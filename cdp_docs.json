{
    "segment": [
        "Log in\nSign Up\nHome\n/\nGuides\n/\nHow To Guides\n/\nSetting Up a Dynamic Coupon Program to Reward Loyal Customers\nSetting Up a Dynamic Coupon Program to Reward Loyal Customers\n\nOne component of building a successful and engaging e-commerce brand is rewarding your most loyal customers. With Segment Warehouses and SQL, you can retrieve a table of your most valuable customers, then reward them.\n\nThis guide will walk you through setting up a dynamic and automated coupon program based on conditions that define your most valuable customers, as well as how to measure the program\u2019s performance.\n\nTalk to a product specialist today about using data to tailor your brand experience.\n\nTools used\n\nEmails with Customer.io: Customer.io is a flexible email provider that allows you to create cohorts based on customer actions. You can build complex onboarding emails, nurture email campaigns, as well as marketing automation workflows.\n\nRetention Analytics with Amplitude: Amplitude is an analytics tool that focuses on understanding retention and funnel analysis.\n\nIt\u2019s important to register for these tools and enable them on your Segment source project. When Segment collects tracking data, it routes it to all of your enabled tools, meaning that they get a single consistent data set. Most importantly, the data generated by users interacting with emails is sent through Segment so you can analyze email performance, and how it impacts conversion with Amplitude.\n\nNot using Customer.io or Amplitude? Check out the other Segment Supported Email Marketing and Analytics tools.\n\nThe Loyalty Program\n\nSay, as the marketing manager of our fictitious, on-demand artisanal toast company, Toastmates, you want to experiment with a coupon program to retain your best customers.\n\nThrough a combination of SQL and statistical analysis on a set of historical data, you\u2019ve identified the conditions for our most valuable customers as:\n\nshops over twice a month\npays over $20 per order\n\nLearn how to define these conditions in How to Forecast LTV for e-commerce with Excel and SQL.\n\nWill rewarding a $5 coupon to this cohort after they make the second purchase a month lead to higher engagement and LTV? Set up this program using Customer.io as the email provider and measure it\u2019s performance on engagement and LTV with Amplitude.\n\nConduct a split test (half of the cohort will represent the control group and will not receive any emails; the other half will receive an email with the $5 coupon) for one month. After which, use Amplitude to see if there were any correlations between the coupon email and conversions.\n\nSet it up\n\nFirst, register for an account with Customer.io and Amplitude. Then, enable Customer.io and enable Amplitude on your Segment project. Finally, go into your Customer.io account and enable \u201csending data to Segment\u201d:\n\nYou can find those destination settings in Customer.io here.\n\nWhen everything is enabled, customer event data such as Order Completed and Product Added, as well as their properties, will all be sent to your configured destinations, including Customer.io and Amplitude. Then you can define cohorts based on these events in Customer.io to add to email campaigns or conduct funnel analytics in Amplitude.\n\nTalk to a product specialist to learn what else you can accomplish with these tools.\n\nDefine the cohort in Customer.io\n\nNow define the specific cohort in Customer.io as per our conditions listed earlier: someone who spends over $20 per order and shops over twice a month. In Customer.io, go to \u201cSegments\u201d and \u201cCreate Segment\u201d:\n\nAfter this cohort is created, then when a customer makes the third purchase in a month and it\u2019s over $20, they will be added to this segment.\n\nNext, create a \u201csegment trigger campaign\u201d, where Customer.io will send a message the first time someone enters a segment. The segment in this case will be the one you just created: Coupon Loyalty Experiment.\n\nSave the changes and enable the campaign. Then, make sure that your e-commerce backend is set up properly to handle the coupons. If it\u2019s available in your system, create a coupon that only works for a specific set of customers.\n\nMeasure performance\n\nAfter a month has passed for the split test, you can measure the performance of the email coupon program to see whether it\u2019s making a material impact on conversions.\n\nIn Amplitude, create a funnel that compares the two cohorts\u2014one who received this coupon email vs. the control group who did not\u2014and see its impact on conversions and revenue generated.\n\nFirst, define a behavioral cohort with the conditions of being loyal customers so you can use it when analyzing the conversion funnel:\n\nYou\u2019ll also have to create a second identical cohort, except with the only difference that these customers did not receive the coupon email. You need this cohort to create the conversion funnel with the control group.\n\nAfter you\u2019ve created these two cohorts, create two funnel charts. The first funnel will look at the control group. The second funnel will look at the group that received the coupon email.\n\nResulting in:\n\nThe control group that did not receive the email for the coupon resulted in 233 people visiting the store, with 66 conversions.\n\nThe funnel for the group who did receive the emails can be created with these parameters:\n\nResulting in:\n\nThe email itself drove 168 customers to the store, which also saw higher conversions to Product Added and ultimately Order Completed.\n\nNote that this funnel is only looking customers who went through these events in this specific order. This analysis doesn\u2019t consider customers who are part of the emailed cohort, yet didn\u2019t open the email, but still visited the site and/or made a purchase.\n\nAt first glance, it appears that the group that was emailed did receive an absolute number of more conversions. However, these funnels are still inconclusive, given that you haven\u2019t explored the impact on the top line revenue, as well as overall engagement with the brand. Fortunately, you can continue to use Amplitude to analyze impact on revenue itself.\n\nFind new ways and channels to retain your most valuable customers\n\nRetaining and rewarding your customers is paramount to a strong and engaging brand. This example is just one of millions that you can employ to find new ways to delight and excite your customer base.\n\nOther ideas can be to send messages to your customers with a referral code to invite their friends. Or set up a coupon for customers who are just shy of entering your most valuable customers cohort. Or, if you\u2019re hosting a pop up shop event, sending a special and personalized invite to your strongest users first, as a way to thank them for their business.\n\nThe possibilities are endless when you use your customer data to drive sales.\n\nTalk to a product specialist today about using data to tailor your brand experience.\n\nThis page was last modified: 25 Oct 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nTools used\nThe Loyalty Program\nSet it up\nDefine the cohort in Customer.io\nMeasure performance\nFind new ways and channels to retain your most valuable customers\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nFunctions\n/\nDestination Functions\nDestination Functions\n\nDestination functions allow you to transform and annotate your Segment events and send them to any external tool or API without worrying about setting up or maintaining any infrastructure.\n\nAll functions are scoped to your workspace, so members of other workspaces can\u2019t view or use them.\n\nFunctions is available to all customer plan types with a free allotment of usage hours. Read more about Functions usage limits, or see your workspace\u2019s Functions usage stats.\n\nDestination functions doesn\u2019t accept data from Object Cloud sources. Destination functions don\u2019t support IP Allowlisting.\n\nCreate a destination function\nFrom your workspace, go to Connections > Catalog and click the Functions tab.\nClick New Function.\nSelect Destination as the function type and click Build.\n\nAfter you click Build, a code editor appears. Use the editor to write the code for your function, configure settings, and test the function\u2019s behavior.\n\nTip: Want to see some example functions? Check out the templates available in the Functions UI, or in the open-source Segment Functions Library. (Contributions welcome!)\n\nCode the destination function\n\nSegment invokes a separate part of the function (called a \u201chandler\u201d) for each event type that you send to your destination function.\n\nYour function isn\u2019t invoked for an event if you\u2019ve configured a destination filter, and the event doesn\u2019t pass the filter.\n\nThe default source code template includes handlers for all event types. You don\u2019t need to implement all of them - just use the ones you need, and skip the ones you don\u2019t.\n\nDestination functions can define handlers for each message type in the Segment spec:\n\nonIdentify\nonTrack\nonPage\nonScreen\nonGroup\nonAlias\nonDelete\nonBatch\n\nEach of the functions above accepts two arguments:\n\nevent - Segment event object, where fields and values depend on the event type. For example, in \u201cIdentify\u201d events, Segment formats the object to match the Identify spec.\nsettings - Set of settings for this function.\n\nThe example below shows a destination function that listens for \u201cTrack\u201d events, and sends some details about them to an external service.\n\nasync function onTrack(event) {\n  await fetch('https://example-service.com/api', {\n    method: 'POST',\n    headers: {\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n      event_name: event.event,\n      event_properties: event.properties,\n      timestamp: event.timestamp\n    })\n  })\n}\n\n\nTo change which event type the handler listens to, you can rename it to the name of the message type. For example, if you rename this function onIdentify, it listens for \u201cIdentify\u201d events instead.\n\nFunctions\u2019 runtime includes a fetch() polyfill using a node-fetch package. Check out the node-fetch documentation for usage examples.\n\nErrors and error handling\n\nSegment considers a function\u2019s execution successful if it finishes without error. You can also throw an error to create a failure on purpose. Use these errors to validate event data before processing it, to ensure the function works as expected.\n\nYou can throw the following pre-defined error types to indicate that the function ran as expected, but that data was not deliverable:\n\nEventNotSupported\nInvalidEventPayload\nValidationError\nRetryError\n\nThe examples show basic uses of these error types.\n\nasync function onGroup(event) {\n  if (!event.traits.company) {\n    throw new InvalidEventPayload('Company name is required')\n  }\n}\n\nasync function onPage(event) {\n  if (!event.properties.pageName) {\n    throw new ValidationError('Page name is required')\n  }\n}\n\nasync function onAlias(event) {\n  throw new EventNotSupported('Alias event is not supported')\n}\n\nasync function onTrack(event) {\n  let res\n  try {\n    res = await fetch('http://example-service.com/api', {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json'\n      },\n      body: JSON.stringify({ event })\n    })\n  } catch (err) {\n    // Retry on connection error\n    throw new RetryError(err.message)\n  }\n  if (res.status >= 500 || res.status === 429) {\n    // Retry on 5xx and 429s (ratelimits)\n    throw new RetryError(`HTTP Status ${res.status}`)\n  }\n}\n\n\n\nIf you don\u2019t supply a function for an event type, Segment throws an EventNotSupported error by default.\n\nYou can incorporate a a try-catch block to ensure smooth operation of functions even when fetch calls fail. This allows for the interception of any errors during the API call, enabling the application of specific error handling procedures, such as error logging for future debugging, or the assignment of fallback values when the API call is unsuccessful. By positioning the continuation logic either outside the try-catch block or within a finally block, the function is guaranteed to proceed with its execution, maintaining its workflow irrespective of the outcome of the API call.\n\nYou can read more about error handling below.\n\nRuntime and dependencies\n\nOn March 26, 2024, Segment is upgrading the Functions runtime environment to Node.js v18, which is the current long-term support (LTS) release.\n\nThis upgrade keeps your runtime current with industry standards. Based on the AWS Lambda and Node.js support schedule, Node.js v16 is no longer in Maintenance LTS. Production applications should only use releases of Node.js that are in Active LTS or Maintenance LTS.\n\nAll new functions will use Node.js v18 starting March 26, 2024.\n\nFor existing functions, this change automatically occurs as you update and deploy an existing function. Segment recommends that you check your function post-deployment to ensure everything\u2019s working. Your function may face issues due to the change in sytax between different Node.js versions and dependency compatibility.\n\nLimited time opt-out option\n\nIf you need more time to prepare, you can opt out of the update before March 19, 2024.\n\nNote that if you opt out:\n- The existing functions will continue working on Node.js v16.\n- You won\u2019t be able to create new functions after July 15, 2024.\n- You won\u2019t be able to update existing functions after August 15, 2024.\n- You won\u2019t receive future bug fixes, enhancements, and dependency updates to the functions runtime.\n\nContact Segment to opt-out or with any questions.\n\nNode.js 18\n\nSegment strongly recommends updating to Node.js v18 to benefit from future runtime updates, the latest security, and performance improvements.\n\nFunctions do not currently support importing dependencies, but you can contact Segment Support to request that one be added.\n\nThe following dependencies are installed in the function environment by default.\n\natob v2.1.2 exposed as atob\naws-sdk v2.488.0 exposed as AWS\nbtoa v1.2.1 exposed as btoa\nfetch-retry exposed as fetchretrylib.fetchretry\nform-data v2.4.0 exposed as FormData\n@google-cloud/automl v2.2.0 exposed as google.cloud.automl\n@google-cloud/bigquery v5.3.0 exposed as google.cloud.bigquery\n@google-cloud/datastore v6.2.0 exposed as google.cloud.datastore\n@google-cloud/firestore v4.4.0 exposed as google.cloud.firestore\n@google-cloud/functions v1.1.0 exposed as google.cloud.functions\n@google-cloud/pubsub v2.6.0 exposed as google.cloud.pubsub\n@google-cloud/storage v5.3.0 exposed as google.cloud.storage\n@google-cloud/tasks v2.6.0 exposed as google.cloud.tasks\nhubspot-api-nodejs exposed as hubspotlib.hubspot\njsforce v1.11.0 exposed as jsforce\njsonwebtoken v8.5.1 exposed as jsonwebtoken\nlibphonenumber-js exposed as libphonenumberjslib.libphonenumberjs\nlodash v4.17.19 exposed as _\nmailchimp marketing exposed as mailchimplib.mailchimp\nmailjet exposed as const mailJet = nodemailjet.nodemailjet;\nmoment-timezone v0.5.31 exposed as moment\nnode-fetch v2.6.0 exposed as fetch\noauth v0.9.15 exposed as OAuth\n@sendgrid/client v7.4.7 exposed as sendgrid.client\n@sendgrid/mail v7.4.7 exposed as sendgrid.mail\nskyflow exposed as skyflowlib.skyflow\nstripe v8.115.0 exposed as stripe\ntwilio v3.68.0 exposed as twilio\nuuidv5 v1.0.0 exposed as uuidv5.uuidv5\nwinston v2.4.6 exposed as const winston = winstonlib.winston\nxml v1.0.1 exposed as xml\nxml2js v0.4.23 exposed as xml2js\n\nzlib v1.0.5 exposed as zlib.zlib\n\n\nuuidv5 is exposed as an object. Use uuidv5.uuidv5 to access its functions. For example:\n\n  async function onRequest(request, settings) {\n       uuidv5 = uuidv5.uuidv5;\n       console.log(typeof uuidv5);\n\n        //Generate a UUID in the default URL namespace\n        var urlUUID = uuidv5('url', 'http://google/com/page');\n        console.log(urlUUID);\n\n        //Default DNS namespace\n        var dnsUUID = uuidv5('dns', 'google.com');\n        console.log(dnsUUID);\n    }\n\n\nzlib\u2019s asynchronous methods inflate and deflate must be used with async or await. For example:\n\nzlib = zlib.zlib;  // Required to access zlib objects and associated functions\nasync function onRequest(request, settings) {\n  const body = request.json();\n\n  const input = 'something';\n\n  // Calling inflateSync method\n  var deflated = zlib.deflateSync(input);\n\n  console.log(deflated.toString('base64'));\n\n  // Calling inflateSync method\n  var inflated = zlib.inflateSync(new Buffer.from(deflated)).toString();\n\n  console.log(inflated);\n\n  console.log('Done');\n  }\n\n\nThe following Node.js modules are available:\n\ncrypto Node.js module exposed as crypto.\nhttps Node.js module exposed as https.\n\nOther built-in Node.js modules aren\u2019t available.\n\nFor more information on using the aws-sdk module, see how to set up functions for calling AWS APIs.\n\nCaching\n\nBasic cache storage is available through the cache object, which has the following methods defined:\n\ncache.load(key: string, ttl: number, fn: async () => any): Promise<any>\nObtains a cached value for the provided key, invoking the callback if the value is missing or has expired. The ttl is the maximum duration in milliseconds the value can be cached. If omitted or set to -1, the value will have no expiry.\ncache.delete(key: string): void\nImmediately remove the value associated with the key.\n\nSome important notes about the cache:\n\nWhen testing functions in the code editor, the cache will be empty because each test temporarily deploys a new instance of the function.\nValues in the cache are not shared between concurrently-running function instances; they are process-local which means that high-volume functions will have many separate caches.\nValues may be expunged at any time, even before the configured TTL is reached. This can happen due to memory pressure or normal scaling activity. Minimizing the size of cached values can improve your hit/miss ratio.\nFunctions that receive a low volume of traffic may be temporarily suspended, during which their caches will be emptied. In general, caches are best used for high-volume functions and with long TTLs. The following example gets a JSON value through the cache, only invoking the callback as needed:\nconst ttl = 5 * 60 * 1000 // 5 minutes\nconst val = await cache.load(\"mycachekey\", ttl, async () => {\n    const res = await fetch(\"http://echo.jsontest.com/key/value/one/two\")\n    const data = await res.json()\n    return data\n})\n\nCreate settings and secrets\n\nSettings allow you to pass configurable variables to your function, which is the best way to pass sensitive information such as security tokens. For example, you might use settings as placeholders to use information such as an API endpoint and API key. This way, you can use the same code with different settings for different purposes. When you deploy a function in your workspace, you are prompted to fill out these settings to configure the function.\n\nFirst, add a setting in Settings tab in the code editor:\n\nClick Add Setting to add your new setting.\n\nYou can configure the details about this setting, which change how it\u2019s displayed to anyone using your function:\n\nLabel - Name of the setting, which users see when configuring the function.\nName - Auto-generated name of the setting to use in function\u2019s source code.\nType - Type of the setting\u2019s value.\nDescription - Optional description, which appears below the setting name.\nRequired - Enable this to ensure that the setting cannot be saved without a value.\nEncrypted - Enable to encrypt the value of this setting. Use this setting for sensitive data, like API keys.\n\nAs you change the values, a preview to the right updates to show how your setting will look and work.\n\nClick Add Setting to save the new setting.\n\nOnce you save a setting, it appears in the Settings tab for the function. You can edit or delete settings from this tab.\n\nNext, fill out this setting\u2019s value in the Test tab, so you can run the function and verify that the correct setting value is passed. (This value is only for testing your function.)\n\nNow that you\u2019ve configured a setting and entered a test value, you can add code to read its value and run the function, as in the example below:\n\nasync function onTrack(request, settings) {\n  const apiKey = settings.apiKey\n  //=> \"super_secret_string\"\n}\n\n\nWhen you deploy your destination function in your workspace, you fill out the settings on the destination configuration page, similar to how you would configure a normal destination.\n\nYou must pass the settings object to the function at runtime. Functions can\u2019t access the settings object when it\u2019s stored as a global variable.\n\nTest the destination function\n\nYou can test your code directly from the editor in two ways:\n\nUse sample events for testing\n\nClick Use Sample Event and select the source to use events from.\n\nClick Run to test your function with the event you selected.\n\nTest using manual input\n\nYou can also manually include your own JSON payload of a Segment event, instead of fetching a sample from one of your workspace sources.\n\nIf your function fails, you can check the error details and logs in the Output section.\n\nError Message - This shows the error surfaced from your function.\nLogs - This section displays any messages to console.log() from the function.\nBatching the destination function\n\nBatch handlers are an extension of destination functions. When you define an onBatch handler alongside the handler functions for single events (for example: onTrack or onIdentity), you\u2019re telling Segment that the destination function can accept and handle batches of events.\n\nBatching is available for destination and destination insert functions only.\n\nWhen to use batching\n\nConsider creating a batch handler if:\n\nYour function sends data to a service that has a batch endpoint. Batch endpoints may allow you both to send more data downstream and stay within the rate limits imposed by the service. Batch handlers that use one or more batch endpoints improve the efficiency of the function, and enable it to scale more easily. Specifically, you can use batch handlers to build list-based Engage destinations.\nYou have a high-throughput function and want to reduce cost. When you define a batch handler, Segment invokes the function once per batch, rather than once per event. As long as the function\u2019s execution time isn\u2019t adversely affected, the reduction in invocations should lead to a reduction in cost.\n\nIf a batched function receives too low a volume of events (under one event per second) to be worth batching, Segment may not invoke the batch handler.\n\nDefine the batch handler\n\nSegment collects the events over a short period of time and combines them into a batch. The system flushes them when the batch reaches a certain number of events, or when the batch has been waiting for a specified wait time.\n\nTo create a batch handler, define an onBatch function within your destination function. You can also use the \u201cDefault Batch\u201d template found in the Functions editor to get started quickly.\n\nasync function onBatch(events, settings){\n  // handle the batch of events\n}\n\n\nThe onBatch handler is an optional extension. Destination functions must still contain single event handlers as a fallback, in cases where Segment does not receive enough events to execute the batch.\n\nThe handler function receives an array of events. The events can be of any supported type and a single batch may contain more than one event type. Handler functions can also receive function settings. Here is an example of what a batch can look like:\n\n[\n    {\n      \"type\": \"identify\",\n      \"userId\": \"019mr8mf4r\",\n      \"traits\": {\n        \"email\": \"jake@yahoo.com\",\n        \"name\": \"Jake Peterson\",\n        \"age\": 26\n      }\n    },\n    {\n      \"type\": \"track\",\n      \"userId\": \"019mr8mf4r\",\n      \"event\": \"Song Played\",\n      \"properties\": {\n        \"name\": \"Fallin for You\",\n        \"artist\": \"Dierks Bentley\"\n      }\n    },\n    {\n      \"type\": \"track\",\n      \"userId\": \"971mj8mk7p\",\n      \"event\": \"Song Played\",\n      \"properties\": {\n        \"name\": \"Get Right\",\n        \"artist\": \"Jennifer Lopez\"\n      }\n    }\n]\n\n\nFor example, you could send the array of events to an external services batch endpoint:\n\nasync function onBatch(events, settings) {\n  await fetch('https://example-service.com/batch-api', {\n    method: 'POST',\n    headers: {\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify(events) // send a JSON array\n  })\n}\n\nConfigure the event types within a batch\n\nSegment batches together any event of any type that it sees over a short period of time to increase batching efficiency and give you the flexibility to decide how batches are created. If you want to split batches by event type, you can implement this in your functions code by writing a handler.\n\nIf your downstream endpoint requires events of a single type, you can write a handler that groups events by type, and then handles the events.\n\nasync function onBatch(events, settings) {\n  // group events by type\n  const eventsByType = {}\n  for (const event of events) {\n    if (!(event.type in eventsByType)) {\n      eventsByType[event.type] = []\n    }\n    eventsByType[event.type].push(event)\n  }\n\n  // concurrently process sub-batches of a specific event type\n  const promises = Object.entries(eventsByType).map(([type, events]) => {\n    switch (type) {\n    case 'track':\n      return onTrackBatch(events, settings)\n    case 'identify':\n      return onIdentifyBatch(events, settings)\n    // ...handle other event types here...\n    }\n  })\n  return Promise.all(promises)\n}\n\nasync function onTrackBatch(events, settings) {\n  // handle a batch of track events\n}\n\nasync function onIdentifyBatch(events, settings) {\n  // handle a batch of identify events\n}\n\nConfigure your batch parameters\n\nBy default, Functions waits up to 10 seconds to form a batch of 20 events. You can increase the number of events included in each batch (up to 400 events per batch) by contacting Segment support. Segment recommends users who wish to include fewer than 20 events per batch use destination functions without the onBatch handler.\n\nTest the batch handler\n\nThe Functions editing environment supports testing batch handlers.\n\nTo test the batch handler:\n\nIn the right panel of the Functions editor, click customize the event yourself to enter Manual Mode.\nAdd events as a JSON array, with one event per element.\nClick Run to preview the batch handler with the specified events.\n\nThe Sample Event option tests single events only. You must use Manual Mode to add more than one event so you can test batch handlers.\n\nThe editor displays logs and request traces from the batch handler.\n\nThe Public API Functions/Preview endpoint also supports testing batch handlers. The payload must be a batch of events as a JSON array.\n\nHandling batching errors\n\nStandard function error types apply to batch handlers. Segment attempts to retry the batch in the case of Timeout or Retry errors. For all other error types, Segment discards the batch. It\u2019s also possible to report a partial failure by returning status of each event in the batch. Segment retries only the failed events in a batch until those events are successful or until they result in a permanent error.\n\n[\n\t{\n\t\t\"status\": 200\n\t},\n\t{\n\t\t\"status\": 400,\n\t\t\"errormessage\": \"Bad Request\"\n\t},\n\t{\n\t\t\"status\": 200\n\t},\n\t{\n\t\t\"status\": 500,\n\t\t\"errormessage\": \"Error processing request\"\n\t},\n\t{\n\t\t\"status\": 500,\n\t\t\"errormessage\": \"Error processing request\"\n\t},\n\t{\n\t\t\"status\": 200\n\t},\n]\n\n\nFor example, after receiving the responses above from the onBatch handler, Segment only retries event_4 and event_5.\n\nERROR TYPE\tRESULT\nBad Request\tDiscard\nInvalid Settings\tDiscard\nMessage Rejected\tDiscard\nRetryError\tRetry\nTimeout\tRetry\nUnsupported Event Type\tDiscard\nSave and deploy the function\n\nOnce you finish building your destination function, click Configure to name it, then click Create Function to save it.\n\nOnce you do that, the destination function appears on the Functions page in your workspace\u2019s catalog.\n\nIf you\u2019re editing an existing function, you can Save changes without updating instances of the function that are already deployed and running.\n\nYou can also choose to Save & Deploy to save the changes, and then choose which of the already-deployed functions to update with your changes. You might need additional permissions to update existing functions.\n\nDestination functions logs and errors\n\nIf your function throws an error, execution halts immediately. Segment captures the event, any outgoing requests/responses, any logs the function might have printed, as well as the error itself.\n\nSegment then displays the captured error information in the Event Delivery page for your destination. You can use this information to find and fix unexpected errors.\n\nYou can throw an error or a custom error and you can also add helpful context in logs using the console API. For example:\n\nasync function onTrack(event, settings) {\n  const userId = event.userId\n\n  console.log('User ID is', userId)\n\n  if (typeof userId !== 'string' || userId.length < 8) {\n    throw new ValidationError('User ID is invalid')\n  }\n\n  console.log('User ID is valid')\n}\n\n\nWarning: Do not log sensitive data, such as personally-identifying information (PII), authentication tokens, or other secrets. Avoid logging entire request/response payloads. The Function Logs tab may be visible to other workspace members if they have the necessary permissions.\n\nCaching in destination functions\n\nFunctions execute only in response to incoming data, but the environments that functions run in are generally long-running. Because of this, you can use global variables to cache small amounts of information between invocations. For example, you can reduce the number of access tokens you generate by caching a token, and regenerating it only after it expires. Segment cannot make any guarantees about the longevity of environments, but by using this strategy, you can improve the performance and reliability of your Functions by reducing the need for redundant API requests.\n\nThis example code fetches an access token from an external API and refreshes it every hour:\n\nconst TOKEN_EXPIRE_MS = 60 * 60 * 1000 // 1 hour\nlet token = null\nasync function getAccessToken () {\n  const now = new Date().getTime()\n  if (!token || now - token.ts > TOKEN_EXPIRE_MS) {\n    const resp = await fetch('https://example.com/tokens', {\n      method: 'POST'\n    }).then(resp => resp.json())\n    token = {\n      ts: now,\n      value: resp.token\n    }\n  }\n  return token.value\n}\n\nManaging destination functions\nFunctions permissions\n\nFunctions have specific roles which can be used for access management in your Segment workspace.\n\nAccess to functions is controlled by two permissions roles:\n\nFunctions Admin: Create, edit, and delete all functions, or a subset of specified functions.\nFunctions Read-only: View all functions, or a subset of specified functions.\n\nYou also need additional Source Admin permissions to enable source functions, connect destination functions to a source, or to deploy changes to existing functions.\n\nEditing and deleting functions\n\nIf you are a Workspace Owner or Functions Admin, you can manage your function from the Functions page.\n\nMonitoring destination functions\n\nYou can use Destination Event Delivery to understand if Segment encounters any issues delivering your source data to destinations. Errors that the Function throws appear here.\n\nIf any of your deployed function instances are failing consistently, they will also appear in Connection Health.\n\nData control\n\nIn addition to using Destination Filters and the Privacy Portal to manage which events and properties are sent to your destination function, you can reference the destination function directly in the integrations object of the Segment payload. For example:\n\n...\n\"integrations\": {\n  \"All\": false,\n  \"Amplitude\": true,\n  \"Customer.io\": true,\n  \"Google Analytics\": true,\n  \"My Destination Function (My Workspace)\": true\n}\n...\n\n\nIn the example above, the integrations object directly references and enables the destination function (My Destination Function), located inside your workspace (My Workspace). Include the workspace name in parentheses, as shown in the example above. Like all items in the integration object, destination function and workspace names are case sensitive.\n\nDestination functions FAQs\nCan I see who made changes to a function?\n\nYes, Functions access is logged in the Audit Trail, so user activity related to functions appears in the logs.\n\nDoes Segment retry failed function invocations?\n\nYes, Segment retries invocations that throw RetryError or Timeout errors (temporary errors only). Segment\u2019s internal system retries failed functions API calls for four hours with a randomized exponential backoff after each attempt. This substantially improves delivery rates.\n\nRetries work the same for both functions and cloud-mode destinations in Segment.\n\nAre events guaranteed to send data in order?\n\nNo, Segment can\u2019t guarantee the order in which the events are delivered to an endpoint.\n\nCan I create a device-mode destination?\n\nNo, destination functions are currently available as cloud-mode destinations only. Segment is in the early phases of exploration and discovery for supporting customer \u201cweb plugins\u201d for custom device-mode destinations and other use cases, but this is unsupported today.\n\nHow do I publish a destination to the public Segment catalog?\n\nIf you are a partner, looking to publish your destination and distribute your app through Segment catalog, visit the Developer Center and check out the Segment partner docs.\n\nHow does batching affect visibility?\n\nThe Event Delivery tab continues to show metrics for individual events, even if they are batched by your function code. For more information, see Destination functions logs and errors.\n\nHow does batching impact function use and cost?\n\nA function\u2019s use depends on the number of times it\u2019s invoked, and the amount of time it takes to execute. When you enable batching, Segment invokes your function once per batch rather than once per event. The volume of events flowing through the function determines the number of batches, which determines the number of invocations.\n\nIf you\u2019re sending your batch to an external service, the execution time of the function depends on the end-to-end latency of that service\u2019s batch endpoint, which may be higher than an endpoint that receives a single event.\n\nWhich IP addresses should be allowlisted?\n\nWhen data leaves Segment\u2019s servers to go to various destinations (not including warehouses), Segment uses Amazon Web Services (AWS) and utilizes many different machines in order to send requests.\n\nThe IP addresses that are used to send these requests can be found on Amazon\u2019s website. If you want to allowlist these specific IP addresses, you need to allowlist all of the IP addresses from your workspace\u2019s location range. Below are the ranges:\n\nFor a US workspace: AWS us-west-2\nFor an EU workspace: AWS eu-west-1 \nCan I use a Destination Function to send data to another Segment source?\n\nYes, to do so, remove the messageId and the writeKey from the payload in your Function code. Leaving either field on your payload will cause unexpected behavior that may cause your event to be delivered to the wrong source or to not be delivered at all.\n\nCan I view console.log() outputs in Destination Functions?\n\nIncorporating console.log() statements in your Destination Function code aids in debugging. However, logs generated by these statements will only be accessible in the Event Delivery view if the payloads encounter errors during processing. Logs from successfully processed payloads are not displayed.\n\nWhat is the maximum data size that can be displayed in console.logs() when testing a Function?\n\nThe test function interface has a 4KB console logging limit. Outputs larger than this limit are not visible in the user interface.\n\nThis page was last modified: 19 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nCreate a destination function\nCode the destination function\nCreate settings and secrets\nTest the destination function\nBatching the destination function\nSave and deploy the function\nDestination functions logs and errors\nCaching in destination functions\nManaging destination functions\nDestination functions FAQs\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nStorage\n/\nWarehouses\n/\nAdding Warehouse Users\nAdding Warehouse Users\n\nIf you have more than one person working with your Segment Warehouse, you might want to create users for your team so that each person can have a discrete login. The three steps in this section will show you how to create a user, grant usage on a schema and then grant the privileges that the user will need to interact with that schema.\n\n1. Creating a user with the\u00a0CREATE USER\u00a0command\nCREATE USER <name>\n[IN GROUP <group>]\nWITH PASSWORD <password>\n[VALID UNTIL] <abstime>\n\n\nThe code above in [] is optional, you don\u2019t need to group your users or give their credentials an expiration date, the code works without it. However, if you do choose to use those parameters,\u00a0<abstime>\u00a0should be formatted as \u20182015-09-13\u2019 which translates to September 13th, 2015.\n\nFor instance, you can create a user named\u00a0flashthesloth\u00a0as\n\nCREATE USER flashthesloth\nWITH PASSWORD 'slow_is_beautiful'\n\n\nThis creates a user, you can run the following to get a list of users in your database.\n\nSELECT * FROM pg_user\n\n\nNow that we\u2019ve confirmed that the user has been created, they already have access to the public schema that contains systems-level information about the cluster but we need to give them access to the specific schemas that they\u2019ll be working in.\n\n2.\u00a0Grant usage on the schema\n\nNext, GRANT USAGE\u00a0on the schema to the user we just created\n\nGRANT USAGE ON SCHEMA <schema_name> TO <user>\n\n\nThe above SQL command grants the user USAGE privileges on a schema. Let\u2019s assume you want to grant\u00a0flashthesloth\u00a0access to your development schema, it would look like below\n\nGRANT USAGE ON SCHEMA development TO flashthesloth\n\n\nOur new user now has usage rights on the\u00a0development\u00a0schema, now we need to grant the type of SQL commands they\u2019ll be able to run against the cluster. For the purposes of this example, we\u2019re going to give the user read only privileges.\n\n3.\u00a0Grant select\u00a0privileges\n\nGRANT SELECT\u00a0privileges so the user can query the tables\n\nGRANT SELECT ON ALL TABLES IN SCHEMA <schema_name> TO <user>\n\n\nThe above SQL command grants the user SELECT rights on all tables in the chosen schema. For our\u00a0flashthesloth user and the\u00a0development\u00a0schema, it would look like below.\n\nGRANT SELECT ON ALL TABLES IN SCHEMA development TO flashthesloth\n\n\nDoing these three steps will result in a new user that can query all the tables in a given schema. If you want to give access to more than one schema then you can simply repeat steps 2 and 3 for each additional schema. If you have any questions or if you\u2019re running into any issues getting this set up,\u00a0contact us.\n\nThis page was last modified: 14 Jul 2021\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nAudiences\n/\nOrganizing Your Audiences\nOrganizing Your Audiences\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nTo add structure to your Spaces, you can organize Audiences into folders and clone Audiences within, and between, Spaces.\n\nWorking with folders\n\nFolders allow you to group Audiences together. You can create, edit, and search through folders directly within the Engage Audiences page.\n\nCreating a folder\n\nTo create a Folder, follow the steps below:\n\nNavigate to the Audiences tab within your Space.\nClick Create, then select Folder from the dropdown menu.\nGive your Folder a unique name, then click Add Audiences.\nSearch for and select the Audience(s) you want to add to the Folder.\nTo confirm the new Folder, click Add Audiences.\nEditing and disbanding folders\n\nTo edit the name or description of a Folder you\u2019ve created, click the More Options icon and select Edit. Once you\u2019ve made your desired changes, click Save.\n\nTo disband a Folder you\u2019ve made, click the More Options icon and select Disband. Audiences from the disbanded Folder return to your main Audience list.\n\nDisbanding folders does not delete audiences.\n\nMoving Audiences into folders\n\nTo move an Audience to a Folder you\u2019ve already created, follow the steps below:\n\nNavigate to the Audiences tab within your Space.\nHover over the Audience you want to move.\nCheck the selection box that appears next to the Audience name.\n(Optional): Repeat Steps 2 and 3 to move multiple Audiences.\nClick the Move icon that appears in the Audiences header.\nSelect your destination Folder from the modal window.\nClick Move to confirm and move the selected Audiences.\nClone Audiences\n\nAudience cloning creates a copy of your Audience. You can clone an Audience within the same space, or clone an Audience to a different space.\n\nClone an Audience inside a Space\n\nTo clone an Audience within the same Space, follow the steps below:\n\nNavigate to the Audiences tab within your Space.\nClick the More Options icon next to the Audience you want to clone.\nFrom the dropdown menu, click Clone.\nSelect Current Space, then click Continue.\nConfigure the Audience, click Preview Results, then click Select Destination.\n(Optional): On the next screen, connect the Audience to a Destination. Click Review & Create.\nGive your Audience a unique name, then click Create Audience.\nCloning an Audience between Spaces\n\nYou may wish to clone an Audience between spaces for a number of use cases, including the following:\n\nCopying an Audience between testing and production spaces\nCopying an Audience between business units\nCopying an Audience between teams\n\nNote\n\nWhen you clone an Audience to a different space, first verify that the target Space includes the same events and traits for the cloned Audience.\n\nTo clone an Audience between Spaces, follow the steps below:\n\nNavigate to the Audiences tab within your Space.\nClick the More Options icon next to the Audience you want to clone.\nFrom the dropdown menu, click Clone.\nSelect Different Space, choose your target Space, then click Continue.\nConfigure the Audience, click Preview Results, then click Select Destination.\n(Optional): On the next screen, connect the Audience to a Destination. Click Review & Create.\nGive your Audience a unique name, then click Create Cloned Audience.\n\nIf your target Space doesn\u2019t include the cloned Audience\u2019s events and traits, Engage prompts you to resolve the Space incompatibilities during Step 5. As a best practice, verify that the target Space includes the Audience\u2019s traits and events before cloning.\n\nDelete an Audience\n\nTo delete an Audience, follow the steps below:\n\nNavigate to the Audiences tab within your Space.\nSelect the Audience you want to delete, then click Settings.\nClick Enabled to toggle to Disabled, then click Delete Audience\u2026.\nOn the Delete Audience prompt, click Delete Audience to confirm.\n\nThis page was last modified: 28 Jun 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nWorking with folders\nEditing and disbanding folders\nMoving Audiences into folders\nClone Audiences\nDelete an Audience\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nEngage FAQs\nEngage FAQs\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\nDo you have an Audiences API?\n\nYes. You can learn more about the Audience API by visiting the Segment Public API documentation.\n\nCan I programmatically determine if a user belongs to a particular audience?\n\nYes. Because Engage creates a trait with the same name as your audience, you can query the Profile API to determine if a user belongs to a particular audience. For example, to determine if the user with an email address of bob@example.com is a member of your high_value_users audience, you could query the following Profile API URL:\n\nhttps://profiles.segment.com/v1/namespaces/<namespace_id>/collections/users/profiles/email:bob@segment.com/traits?include=high_value_users\n\n\nThe following response indicates that Bob is a high-value user:\n\n{\n  \"traits\": {\n    \"high_value_users\": true,\n  },\n  \"cursor\": {\n    \"has_more\": false,\n  }\n}\n\n\nFor more information on profile queries, visit the Profile API documentation.\n\nCan I modify audience keys?\n\nYou can\u2019t change the audience key after it\u2019s created. To change the key, you need to re-create the audience.\n\nCan I reuse audience keys?\n\nAvoid using the same audience key twice, even if you\u2019ve deleted the key\u2019s original audience. Downstream tools and destinations might have trouble distinguishing between different audiences that once shared the same key. This may create mismatch in audience size between Segment and the destination because the destination may count users of the old audience, resulting in a larger audience size.\n\nHow do historical lookback windows work?\n\nEngage allows you to compute new traits and audiences of your users based on their entire customer journey, and all historical data you\u2019ve tracked with Segment.\n\nWhen you create a new computed trait or audience, you include a lookback window that determines how far back into the past the trait or audiences will be computed.\n\nSome important things to keep in mind when setting a lookback window:\n\nHistorical lookback windows are based on the event timestamp field.\n\nLookback windows are precise down to the hour, so a 90-day lookback window will include any events with a timestamp timestamp within the last 2,160 hours (24 hr/day * 90 days).\n\nThe trait and audience will automatically update going forward as historical events exceed the lookback window.\n\nWhat are Funnel Audiences?\n\nFunnel Audiences allow you to use strict, relative ordering for your audience conditions. Common use cases for these audiences are Cart Abandonment (users that triggered the Product Added event but did not trigger the Order Completed event after the Product Added event occurred) and onboarding steps (users that Added Credit Card but did not Subscribe afterward).\n\nTo get started with Funnel Audiences, go to:\n\nAudiences > New > Select Funnel Condition (\u201cand then did not\u201d/\u201dand then did\u201d)\n\nThe funnel condition will now be relative to the parent condition.\n\nThe audience in the image below includes all users that have Product Added in the last week, but not Order Completed within a day of doing so.\n\nFunnel Audiences compute based on all instances of the parent event within the lookback period. This means that if you have a user that Product Added \u27f6 Order Completed \u27f6 Product Added, this user would be entered into the Abandoned Cart state despite having previously completed an order.\n\nWhat is Engage Merge Protection?\n\nEngage\u2019s merge protection algorithm protects your identity graph from unnecessary merges by finding and removing untrusted external IDs. Here\u2019s an example:\n\nIn this example, anonymous_id: a1 is not reset during a User Logout. Without merge protection rules, Segment would merge user_id u1 and user_id u2. Instead, the identity resolution algorithm detects that such a merge would break user_id uniqueness and prevents the merge.\n\nThis is especially helpful for preventing \u201cblob users\u201d that are merged together by non-unique anonymous IDs or by common group emails like team@company.com.\n\nWhich destinations support syncing the identity graph?\n\nMost destinations on the Segment Platform are built up around a user model. They assume that a user will have a single userId. Further, most Destinations are not built to handle anonymous traffic.\n\nBy default, Segment doesn\u2019t sync the output of the Identity Graph to Destinations. However, Segment computed traits and audiences are based on the entire user profile, including anonymous and merged data. Segment syncs the value of these computations (for example, blog_posts_ready_30_days: 10) using all userIds on the profile.\n\nFor Destinations that support an alias call (for example, Mixpanel), you can emit an alias call on merge.\n\nWhat Sources can I sync to Engage?\n\nThe following list shows just some data sources you can sync to Engage:\n\nWebsite (Analytics.js)\nMobile SDKs (iOS, Android, AMP)\nServer-side libraries (Go, Node, Java, PHP, Python, Ruby, .NET)\nFacebook Lead Ads\nActiveCampaign\nCustomer.io\nDrip\nIterable\nKlaviyo\nMailjet\nNudgespot\nVero\nBlueshift\nDelighted\nBraze\nLooker\nRadar\nAutopilot\nFriendbuy\nCan I send audiences to multiple destination accounts?\n\nYes, Engage supports the ability to send an audience or computed trait to two or more accounts of the same partner. The most common use case is multiple Facebook, or Adwords ad accounts.\n\nWhy am I getting alerts about an audience/computed trait sync failure, but when I look at the specific audience/computed trait it shows a successful sync?\n\nAn audience/computed trait run or sync may fail on its first attempt, but Engage will retry up to five times before considering it a hard failure that displays on the audience/compute trait\u2019s overview page. As long as the runs/syncs within the specific audience\u2019s overview page indicate success, you can ignore any failure alerts.\n\nHow things work internally: Segment\u2019s Engage scheduler fetches audiences/traits from the compute service and then handles the logic of generating tasks. These compute/sync tasks get scheduled and executed by another worker. These tasks are a list of steps to be executed. Each task has a series of steps that Segment marks as complete by saving a timestamp for the completion. If something disrupts the worker, it picks up at the latest step without a completed_at timestamp. In some cases, the step or entire task might fail due to timeout or worker disruption. No matter the cause, Segment will retry any failures.\n\nThe audit trail\u2019s configuration notifies about every task failure, even if the failure later succeeds. In most cases, you won\u2019t need to track these failures, unless you notice actual computation or sync failures.\n\nIf you don\u2019t want to receive notifications for temporary failures, reach out to support. Upon request, Segment can disable temporary failure notifications, which will reduce the number of notifications your workspace receives.\n\nWhy is the user count in a journey step greater than the entry/previous step of the journey?\n\nEach step of a Journey is an Engage audience under the hood. The conditions stack, so a user must be a member of the previous step (audience) and meet all conditions to be added to subsequent steps. However, if the user no longer meets entry conditions for a particular step, they\u2019ll exit and you\u2019ll see the user count reduced. For any subsequent steps a user is still a part of, they\u2019ll remain until they no longer meet entry conditions.\n\nWhy were multiple audience-entered events triggered for the same user?\n\nMultiple audience events can trigger for a user if any of the following conditions occur: 1) There is a merge on the user. 2) An external_id was added to the profile. 3) The user has multiple identifiers of the same type. Segment sends one event per identifier for each audience or computed trait event. 4) The include anonymous users option is selected for an audience. Segment sends an event for every anonymousId on the user profile.\n\nWhy am I not seeing standard source events on the Engage source, even though it has been connected through \u201cUnify -> Unify Settings -> Profile Sources\u201d page?\n\nBased on Engage behavior, standard source events such as Page, Track and Identify calls aren\u2019t visible on the Engage source. The Engage source tracks and manages events related to audiences and computed traits within the Engage space. This includes events generated by changes in audience membership or computed trait calculations or when a user profile has been created in the Engage space. These are distinct from the typical Page calls, Track calls, or Identify calls (user interaction events) that you would observe in a standard Segment source.\n\nWhy can\u2019t I connect the audience/computed trait to an existing destination in my workspace?\n\nEngage will not allow you to connect an audience/computed trait to a destination that is already linked to a Connections-based source. Instead, create a new instance of the destination with the correct Engage space selected as the data source.\n\nHow are the \u201c5 most common values\u201d for traits calculated?\n\nThe \u201c5 most common values\u201d are the most frequently observed values for a given trait across all users, not tied to any individual user.\n\nThis page was last modified: 24 Oct 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nDo you have an Audiences API?\nCan I programmatically determine if a user belongs to a particular audience?\nCan I modify audience keys?\nCan I reuse audience keys?\nHow do historical lookback windows work?\nWhat are Funnel Audiences?\nWhat is Engage Merge Protection?\nWhich destinations support syncing the identity graph?\nWhat Sources can I sync to Engage?\nCan I send audiences to multiple destination accounts?\nWhy is the user count in a journey step greater than the entry/previous step of the journey?\nWhy were multiple audience-entered events triggered for the same user?\nWhy am I not seeing standard source events on the Engage source, even though it has been connected through \u201cUnify -> Unify Settings -> Profile Sources\u201d page?\nWhy can\u2019t I connect the audience/computed trait to an existing destination in my workspace?\nHow are the \u201c5 most common values\u201d for traits calculated?\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nContent\n/\nSms\n/\nSMS Template\nSMS Template\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE PREMIER \u2713\n?\n\nEngage Premier entered an End of Sale (EOS) period effective June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:\n\nTwilio Marketing Campaigns\n\nPreferred ISV Partners:\n\nAirship Blog\nBloomreach Blog\nBraze Blog\nInsider Blog\nKlaviyo Blog\nTwilio Engage Foundations Documentation\n\n\nUse Twilio Engage to build SMS message templates to include throughout your marketing campaigns.\n\nYou can build an SMS template and include personalized content in messages based on user profile traits. Once you build the SMS, Twilio Engage saves the template for you to preview, maintain, and reuse.\n\nUse personalized SMS messages to connect with users in real-time, as they reach a specific step in a journey.\n\nSMS template types\n\nYou can choose between two SMS template types:\n\nMedia, which contains media and text content\nText, which contains text content of up to 1600 characters\nBuild an SMS message template\n\nYou must first configure your SMS service with Twilio to build an SMS template in Engage. Visit the onboarding steps for more on how to connect a Twilio account.\n\nFollow these steps to build an SMS template:\n\nNavigate to Engage > Content and click Create template.\nSelect SMS, then click Configure.\nEnter a template name and select your template\u2019s language.\nSelect your template\u2019s content type, then click Next.\nFor text templates, enter your message\u2019s text in the Body field and add any desired merge tags.\nFor media templates, enter your message\u2019s text in the Body field, add the media URL, then add any desired merge tags. Media templates support PNG, JPEG, and GIF files.\nInclude an opt-out message in the body of your text. For example, \u201cReply STOP to unsubscribe.\u201d See SMS Best Practices for more information.\nOnce you\u2019ve finished adding your template\u2019s content, click Save.\nSegment confirms that your template was saved.\n\nUse the SMS Templates screen to preview and update existing SMS message templates.\n\nEngage content validation\n\nFor all content editors in Engage, you\u2019ll see alerts for any issues in your template, such as invalid profile traits or incorrect liquid syntax. Engage both flags template issue(s), and displays recommended next steps. While you can save these templates, you must fix any issues before using them in Engage campaigns.\n\nTest your SMS template\n\nSend a test SMS message before you include it as a step in your Journey.\n\nAfter you build your SMS template, click Test SMS.\nIf your template has profile traits, enter a trait value for the test SMS. This ensures that your merge tags work as expected.\nEmpty fields show the default value that you\u2019ve assigned. For example, loyal customer would be the default for the following merge tag: {{profile.traits.first_name | default: \"loyal customer\"}}. If there\u2019s no default value, the field will be blank.\nEnter recipient phone numbers for the test message.\nProfiles that you send test messages to must have a userId in Segment.\nClick Send test SMS.\n\nYou can also test SMS templates directly within Journeys before you send them.\n\nPersonalize with merge tags\n\nPersonalize SMS content in Engage using profile traits as merge tags in your messages.\n\nTo personalize an SMS, click Merge Tags in the SMS builder and select the profile traits to include in your message.\n\nEngage inserts the selected traits inside merge tags based on cursor placement in the message. This allows you to personalize each SMS you send to recipients. You can also use liquid templating to create dynamic content in the SMS editor.\n\nTo learn more about profile traits, visit Segment\u2019s Computed Traits and SQL Traits documentation.\n\nConfigure Link Shortening\n\nUse Link Shortening to send shorter, more manageable link URLs in your Engage SMS campaigns.\n\nConfigure Link Shortening in your Twilio Console in six steps:\n\nSet up an Organization\nRegister Domains\nAdd Domain Name System (DNS) records\nGenerate a TLS certificate\nUpload your TLS certificate\nConfigure fallback and callback URLs (Optional)\n\nOnce you\u2019ve configured Link Shortening, Twilio automatically shortens the link URLs for recipients of your SMS messages. Link shortening occurs during the message sending process, so shortened links don\u2019t appear in the message editor.\n\nLink Shortening is only available for SMS messages.\n\nWorking with SMS message templates\n\nYou can edit, duplicate, and delete SMS templates within your Engage workspace.\n\nEdit an SMS message template\n\nTo edit an SMS template:\n\nNavigate to Engage > Content.\nSelect the \u2026 icon next to template you want to edit. Click Edit.\nFrom the template\u2019s overview page, select Edit or Settings.\nEdit your template, then click Update Template to save your changes.\nDuplicate an SMS message template\n\nTo duplicate an SMS template:\n\nNavigate to Engage > Content.\nSelect the \u2026 icon next to template you want to duplicate. Click Duplicate.\nFrom the Duplicate Template popup, click Duplicate.\n\nAfter you duplicate a template, you can edit it from the Templates page.\n\nDelete an SMS message template\n\nTo delete an SMS template:\n\nNavigate to Engage > Content.\nSelect the \u2026 icon next to template you want to delete. Click Delete.\nFrom the Confirm Template Deletion popup, click Delete Template.\nSMS best practices and limitations\nInclude an SMS opt-out message\n\nWhen you build an SMS, include an opt-out message in the body of your text that informs recipients they can unsubscribe from a message channel.\n\nWhen an SMS recipient replies \u201cStop\u201d to an SMS, they\u2019ll receive an opt-out confirmation, and Engage updates their phone number subscription status. Visit the User Subscription States documentation to learn more about user subscriptions in Engage.\n\nSMS character limit\n\nNote that there\u2019s a 1,600 character count limit for SMS messages. Visit Twilio\u2019s SMS Character Limit documentation for more information.\n\nNext steps\n\nUse the Templates screen in Twilio Engage to build personalized email templates.\n\nThis page was last modified: 15 Jul 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSMS template types\nBuild an SMS message template\nTest your SMS template\nPersonalize with merge tags\nConfigure Link Shortening\nWorking with SMS message templates\nSMS best practices and limitations\nNext steps\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nPrivacy\n/\nPrivacy Controls & Alerts\nPrivacy Controls & Alerts\nFREE \u2713\nTEAM \u2713\nBUSINESS \u2713\nADD-ON X\n?\n\nThe Privacy Portal gives you control over whether specific data is allowed to enter Segment.\n\nThe controls are available to all customers, across all plan types. With these, you can block data at the source level, which means data from those sources never enters Segment once blocked. This does not support blocking to device-mode destinations.\n\nFor example, if you want to prevent certain types of PII (like Credit Card Number) from ever being ingested by Segment, you can block it using standard controls by classifying those fields as \u201cRed\u201d in your Data Inventory, and then blocking them in your standard controls settings. Blocking fields blocks the properties from entering Segment, but does not block the rest of the event data from flowing through. Remember that any data you classify as \u201cRed\u201d in your Data Inventory is blocked if you enable these controls.\n\nIf you block Segment data at the source level using these controls, the data does not enter Segment and we can not Replay it. Additionally, if you have Privacy Controls configured to change how you route Red and Yellow data into or out of Segment, the standard controls respect the rules set by those Controls. For example, if you have a Privacy Control set up to block Red data at the Source-level, any new fields you classify in the Data Inventory as Red are also blocked from that Source. Only fields added to the Data Inventory are blocked by a Privacy Control.\n\nPrivacy Alerts\n\nAlerts notify you when a new, unclassified data type appears in your Privacy Portal inbox. You can set up Slack alerts or a generic Source to pipe alerts to other tools you might prefer for events and notifications. We recommend setting up alerts to help you ensure your Inventory is always up to date.\n\nTo set up a Slack Alert:\n\nFrom the Privacy Portal screen, click the Settings tab, and if necessary click Alerts.\nOn the Slack Alerting settings, set the toggle to Enabled.\nEnter the Slack webhook by following the in-app hyperlink.\nEnter the name of the #slack-channel to be send alerts to.\nClick Save.\n\nyou\u2019re all set!\n\nTo send alerts to a Segment Source:\n\nFrom the Privacy Portal screen, click the Settings tab, and if necessary click Alerts.\nOn the Segment Source settings, set the toggle to Enabled.\nFrom the dropdown menu in the settings, select an existing Segment Source, or click the link to create a new Source. We recommend that you use the HTTP Source so you can easily send the data to wherever you prefer to consume alerts.\nClick Save.\n\nyou\u2019re all set!\n\nThis page was last modified: 24 Apr 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nPrivacy Alerts\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nAdd or Update Profiles and Traits with a CSV\nAdd or Update Profiles and Traits with a CSV\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\n\nYou can use the Profiles CSV Uploader to add or update user profiles and traits. This page contains guidelines for your CSV upload and explains how to upload a CSV file to Unify.\n\nCSV file upload guidelines\n\nKeep the following guidelines in mind as you upload CSV files:\n\nYou can only upload .csv files.\nFiles can\u2019t be empty and must have at least one header and one row.\nYou can\u2019t have multiple columns with the same header.\nCSV files cannot exceed 1 million rows (plus one header row), 299 columns, or 100 MB in file size.\nYou can only upload one file at a time.\nAdd an identifier column or anonymous_id in your identity resolution configuration.\nLeave any unknown values blank to avoid bad data. Segment can create a user profile from a single identifier in your CSV.\nThe template won\u2019t include duplicate custom traits, traits with trailing, leading, or multiple consecutive spaces between characters, or unallowed characters.\nCustom traits column headers are case-sensitive. For example, first Name, FIRST Name, and First Name would all be different traits in the template.\nTrailing, leading, or multiple consecutive spaces between characters are not allowed.\nThe CSV uploader shares Unify product limits.\nUpload a CSV file\n\nUse the Upload CSV page to upload a CSV file in your Segment space:\n\nNavigate to Unify > Profile explorer or Engage > Audiences > Profile explorer.\nClick +Add Profiles.\nDownload and fill out the CSV template.\nUpload your CSV file.\n1. Download your CSV template\n\nClick Download Template to download a CSV template with identifier columns from your identity resolution configuration.\n\n2. Fill out your CSV file\n\nEnter values for the identifiers in your CSV file.\n\n3. Upload your CSV file\n\nYou can upload a CSV file in two ways:\n\nDrag and drop the CSV file in the dropzone.\nClick Browse to locate the CSV file.\nWork with the CSV template\n\nKeep the following in mind as you fill out your CSV template.\n\nAllowed CSV file characters\n\nYou can use these characters in your CSV file:\n\nAlphabetic English characters in both upper and lower case\nThe numerals 0-9\nThese special characters: !@#$%^&*()_+-=[]{}:\\\\|.`~<>\\/?\nThe following non-English characters:\n\n\u00e0\u00e1\u00e2\u00e4\u01ce\u00e6\u00e3\u00e5\u0101\u00e7\u0107\u010d\u010b\u010f\u00f0\u1e0d\u00e8\u00e9\u00ea\u00eb\u011b\u1ebd\u0113\u0117\u0119\u011f\u0121gg\u035fh\u0127\u1e25h\u0324\u00ec\u00ed\u00ee\u00ef\u01d0\u0129\u012b\u0131\u012f\u0137k\u035fh\u0142\u013c\u013el\u0325\u1e41m\u0310\u00f2\u00f3\u00f4\u00f6\u01d2\u0153\u00f8\u00f5\u014d\u0159\u1e5br\u0325\u027d\u00df\u015f\u0219\u015b\u0161\u1e63s\u0324s\u0331s\u021b\u0165\u00fe\u1e6dt\u0324\u0288\u00f9\u00fa\u00fb\u00fc\u01d4\u0169\u016b\u0171\u016f\u0175\u00fd\u0177\u00ff\u017a\u017e\u017c\u1e93z\u0324\u00c0\u00c1\n\u00c4\u01cd\u00c6\u00c3\u00c5\u0100\u00c7\u0106\u010c\u010a\u010e\u00d0\u1e0c\u00c8\u00c9\u00ca\u00cb\u011a\u1ebc\u0112\u0116\u0118\u011e\u0120GG\u035fH\u0126\u1e24H\u0324\u00cc\u00cd\u00ce\u00cf\u01cf\u0128\u012aI\u012e\u0136K\u035fH\u0141\u013b\u013dL\u0325\u1e40M\u0310\u00d2\u00d3\u00d4\u00d6\u01d1\u0152\u00d8\u00d5\u014c\u0158\u1e5aR\u0325\u024cS\u1e9e\u015a\u0160\u015e\u0218\u1e62S\u0324S\u0331\u021a\u0164\u00de\u1e6cT\u0324\u01ae\u00d9\u00da\u00db\u00dc\u01d3\u0168\u016a\u0170\u016e\u0174\u00dd\u0176\u0178\u0179\u017d\u017b\u1e92Z\n\nView Update History\n\nUse the Update History page to view CSV file uploads in your workspace over the last 30 days.\n\nTo view the Update History page:\n\nNavigate to Unify > Profile explorer or Engage > Audiences > Profile explorer.\nClick View update history.\nValidation errors\n\nThe following table lists validation errors you may run into with your profiles and traits CSV upload:\n\nERROR\tERROR MESSAGE\nInvalid file types\tYou can upload only .csv files. Change your file format, then try again.\nEmpty files\tThis file contains no data. Add data to your CSV, then try again.\nCSV parsing error\tWe encountered an issue while parsing your CSV file. Validate the CSV file and try again.\nUnexpected/fallback\tSomething went wrong. Try again later.\nEmpty header row\tThis file contains empty header(s). Remove the empty header(s), then try again.\nFile exceeds one million rows\tToo many rows. You can upload up to 1000000 rows.\nFile exceeds 299 columns\tYour CSV file is exceeding the limit of 299 columns.\nFile exceeds 100 MB\tFiles can be up to 100 MB.\nFile contains a header with unallowed spaces\tThis file contains leading, trailing or consecutive spaces. Remove leading, trailing or consecutive spaces, then try again.\nFile contains duplicate headers\tThis file contains duplicate header(s). Remove duplicate header(s), then try again.\nFile contains invalid characters\tThis file contains invalid character(s). Remove invalid character(s), then try again.\nUnconfigured anonymous_id or missing Identifier column\tThis file is missing an identifier column and does not have anonymous_id configured. Add an identifier column or add anonymous_id in your identity resolution configuration, then try again.\n\nThis page was last modified: 12 Mar 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nCSV file upload guidelines\nUpload a CSV file\nWork with the CSV template\nView Update History\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nHow To Guides\n/\nTracking Customers Across Channels and Devices\nTracking Customers Across Channels and Devices\n\nThe paths consumers take to your app or website are more complex than ever, often involving a variety of online communities and multiple devices. Your next repeat customer might stumble across your display ad on a newsletter you\u2019ve never heard about, or receive a recommendation from a co-worker in a Slack channel.\n\nBut these off-domain and cross-device brand interactions are equally, if not more, important to track and understand. With this data, you can identify more sources of qualified traffic and determine the best shopping experiences for conversion.\n\nIn this guide, you\u2019ll learn where and how to track these critical events so that you can understand your customer\u2019s journey before they even get to your storefront, as well as their preferred shopping experiences.\n\nIf you\u2019re interested in learning about what to track, check out Segment\u2019s guide on creating an e-commerce tracking plan.\n\nTalk to a product specialist today about building a clean, high-quality data spec so you can focus on brand engagement and sales growth.\n\nWhere are they coming from? Off-domain tracking\n\nDigital marketing consists of owned marketing, earned marketing, and paid marketing.\n\nTYPE OF MARKETING\tHOW TO TRACK\nOwned (domain, app)\tFirst-party data sources (on-page or in-app analytics)\nOwned (email, push notifications)\tSecond-party data sources\nEarned (blogs, PR, partners, news)\tUTM params, deep links on mobile\nPaid aquisition\tUTM params, deep links on mobile\n\nOwned marketing encompasses all activities you have full control over. It can be further split into first- and second-party data. First-party data is customer data generated on your site or in your app. Second-party data is customer data generated when your customers interact with your email or push notifications (for example, \u201cEmail Opened\u201d or \u201cPush Notification Received\u201d).\n\nEarned marketing is when publications, newsletters, or blogs organically create some content that refers to, or promotes you.\n\nPaid acquisition, like display ads or embedded advertorials, don\u2019t exist on your domain. To track the inbound traffic from both \u201cearned\u201d and paid acquisition sources, Segment uses UTM parameters (and deep links if you\u2019re directing a customer to a specific screen in your mobile app that has the product to purchase).\n\nTrack engagement on your email channels\n\nWhile these are still under \u201cowned\u201d marketing, they happen off your domain. An example is sending an engagement email to your customer base with a call-to-action to visit your store. If you\u2019re using Segment and an email or push notification tool on Segment\u2019s platform, you can easily collect second-party data such as \u201cEmail Sent\u201d and \u201cPush Notification Opened\u201d.\n\nLearn more about which email and push notification tools Segment supports.\n\nHere are some of the most commonly used and popular events tracked through email and push notifications on Segment:\n\nEmail Delivered\n\nEmail Opened\n\nPush Notification Received\n\nPush Notification Opened\n\nDeep Link Clicked\n\nIf your email tool is not supported on Segment, you can still track email opens with Segment\u2019s tracking pixel. This pixel functions like an advertising pixel in that it embeds an image onto pages where JavaScript and POST requests are disabled.\n\nView a list of tools Segment supports.\n\nIn your email template HTML, include an image tag where the src is a URL that is carefully constructed to hit Segment\u2019s appropriate endpoint with a JSON payload that is base64 encoded.\n\nAn example of the payload that will be sent to Segment upon an email open is:\n\n{\n  \"writeKey\": \"YOUR_WRITE_KEY\",\n  \"userId\": \"025waflo3d65\",\n  \"event\": \"Email Opened\",\n  \"properties\": {\n    \"subject\": \"Try Our New $10 Toast\",\n    \"email\": \"andy@segment.com\"\n  }\n}\n\n\nThen, you would base64 encode that and append it to the Segment endpoint:\n\nhttps://api.segment.io/v1/pixel/track?data=<base64-ENCODED-JSON>\n\n\nAdd the complete URL as the src in the image tag.\n\n<img src=\"https://api.segment.io/v1/pixel/track?data=eyJ3cml0ZUtleSI6ICJZT1VSX1dSSVRFX0tFWSIsICJ1c2VySWQiOiAiMDI1cGlrYWNodTAyNSIsICJldmVudCI6ICJFbWFpbCBPcGVuZWQiLCAicHJvcGVydGllcyI6IHsgICAic3ViamVjdCI6ICJUaGUgRWxlY3RyaWMgRGFpbHkiLCAgICJlbWFpbCI6ICJwZWVrQXRNZUBlbWFpbC5wb2tlIiB9fQ\">\n\n\nLearn more about Segment\u2019s Pixel API.\n\nTrack earned traffic with UTM Parameters\n\nUTM parameters are types of query strings added to the end of a URL. When clicked, they let the domain owners track where incoming traffic is coming from and understand what aspects of their marketing campaigns are driving traffic.\n\nUTM parameters are only used when linking to your site from outside of your domain. When a visitor arrives to your site using a link containing UTM parameters, Segment\u2019s client-side analytics.js library will automatically parse the URL\u2019s query strings, and store them within the context object as outlined in the Spec: Common docs. These parameters do not persist to subsequent calls unless you pass them explicitly.\n\nUTM parameters contain three essential components:\n\nutm_campaign: This is the name of your campaign. All marketing activities that support this campaign, needs to have the same utm_campaign so that downstream analysis to measure performance for this specific campaign can be done off this primary key. (Example: \u201cnational-toastday\u201d)\n\nutm_medium: How the traffic is coming to your site. Is it through email, a display ad, or an online forum? This ensures Segment\u2019s downstream analysis can easily see which channel performs the best. (Examples: \u201cemail\u201d, \u201cpaid-display\u201d, \u201cpaid-social\u201d, \u201corganic-social\u201d)\n\nutm_source: Where the traffic is specifically coming from. You can be specific here. This ensures Segment\u2019s downstream analysis can measure which specific source brings the most conversions. (Examples: \u201ctwitter\u201d, \u201ccustomer.io\u201d (email tool), \u201cfacebook\u201d, \u201cadroll\u201d)\n\nWith these being optional:\n\nutm_content: For multiple calls to action on a single page, utm_content indicates which one. For example, on a website, there may be three different display ads. While the link on each display ad will have the same utm_campaign, utm_medium, and utm_source, the utm_content will be different. (Examples: \u201cbanner\u201d, \u201cleft-side\u201d, \u201cbottom-side\u201d)\n\nutm_term: This is the parameter suggested for paid search to identify keywords for your ad. If you\u2019re using Google Adwords and have enabled \u201cautotagging\u201d, then you don\u2019t need to worry about this. Otherwise, you can manually pass the keywords from your search terms through this parameter so that you can see which keywords convert the most. Note that this parameter is reserved explicitly for search. (Examples: \u201ctoast\u201d, \u201cbutter\u201d, \u201cjam\u201d)\n\nIf you\u2019d like UTM parameters to persist in subsequent calls, you\u2019ll need to manually add those fields in the\u00a0context.campaign\u00a0object of your event call. For example:\n\nanalytics.page(\"97980cfea0067\", {}, {\u00a0 campaign: {\n   name: \"TPS Innovation Newsletter\",\n   source: \"Newsletter\",\n   medium: \"email\",\n   term: \"tps reports\",\n   content: \"image link\"\n  },\n});\n\n\nYou can also store the values in cookies and/or localStorage and use Analytics.js Middleware to enrich the payload for subsequent calls.\n\nLearn more about the semantics with each UTM parameter. The key isn\u2019t to stick with the definitions that closely, but to be consistent within your own analytics system.\n\nProper UTMs use\n\nA marketing campaign is a single marketing message across several platforms, media, and channels, with a consistent and clear call-to-action.\n\nSince the marketing campaign is from off-domain to your storefront (on your property or domain), then it\u2019s critical to use the proper and consistent UTM params across all of your channels:\n\nEmails\n\nPaid acquisition\n\nGuest blog post in partner\u2019s newsletter\n\nArticle in the news\n\nOffline events / in real life / meat space\n\nYour UTM parameters would match a pattern such as:\n\nHaving the same utm_campaign across all channels\n\nDifferent utm_source and utm_medium depending on the channel\n\nIf you were on paid acquisition, the placement of the display ad would determine what goes in utm_content\n\nIf you were using paid search, the term would be utm_term\n\nAn example would be a National Toast Day campaign. This campaign would include emails, paid acquisition (with AdRoll and Facebook Ads), organic social (Twitter), and promotional content on partners\u2019 blogs.\n\nCHANNEL\tUTM CAMPAIGN\tUTM MEDIUM\tUTM SOURCE\nEmail\tnational-toastday\temail\tcustomer.io\nNews\tnational-toastday\tnews\ttoastnation\nAdRoll\tnational-toastday\tdisplay\tadroll\nFacebook\tnational-toastday\tpaid-social\tfacebook\nTwitter\tnational-toastday\torganic-social\ttwitter\n\nHaving the consistent UTM parameters naming convention simplifies the downstream analysis and the ease of querying across dimensions, such as within the campaign, which medium or source was the best. Or which placement of the display ad led to the most conversions.\n\nLearn more about measuring ROI of marketing campaigns with SQL and UTM parameters.\n\nWhat device are they using? Cross-device tracking\n\nIt\u2019s common for customers to discover you on their desktop before making the purchase much later on their phone. How do you tie all of these events back to the same customer so you can understand which marketing activities on what screens are responsible for conversions?\n\nTrack server-side when possible\n\nTracking with JavaScript in the browser has its benefits, such as using browser technologies to automatically track things like UTM parameters, referring domain, IP address, and user agent. But here are a few reasons why it might make sense for your store to track on the server side.\n\nAre your customers technically savvy and use ad blockers? Ad blockers restrict requests from a list of blocklisted domains to your browser, which means that none of your event tracking will work properly. If you sell to a technical audience, it is possible that you may be underreporting your analytics by a material amount.\n\nDo you have multiple devices? If you have multiple devices with the same customer check out flow, moving those events to the server-side will reduce your surface area of your code base. This means less maintenance and faster changes.\n\nLearn more about client vs server tracking.\n\nIf you do move key checkout events to the server side, you will have to manually send the data automatically collected by Segment\u2019s client-side JavaScript library to your server. These pieces of tracking data are still important for the following reasons:\n\nUTM parameters: Collecting the UTM params will allow you to tie conversion events to your marketing campaign or activities. This is valuable in that you can immediately measure performance and calculate ROI on your campaigns.\n\nIP address: The IP address can provide location intelligence for your customers. This means you can personalize your shopping experience or engagement emails with inventory that might be more relevant depending on your customers\u2019 locations.\n\nUser Agent: The User Agent will inform you of your customers\u2019 preferred device and shopping experience. Are they converting on a mobile web browser? Native app? Or on their laptop?\n\nLearn how to usecontext to manually send this information on the server side.\n\nTrack the same user across devices\n\nIf your store allows user registration and users are logged in when they shop on your site or app, then you can track them across devices.\n\nThis works by using a userId instead of an anonymousId to track key events and where they occur. This userId serves as the primary key in your downstream tools and data warehouse, allowing you to join all of a profile\u2019s anonymous activities with logged in activities. You also can get a complete picture of a profiles location, and what device they are on while using your app or website.\n\nLearn more about pulling the entire user journey for a single user given a userId.\n\nUnfortunately, tracking the same user across devices only works if they log in to each device. Anonymous browsing in each distinct \u201cexperience\u201d (for example, mobile safari, native iPhone, browser on laptop) generates its own unique anonymousId . Each anonymousId is limited to the scope of that browser or app, only measuring activities in those sessions. It\u2019s not until the user logs in when the userId is generated (if registering for a new account) or the userId is retrieved from your database, and then mapped to the anonymousId of that session. Segment keeps a table of anonymousIds mapped to a single userId so you can analyze a user\u2019s activity across multiple devices.\n\nIf a user logs in on multiple devices, then you would be able to analyze even the anonymous activity across those devices. Consequently, it\u2019s important to encourage your users to log in so that you have this capability.\n\nAttribute offline conversions to online impressions\n\nOne of the biggest challenges for brick-and-mortar stores is to measure the impact of their online advertising campaigns on their in-store purchases. Attributing offline conversions has traditionally been difficult to achieve, due to the lack of offline data and robust infrastructure to route that data.\n\nFor Facebook advertisers, Facebook Offline Conversions allow you to tie offline conversions to your campaigns. It\u2019s important to note that the offline data is labeled to an event set that has been assigned to a Facebook campaign. Here are the two ways to attribute offline conversions to Facebook advertisements:\n\nUploading offline event data about actions that aren\u2019t captured with Facebook Pixel or App Events to Facebook for them to match actions to your Facebook ads\n\nEnable and configure Segment\u2019s Facebook Offline Conversions destination, which automates attributing offline events to your Facebook ads in real-time\n\nLearn more about the benefits of Segment\u2019s Facebook Offline Conversions destination.\n\nMost other advertising networks provide some functionality of manually uploading offline data to match with their online advertising data. Here is a short list of other services:\n\nGoogle Adwords provides the functionality to attribute offline conversions to your ads.\n\nAttributing in-store purchases to an impression from a display ad online is critical to help marketers and advertisers understand which campaigns or creatives are driving sales. The more real-time the data and insights, the more nimble your business can be in altering course so that additional resources can be put towards the right marketing actions.\n\nLearn about the funnel before your website or app\n\nThe internet has made it easy for customers to come from nearly anywhere to your digital storefront. But there are ways to track and collect data to better understand these complicated paths so you can be intentional with your marketing efforts to tap into these communities.\n\nBy tracking in these locations with the above mentioned techniques, your downstream analysis will also be simpler. With UTM params, you\u2019ll be able to quickly measure the performance of a campaign or a particular channel. By properly tracking on multiple devices, you can understand which shopping experiences are most preferred. These tracking techniques are invaluable to understanding the source of your highest quality customers.\n\nTalk to a product specialist today about building a clean, high-quality data spec so you can focus on brand engagement and sales growth.\n\nThis page was last modified: 12 Aug 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nWhere are they coming from? Off-domain tracking\nWhat device are they using? Cross-device tracking\nAttribute offline conversions to online impressions\nLearn about the funnel before your website or app\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSpec\n/\nWhat is the native mobile spec?\nWhat is the native mobile spec?\n\nThe\u00a0Native Mobile Spec\u00a0is a common blueprint for the mobile user lifecycle. The Spec outlines the most important events for mobile apps to be tracking and automatically collects many of these events with the Segment Android and iOS SDKs.\n\nThis article outlines common questions about our Native Mobile Spec. To learn what the benefits are of the feature, check out our\u00a0blog. For technical set up and documentation, see our\u00a0spec docs.\n\nHow does the Native Mobile Spec help me?\n\nSmaller SDK: Move more destinations to the server-side, including the Facebook App Events destination.\n\nLess engineering time: Automatically collect key user events instead of coding them in yourself.\n\nFaster time to value: Set up your destinations with key metrics like Daily Active Users, sessions, and in-app purchases right away!\n\nMeasure ROI of campaigns: Analyze campaign performance with spec\u2019d events like \u201ccampaign hit\u201d, \u201cinstall attributed\u201d and \u201cpush notification opened\u201d in your favorite analytics or BI tool.\n\nWhich destinations currently take advantage of the mobile spec?\n\nOur\u00a0Facebook App Events cloud-mode destination currently takes advantage of the \u201cApplication Installed\u201d event to power new features like custom audience creation, dynamic ads and conversion tracking, without needing to sit on the device. Soon, more destinations like Google Adwords and Salesforce Marketing Cloud and attribution providers will offer similar functionality.\n\nHow does all of this work?\n\nWhen an engineer installs the SDK, the SDK will automatically register on iOS and Android operation system handlers.\n\nWhen the application is foregrounded on the phone, our SDK will be called and emit the Application Opened event. Similarly, when the user updates the app, on the next app open, the iOS and Android operation system will tell us and we\u2019ll emit a track event called Application Updated event. In-app purchases will trigger Order Completed, etc.\n\nHow do I opt-in to the new feature?\n\nThis feature is opted out by default. You have to opt in to collect these events as mentioned in our Quick Start guides (iOS,\u00a0Android). You\u2019ll be doing this in code by altering the configuration you pass into the SDK initialization methods (telling the SDK to collect these events automatically).\n\nWhat happens if I\u2019m already tracking these events? Will they be double counted?\n\nYes, they will be double counted, but that\u2019s only if you opt into this feature. You can either remove your own tracking code for these events or not opt into auto collection at all.\n\nDo I still benefit from this new SDK if I opt out of automatic tracking?\n\nYes. If you follow the Spec when you write your own custom events, you will be able to take advantage of certain features in downstream destinations on the server-side, like with our\u00a0Facebook App Events destination.\n\nWill I need to change the names of the events I am currently tracking?\n\nWe recommend migrating to these event names if you\u2019re tracking similar events so that you can take advantage of available features in our integrations which will depend on the spec as they become available.\n\nCan I send custom properties inside of automatic events?\n\nNot currently.\n\nIs there a way to link the old event name with the new event name?\n\nNot currently.\u00a0Contact us\u00a0for alternative options.\n\nCan I do this later?\n\nYou can, but the sooner you switch to the spec\u2019d events, the further back you\u2019ll be able to look in your reporting with the same event name!\n\nHow will I be able to take advantage of new campaign events?\n\nIn the coming months, we\u2019ll be updating our mobile marketing destinations to automatically capture campaign events around attribution, deep linking, and push notifications. These events will go to\u00a0destinations, including\u00a0warehouses.\n\nWhy don\u2019t Push Notification events reach Segment when my Android App is backgrounded?\n\nAndroid applications can\u2019t receive Push Notifications when the process is not running, and when apps are put into background they are eligible to have their Process killed when there is memory pressure. For more more on Android processes, view Android\u2019s Processes and app lifecycle documentation.\n\nSegment tracks messages delivered to the application. So if the process has been killed for any reason, messages won\u2019t be delivered.\n\nWhy do Push Notifications work with Firebase Cloud Messaging when the app is backgrounded?\n\nFirebase Cloud Messaging (FCM) has its own servers. When an FCM message is created and sent to a device, that message travels through FCM servers and is delivered to the local FCM client device that is part of Android OS. This FCM client is almost always running, even when the app is backgrounded. From there, the FCM message is intended to be delivered to a local application. If the app process is running, the message gets delivered. Otherwise, the user must tap a notification that starts the app and delivers the FCM message.\n\nFor more context on how this process works, view the Firebase FCM architecture documentation.\n\nThis page was last modified: 25 Sep 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nJourneys\n/\nSend Journeys data to a Destination\nSend Journeys data to a Destination\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nWhen you send data to destinations, you send a series of events or user lists, depending on the destination type.\n\nBefore you begin\n\nEnsure you have connected and enabled destinations in your Space.\n\nSend data to destinations\nAdd a Send to destinations step to the journey.\nEnter a Step name. This name should be descriptive of the users you send to the destination. For example, New subscribed users. Journeys generates a key based on the step name you enter. Destinations use this key to references the users that Journeys sends to it. For track events, the property name uses this key. For Identify events, the trait name uses the key.\nClick Connect destinations to select the destination you\u2019ll send the data to.\nClick Save.\n\nTo include an advertising destination in a Journey, ensure you have connected and enabled the destination within your Space, then utilize the Show an Ad step.\n\nTest event payloads\n\nWith the Engage event tester, you can send a test event payload to a Destination. As a result, you can confirm that you\u2019ve correctly configured Journey Audiences before you publish your Journey.\n\nFollow these steps to send a test event:\n\nFrom the Send to destinations window, select + Add destination.\nChoose the Destination that you want to connect.\nIn the Destination pane, select Event tester. This is only available for Event Destinations.\nFrom the Event Type drop-down, select the event you want to test. Segment generates a test user ID.\nSelect Send Event, then view the test event results in the Event lifecycle section.\n\nIf your Destination successfully handled the event, Segment displays a 200 OK HTTP status code along with the full response. If an error occurred, Segment displays any available details in the Event lifecyle section.\n\nUse Trait Activation with Journeys\n\nUse Trait Enrichment and ID Sync to configure sync payloads that you send from Journeys to your destination.\n\nWith Trait Enrichment, use custom, SQL, computed, and predictive traits to enrich the data you map to your destinations.\nUse ID Sync to select identifiers and a sync strategy for the data you send to your destination.\n\nTo use Trait Activation with Journeys:\n\nNavigate to the Journeys builder of a new or existing Journey.\nSelect a supported destination from a journey step.\nSelect Customized Setup, then add identifier and trait mappings to customize the way you send data to your destination. For more, visit the Trait Enrichment and ID Sync setup docs.\n\nUse Segment\u2019s Duplicate mappings feature to create an exact copy of an existing mapping. The copied mapping has the same configurations and enrichments as your original mapping.\n\nWhat events are sent to destinations?\n\nThe data type you send to a destination depends on whether the destination is an Event destination or a List destination.\n\nTo view the events that get generated by an Engage Space\u2019s Journeys, navigate to Unify settings > Debugger to view the list of sources that are configured to generate events for each destination instance. Each source generates events only to its connected destinations. Under the source\u2019s Debugger tab, you\u2019ll find the most recent events generated by that source according the connected destinations\u2019 audiences and computed traits.\n\nThe full JSON body of a journey event will have the journey\u2019s specific details found under the context.personas object. These fields can be useful when building out Destination Filters, Actions destination mappings, and Functions.\n\nThe integrations object in these payloads will appear as {\"All\" : false,} and only list some destinations. This is due to the fact that each source has multiple destinations connected, while each journey may only have a subset of destinations connected to it. See Filtering with the Integrations Object for more information. The integrations object routing specific events to its specified destinations is also why a destination\u2019s Delivery Overview tab will show a large number of events under the Filtered at destination box, as that destination will only receive the events intended to be sent to it according to the journeys that are connected to that specific destination.\n\nEvent destination\n\nThe format in which the destination receives updates depends on the call type.\n\nTrack calls\n\nWhen the user enters the step:\n\n{\n  \"context\": {\n    \"personas\": {\n      \"computation_class\": \"audience\", // the type of computation\n      \"computation_id\": \"aud_###\", // the audience's ID, found in the URL\n      \"computation_key\": \"j_o_###\", // the configured journey key that appears on user profile\n      \"namespace\": \"spa_###\", // the Engage Space's ID\n      \"space_id\": \"spa_###\" // the Engage Space's ID\n    }\n  },\n  \"type\": \"track\",\n  \"event\": \"Audience Entered\",\n  \"properties\": {\n    \"j_o_first_purchase__opened_email_dje83h\": \"true\"\n  }\n}\n\nIdentify calls\n\nWhen the user enters the step:\n\n{\n  \"context\": {\n    \"personas\": {\n      \"computation_class\": \"audience\", // the type of computation\n      \"computation_id\": \"aud_###\", // the audience's ID found in the URL\n      \"computation_key\": \"j_o_###\", // the configured journey key that appears on user profile\n      \"namespace\": \"spa_###\", // the Engage Space's ID\n      \"space_id\": \"spa_###\" // the Engage Space's ID\n    }\n  },\n  \"type\": \"identify\",\n  \"traits\": {\n    \"j_o_first_purchase__opened_email_dje83h\": \"true\"\n  }\n}\n\nList destination\n\nThe destination receives a list of users who qualify for the associated journey step. Unlike lists associated with Engage Audiences, users who are added to a journey list cannot be subsequently removed. See best practices for techniques to suppress targeting with journey lists. List destinations do not have access to the Event tester.\n\nFor more information, see Using Engage Data.\n\nThis page was last modified: 11 Sep 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nBefore you begin\nSend data to destinations\nTest event payloads\nUse Trait Activation with Journeys\nWhat events are sent to destinations?\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGetting Started\n/\nA Basic Segment Installation\nA Basic Segment Installation\n\nWhen you implement Segment, you add Segment code to your website, app, or server. This code generates messages based on specific triggers you define.\n\nIn a basic implementation, the code can be a snippet of JavaScript that you copy and paste into the HTML of a website to track page views. It can also be as complex as Segment calls embedded in a React mobile app to send messages when the app is opened or closed, when the user performs different actions, or when time based conditions are met (for example \u201cticket reservation expired\u201d or \u201ccart abandoned after 2 hours\u201d).\n\nThe best way to learn about how Segment works is to see it in action. This tutorial walks you through an installation using one of Segment\u2019s libraries: JavaScript, PHP, or the iOS library.\n\nBefore you begin\n\nBefore you start your Segment implementation, you need:\n\nA Segment user account and a workspace. If you\u2019re not already part of an organization with a Segment Workspace, you can sign up for a free account and workspace.\nAccess to the code for a basic website, PHP website, or an iOS app.\n\nTip! If you don\u2019t have any of those things, consider creating a simple GitHub Pages website.\n\nCreate separate dev and prod sources\n\nWhen you develop and test sources, Segment recommends you to create and use separate sources for each of your environments (production, development, staging) to prevent testing and development activities from filling production systems with invalid data.\n\nYou can give each source an environment label when you create it, and Segment strongly suggests that you use these labels to sort your sources. When you create a source during the steps below, make sure you enter an environment label.\n\nDouble-check when you enter write keys for dev and production environments to make sure that you send the right data to the right place.\n\nCreate a Segment source\n\nTo create a Segment source:\n\nGo to your Segment workspace, and navigate to the Sources catalog.\nSelect your source. You can choose from either the Javascript source, the PHP source, or the iOS source.\nClick Add Source.\nEnter a name for the source. Segment recommends that you include the word demo, test, or quickstart in the name so you can easily find and delete this source later.\n(Optional) Add an Environment label of dev to the source in the Labels field. Segment recommends you do this so that you know this demo source isn\u2019t part of a production installation.\n(Optional) Add the website URL. Segment provides this field so that you can flag the website being tracked to the source. Segment does not use this URL anywhere else.\nFind your write key\n\nThe write key is a unique identifier for a source that tells Segment which source the data comes from, to which workspace the data belongs, and which destinations should receive the data.\n\nTo find your write key:\n\nGo to Connections > Sources and select your source.\nClick the Settings tab for the source and click API Keys.\n\nMake note of or write down your write key, as you\u2019ll need it in the next steps.\n\nAny time you change a library\u2019s settings in the Segment App, the write key regenerates.\n\nCloud-sources do not have write keys, as they use a token or key from your account with that service. Cloud-sources have other considerations and aren\u2019t part of this tutorial.\n\nInstalling Segment\n\nClick a tab below to see the tutorial content for the specific library you chose.\n\nJavascript quickstart\niOS Mobile quickstart\nPHP quickstart\nStep 1: Copy the Snippet\n\n\nNavigate Connections > Sources > JavaScript in the Segment app and copy the snippet from the JavaScript Source overview page and paste it into the <head> tag of your site.\n\nThat snippet loads Analytics.js onto the page asynchronously, so it won\u2019t affect your page load speed. Once the snippet runs on your site, you can turn on destinations from the destinations page in your workspace and data starts loading on your site automatically.\n\n\n\nNote: If you only want the most basic Google Analytics setup you can stop reading right now. You\u2019re done! Just toggle on Google Analytics from the Segment App.\n\nThe Segment snippet version history available on GitHub. Segment recommends that you use the latest snippet version whenever possible.\n\n\n\n\nStep 2: Identify Users\n\n\nThe identify method is how you tell Segment who the current user is. It includes a unique User ID and any optional traits you know about them. You can read more about it in the identify method reference.\n\n\n\nNote: You don\u2019t need to call identify for anonymous visitors to your site. Segment automatically assigns them an anonymousId, so just calling page and track works just fine without identify.\n\n\n\nHere\u2019s an example of what a basic call to identify might look like:\n\nanalytics.identify('f4ca124298', {\n  name: 'Michael Brown',\n  email: 'mbrown@example.com'\n});\n\n\n\nThis identifies Michael by his unique User ID (in this case, f4ca124298, which is what you know him by in your database) and labels him with name and email traits. When you put that code on your site, you need to replace those hard-coded trait values with the variables that represent the details of the currently logged-in user.\n\nTo do that, Segment recommends that you use a backend template to inject an identify call into the footer of every page of your site where the user is logged in. That way, no matter what page the user first lands on, they will always be identified. You don\u2019t need to call identify if your unique identifier (userId) is not known.\n\nDepending on your templating language, your actual identify call might look something like this:\n\n\nanalytics.identify(' {{user.id}} ', {\n  name: '{{user.fullname}}',\n  email: '{{user.email}}'\n});\n\n\n\n\nWith that call in your page footer, you successfully identify every user that visits your site.\n\n\n\nNote: If you only want to use a basic CRM set up, you can stop here. Just enable Salesforce, Intercom, or any other CRM system from your Segment workspace, and Segment starts sending all of your user data to it.\n\n\n\n\nStep 3: Track Actions\n\n\nThe track method is how you tell Segment about the actions your users are performing on your site. Every action triggers what Segment calls an \u201cevent\u201d, which can also have associated properties. You can read more about track in the track method reference.\n\nHere\u2019s an example of what a call to track might look like when a user signs up:\n\nanalytics.track('Signed Up', {\n  plan: 'Enterprise'\n});\n\n\n\nThis example shows that your user triggered the Signed Up event and chose your hypothetical 'Enterprise' plan.\n\nProperties can be anything you want to record, for example:\n\nanalytics.track('Article Bookmarked', {\n  title: 'Snow Fall',\n  subtitle: 'The Avalanche at Tunnel Creek',\n  author: 'John Branch'\n});\n\n\n\nIf you\u2019re just getting started, some of the events you should track are events that indicate the success of your site, like Signed Up, Item Purchased or Article Bookmarked. Segment recommends that you track a few important events as you can always add more later.\n\nOnce you add a few track calls, you\u2019re done with setting up Segment. You successfully installed Analytics.js tracking. Now you\u2019re ready to turn on any destination you like from the Segment App.\n\nTest that it\u2019s working\n\nOnce you\u2019ve set up your Segment library, and instrumented at least one call, you can look at the Debugger tab for the Source to check that it produces data as you expected.\n\nThe Source Debugger is a real-time tool that helps you confirm that API calls made from your website, mobile app, or servers arrive at your Segment Source, so you can quickly see how calls are received by your Segment source, so you can troubleshoot quickly without having to wait for data processing.\n\nThe Debugger is separate from your workspace\u2019s data pipeline, and is not an exhaustive view of all the events ever sent to your Segment workspace. The Debugger only shows a sample of the events that the Source receives in real time, with a cap of 500 events. The Debugger is a great way to test specific parts of your implementation to validate that events are being fired successfully and arriving to your Source.\n\nTip: To see a more complete view of all your events, you might consider setting up either a warehouse or an S3 destination.\n\nThe Debugger shows a live stream of sampled events arriving at the Source, but you can also toggle from \u201cLive\u201d to \u201cPause\u201d to stop the stream and prevent it from displaying new events. Events continue to arrive to your Source while you Pause the stream, they just are not displayed.\n\nYou can search on any information you know is available in an event payload to search in the Debugger and show only matching payloads. You can also use advanced search options to limit the results to a specific event.\n\nTwo views are available when viewing a payload:\n\nThe Pretty view is a recreation of the API call you made that was sent to Segment.\nThe Raw view is the complete JSON object Segment received from the calls you sent. These calls include all the details about what is being tracked: timestamps, properties, traits, ids, and contextual information Segment automatically collects the moment the data is sent.\nSet up your first destination\n\nOnce you\u2019re satisfied that data is arriving from your new source, it\u2019s time to set up your first destination! As long as you have page or screen data coming from the source, you can quickly enable Google Analytics to look at the page view statistics.\n\nIf you don\u2019t have a Google Analytics account, you can either set up a free account, or look at the Destination Catalog for a different destination to enable.\n\nYou\u2019ll need a tracking ID for Google Analytics (either a \u201cwebsite\u201d or \u201cserverside\u201d tracking ID), or another API key if you\u2019re substituting another destination. Make a note of this ID or key as you\u2019ll need it to connect your destination.\n\nTo set up your first destination:\n\nGo to your Segment workspace, click Destinations, and click Add Destination to go to the Catalog.\nSearch for the destination you want to add. In this case, search for Google Analytics.\nClick the tile for the destination to see information about it.\nClick Configure Google Analytics.\nSelect the source that you set up earlier in this quickstart, then click Confirm Source.\nOn the settings page, locate the setting line for the tracking ID or other API key to connect to your destination.\nEnter the ID or API key and click Save.\nClick Back to Destination, then click the toggle to enable the destination.\n\nCongratulations! Data is now flowing from the source you set up, to the first destination. Do some test browsing on your site or app, then log in to your downstream tool to see the data in place.\n\nYou can click around and load pages to see your Segment calls in action, watch them arrive in the Debugger, and see them arrive in the destination tool.\n\nNote: When you\u2019re done with this test source and destination, you can delete them. This prevents you from getting unplanned \u201cdemo\u201d data in your production environment later.\n\nBACK\nWhat is Segment\n\nThe basics of the Segment platform and what you can do with it.\n\nNEXT\nPlanning a Full Installation\n\nThink through your goals, plan your calls, and set yourself up for success.\n\nThis page was last modified: 13 Aug 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nBefore you begin\nCreate a Segment source\nFind your write key\nInstalling Segment\nTest that it\u2019s working\nSet up your first destination\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSpec\n/\nVideo Spec\nVideo Spec\n\nSegment\u2019s video spec helps you define how customers engage with your video and ad content. This document covers the naming syntax and conventions for how you should send events when tracking video analytics.\n\nNote: not all destinations support video tracking, and you should always check with the individual destination documentation to confirm.\n\nGetting started\n\nBefore you start implementing the Segment video spec, you should understand the overall structure and classification of events. The video spec will be organized into three distinct event categories:\n\nPlayback\nContent\nAds\nPlayback\n\nYou can think of playback events being related to the actual playback of the video content. This means that these events are meant to track information about the video player (such as pause, resume, or play). Thus, you can think of playback events to be at the session level. For example, when a customer presses play on your video, you would start by sending a Video Playback Started event with a unique session_id. In particular, this event should fire after the last user action required for playback to begin.\n\nThen, for the duration of that user\u2019s session with that specific video player, all subsequent events generated from this session/playback should be tied with the same aforementioned session_id. So if you had a web page that had two video players, you would have two separate sessions and session_ids while contrastingly if you only had one video player on the page but the playback played two video contents in a row, you would only have one session but two contents tied to it.\n\nPlayback event object\n\nAll playback events share the same event properties that describe information about the current state of the player. Below is a table of the supported properties of this object.\n\nPROPERTY\tTYPE\tDESCRIPTION\nsession_id\tString\tThe unique ID of the overall session used to tie all events generated from a specific playback. This value should be the same across all playback, content, and ad events if they are from the same playback session.\ncontent_asset_id, content_asset_ids\tString, Array[string]\tThe Content Asset Id(s) of the video/videos playing or about to be played in the video player. For Video Playback Started events only, you should send the plural form with an Array of unique asset IDs. For all other playback events, you should send the singular form with the ID of the current content asset playing at the time of the event.\ncontent_pod_id, content_pod_ids\tString, Array[string]\tThe Content Pod Id(s) of the video/videos playing or about to be played in the video player. For Video Playback Started events only, you should send the plural form with an Array of unique pod IDs. For all other playback events, you should send the singular form with the ID of the current content pod playing at the time of the event.\nad_asset_id\tString, Array[string]\tThe Ad Asset Id(s) of the ad/ads playing or about to be played in the video player. For Video Playback Started events only, you should send an Array of unique ad asset IDs. For all other playback events, you should send a string with the ID of the current ad asset playing at the time of the event.\nad_pod_id\tString, Array[string]\tThe Ad Pod Id(s) of the ad/ads playing or about to be played in the video player. For Video Playback Started events only, you should send an Array of unique ad pod IDs. For all other playback events, you should send a string with the ID of the current ad pod playing at the time of the event.\nad_type\tEnum {pre-roll, mid-roll, post-roll}\tThe type of ad playing at the time of the event. Values can include pre-roll, mid-roll, and post-roll.\nposition\tInteger\tThe current index position in seconds of the playhead, including the duration of any ads seen (if available). If the playback is a livestream, check the documentation for relevant destinations for details on how to correctly pass the playhead position.\ntotal_length\tInteger\tThe total duration of the playback in seconds. This should include the duration of all your content and ad included in this playback session. For livestream playback, send null.\nbitrate\tInteger\tThe current kbps.\nframerate\tFloat\tThe average fps.\nvideo_player\tString\tThe name of the video player (for example, youtube or vimeo).\nsound\tInteger\tThe sound level of the playback represented in a 0 to 100 scale where 0 is muted and 100 is full volume.\nfull_screen\tBoolean\ttrue if playback is currently in full screen mode and false otherwise.\nad_enabled\tBoolean\tfalse if the user has adblock or any other ad blockers, otherwise true if they can view your video ads.\nquality\tString\tThe quality of the video, for example, highres, hd1080, or 480p.\nmethod\tString\tFor Video Playback Interrupted events only, you can send this property denoting how the playback was interrupted (such as browser redirect, device lock, or call).\nlivestream\tBoolean\tIf the playback will be a livestream, send true, otherwise false.\nPlayback events\n\nBelow is the full list of Video Playback Events.\n\nVideo playback started\n\nWhen a user presses Play; after the last user action required for playback to begin (for example, after user login/authentication).\n\n{\n    \"action\": \"track\",\n    \"event\": \"Video Playback Started\",\n    \"userId\": \"userId\",\n    \"properties\": {\n      \"session_id\": \"12345\",\n      \"content_asset_ids\": [\"0129370\"],\n      \"content_pod_ids\": [\"segA\", \"segB\"],\n      \"ad_asset_id\": [\"ad123\", \"ad097\"],\n      \"ad_pod_id\": [\"adSegA\", \"adSegB\"],\n      \"ad_type\": [\"mid-roll\", \"post-roll\"],\n      \"position\": 0,\n      \"total_length\": 392,\n      \"bitrate\": 100,\n      \"framerate\": 29.00,\n      \"video_player\": \"youtube\",\n      \"sound\": 88,\n      \"full_screen\": false,\n      \"ad_enabled\": true,\n      \"quality\": \"hd1080\",\n      \"livestream\": false\n    }\n}\n\nVideo playback paused\n\nWhen a user presses Pause.\n\n{\n    \"action\": \"track\",\n    \"event\": \"Video Playback Paused\",\n    \"userId\": \"userId\",\n    \"properties\": {\n      \"session_id\": \"12345\",\n      \"content_asset_id\": \"0129370\",\n      \"content_pod_id\": \"segA\",\n      \"position\": 278,\n      \"total_length\": 392,\n      \"bitrate\": 100,\n      \"framerate\": 29.00,\n      \"video_player\": \"youtube\",\n      \"sound\": 55,\n      \"full_screen\": false,\n      \"ad_enabled\": false,\n      \"quality\": \"hd1080\",\n      \"livestream\": false\n    }\n}\n\nVideo playback interrupted\n\nWhen the playback stops unintentionally (such as from network loss, browser close/redirect, or app crash). With this event you can pass method as a property to denote the cause of the interruption.\n\n{\n    \"action\": \"track\",\n    \"event\": \"Video Playback Interrupted\",\n    \"userId\": \"userId\",\n    \"properties\": {\n      \"session_id\": \"12345\",\n      \"content_asset_id\": \"0129370\",\n      \"content_pod_id\": \"segA\",\n      \"position\": 278,\n      \"total_length\": 392,\n      \"bitrate\": 100,\n      \"framerate\": 29.00,\n      \"video_player\": \"youtube\",\n      \"sound\": 55,\n      \"full_screen\": false,\n      \"ad_enabled\": false,\n      \"quality\": \"hd1080\",\n      \"livestream\": false,\n      \"method\": \"network loss\"\n    }\n}\n\nVideo playback buffer started\n\nWhen playback starts buffering content or an ad.\n\n{\n    \"action\": \"track\",\n    \"event\": \"Video Playback Buffer Started\",\n    \"userId\": \"userId\",\n    \"properties\": {\n      \"session_id\": \"12345\",\n      \"content_asset_id\": \"0129370\",\n      \"content_pod_id\": \"segA\",\n      \"position\": 278,\n      \"total_length\": 392,\n      \"bitrate\": 100,\n      \"framerate\": 29.00,\n      \"video_player\": \"youtube\",\n      \"sound\": 55,\n      \"full_screen\": false,\n      \"ad_enabled\": false,\n      \"quality\": \"hd1080\",\n      \"livestream\": false\n    }\n}\n\nVideo playback buffer completed\n\nWhen playback finishes buffering content or an ad.\n\n{\n    \"action\": \"track\",\n    \"event\": \"Video Playback Buffer Completed\",\n    \"userId\": \"userId\",\n    \"properties\": {\n      \"session_id\": \"12345\",\n      \"content_asset_id\": \"0129370\",\n      \"content_pod_id\": \"segA\",\n      \"position\": 278,\n      \"total_length\": 392,\n      \"bitrate\": 100,\n      \"framerate\": 29.00,\n      \"video_player\": \"youtube\",\n      \"sound\": 55,\n      \"full_screen\": false,\n      \"ad_enabled\": false,\n      \"quality\": \"hd1080\",\n      \"livestream\": false\n    }\n}\n\nVideo playback seek started\n\nWhen a user manually seeks a certain position of the content or ad in the playback. Pass in the seek_position to denote where the user is seeking to, and pass in the position property to denote where the user is seeking from.\n\n{\n    \"action\": \"track\",\n    \"event\": \"Video Playback Seek Started\",\n    \"userId\": \"userId\",\n    \"properties\": {\n      \"session_id\": \"12345\",\n      \"content_asset_id\": \"0129370\",\n      \"content_pod_id\": \"segA\",\n      \"position\": 278,\n      \"seek_position\": 320,\n      \"total_length\": 392,\n      \"bitrate\": 100,\n      \"framerate\": 29.00,\n      \"video_player\": \"youtube\",\n      \"sound\": 55,\n      \"full_screen\": false,\n      \"ad_enabled\": false,\n      \"quality\": \"hd1080\",\n      \"livestream\": false\n    }\n}\n\nVideo playback seek completed\n\nAfter a user manually seeks to a certain position of the content or ad in the playback. Pass in the position property to denote where the user desires to begin the playback from.\n\n{\n    \"action\": \"track\",\n    \"event\": \"Video Playback Seek Completed\",\n    \"userId\": \"userId\",\n    \"properties\": {\n      \"session_id\": \"12345\",\n      \"content_asset_id\": \"0129370\",\n      \"content_pod_id\": \"segA\",\n      \"position\": 320,\n      \"total_length\": 392,\n      \"bitrate\": 100,\n      \"framerate\": 29.00,\n      \"video_player\": \"youtube\",\n      \"sound\": 55,\n      \"full_screen\": false,\n      \"ad_enabled\": false,\n      \"quality\": \"hd1080\",\n      \"livestream\": false\n    }\n}\n\nVideo playback resumed\n\nWhen playback is resumed, by the user, after being paused.\n\n{\n    \"action\": \"track\",\n    \"event\": \"Video Playback Resumed\",\n    \"userId\": \"userId\",\n    \"properties\": {\n      \"session_id\": \"12345\",\n      \"content_asset_id\": \"0129370\",\n      \"content_pod_id\": \"segA\",\n      \"position\": 278,\n      \"total_length\": 392,\n      \"bitrate\": 100,\n      \"framerate\": 29.00,\n      \"video_player\": \"youtube\",\n      \"sound\": 55,\n      \"full_screen\": false,\n      \"ad_enabled\": false,\n      \"quality\": \"hd1080\",\n      \"livestream\": false\n    }\n}\n\nVideo playback completed\n\nWhen playback is complete and only when the session is finished.\n\n{\n    \"action\": \"track\",\n    \"event\": \"Video Playback Completed\",\n    \"userId\": \"userId\",\n    \"properties\": {\n      \"session_id\": \"12345\",\n      \"content_asset_id\": \"0129370\",\n      \"content_pod_id\": \"segA\",\n      \"position\": 392,\n      \"total_length\": 392,\n      \"bitrate\": 100,\n      \"framerate\": 29.00,\n      \"video_player\": \"youtube\",\n      \"sound\": 55,\n      \"full_screen\": false,\n      \"ad_enabled\": false,\n      \"quality\": \"hd1080\",\n      \"livestream\": false\n    }\n}\n\nVideo playback exited\n\nWhen user navigates away from a playback/stream.\n\n{\n    \"action\": \"track\",\n    \"event\": \"Video Playback Exited\",\n    \"userId\": \"userId\",\n    \"properties\": {\n      \"session_id\": \"12345\",\n      \"content_asset_id\": \"0129370\",\n      \"content_pod_id\": \"segA\",\n      \"position\": 392,\n      \"total_length\": 392,\n      \"bitrate\": 100,\n      \"framerate\": 29.00,\n      \"video_player\": \"youtube\",\n      \"sound\": 55,\n      \"full_screen\": false,\n      \"ad_enabled\": false,\n      \"quality\": \"hd1080\",\n      \"livestream\": false\n    }\n}\n\nContent\n\nUnderneath the playback level, we now have the pod level. A pod can be seen as a \u201cgroup\u201d or \u201csegment\u201d of either the content or advertisement.\n\nConsider, for example, a playback session that might have some content and one mid-roll advertisement. This would mean that you would have two content pods (since the mid-roll ad split the content playback into two sections) while you might have one ad pod for the mid-roll ad. In this instance, you\u2019d start and complete the first pod of content; you\u2019d start and complete the ad; you\u2019d start and complete the second pod of content. All of this would happen within one playback start.\n\nContent event object\n\nAll content events share the same event properties that describe information about the current video content the user is interacting with. Below is a table of the supported properties of this object.\n\nPROPERTY\tTYPE\tDESCRIPTION\nsession_id\tString\tThe unique ID of the overall session used to tie all events generated from a specific playback. This value should be same across all playback, content, and ad events if they are from the same playback session.\nasset_id\tString\tThe unique ID of the content asset.\npod_id\tString\tThe unique ID of the content pod.\ntitle\tString\tThe title of the video content.\ndescription\tString\tShort description of the video content.\nkeywords\tArray[string]\tAn array of arbitrary keywords or tags that describe or categorize the video content.\nseason\tString\tThe season number if applicable.\nepisode\tString\tThe episode number if applicable.\ngenre\tString\tThe genre of the content. For example,comedy or action.\nprogram\tString\tThe name of the content program or show if applicable.\npublisher\tString\tThe content creator, author, producer, or publisher.\nposition\tInteger\tThe current index position in seconds of the playhead into the content/asset. This position must exclude the duration of any ads played. If the playback is a livestream, check the documentation for relevant destinations for details on how to correctly pass the playhead position.\ntotal_length\tInteger\tThe total duration of the content/asset in seconds. This should exclude the duration of any ads included in the playback of this asset. For livestream playback, send null.\nchannel\tString\tThe channel in which the video content is playing, such as espn or my blog.\nfull_episode\tBoolean\ttrue if content is a full episode and false otherwise.\nlivestream\tBoolean\tIf the playback is a livetream, send true, otherwise false.\nairdate\tISO 8601 Date String\tAn ISO 8601 Date String representing the original air date or published date.\nbitrate\tInteger\tThe current kbps.\nframerate\tFloat\tThe average fps.\nContent events\n\nBelow is the full list of Video Content Events.\n\nVideo content started\n\nWhen a video content segment starts playing within a playback.\n\n{\n    \"action\": \"track\",\n    \"event\": \"Video Content Started\",\n    \"userId\": \"userId\",\n    \"properties\": {\n      \"session_id\": \"12345\",\n      \"asset_id\": \"0129370\",\n      \"pod_id\": \"segA\",\n      \"program\": \"Planet Earth\",\n      \"title\": \"Seasonal Forests\",\n      \"description\": \"David Attenborough reveals the greatest woodlands on earth.\",\n      \"season\": \"1\",\n      \"position\": 0,\n      \"total_length\": 3600,\n      \"genre\": \"Documentary\",\n      \"publisher\": \"BBC\",\n      \"full_episode\": true,\n      \"keywords\": [\"nature\", \"forests\", \"earth\"]\n    }\n}\n\nVideo content playing\n\nHeartbeats that you can fire every n seconds to track how far into the content the user is currently viewing as indicated by the position.\n\n{\n    \"action\": \"track\",\n    \"event\": \"Video Content Playing\",\n    \"userId\": \"userId\",\n    \"properties\": {\n      \"session_id\": \"12345\",\n      \"asset_id\": \"0129370\",\n      \"pod_id\": \"segA\",\n      \"program\": \"Planet Earth\",\n      \"title\": \"Seasonal Forests\",\n      \"description\": \"David Attenborough reveals the greatest woodlands on earth.\",\n      \"season\": \"1\",\n      \"position\": 10,\n      \"total_length\": 3600,\n      \"genre\": \"Documentary\",\n      \"publisher\": \"BBC\",\n      \"full_episode\": true,\n      \"keywords\": [\"nature\", \"forests\", \"earth\"]\n    }\n}\n\nVideo content completed\n\nWhen a video content segment completes playing within a playback. That is, position and total_length are equal.\n\n{\n    \"action\": \"track\",\n    \"event\": \"Video Content Completed\",\n    \"userId\": \"userId\",\n    \"properties\": {\n      \"session_id\": \"12345\",\n      \"asset_id\": \"0129370\",\n      \"pod_id\": \"segA\",\n      \"program\": \"Planet Earth\",\n      \"title\": \"Seasonal Forests\",\n      \"description\": \"David Attenborough reveals the greatest woodlands on earth.\",\n      \"season\": \"1\",\n      \"position\": 3600,\n      \"total_length\": 3600,\n      \"genre\": \"Documentary\",\n      \"publisher\": \"BBC\",\n      \"full_episode\": true,\n      \"keywords\": [\"nature\", \"forests\", \"earth\"]\n    }\n}\n\nAds\n\nJust like Content events, Ad Events also live underneath the playback level and at the pod level. A given ad pod can have multiple ad assets (or just one) and a playback session might have multiple ad pods. For example, if your video playback has two pre-roll, one mid-roll, and one post-roll ads, you would have three ad pods:\n\nad pod 1: plays the two pre-roll ads\nad pod 2: plays the one mid-roll ad\nad pod 3: plays the one post-roll ad\nAd event object\n\nAll ad events share the same event properties that describe information about the current ad content the user is interacting with. Below is a table of the supported properties of this object.\n\nPROPERTY\tTYPE\tDESCRIPTION\nsession_id\tString\tThe unique ID of the overall session used to tie all events generated from a specific playback. This value should be same across all playback, content, and ad events if they are from the same playback session.\nasset_id\tString\tThe unique ID of the ad asset.\npod_id\tString\tThe unique ID of the ad pod.\npod_position\tInteger\tThe position of the ad asset relative to other assets in the same pod.\npod_length\tInteger\tThe number of ad assets the current ad pod contains.\ntype\tEnum {pre-roll, mid-roll, post-roll}\tThe ad type. You can send either pre-roll, mid-roll, or post-roll.\ntitle\tString\tThe title of the video ad.\npublisher\tString\tThe ad creator, author, producer, or publisher.\nposition\tInteger\tThe current index position, in seconds, of the playhead with respect to the length of the ad.\ntotal_length\tInteger\tThe total duration of the current ad asset in seconds.\nload_type\tEnum {linear, dynamic}\tdynamic if ads are loaded dynamically and linear if ads are same for all users.\ncontent\tObject[ContentEventObject]\tFor video destinations that require content metadata to be sent with ad events, you can send all the content metadata nested under this property (such as content.asset_id or content.title) as a Content Event Object.\nquartile\tInteger\tFor Video Ad Playing events, this property can be set to indicate when a specific ad quartile has been reached (1,2, or 3). If you are using a Segment client-side library to track your video events you don\u2019t need to send this property as Segment\u2019s libraries will automatically track quartiles.\n\nSince some video destinations require sending Content metadata along with Ad metadata, you may need to send your content properties also in all your ad events under properties.content depending on the video destination you\u2019re using.\n\nAd events\nVideo ad started\n{\n    \"action\": \"track\",\n    \"event\": \"Video Ad Started\",\n    \"userId\": \"userId\",\n    \"properties\": {\n      \"session_id\": \"12345\",\n      \"asset_id\": \"0129370\",\n      \"pod_id\": \"segA\",\n      \"type\": \"pre-roll\",\n      \"title\": \"The New New Thing!\",\n      \"position\": 0,\n      \"total_length\": 30,\n      \"publisher\": \"Apple\",\n      \"load_type\": \"dynamic\"\n    }\n}\n\nVideo ad playing\n{\n    \"action\": \"track\",\n    \"event\": \"Video Ad Playing\",\n    \"userId\": \"userId\",\n    \"properties\": {\n      \"session_id\": \"12345\",\n      \"asset_id\": \"0129370\",\n      \"pod_id\": \"segA\",\n      \"type\": \"pre-roll\",\n      \"title\": \"The New New Thing!\",\n      \"position\": 5,\n      \"total_length\": 30,\n      \"publisher\": \"Apple\",\n      \"load_type\": \"dynamic\"\n    }\n}\n\nVideo ad completed\n{\n  \"action\": \"track\",\n  \"event\": \"Video Ad Completed\",\n  \"userId\": \"userId\",\n  \"properties\": {\n    \"session_id\": \"12345\",\n    \"asset_id\": \"0129370\",\n    \"pod_id\": \"segA\",\n    \"type\": \"pre-roll\",\n    \"title\": \"The New New Thing!\",\n    \"position\": 30,\n    \"total_length\": 30,\n    \"publisher\": \"Apple\",\n    \"load_type\": \"dynamic\"\n  }\n}\n\nResuming playback\n\nWhen you fire a Video Playback Resumed event, you should immediately call a Segment heartbeat event (Video Content Playing or Video Ad Playing depending on what the playback resumed to). This should effectively mean that you are also resuming your 10 second heartbeats (since they should\u2019ve been paused after sending Video Playback Paused event).\n\nVideo quality event\n\nIt\u2019s important to analyze the performance of your video content. To keep track of quality changes, you can track a Video Quality Updated event when there is a change in video quality with the following properties:\n\nbitrate\nframerate\nstartupTime\ndroppedFrames\nExample event lifecycle\n\nBelow is an example of how one might implement the video spec:\n\n1) User presses play on a video:\n\nanalytics.track('Video Playback Started', {\n  session_id: '12345',\n  content_asset_ids: ['0129370'],\n  content_pod_ids: ['segA', 'segB'],\n  ad_asset_ids: [ 'ad123', 'ad097' ],\n  ad_pod_ids: ['adSegA', 'adSegB'],\n  ad_types: ['mid-roll', 'post-roll'],\n  position: 0,\n  total_length: 392,\n  bitrate: 100,\n  video_player: 'youtube',\n  sound: 88,\n  full_screen: false,\n  ad_enabled: false,\n  quality: 'hd1080'\n});\n\n\n2) Playback starts to play content:\n\nanalytics.track('Video Content Started', {\n  session_id: '12345',\n  asset_id: '0129370',\n  pod_id: 'segA',\n  title: 'Interview with Tony Robbins',\n  description: 'short description',\n  keywords: ['entrepreneurship', 'motivation'],\n  season: '2',\n  episode: '177',\n  genre: 'entrepreneurship',\n  program: 'Tim Ferris Show',\n  publisher: 'Tim Ferris',\n  position: 0,\n  total_length: 360,\n  channel: 'espn',\n  full_episode: true,\n  livestream: false,\n  airdate: '1991-08-13'\n});\n\n\n3) User views content for 20 seconds and we have 10 second heartbeats:\n\nanalytics.track('Video Content Playing', {\n  session_id: '12345',\n  asset_id: '0129370',\n  pod_id: 'segA',\n  title: 'Interview with Tony Robbins',\n  description: 'short description',\n  keywords: ['entrepreneurship', 'motivation'],\n  season: '2',\n  episode: '177',\n  genre: 'entrepreneurship',\n  program: 'Tim Ferris Show',\n  publisher: 'Tim Ferris',\n  position: 10,\n  total_length: 360,\n  channel: 'espn',\n  full_episode: true,\n  livestream: false,\n  airdate: '1991-08-13'\n});\n\nanalytics.track('Video Content Playing', {\n  session_id: '12345',\n  asset_id: '0129370',\n  pod_id: 'segA',\n  title: 'Interview with Tony Robbins',\n  description: 'short description',\n  keywords: ['entrepreneurship', 'motivation'],\n  season: '2',\n  episode: '177',\n  genre: 'entrepreneurship',\n  program: 'Tim Ferris Show',\n  publisher: 'Tim Ferris',\n  position: 20,\n  total_length: 360,\n  channel: 'espn',\n  full_episode: true,\n  livestream: false,\n  airdate: '1991-08-13'\n});\n\n\n4) Playback is paused and resumed:\n\nanalytics.track('Video Playback Paused', {\n  session_id: '12345',\n  content_asset_id: '0129370',\n  content_pod_id: 'segA',\n  ad_asset_id: null,\n  ad_pod_id: null,\n  ad_type: null,\n  position: 21,\n  total_length: 392,\n  video_player: 'youtube',\n  sound: 88,\n  bitrate: 100,\n  full_screen: false,\n  ad_enabled: false,\n  quality: 'hd1080'\n});\n\nanalytics.track('Video Playback Resumed', {\n  session_id: '12345',\n  content_asset_id: '0129370',\n  content_pod_id: 'segA',\n  ad_asset_id: null,\n  ad_pod_id: null,\n  ad_type: null,\n  position: 21,\n  total_length: 392,\n  sound: 88,\n  bitrate: 100,\n  full_screen: false,\n  video_player: 'youtube',\n  ad_enabled: false,\n  quality: 'hd1080'\n});\n\n\n5) Mid-roll ad starts playing:\n\nanalytics.track('Video Ad Started', {\n  session_id: '12345',\n  asset_id: 'ad123',\n  pod_id: 'adSegA',\n  type: 'mid-roll',\n  title: 'Segment Connection Modes',\n  publisher: 'Segment',\n  position: 0,\n  total_length: 21,\n  load_type: 'dynamic'\n});\n\n\n6) User watches the full 21 second ad and we have 10 second heartbeats:\n\nanalytics.track('Video Ad Playing', {\n  session_id: '12345',\n  asset_id: 'ad123',\n  pod_id: 'adSegA',\n  type: 'pre-roll',\n  title: 'Segment Connection Modes',\n  publisher: 'Segment',\n  position: 10,\n  total_length: 21,\n  load_type: 'dynamic'\n});\n\nanalytics.track('Video Ad Playing', {\n  session_id: '12345',\n  asset_id: 'ad123',\n  pod_id: 'adSegA',\n  type: 'pre-roll',\n  title: 'Segment Connection Modes',\n  publisher: 'Segment',\n  position: 20,\n  total_length: 21,\n  load_type: 'dynamic'\n});\n\nanalytics.track('Video Ad Completed', {\n  session_id: '12345',\n  asset_id: 'ad123',\n  pod_id: 'adSegA',\n  type: 'pre-roll',\n  title: 'Segment Connection Modes',\n  publisher: 'Segment',\n  position: 21,\n  total_length: 21,\n  load_type: 'dynamic'\n});\n\n\n7) Content resumes, user finishes the full content:\n\nanalytics.track('Video Content Playing', {\n  session_id: '12345',\n  asset_id: '0129370',\n  pod_id: 'segB',\n  title: 'Interview with Tony Robbins',\n  description: 'short description',\n  keywords: ['entrepreneurship', 'motivation'],\n  season: '2',\n  episode: '177',\n  genre: 'entrepreneurship',\n  program: 'Tim Ferris Show',\n  publisher: 'Tim Ferris',\n  position: 31,\n  total_length: 360,\n  channel: 'espn',\n  full_episode: true,\n  livestream: false,\n  airdate: '1991-08-13'\n});\n\n\n(Video Content Playing hearbeats every 10 seconds)\n\nanalytics.track('Video Content Completed', {\n  session_id: '12345',\n  asset_id: '0129370',\n  pod_id: 'segB',\n  title: 'Interview with Tony Robbins',\n  description: 'short description',\n  keywords: ['entrepreneurship', 'motivation'],\n  season: '2',\n  episode: '177',\n  genre: 'entrepreneurship',\n  program: 'Tim Ferris Show',\n  publisher: 'Tim Ferris',\n  position: 360,\n  total_length: 360,\n  channel: 'espn',\n  full_episode: true,\n  livestream: false,\n  airdate: '1991-08-13'\n});\n\n\n8) 11 second post-roll ad plays after content finishes:\n\nanalytics.track('Video Ad Started', {\n  session_id: '12345',\n  asset_id: 'ad097',\n  pod_id: 'adSegB',\n  type: 'post-roll',\n  title: 'Segment Cross Domain IDs',\n  publisher: 'Segment',\n  position: 0,\n  total_length: 11,\n  load_type: 'dynamic'\n});\n\nanalytics.track('Video Ad Playing', {\n  session_id: '12345',\n  asset_id: 'ad097',\n  pod_id: 'adSegB',\n  type: 'post-roll',\n  title: 'Segment Cross Domain IDs',\n  publisher: 'Segment',\n  position: 10,\n  total_length: 11,\n  load_type: 'dynamic'\n});\n\nanalytics.track('Video Ad Completed', {\n  session_id: '12345',\n  asset_id: 'ad097',\n  pod_id: 'adSegB',\n  type: 'post-roll',\n  title: 'Segment Cross Domain IDs',\n  publisher: 'Segment',\n  position: 11,\n  total_length: 11,\n  load_type: 'dynamic'\n});\n\n\n9) Playback finishes:\n\nanalytics.track('Video Playback Completed', {\n  session_id: '12345',\n  content_asset_id: null,\n  content_pod_id: null,\n  ad_asset_id: 'ad907',\n  ad_pod_id: 'adSegB',\n  ad_type: null,\n  position: 392,\n  total_length: 392,\n  sound: 88,\n  bitrate: 100,\n  full_screen: false,\n  video_player: 'youtube',\n  ad_enabled: false,\n  quality: 'hd1080'\n});\n\n\nBelow is an example of how a playback that has three mid-roll ads interspersed within the content:\n\nThis page was last modified: 25 Oct 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nGetting started\nPlayback\nContent\nAds\nResuming playback\nVideo quality event\nExample event lifecycle\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nStorage\n/\nWarehouses\n/\nWarehouse Syncs\nWarehouse Syncs\n\nInstead of constantly streaming data to the warehouse destination, Segment loads data to the warehouse in bulk at regular intervals. Before the data loads, Segment inserts and updates events and objects, and automatically adjusts the schema to make sure the data in the warehouse is inline with the data in Segment.\n\nWhen Segment loads data into your warehouse, each sync goes through two steps:\n\nPing: Segment servers connect to your warehouse. For Redshift warehouses, Segment also runs a query to determine how many slices a cluster has. Common reasons a sync might fail at this step include a blocked VPN or IP, a warehouse that isn\u2019t set to be publicly accessible, or an issue with user permissions or credentials.\nLoad: Segment de-duplicates the transformed data and loads it into your warehouse. If you have queries set up in your warehouse, they run after the data is loaded into your warehouse.\n\nWarehouses sync with all data coming from your source. However, Business plan members can manage the data that is sent to their warehouses using Selective Sync.\n\nSync Frequency\n\nYour plan determines how frequently data is synced to your warehouse.\n\nPLAN\tFREQUENCY\nFree\tOnce a day (every 86,400 seconds)\nTeam\tTwice a day (every 43,200 seconds)\nBusiness*\tUp to 24 times a day. Generally, these syncs are fixed to the top of the hour (:00), but these times can vary.\n\n*If you\u2019re a Business plan member and would like to adjust your sync frequency, you can do so using the Selective Sync feature. To enable Selective Sync, please go to Warehouse > Settings > Sync Schedule.\n\nWhy can't I sync more than 24 times per day?\n\nWe do not set syncs to happen more than once per hour (24 times per day). The warehouse product is not designed for real-time data, so more frequent syncs would not necessarily be helpful.\n\nSync History\n\nYou can use the Sync History page to see the status and history of data updates in your warehouse. The Sync History page is available for every source connected to each warehouse. This page helps you answer questions like, \u201cHas the data from a specific source been updated recently?\u201d \u201cDid a sync completely fail, or only partially fail?\u201d and \u201cWhy wasn\u2019t this sync successful?\u201d\n\nThe Sync History includes the following information:\n\nSync Status: The possible statuses are:\nSuccess: The sync run completed without any notices and all rows synced, OR no rows synced because no data was found.\nPartial: The sync run completed with some notices and some rows synced.\nFailure: The sync run completed with some notices and no rows synced.\nStart Time: The time at which the sync began. This is shown in your local timezone.\nDuration: The length of time the sync took.\nSynced Rows: Number of rows successfully synced from the sync run.\nNotices: A list of errors or warnings found, which could indicate problems with the sync run. Click a notice message to show details about the result, and any errors or warnings for each collection included in the sync run.\n\nIf a sync run shows a partial success or failure, the next sync attempts to sync any data that was not successfully synced in the prior run.\n\nView the Sync History\n\nTo view the Sync History:\n\nGo to Connections > Destinations and choose the warehouse destination you want to view the sync history for.\nClick the source you want to view the sync history for.\n(Optional) Click on any of the rows in the Sync History table to see additional details related to that sync. You can view:\nThe Results of your sync which shows the number of rows synced for each collection.\nThe Sync Duration which shows the Preparation and Loading times of your sync.\nWarehouse Selective Sync\n\nWarehouse Selective Sync allows you to manage the data that you send to your warehouses. You can use this feature to stop syncing specific events (also known as collections) or properties that aren\u2019t relevant, and may slow down your warehouse syncs.\n\nThis feature is only available to Business Tier customers.\n\nYou must be a Workspace Owner to change Selective Sync settings.\n\nWith Selective Sync, you can customize which collections and properties from a source are sent to each warehouse. This helps you control the data that is sent to each warehouse, allowing you to sync different sets of data from the same source to different warehouses.\n\nNOTE: This feature only affects warehouses, and doesn\u2019t prevent data from going to any other destinations.\n\nWhen you disable a source, collection or property, Segment no longer syncs data from that source. Segment won\u2019t delete any historical data from your warehouse. When you re-enable a source, Segment syncs all events since the last sync. This doesn\u2019t apply when a collection or property is re-enabled. Only new data generated after re-enabling a collection or property will sync to your warehouse.\n\nFor each warehouse only the first 5,000 collections per source and 5,000 properties per collection are visible in the Selective Sync user interface. Learn more about the limits.\n\nDisabling the received_at column will cause your syncs to fail, as all tables use received_at as the sort key.\n\nWhen to use Selective Sync\n\nBy default, all sources and their collections and properties are sent, and no data is prevented from reaching warehouses.\n\nWhen you disable sources, collections, or properties using Selective Sync, Segment stops sending new data for these sources, collections, or properties to your warehouse. It doesn\u2019t delete any existing data in the warehouse.\n\nIf you choose to re-enable a source to begin syncing again, Segment loads all data that arrived since the last sync into the warehouse, but doesn\u2019t backfill data that was omitted while these were disabled. When a collection or property is re-enabled, data only syncs going forward. It will not be loaded from the last sync.\n\nEnable Selective Sync\n\nTo use Selective Sync:\n\nGo to Connections > Destinations and select the warehouse you want to enable Selective Sync for.\nClick the Settings tab and click Selective Sync in the left menu.\nSelect which sources, collections, and properties to sync. All that is not selected won\u2019t be synced to your warehouse.\nClick Save Changes.\nChange sync settings to a single warehouse from multiple sources\n\nTo change the sync settings to a single warehouse from multiple sources, follow the same steps as above.\n\nThis may be valuable if you\u2019re looking to make changes in bulk, such as when setting up a new warehouse.\n\nChange sync settings on a specific Warehouse to Source connection\n\nTo manage data from one specific source to an individual warehouse:\n\nGo to Connections > Destinations and select the warehouse you want to change the sync settings for.\nOn the Warehouse Overview page, click the Schema you want to change the sync settings for.\nOn the Settings tab of the Sync History page for that source, select the data you want synced to your warehouse, or deselect the data you don\u2019t want synced.\n\nThis may be valuable when you\u2019re making smaller changes, for example, disabling all properties from one unnecessary collection.\n\nAll changes made through Selective Sync only impact an individual warehouse. They don\u2019t impact multiple warehouses at once. To make changes to multiple warehouses, you need to enable/disable data for each individual warehouse.\n\nSelective Sync User Interface Limits\n\nRegardless of schema size, for each warehouse only the first 5,000 collections per source and 5,000 properties per collection can be managed using the Selective Sync user interface. After you hit any of these limits, all future data is still tracked and sent to your warehouse. New collections created after hitting this limit is not displayed in the Selective Sync table.\n\nYou will see a warning in the Selective Sync user interface when the warehouse schema has reached 80% of the limit for collections and/or properties. An error message will appear when you\u2019ve reached the limit.\n\nContact Support to edit Selective Sync settings for any collections and/or properties which exceed the limit.\n\nOnly Workspace Owners can change Selective Sync settings.\n\nThis page was last modified: 20 Mar 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSync Frequency\nSync History\nWarehouse Selective Sync\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nAudiences\n/\nProduct Based Audiences\nProduct Based Audiences\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nProduct Based Audiences lets you select a product, article, song, or other piece of content from your catalog, and then build an audience of the people that are most likely to engage with it. Segment optimized the personalized recommendations built by Product Based Audiences for user-based commerce, media, and content affinity use cases.\n\nYou can use Product Based Audiences to power the following common marketing campaigns:\n\nCross-selling: Identify an audience of users who recently purchased a laptop and send those customers an email with a discount on items in the \u201claptop accessories\u201d category.\nUpselling: Identify an audience of users who regularly interact with your free service and send them a promotion for your premium service.\nRanking: Identify an audience of users who frequently interact with one category of your website and send them a promotion that contains only items from this category.\nMoving excess inventory: Identify an audience of users who are in the top 5% of purchasers for a specific brand you sell and send them a coupon for the excess inventory you have of that brand.\nNext best action: Identify an audience of users who frequently read articles in your website\u2019s \u201cSports\u201d category and recommend those users your latest sports article.\nIncreasing average order value (AOV): Identify an audience of users who frequently interact with the \u201cFor Kids\u201d section of your website and send them a back to school promotion in August, with free shipping after a set price threshold.\nCreate a Product Based Audience\nSet up your Recommendation Catalog\n\nSegment utilizes your interaction events (order_completed, product_added, product_searched, song_played, article_saved) and the event metadata of those interaction events to power our CustomerAI Recommendations workflow.\n\nTo create your Recommendation Catalog:\n\nOpen your Engage space and navigate to Engage > Engage Settings > Recommendation catalog.\nOn the Recommendation catalog page, click Create catalog.\nSelect up to 10 product-related events you\u2019d like Segment to use as a basis for recommendations. Segment recommends selecting 3-7 different events that represent user interaction. For example: Product Added to Cart, Product Searched, or Product Viewed.\nSelect a product ID for each product-related event you previously selected.\nClick Next.\nMap event properties to the suggested model columns. Segment recommends mapping all properties of a product hierarchy to allow for increased granularity when building your Recommendation Audience.\n(Optional): To add an additional column to your model, click + Add column on the Map properties page.\nWhen you\u2019ve completed your mappings, click Save.\n\nSegment can take several hours to create your Recommendation Catalog.\n\nCreate your Product Based Audience\n\nOnce you\u2019ve created your Recommendation Catalog, you can build a Recommendation Audience. A Recommendation Audience lets you select a parameter and then build an audience of the people that are most likely to engage with that parameter.\n\nTo create a Product Based Audience:\n\nOpen your Engage space and click + New audience.\nSelect Recommendation Audience and click Next.\nSelect a property and value that you\u2019d like to build your audience around (for example, if the property was \u201cCompany\u201d, you could select a value of \u201cTwilio\u201d). For values that haven\u2019t updated yet, enter an exact value into the Enter value field. If you\u2019re missing a property, return to your Recommendation catalog and update your mapping to include the property.\nSet a maximum audience size by selecting one of the pre-populated options, or move the slider to create a custom audience. Segment recommends audiences that contain less than the top 20% of your audience because as the size of your audience increases, the propensity to purchase typically decreases. See Best practices for more information.\nWhen you\u2019ve filled out all fields, click Next to continue.\nOn the Select Destinations page, select any destinations you\u2019d like to sync your audience to and click Next.\nEnter a name for your destination, update any optional fields, and click Create Audience to create your audience.\n\nSegment can take up to a day to calculate your Product Based Audience.\n\nBest practices\nWhen mapping events to the model column during the setup process for your Recommendation catalog, select the event property that matches the model column. For example, if you are mapping to model column \u2018Brand\u2019, select the property that refers to \u2018Brand\u2019 for each of the selected interaction events.\nWhen you complete your audience creation, the status will display as \u201clive\u201d with 0 customers. This means the audience is still computing, and the model is determining which customers belong to it. Segment recommends waiting at least 24 hours for the audience to finish computing. Once the computation is complete, the audience size will update from 0 customers to reflect the finalized audience.\nAs the size of your audience increases, the propensity to purchase typically decreases. For example, an audience of a hundred thousand people that represents the top 5% of your customers might be more likely to purchase your product, but you might see a greater number of total sales if you expanded the audience to a million people that represent the top 50% of your customer base.\n\nThis page was last modified: 20 Nov 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nCreate a Product Based Audience\nBest practices\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nTraits\n/\nPredictions\n/\nPredictions Nutrition Facts Label\nPredictions Nutrition Facts Label\n\nTwilio\u2019s AI Nutrition Facts provide an overview of the AI feature you\u2019re using, so you can better understand how the AI is working with your data. Predictions\u2019s AI qualities are outlined in the following Nutrition Facts label. For more information, including the glossary regarding the AI Nutrition Facts label, refer to the AI Nutrition Facts page.\n\nAI Nutrition Facts\n\nCustomer AI Predictions\n\n\n\n\nDescription\n\nPredictions creates propensity models that predict if a customer will purchase, churn, or perform any other conversion event\n\n\n\n\nPrivacy Ladder Level\n2\n\n\n\n\nFeature is Optional\nYes\n\n\n\n\nModel Type\nPredictive\n\n\n\n\nBase Model\nGradient Boosted Trees\n\n\n\n\nTrust Ingredients\n\n\n\n\nBase Model Trained with Customer Data\nN/A\n\n\nCustomer Data is used to develop a model created specifically for each customer and is never reused for other customers\n\n\n\n\nCustomer Data Shared with Model Vendor\nNo\n\n\nCustomer Data is used to build the model, but it is built by Twilio Segment for the customer and not by a third party vendor\n\n\n\n\nTraining Data Anonymized \u00a0\nNo\n\n\n\n\nData Deletion\nYes\n\n\n\n\nHuman in the Loop\nN/A\n\n\n\n\nData Retention\n30 days\n\n\nCompliance \u00a0 \u00a0\nLogging & Auditing\nYes\n\nGuardrails\nN/A\n\n\n\nInput/Output Consistency\nN/A\n\n\n\n\nOther Resources\n\nLearn more at: https://twilio.com/en-us/customer-ai\n\nThis page was last modified: 26 Feb 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nPrivacy\n/\nPrivacy Portal\nPrivacy Portal\nFREE \u2713\nTEAM \u2713\nBUSINESS \u2713\nADD-ON X\n?\n\nWhen preparing for new privacy regulations (like HIPAA, the GDPR, or the CCPA), the best practice is to create a comprehensive data inventory which includes details about what personal information you collect, where you collect it from, where you store the data, and who has access to it. The Privacy Portal helps automate this process for the data you collect with Segment.\n\nWhen you use Segment as the single point of collection for your customer data, you can use the Privacy Portal to:\n\nAutomatically detect and classify your customer data to create a dynamic data inventory\nMonitor changes to your inventory with real-time alerts\nEnforce your company\u2019s data privacy policies with standard privacy controls\nStreamline regulatory compliance with tools for user deletion and suppression\n\nPrivacy Portal features are available to all Segment workspaces, however only workspace owners can access the Privacy Portal.\n\nPrivacy Detection\n\nThe Detection page in the Privacy Portal is where you can find out more about exactly how data is being detected and classified in your workspace. You can think of it as the brain behind the entire Privacy Portal, filled with the logic that detects and classifies the data in the first place.\n\nOn this page, you can also modify your Detection settings and tell Segment how you want us to match data, so that it is meets your unique business needs.\n\nDefault PII Matchers\n\nOut of the box, the Privacy Portal contains matchers for the most common PII fields. These matchers scan data coming from your Sources for PII based on both exact-matching (looking for an exact match, such as a field name) and fuzzy-matching (looking for both exact matches, and any values which are similar).\n\nIn this section of Segment\u2019s Privacy Portal, you can see the fields we match against by default. The display lists whether we match on the key (for example the label \u201cCCN\u201d) or value (for example, the payload 123-456-7890) in the Matches On column. You can also see how we classify these matchers by default in the Default Classification column.\n\nBelow is a full list of automatically detected restricted fields.\n\nMATCHER\tCLASSIFICATION\nsocial security number\tred\npassword\tred\nvisa\tred\nveteran\tred\ndisability\tred\ncredit card\tred\npassport\tred\ntoken\tred\nrace\tyellow\nbirthdate\tyellow\nphone\tyellow\naddress\tyellow\ngender\tyellow\nethnicity\tyellow\ncitizenship\tyellow\nname\tyellow\nstreet\tyellow\ncity\tyellow\nzipcode\tyellow\nemail\tyellow\ncertificate\tyellow\nlicense\tyellow\nidentification\tyellow\nserial\tyellow\nip\tyellow\nphoto\tyellow\nsalary\tyellow\nreligion\tyellow\nemail\tyellow\nmac\tyellow\nsex\tyellow\ngender\tyellow\nsexual orientation\tyellow\nmedication\tyellow\nallergy\tyellow\ncondition\tyellow\ndiagnosis\tyellow\nprocedure\tyellow\n\nWhen Segment detects data that meets the criteria for one of the default matchers (in the list above) in properties in your Web, Mobile, Server, or Cloud Event Sources, we display it in the Privacy Portal Inbox.\n\nDefault PII matchers are currently uneditable. If you want to change the behavior of a default matcher, you can create a custom PII matcher that replicates and overwrites the default matcher.\n\nCustom PII Matchers\n\nThis is where you can create your very own matchers to tell Segment what to scan for in your workspace. You can use this feature to detect properties that are unique to your company or region, or that aren\u2019t already handled by the default matchers above. You can have up to 100 custom matchers per workspace. Custom Matchers detect data in your Web, Mobile, Server, and Cloud Event Sources for fields under context, traits and properties objects, and the data they detect appears in the Inbox.\n\nFor example, if you have a restricted data point at your company called \u201cSIN\u201d (for \u201csocial insurance number\u201d) you can tell Segment\u2019s Privacy Portal how to treat that property whenever it is appears in data Segment processes.\n\nTo create a Custom Matcher:\n\nClick Add a Custom Matcher.\nEnter the Matcher Name (for example the property name, like \u201cSocial Insurance Number\u201d). Segment matches against the Matcher Name, as well as the other context you provide in the next steps.\nSet the default classification:\nRed for highly restricted\nYellow for moderately restricted\nChoose whether to match on a Key (for example, \u201cSIN\u201d, \u201cSocial Insurance Number\u201d, \u201cSocial Insurance No.\u201d, \u201cSocInsNo\u201d) or on a Value (for example. \u201c123-456-789\u201d, \u201c1234567\u201d)\nSelect how precise the match should be, by choosing Exact or Similar match.\nExact matches mean that a key matches the term exactly (for example \u201cphone number\u201d but never \u201cphne number\u201d)\nSimilar to matches a Key that is similar to a term within a fuzzy string distance (for example \u201cemail\u201d and \u201ce-mail\u201d). We built fuzzy matching using this public GitHub repository. If the score is > 0.7, then we say it\u2019s a match.\n\nUnless the field value pattern is unique, we recommend matching on the Key. For example, for Credit Card Number, it\u2019s better to detect on Keys that look like \u201cCCN\u201d or \u201cCredit Card Number\u201d instead of trying to detect any values that look like \u201c1234567890\u201d, because a 10-digit string can be found in all kinds of data even when it\u2019s not an CCN. For example, the key \u201cProduct_ID\u201d could contain a 10-digit string, even though Product_ID does not actually contain an SSN. A North American phone number (without country code) is also ten digits.\n\nPII Access\n\nUse Access Roles to control who has access to the PII identified by your matchers.\n\nAn intro to Regular expressions\n\nCustom Matchers use regular expressions (using the Golang Regex Package) to provide you great flexibility for your matching patterns.\n\nRegular expressions (or regexes) are a way to describe to a computer a pattern that we\u2019re looking for, using a series of special symbols. For example, if we want to match all Gmail emails, we\u2019d write the following regex:\n\n@example.com\n\n\nThis pattern matches jane@example.com, mike@example.com and so on. Regular expressions can also contain special symbols. One of them is \\d and it tells the computer to match a single-digit number. In that case, regular expression Number \\d would match Number 1, Number 2 and so on. You can match multiple digits by adding a plus sign (+) at the end: Year \\d+. This pattern would match Year 2019, Year 2020, etc.\n\nIf you need to match a specific word, you can type the word without any modifiers. For example, a regular expression of Apple would only match strings that contain Apple. You can also change it to match all Apple or Orange words, by separating the search terms with a \u201cpipe\u201d character, like so: Apple|Orange.\n\nRegular expressions have much more flexibility than we can describe here. Check out the following resources to learn more about regular expressions so you can build new custom matchers:\n\nRegExr - an online tool to experiment with regular expressions and test them\nRegexOne - a tutorial which takes you from regular expression basics to advanced topics\nRegexp Cheatsheet - a handy cheatsheet to have nearby when you\u2019re writing regular expressions\nUsing Synonyms\n\nSegment\u2019s exact matching and fuzzy matching do not detect all variations in the received keys and for those scenarios, you can use synonyms. For example, for the value credit card number, you can add credit card no, debit card number, debit card no, or similar variations in the synonyms section to classify those fields.\n\nPrivacy Inbox\n\nThe Inbox helps you keep track of new restricted data types as they are captured, quickly classify them, and build a data Inventory.\n\nSegment detects these fields by scanning data from your Web, Mobile, Server, and Cloud Event Sources to detect PII based on the default PII matchers. New properties sent into Segment appear in the Inbox in realtime.\n\nWhen you view the Inbox, it displays every property that was sent into Segment from Web, Mobile, Server, and Cloud Event Sources for the past 7 days. (Object Cloud Sources and Reverse ETL Sources do not appear in the Inbox at this time.)\n\nYou can click a row in the Inbox to learn more about a field and where it was collected. The expanded view shows:\n\nwhich events contain the field\nwhich sources are sending the field\nwhich matcher (and what type of matcher) detected the field\nan example code snippet containing a payload that the field appears in\n\nTo streamline the classification process, Segment pre-classifies the data in the Privacy Portal Inbox as Red (likely highly restricted data), Yellow (likely moderately restricted data), and Green (likely least restricted data). These colors indicate how restricted the data is for your business. You can also send and block data from flowing based on its color classification and how restricted it is.\n\nSegment makes recommendations about how a field should be classified using built-in PII matcher detection, however, you can always update the classification in the Inbox based on your company\u2019s requirements.\n\nChange a recommended classification\n\nYou can update the classifications to suit your needs by clicking on the color dropdown menu to change. For example, you might manually change a field that does not contain personal information in your implementation from a \u201cYellow\u201d classification to \u201cGreen.\u201d\n\nWhen you\u2019re satisfied that the fields have been classified appropriately, you can click Add to Inventory to officially apply the classification to the field. This moves the field into your Data Inventory, which is a central repository of all of the properties you classified as Red, Yellow, and Green. Any time you send this field from a Web, Mobile, Server, or Cloud Event Source \u2014 whether from another Source or event type \u2014 the Privacy Portal automatically classifies it and adds it to the Inventory.\n\nUnderstanding Classification types:\n\nRed Classification: Fields that are classified as \u2018Red\u2019 are masked for users that do not have PII Access enabled. These fields are also blocked if you have set Standard Controls under Privacy > Settings section.\n\nKeep in mind that if you have set Standard Controls to block fields from any of your sources, any new classifications you create in the Inbox will start to take affect immediately. For example, if you have a Privacy Control set up to block Red data from your Android source, any new fields you classify in the Inbox as Red will be blocked from entering Segment from your Android source.\n\nYellow Classification: Fields that are classified as \u2018Yellow\u2019 are masked for users that do not have PII Access enabled.\n\nGreen Classification: Classifying a field as \u2018Green\u2019 does not have any impact on the behavior of masking of fields within the Segment App, it is only available for the housekeeping purposes.\n\nOnce a field has been classified as \u201cYellow\u201d or \u201cRed\u201d, marking it \u201cGreen\u201d will not make it visible for users that don\u2019t have PII access.\n\nPrivacy Inventory\n\nThe Inventory is a central repository of all of the properties you classified as Red, Yellow, and Green. Where the Inbox shows new, unclassified data with Segment\u2019s recommended classifications, the Inventory only contains data that you explicitly applied Classifications to.\n\nThe Inventory is intended to be a Single Source of Truth so you can answer common regulatory questions about the data you\u2019re sending through Segment, for example:\n\nWhat data am I sending into Segment, and how frequently?\nHow restricted is the data I\u2019m sending through Segment?\nWhere is the data coming from, on a property-by-property level?\nWhere am I sending this data?\nWho within my organization has access to each property within Segment?\n\nOnce you\u2019ve classified the fields as Red, Yellow, and Green in the Inbox, the classified fields appear in the Inventory. You can use the filter at the top left to filter down to specific categories of data (for example, Red data, data from a production environment, data from specific sources).\n\nClick into a field (for example, ip) in the Inventory to open the Inventory details. The details sheet displays how many times a specific field has been sent from each Source it comes from. You can click the Events tab to see which events contained the event, along with the Sources which sent the event. The data in the side sheet updates in realtime, and includes a limited historical view.\n\nYou can click Connected Destinations to see which Destinations are connected to the Source that contains the field. The Access tab displays a list of who within your organization has access to this field.\n\nFinally, workspace owners can use the Download CSV button to export a CSV of their data Inventory to share with their Data Protection Officer (DPO), Chief Information Security Officer (CISO), legal teams, and more! Note that the CSV download button includes all data from your Inventory, and ignores any filters you applied in the UI.\n\nThis page was last modified: 25 Jun 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nPrivacy Detection\nPrivacy Inbox\nPrivacy Inventory\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nPrivacy Tools Overview\nPrivacy Tools Overview\n\nSegment includes a suite of Privacy tools to help your organization comply with regulations like HIPAA, the GDPR, and the CCPA.\n\nProactively identify PII with the Privacy Portal\nDetect and classify customer data\n\nThe Privacy Portal helps automate preparing for new privacy regulations.\n\nControl what data you collect\n\nTake control over whether specific data is allowed to enter Segment\n\nPrepare for GDPR & CCPA\nUnderstand and Comply with regulations\n\nSegment is committed to making it easier for you to comply with the GDPR.\n\nDelete and suppress data about end-users\n\nEasily remove your company's end-users from Segment and supported connections.\n\nThis page was last modified: 14 Nov 2022\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nProactively identify PII with the Privacy Portal\nPrepare for GDPR & CCPA\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nEngage Default Limits\nEngage Default Limits\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nTo provide consistent performance and reliability at scale, Segment enforces default use and rate limits within Engage. Most customers do not exceed these limits.\n\nTo learn more about custom limits and upgrades, contact your dedicated Customer Success Manager or friends@segment.com.\n\nBeginning August 18, 2023, Segment has updated product limits that apply to new Engage and Unify users.\n\nDefault limits\nNAME\tLIMIT\tDETAILS\nInbound Data Throughput\t1000 events per second\tTotal event stream from sources connected to Engage, including historical data replays. Segment may slow request processing once this limit is reached.\nOutbound Downstream Destination Rate Limits\tReduced retries when failures exceed 1000 events per second\tOutbound Destination requests may fail for reasons outside of Segment\u2019s control. For example, most Destinations enforce their own rate limits. As a result, Segment may deliver data faster than the Destination can accept.\n\nWhen Destination requests fail, Segment tries to deliver the data again. However, if more than 1000 requests per second fail or if the failure rate exceeds 50% for over 72 hours, Segment may reduce additional delivery attempts until the failure condition resolves.\nAudiences and Computed Traits\nNAME\tLIMIT\tDETAILS\nCompute Concurrency\t5 new concurrent audiences or computed traits\tSegment computes five new audiences or computed traits at a time. Once the limit is reached, Segment queues additional computations until one of the five finishes computing.\nEdit Concurrency\t2 concurrent audiences or computed traits\tYou can edit two concurrent audiences or computed traits at a time. Once the limit is reached, Segment queues and locks additional computations until one of the two finishes computing.\nBatch Compute Concurrency Limit\t10 (default) per space\tThe number of batch computations that can run concurrently per space. When this limit is reached, Segment delays subsequent computations until current computations finish.\nCompute Throughput\t10000 computations per second\tComputations include any Track or Identify call that triggers an audience or computed trait re-computation. Once the limit is reached, Segment may slow audience processing.\nEvents Lookback History\t3 years\tThe period of time for which Segment stores audience and computed traits computation events.\nReal-time to batch destination sync frequency\t12-15 hours\tThe frequency with which Segment syncs real-time audiences to batch destinations.\nEvent History\t1970-01-01\tEvents with a timestamp less than 1970-01-01 aren\u2019t always ingested, which could impact audience backfills with event timestamps prior to this date.\nEngage Data Ingest\t1x the data ingested into Connections\tThe amount of data transferred into the Compute Engine.\nAudience Frequency Update\t1 per 8 hours\tAudiences that require time windows (batch audiences), funnels, dynamic properties, or account-level membership are processed on chronological schedules. The default schedule is once every eight hours; however, this can be delayed if the \u201cBatch Compute Concurrency Limit\u201d is reached. Unless otherwise agreed upon, the audiences will compute at the limit set forth.\nEvent Properties (Computed Traits)\t10,000\tFor Computed Traits that exceed this limit, Segment will not persist any new Event Properties and will drop new trait keys and corresponding values.\nSQL Traits\nNAME\tLIMIT\tDETAILS\nSQL Traits\t25\tThe number of SQL traits you can sync to your Space.\nSQL Traits - Sync Frequency\tcustomizable, up to hourly\tThe frequency with which Segment runs your SQL traits. Contact your account team to customize your schedule.\nSQL Traits - Rows\t25 million\tThe number of rows each SQL trait can return.\nSQL Traits - Columns\t25\tThe number of columns each SQL trait can return.\nJourneys\nITEM\tLIMIT DESCRIPTION\tDETAILS\nSteps\t100\tThe maximum number of steps per Journey.\nStep Name\tMaximum length of 170 characters\tOnce the limit is reached, you cannot add additional characters to the name.\nKey\tMaximum length of 255 characters\tOnce the limit is reached, you cannot add additional characters to the key.\nJourney Name\tMaximum length of 73 characters\tOnce the limit is reached, you cannot add additional characters to the name.\nCompute credits\tHalf a credit for each step (up to 250 compute credits)\tEach step in a published Journey consumes half of one compute credit.\nChannels\nITEM\tLIMIT DESCRIPTION\tDETAILS\nChannels\tDoes not support Regional Segment\tWorkspaces with Channels functionality enabled must be deployed in the default region (Oregon, US).\n\nThis page was last modified: 31 Oct 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nDefault limits\nAudiences and Computed Traits\nSQL Traits\nJourneys\nChannels\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nProduct Limits\nProduct Limits\n\nThese limits were updated on January 25, 2024.\n\nEvent properties ingestion limit\n\nEvents ingested by Segment have a limit of 10,000 properties per individual event received. For example, two Track events named \u201cPage Viewed\u201d and \u201cSignup completed\u201d each have their own limit. Segment will not persist properties beyond this limit, and will drop any corresponding values.\n\nInbound data ingestion API rate limit\n\nIf any sources send more than 1,000 events per second in a workspace without prior arrangement, Segment reserves the right to queue any additional events and process those at a rate that doesn\u2019t exceed this limit. To request a higher limit, contact Segment.\n\nEngage rate limit\n\nEngage has a limit of 1,000 events per second for inbound data. Visit the Engage Default Limits documentation to learn more.\n\nOutbound downstream destination rate limits\n\nMost destinations have their own rate limits that Segment cannot control. In some instances, Segment is able to ingest and attempt to deliver data faster than the downstream destination is able to accept data. Outbound requests to a destination may also fail for other reasons outside of Segment\u2019s control. When requests to downstream destinations fail, Segment makes additional attempts to deliver the data (retries). However, when more than 1,000 requests per second to a downstream destination fail or when the failure rate for a downstream destination exceeds 50% for more than 72 hours, Segment reserves the right to reduce the number of retries until the condition is resolved.\n\nThis page was last modified: 25 Jan 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nEvent properties ingestion limit\nInbound data ingestion API rate limit\nOutbound downstream destination rate limits\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nAmazon Web Services PrivateLink\nAmazon Web Services PrivateLink\n\nAmazon Web Services\u2019 PrivateLink is an AWS service that provides private connectivity between VPCs without exposing traffic to the public Internet. Keeping traffic in the Amazon network reduces the data security risk associated with exposing your Warehouse traffic to the Internet.\n\nSegment\u2019s PrivateLink integration is currently in private beta and is governed by Segment\u2019s First Access and Beta Preview Terms. You might incur additional networking costs while using AWS PrivateLink.\n\nYou can configure AWS PrivateLink for Databricks, RDS Postgres, Redshift, and Snowflake. Only warehouses located in regions us-east-1, us-west-2, or eu-west-1 are eligible.\n\nUsage limits for each customer during the AWS PrivateLink Private Beta include the following:\n\nUp to 2 AWS PrivateLink VPC endpoints.\nA monthly data transfer limit of 300GB total for all PrivateLink VPC endpoints connected to Segment.\nDatabricks\n\nThe following Databricks integrations support PrivateLink:\n\nDatabricks storage destination\nDatabricks Reverse ETL source\nDatabricks Profiles Sync\nDatabricks Data Graph\n\nSegment recommends reviewing the Databricks documentation before attempting AWS PrivateLink setup\n\nThe setup required to configure the Databricks PrivateLink integration requires front-end and back-end PrivateLink configuration. Review the Databricks documentation on AWS PrivateLink to ensure you have everything required to set up this configuration before continuing.\n\nPrerequisites\n\nBefore you can implement AWS PrivateLink for Databricks, complete the following prerequisites in your Databricks workspace:\n\nDatabricks account must be on the Enterprise pricing tier and use the E2 version of the platform.\nDatabricks workspace must use a Customer-managed VPC and Secure cluster connectivity.\nConfigure your VPC with DNS hostnames and DNS resolution\nConfigure a security group with bidirectional access to 0.0.0.0/0 and ports 443, 3306, 6666, 2443, and 8443-8451.\nImplement PrivateLink for Databricks\n\nTo implement Segment\u2019s PrivateLink integration for Databricks:\n\nFollow the instructions in Databricks\u2019 Enable private connectivity using AWS PrivateLink documentation. You must create a back-end connection to integrate with Segment\u2019s front-end connection.\nAfter you\u2019ve configured a back-end connection for Databricks, let your Customer Success Manager (CSM) know that you\u2019re interested in PrivateLink.\nSegment\u2019s engineering team creates a custom VPC endpoint on your behalf. Segment then provides you with the VPC endpoint\u2019s ID.\nRegister the VPC endpoint in your Databricks account and create or update your Private Access Setting to include the VPC endpoint. For more information, see Databricks\u2019 Register PrivateLink objects documentation.\nConfigure your Databricks workspace to use the Private Access Setting object from the previous step.\nReach back out to your CSM and provide them with your Databricks Workspace URL. Segment configures their internal DNS to reroute Segment traffic for your Databricks workspace to your VPC endpoint.\nYour CSM notifies you that Segment\u2019s PrivateLink integration is complete. If you have any existing Segment Databricks integrations that use your Databricks workspace URL, they now automatically use PrivateLink. Any new Databricks integrations created in the Segment app using your Databricks workspace URL will also automatically use PrivateLink.\nRDS Postgres\n\nThe following RDS Postgres integrations support PrivateLink:\n\nRDS Postgres storage destination\nRDS Postgres Reverse ETL source\nRDS Postgres Profiles Sync\nPrerequisites\n\nBefore you can implement AWS PrivateLink for RDS Postgres, complete the following prerequisites:\n\nSet up a Network Load Balancer (NLB) to route traffic to your Postgres database: Segment recommends creating a NLB that has target group IP address synchronization, using a solution like AWS Lambda. If any updates are made to the Availability Zones (AZs) enabled for your NLB, please let your CSM know so that Segment can update the AZs of your VPC endpoint.\nConfigure your NLB with one of the following settings:\nDisable the Enforce inbound rules on PrivateLink traffic setting\nIf you must enforce inbound rules on PrivateLink traffic, add an inbound rule that allows traffic belonging to Segment\u2019s PrivateLink/Edge CIDR: 10.0.0.0/8\nImplement PrivateLink for RDS Postgres\n\nTo implement Segment\u2019s PrivateLink integration for RDS Postgres:\n\nCreate a Network Load Balancer VPC endpoint service using the instructions in the Create a service powered by AWS PrivateLink documentation.\nLet your Customer Success Manager (CSM) know that you\u2019re interested in PrivateLink. They will share information with you about Segment\u2019s AWS principal.\nAdd the Segment AWS principal as an \u201cAllowed Principal\u201d to consume the Network Load Balancer VPC endpoint service you created in step 1.\nReach out to your CSM and provide them with the Service Name for the service that you created above. Segment\u2019s engineering team provisions a VPC endpoint for the service in the Segment Edge VPC.\nSegment provides you with the VPC endpoint\u2019s private DNS name. Use the DNS name as the Host setting to update or create new Postgres integrations in the Segment app.\nRedshift\n\nThe following Redshift integrations support PrivateLink:\n\nRedshift storage destination\nRedshift Reverse ETL source\nRedshift Profiles Sync\nRedshift Data Graph\nPrerequisites\n\nBefore you can implement AWS PrivateLink for Redshift, complete the following prerequisites:\n\nYou\u2019re using the RA3 node type: To access Segment\u2019s PrivateLink integration, use an RA3 instance.\nYou\u2019ve enabled cluster relocation: Cluster relocation migrates your cluster behind a proxy and keeps the cluster endpoint unchanged, even if your cluster needs to be migrated to a new Availability Zone. A consistent cluster endpoint makes it possible for Segment\u2019s Edge account and VPC to remain connected to your cluster. To enable cluster relocation, follow the instructions in the AWS Relocating your cluster documentation.\nYour cluster is using a port within the ranges 5431-5455 or 8191-8215: Clusters with cluster relocation enabled might encounter an error if updated to include a port outside of this range.\nImplement PrivateLink for Redshift\n\nTo implement Segment\u2019s PrivateLink integration for Redshift:\n\nLet your Customer Success Manager (CSM) know that you\u2019re interested in PrivateLink. They will share information with you about Segment\u2019s Edge account and VPC.\nAfter you receive the Edge account ID and VPC ID, grant cluster access to Segment\u2019s Edge account and VPC.\nReach back out to your CSM and provide them with the Cluster Identifier for your cluster and your AWS account ID.\nSegment\u2019s engineering team creates a Redshift managed VPC endpoint within the Segment Redshift subnet on your behalf, which creates a PrivateLink Endpoint URL. Segment then provides you with the internal PrivateLink Endpoint URL.\nUse the provided PrivateLink Endpoint URL as the Hostname setting to update or create new Redshift integrations in the Segment app.\nSnowflake\n\nThe following Snowflake integrations support PrivateLink:\n\nSnowflake storage destination\nSnowflake Reverse ETL source\nSnowflake Profiles Sync\nSnowflake Data Graph\nPrerequisites\n\nBefore you can implement AWS PrivateLink for Snowflake, complete the following prerequisites:\n\nYour Snowflake account is on the Business Critical Edition or higher.\nYour Snowflake account is hosted on the AWS cloud platform.\nImplement PrivateLink for Snowflake\n\nTo implement Segment\u2019s PrivateLink integration for Snowflake:\n\nFollow Snowflake\u2019s PrivateLink documentation to enable AWS PrivateLink for your Snowflake account.\nLet your Customer Success Manager (CSM) know that you\u2019re interested in PrivateLink. They will provide you with Segment\u2019s AWS Edge account ID.\nCreate a Snowflake Support Case to authorize PrivateLink connections from Segment\u2019s AWS account ID as a third party vendor to your Snowflake account.\nAfter Snowflake support authorizes Segment, call the SYSTEM$GET_PRIVATELINK_CONFIG function while using the Snowflake ACCOUNTADMIN role. Reach back out to your Segment CSM and provide them with the privatelink-vpce-id and privatelink-account-url values from the function output. Note down for yourself the privatelink-account-name value.\nSegment\u2019s engineering team creates a custom VPC endpoint on your behalf. Segment also creates a CNAME record to reroute Segment traffic to use your VPC endpoint. This ensures that Segment connections to your privatelink-account-name are made over PrivateLink.\nYour CSM notifies you that the setup on Segment\u2019s side is complete. Use your privatelink-account-name as the Account setting to update or create new Snowflake integrations in the Segment app.\n\nThis page was last modified: 30 Oct 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nDatabricks\nRDS Postgres\nRedshift\nSnowflake\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nHow To Guides\n/\nCollecting Data on the Client or Server\nCollecting Data on the Client or Server\n\nOne of the most common questions Segment receives is: \u201cShould I use one of your client-side libraries or one of your server-side libraries?\u201d\n\nThis is such an important topic that you\u2019ll find an in-depth article in Segment\u2019s Analytics Academy: \u00a0When to Track on the Client vs Server. It\u2019s worth a read. Below, you can also read some quick logic around why you may want to choose either option.\n\nClient-side\nNot stored in your database\n\nGood things to send from the client-side are things that you wouldn\u2019t usually store in your database. Things like page views, button clicks, page scroll length, mouse movements, social shares, and likes.\n\nEasier to send client-side\n\nThings like UTM tags, operating system, device type, or cookied data like returning visitors are all easiest to track client-side. Of course, some things like mouse movements are only available on the client-side so you should definitely track that there.\n\nEvents needed for client-side only destinations\n\nSome destinations can only accept data when the event is sent from the browser. They require events on the client since they rely on cookies and most of those tools do not have an API that Segment can send server-side data to. More on this in Segment\u2019s Analytics.js docs.\n\nServer-side\nPayment events\n\nCharging customers often happens when they aren\u2019t online, and accuracy for payments is so important. Server-side tracking tends to be more accurate than user devices since it\u2019s a more controlled environment.\n\nAccuracy\n\nIn general client-side data is fine for watching general trending, but it\u2019s never going to be perfect. Especially if your customers are likely to use things like adblock or old/non-standard browsers.\n\nFor example, if you\u2019re sending triggered emails based on events, it\u2019s probably a good idea to make sure your user profiles are sent through Segment\u2019s servers so no one gets left out or mis-emailed.\n\nCalculated from your database\n\nAnother good type of data to send server-side are things that need to be calculated from a database query. This might be something like \u201cFriend Count\u201d if your site or app is a social network.\n\nSensitive information\n\nSensitive information is also best kept out of browsers. Any data you don\u2019t want exposed to users should be sent server-side.\n\nSelecting Destinations\n\nEach Segment library allows an\u00a0integrations\u00a0object either as a top level object or nested in options object.\u00a0\n\nThis flag may be especially useful in Legacy source types, where an event might be triggered on both the client and server for various reasons. The following will cause the payload to be sent to all enabled tools except Facebook Pixel:\n\n    analytics.identify('user_123', {\n      email: 'jane.kim@example.com',\n      name: 'Jane Kim'\n      }, {\n        integrations: {\n          'Facebook Pixel': false\n        }\n      });\n\n\nThis page was last modified: 07 Mar 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nAudiences\n/\nSend Audience Data to Destinations\nSend Audience Data to Destinations\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nWith the help of sources and destinations in Segment\u2019s catalog, you can create and send audiences and computed traits to third-party services.\n\nSegment\u2019s Connections pipeline first collects and sends events from your source to your destination. Built on top of Connections, Engage then uses the same source events to let you create audiences and computed traits within Segment. You can then send the audience or computed trait you\u2019ve built to your destination(s).\n\nBecause Engage only sends audiences and computed traits to destinations, it doesn\u2019t replace a standard event pipeline. Connect a source directly to a destination if you want the destination to receive all events that Segment gathers.\n\nConnect your audience to a destination\n\nOnce you\u2019ve previewed your audience, you can choose to connect it to a destination or keep the audience in Segment and export it as a CSV file download.\n\nWhen you create an audience, Segment starts syncing your audience to the destinations you selected. Audiences are either sent to destinations as a boolean user-property or a user-list, depending on what the destination supports. Read more about supported destinations in the Engage documentation.\n\nFor account-level audiences, you can send either a Group call and/or Identify call. Group calls send one event per account, whereas Identify calls send an Identify call for each user in the account. This means that even if a user hasn\u2019t performed an event, Segment will still set the account-level computed trait on that user.\n\nBecause most marketing tools are still based at the user level, you\u2019ll usually want to map this account-level trait onto each user within an account. See Account-level Audiences for more information.\n\nWhen you connect a new destination to an existing audience, Engage will backfill historical data for that audience to the new destination.\n\nFollow these steps to connect an audience to a destination:\n\nNavigate to Engage > Audiences, then select the audience you want to connect.\nFrom the audience\u2019s overview page, click + Add destination.\nSelect the destination you want to connect to, then click Add destination.\nSegment then begins its initial sync to the destination.\nView connected destinations\n\nYou can view a list of an audience\u2019s connected destinations in the destination list table of the audience overview tab.\n\nThe Destinations table contains information about the destination\u2019s matching mappings, status, and sync status.\n\nMatching mappings\n\nActions destinations have mappings that can receive granular data from your audience. The Matching mappings column shows the number of mappings that match the data coming from the audience, as well as the number of enabled and disabled mappings. See Working with mappings for more information.\n\nThe Matching mappings column will show Not applicable for classic destinations.\n\nStatus columns\n\nThe Destination status column shows Connected, Disconnected, or Disabled:\n\nConnected indicates that the destination is enabled and receiving data from the audience.\nDisconnected means that either the destination is disabled or the audience isn\u2019t sending it data.\nDisabled means that the destination is disabled and the audience isn\u2019t sending it data.\n\nThe Sync status column shows the current compute status between the audience and connected destination.\n\nWorking with mappings\n\nYou can add and access mappings within your audience\u2019s connected destination by following these steps:\n\nNavigate to Engage > Audiences.\nFrom the Destinations list, click the destination you want to work with, or click +Add mapping.\nIn the destination\u2019s side panel, click Matching mappings.\nIn the Add Mapping popup, select the mapping that you want to add.\nSegment then opens the destination\u2019s mappings tab. Add the mapping(s) you want, then click Save.\n\nSegment then returns you to the audience\u2019s destination side panel, which shows your new mapping(s).\n\nUse Segment\u2019s Duplicate mappings feature to create an exact copy of an existing mapping. The copied mapping has the same configurations and enrichments as your original mapping.\n\nThis page was last modified: 11 Sep 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nConnect your audience to a destination\nView connected destinations\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nThe page you were looking for doesn't exist.\n\nYou may have mistyped the address or the page may have moved. Double-check the URL and try again or search the term.\n\n404",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSpec\n/\nEcommerce Tracking Plans\nEcommerce Tracking Plans\n\nWhen tracking your data, it\u2019s important to set yourself up for success. E-commerce and retail companies want to use their data to understand why some customers fall out of their funnels or why customers become repeat buyers. They want to understand the important lifecycle events that lead up to the sale of a physical item, so they can, for example, test whether personalized shopping experiences yield higher conversions, or build a multi-channel cart abandonment campaign. But first, they need to make sure those lifecycle events are being captured in their datasets.\n\nThat\u2019s where a tracking plan comes in. A tracking plan is a living document that can be used across your organization to record what events and properties to track, where you\u2019ll be tracking them in your code base, and why you\u2019re tracking them.\n\nLearn more about the value and function of a tracking plan.\n\nIn this guide, you\u2019ll learn the core events most relevant to e-commerce companies that can get you started immediately in understanding your customers and driving sales.\n\nTalk to a product specialist today about building a clean, high-quality data spec so you can focus on brand engagement and sales growth.\n\nIdentifying your customers\n\nBefore diving into specific event tracking, you\u2019ll want to make sure you track who your users are with the the .identify() call. You should call .identify() whenever a visitor provides you with a piece of information, at which point they become \u201cknown users.\u201d The .identify() call creates or updates a record of your customer with a set of traits in your tools and warehouse. But how do you choose which traits about your user to include?\n\nTraits are pieces of data that you track about a specific user. Read the guide about selecting traits to learn more.\n\nHere are the most common user traits e-commerce companies include in their tracking:\n\nfirst name\nlast name\nemail\n\nHere are a few examples of other helpful user traits:\n\nlocale\ncurrency\nphone\nlifetime_value\norder_count\n\nHere is a sample .identify() call for Segment:\n\nIn analytics.js:\n\n\u00a0 \u00a0 analytics.identify({\n\u00a0 \u00a0 \u00a0 first_name: 'Andy',\n\u00a0 \u00a0 \u00a0 last_name: 'Jiang',\n\u00a0 \u00a0 \u00a0 email: 'andy@segment.com'\n\u00a0 \u00a0 });\n\n\nIn analytics-ios:\n\n\u00a0 \u00a0 [[SEGAnalytics sharedAnalytics] identify:nil\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 traits:@{ @\"email\": @\"andy@segment.com\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 @\"first_name\": @\"Andy\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 @\"last_name\": @\"Jiang\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 @\"experiment_viewed\": @\"Coupon\" }];\n\n\nIn analytics-android:\n\n\u00a0 \u00a0 Analytics.with(context).identify(new Traits().putValue(\"first_name\", \"andy\").putValue(\"last_name\", \"jiang\").putValue(\"experiment_viewed\", \"Coupon\").putValue(\"email\", \"andy@segment.com\"), null);\n\n\nThe main benefits of using the traits listed above are:\n\nYou can use the traits to personalize content in your email or push notification tools by inserting their information. For example:\n\nYou can create cohorts based on the traits in any of your tools. For example, you could use order_count to determine how many of your users are first or repeat, or experiments to determine how many have seen this particular experiment variant\n\nGiven the power you have in your downstream tools to create cohorts based on these dimensions, you may be tempted to throw more contextual data into the traits , such as UTM params, IP addresses, and userAgents. But if you\u2019re using Segment\u2019s client-side analytics.js library, then all of these contextual pieces of data are automatically collected.\n\nSelecting key e-commerce and marketing events\n\nNow that you are tracking who your users are, you can work on what they\u2019re doing on your website or in your app with a .track() call. Tracking customer events lets you learn about your customers, measure the impact of your marketing efforts and product decisions, and proactively engage your customers in a meaningful way that drives sales.\n\nRead the event tracking guide to learn more about tracking the right events.\n\nE-commerce businesses, unlike SaaS or consumer apps that optimize for product engagement, focus on directing users down their funnels to a conversion goal, like purchasing a product. To best understand why customers convert, it\u2019s important to track and measure all key funnel events.\n\nProduct Viewed\nProduct Added\nCheckout Started\nOrder Completed\n\nThere are also auxiliary actions to track to measure your customer\u2019s engagement with your site. These actions are good to track so you can better understand their intent on your website.\n\nProducts Searched\nProduct List Viewed\nProduct List Filtered\n\nLastly, Segment has a set of semantic campaign events that are automatically collected so you can understand the conversions in these specific channels:\n\nEmail Opened\nEmail Link Clicked\nPush Notification Received\nPush Notification Tapped\nDeep Link Clicked\nDeep Link Opened\n\nCheck out the full list of e-commerce events you should track.\n\nYou may notice a pattern in the event names. Segment selected the \u201cObject Action\u201d naming convention to ensure that all event data is clean and easily analyzable, while choosing \u201csnake_case\u201d for the traits and properties. It doesn\u2019t matter what you choose, so long as it\u2019s consistent. Without a uniform and enforced naming framework to guide developers that add tracking code later, your data could get marred with conflicting naming structures. Learn more about the importance of naming conventions.\n\nSegment recommends tracking core checkout activity on the server-side. Learn more about tracking on the client vs. server.\n\nSelecting your properties\n\nProperties are similar to traits, but they\u2019re associated with specific actions, rather than with an individual user. Each .track() call can accept an optional dictionary of properties, which can contain any key-value pair you want. These properties act as dimensions that allow your end tool to group, filter, and analyze the events. They give you additional detail on broader events.\n\nLearn more about properties and what they mean for your downstream analysis.\n\nFor e-commerce, since most events are customers choosing, browsing, and checking out products, all of the traits must contain key information about the products themselves:\n\nproduct_id\nsku\nprice\nquantity\ncurrency\n\nThese traits must be included because many tools rely on them for analysis. If there was one of them missing, the call would be ignored.\n\nUsing a specific tool and want to see how Segment handles sending calls to it? Check out the documentation.\n\nHere is an example .track() call:\n\nIn analytics-node:\n\n\u00a0 \u00a0 analytics.track({\n\u00a0 \u00a0 \u00a0 userId: '019mr8mf4r',\n\u00a0 \u00a0 \u00a0 event: 'Order Completed',\n\u00a0 \u00a0 \u00a0 properties: {\n\u00a0 \u00a0 \u00a0 \u00a0 order_id: '50314b8e9bcf000000000000',\n\u00a0 \u00a0 \u00a0 \u00a0 total: 20,\n\u00a0 \u00a0 \u00a0 \u00a0 currency: 'USD',\n\u00a0 \u00a0 \u00a0 \u00a0 products: [\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 product_id: '507f1f77bcf86cd799439011',\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sku: '201',\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: 'Folsom',\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 price: 10,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 quantity: 1\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 product_id: '505bd76785ebb509fc183733',\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sku: '204',\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: 'Brennan',\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 price: 10,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 quantity: 1\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\n\u00a0 \u00a0 \u00a0 \u00a0 ]\n\u00a0 \u00a0 \u00a0 }\n\u00a0 \u00a0 });\n\n\nIn analytics-ios\n\n\u00a0 \u00a0 [[SEGAnalytics sharedAnalytics] track:@\"Order Completed\"\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 properties:@{ @\"order_id\": @\"50314b8e9bcf000000000000\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0@\"total\": @\"20\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 @\"currency\": @\"USD\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 @\"products\": @\"Coupon\" }];\n\n\nIn analytics-android:\n\n\u00a0 \u00a0 Analytics.with(context).track(\"Order Completed\", new Properties().putValue(\"order_id\", \"50314b8e9bcf000000000000\").putValue(\"total\", 20).putValue(\"currency\", \"USD\").putValue(\"products\", \"Coupon\"));\n\n\nIt\u2019s important that these events contain particular properties, such as sku , otherwise the downstream tools won\u2019t be able to create out-of-the-box revenue and sales reports. Learn more about semantic properties in the e-commerce spec.\n\nUsing data to understand why your customers don\u2019t convert\n\nThe most successful e-commerce businesses not only efficiently move their customers through the funnel towards conversion, but also have the infrastructure to collect and use customer data. Having a tracking plan focused on key funnel events can help e-commerce businesses get a sense of the health of their funnel.\n\nAside from funnel health, having these key pieces of customer data gives companies the ability to tailor and personalize each interaction, as well as build marketing campaigns around actions taken or omitted.\n\nWithout taking this critical step of mapping out key customer events, businesses often spend too much time revisiting their data model or analyzing impartial data sets. Instead they could spend that time understanding and addressing customers\u2019 needs.\n\nTalk to a product specialist today about building a clean, high-quality data spec so you can focus on brand engagement and sales growth.\n\nThis page was last modified: 01 Jul 2022\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nIdentifying your customers\nSelecting key e-commerce and marketing events\nSelecting your properties\nUsing data to understand why your customers don\u2019t convert\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nStorage\n/\nData Lakes\n/\nData Lakes Sync History and Health\nData Lakes Sync History and Health\nFREE X\nTEAM X\nBUSINESS \u2713\nADD-ON X\n?\n\nThe Segment Data Lakes sync history and health tabs generate real-time information about data syncs so you can monitor the health and performance of your data lakes. These tools provide monitoring and debugging capabilities within the Data Lakes UI, so you can identify and proactively address data sync or data pipeline failures.\n\nSync history\n\nThe Sync History table shows detailed information about the latest 100 syncs to the data lake. The table includes the following fields:\n\nFIELD\tDESCRIPTION\nSync status\tThe status of the sync: either Success, indicating that all rows synced correctly, Partial Success, indicating that some rows synced correctly, or Failed, indicating that no rows synced correctly\nStart time\tThe time the sync began\nDuration\tHow long the sync took to complete\nSynced rows\tThe number of rows that synced to the data lake\nNotices\tAny notes or warnings about the sync\n\nSelecting a row in the Sync History table opens a sidebar showing the number of rows from each collection that synced.\n\nTo access the Sync History page from the Segment app, open the My Destinations page and select the data lake. On the data lakes Settings page, select the Sync History tab.\n\nHealth\n\nThe health tab provides an overview of the rows that synced to your data lake both today and each day for the last 30 days.\n\nThe bar chart, \u2018Daily Synced Rows,\u2019 shows an overview of the rows synced for each of the last 30 days. Hovering over a date shows the number of rows that were synced for that day. Selecting a date from the bar chart opens the Daily Row Volume table, which provides a breakdown of which collections synced, how many rows from each collection synced, and the percentage of all synced rows from each collection.\n\nThe Daily Row Volume table contains the following information:\n\nCollections: The name of each collection of properties synced to the data lake\nRows: The number of rows synced from each collection\n% of Total: The percentage of the total number of rows synced that each collection represents\n\nAbove the Daily Row Volume table is an overview of the total syncs for the current day, showing the number of rows synced, the number of collections that synced, and the current date.\n\nTo access the Sync history page from the Segment app, open the My Destinations page and select the data lake. On the data lakes settings page, select the Health tab.\n\nData Lakes reports FAQ\nHow long is a data point available?\n\nThe health tab shows an aggregate view of the last 30 days worth of data, while the sync history retains the last 100 syncs.\n\nHow do sync history and health compare?\n\nThe sync history feature shows detailed information about the most recent 100 syncs to a data lake, while the health tab shows just the number of rows synced to the data lake over the last 30 days.\n\nWhat timezone is the time and date information in?\n\nAll dates and times on the sync history and health pages are in the user\u2019s local time.\n\nWhen does the data update?\n\nThe sync data for both reports updates in real time.\n\nWhen do syncs occur?\n\nSyncs occur approximately every two hours. Users cannot choose how frequently the data lake syncs.\n\nThis page was last modified: 03 Aug 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSync history\nHealth\nData Lakes reports FAQ\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nSegment Resources\nSegment Resources\nTechnical Support\n\nEmail support is available for all Segment support plans. If you\u2019re experiencing problems, have questions about implementing Segment, or want to report a bug, you can fill out the support contact form and the Success Engineering team will get back to you.\n\nYou need a Segment account to file a support request. If you don\u2019t have one, sign up for a free workspace and then send your request.\n\nSegment Support Business Hours\n\nFor more information about the Segment Support Team\u2019s hours and holidays see Twilio Support business hours.\n\nSegment University\n\nSegment University is Segment\u2019s free, online classroom for learning the basics of Segment.\n\nAnalytics Academy\n\nAnalytics Academy is a series of lessons designed to help you understand the value of analytics as a discipline. Analytics Academy will help you think through your analytics needs so you can start creating robust and flexible analytics systems.\n\nRecipes\n\nWondering what you can do with Segment? Check out Segment Recipes for inspiration on what you can achieve by connecting your Segment workspace to different Destinations, from tailored onboarding emails to joining and cleaning your data with third-party tools.\n\nOther Resources\n\nHead over to Segment Resources for Segment case studies, webinars, courses, and more.\n\nStatus Page\n\nThe Segment Status Page offers details about system metrics and reports on uptime for each part of the Segment product. You can subscribe to receive updates about ongoing incidents and view information about past incidents.\n\nThis page was last modified: 28 Jun 2023\n\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nProfiles Sync\n/\nProfiles Sync Setup\n/\nDatabricks for Profiles Sync\nDatabricks for Profiles Sync\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\n\nWith Databricks for Profiles Sync, you can use Profiles Sync to sync Segment profiles into your Databricks Lakehouse.\n\nGetting started\n\nBefore getting started with Databricks Profiles Sync, note the following prerequisites for setup.\n\nThe target Databricks workspace must be Unity Catalog enabled. Segment doesn\u2019t support the Hive metastore. Visit the Databricks guide enabling the Unity Catalog for more information.\nSegment creates managed tables in the Unity catalog.\nSegment supports only OAuth (M2M) for authentication.\nWarehouse size and performance\n\nA SQL warehouse is required for compute. Segment recommends a warehouse with the the following characteristics:\n\nSize: small\nType Serverless otherwise Pro\nClusters: Minimum of 2 - Maximum of 6\n\nTo improve the query performance of the Delta Lake, Segment recommends creating compact jobs per table using OPTIMIZE following Databricks recommendations.\n\nSegment recommends manually starting your SQL warehouse before setting up your Databricks destination. If the SQL warehouse isn\u2019t running, Segment attempts to start the SQL warehouse to validate the connection and may experience a timeout when you hit the Test Connection button during setup.\n\nSet up Databricks for Profiles Sync\nFrom your Segment app, navigate to Unify > Profiles Sync.\nClick Add Warehouse.\nSelect Databricks as your warehouse type.\nUse the following steps to connect your warehouse.\nConnect your Databricks warehouse\n\nUse the five steps below to connect to your Databricks warehouse.\n\nTo configure your warehouse, you\u2019ll need read and write permissions.\n\nStep 1: Name your schema\n\nPick a name to help you identify this space in the warehouse, or use the default name provided. You can\u2019t change this name once the warehouse is connected.\n\nStep 2: Enter the Databricks compute resources URL\n\nYou\u2019ll use the Databricks workspace URL, along with Segment, to access your workspace API.\n\nCheck your browser\u2019s address bar when inside the workspace. The workspace URL should resemble: https://<workspace-deployment-name>.cloud.databricks.com. Remove any characters after this portion and note the URL for later use.\n\nStep 3: Enter a Unity catalog name\n\nThis catalog is the target catalog where Segment lands your schemas and tables.\n\nFollow the Databricks guide for creating a catalog. Be sure to select the storage location created earlier. You can use any valid catalog name (for example, \u201cSegment\u201d). Note this name for later use.\nSelect the catalog you\u2019ve just created.\nSelect the Permissions tab, then click Grant.\nSelect the Segment service principal from the dropdown, and check ALL PRIVILEGES.\nClick Grant.\nStep 4: Add the SQL warehouse details from your Databricks warehouse\n\nNext, add SQL warehouse details about your compute resource.\n\nHTTP Path: The connection details for your SQL warehouse.\nPort: The port number of your SQL warehouse.\nStep 5: Add the service principal client ID and client secret\n\nSegment uses the service principal to access your Databricks workspace and associated APIs.\n\nService principal client ID: Follow the Databricks guide for adding a service principal to your account. This name can be anything, but Segment recommends something that identifies the purpose (for example, \u201cSegment Profiles Sync\u201d). Segment doesn\u2019t require Account admin or Marketplace admin roles.\n\nThe service principal needs the following setup:\n\nCatalog-level privileges which include:\nUSE CATALOG\nUSE SCHEMA\nMODIFY\nSELECT\nCREATE SCHEMA\nCREATE TABLE\nDatabricks SQL access entitlement at the workspace level.\nCAN USE permissions on the SQL warehouse that will be used for the sync.\n\nClient secret: Follow the Databricks instructions to generate an OAuth secret.\n\nOnce you\u2019ve configured your warehouse, test the connection and click Next.\n\nSet up selective sync\n\nWith selective sync, you can choose exactly which tables you want synced to the Databricks warehouse. Segment syncs materialized view tables as well by default.\n\nSelect tables to sync, then click Next. Segment creates the warehouse and connects databricks to your Profiles Sync space.\n\nYou can view sync status, and the tables you\u2019re syncing from the Profiles Sync overview page.\n\nLearn more about using selective sync with Profiles Sync.\n\nThis page was last modified: 03 Jun 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nGetting started\nSet up Databricks for Profiles Sync\nConnect your Databricks warehouse\nSet up selective sync\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nCampaigns\n/\nWhatsApp Campaigns\nWhatsApp Campaigns\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE PREMIER \u2713\n?\n\nEngage Premier entered an End of Sale (EOS) period effective June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:\n\nTwilio Marketing Campaigns\n\nPreferred ISV Partners:\n\nAirship Blog\nBloomreach Blog\nBraze Blog\nInsider Blog\nKlaviyo Blog\nTwilio Engage Foundations Documentation\n\n\nHow Engage campaigns work\n\nTwilio Engage uses Journeys to send WhatsApp, email, and SMS campaigns. With Journeys, you add conditions and steps that trigger actions like sending a WhatsApp message.\n\nYou\u2019ll build and send your WhatsApp campaign in three stages:\n\nCreate a Journey.\nAdd a Journey condition.\nAdd a WhatsApp step and publish your campaign.\n\nWhatsApp Templates\n\nTo send a WhatsApp campaign, you\u2019ll first need an approved WhatsApp template. For instructions on building a template, view WhatsApp Templates.\n\nCreate a Journey\n\nBecause Engage campaigns exist within Journeys, begin by creating a Journey:\n\nIn Engage, select Journeys, then click Create journey.\nName your Journey and select its entry settings.\nClick Build Journey to create the Journey.\n\nSegment then opens the Journey Builder.\n\nAdd a Journey condition\n\nWith your Journey created, you\u2019ll now set a condition to trigger your WhatsApp campaign:\n\nWithin the Journey builder, click + Add Entry Condition.\nIn the Add entry condition pane, give the step a name.\nClick + Add Condition, select your desired condition, then click Save.\n\nWith your entry condition added, you\u2019re ready to add an approved WhatsApp template to build a campaign.\n\nAdd a WhatsApp step and publish your Journey\nWithin the Journey builder, click the + node below your new condition.\nFrom the Add step window, click Send a WhatsApp.\nPick an approved template from the template list, then choose Select.\nGive the WhatsApp message step a name.\nIn the Sender field, choose WhatsApp, then click Save.\nSegment returns you to the Journey builder. Select Publish, then select Publish journey in the popup.\n\nYour Journey and WhatsApp campaign are now live. Users who trigger the WhatsApp step\u2019s parent Journey condition will receive your SMS campaign.\n\nMessaging limits\n\nWhatsApp limits the number of unique recipients that can receive your campaigns. If your Meta Business Account isn\u2019t verified, you\u2019ll begin with a messaging limit of 250 unique recipients every 24 hours.\n\nOnce your Meta Business Account is verified, the number of unique recipients increases, depending on your messaging limit tier. For more information, view Meta\u2019s messaging limits documentation.\n\nThis page was last modified: 15 Jul 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nHow Engage campaigns work\nMessaging limits\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nContent\n/\nEmail\n/\nDrag and Drop Editor\nDrag and Drop Editor\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE PREMIER \u2713\n?\n\nEngage Premier entered an End of Sale (EOS) period effective June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:\n\nTwilio Marketing Campaigns\n\nPreferred ISV Partners:\n\nAirship Blog\nBloomreach Blog\nBraze Blog\nInsider Blog\nKlaviyo Blog\nTwilio Engage Foundations Documentation\n\n\nUse Twilio Engage to build email templates with a what you see is what you get (WYSIWYG) Drag and Drop Editor. Use drag and drop tools to design the template layout and include user profile traits to personalize the message for each recipient.\n\nYou can navigate to the Drag and Drop Editor from the Select Editor screen:\n\nWhen you build a new email template or edit an existing one.\nFrom a Send Email step in a Journey.\n\nFrom the Select Editor screen, select Drag and Drop Editor and click Build Email.\n\nThe Drag and Drop Editor consists of a left sidebar with design modules and an email canvas.\n\nLeft sidebar\n\nThe left sidebar contains the following tools:\n\nContent\nBlocks\nBody\nImages\nUploads\n\nClick and drag the tools you want to use from the left sidebar in the email canvas.\n\nEmail canvas\n\nUse the canvas to organize and preview the email template for both desktop and mobile. Drag and drop content modules from the sidebar into the canvas and arrange the layout as desired.\n\nSelect content in the canvas and return to the left sidebar to set properties for the selected content.\n\nDesign for desktop or mobile\n\nNavigate between desktop or mobile to design the email template for both formats. Toggle between desktop or mobile in the left sidebar or use the buttons in the bottom right corner of the email canvas.\n\nMobile view doesn\u2019t contain all design options that are available for desktop. For example, upload image capabilities are only available when you edit for desktop. However, content that you add for desktop will also display for mobile devices by default. See content modules for more on content that\u2019s only available to edit for desktop.\n\nResponsive design\n\nUse responsive design settings in the sidebar to hide specific content for either desktop or mobile.\n\nAdd content modules\n\nSelect from the following content modules:\n\nMODULE\t\u00a0\tDESCRIPTION\nColumns\t\u00a0\tAdds columns to the email body. After you place a column in the email canvas, use the left sidebar to set column and row properties, as well as responsive design settings.\n\nColumns allow you to organize the email layout by adding sections to drop other content modules in, such as buttons or images.\nButton\t\u00a0\tUse this tool to add a button to the canvas, then edit button properties in the sidebar:\n\nAction: Use the Action Type dropdown menu to select the action that occurs when the button is clicked. Learn more about link actions.\nAction properties can only be set when you edit for desktop.\n\nButton Options: Select button text and background color along with the width and alignment.\n\nSpacing: Set spacing and border attributes for the button.\n\nGeneral: Adjust the container padding to determine the amount of space between the button and column border.\n\nResponsive Design: Hide the selected button for desktop and mobile.\nDivider\t\u00a0\tThe divider separates columns, rows, or content in the email with a visible divider line.\nHeading\t\u00a0\tAdds heading text to the email. Set text properties, add actionable links, and personalize the text with merge tags.\nHTML\t\u00a0\tUse this tool to add HTML to the email body. Note that you can only add HTML when you edit for desktop.\nImage\t\u00a0\tAdds an image. You can format image properties and add a link. Image uploads are only available when you edit for desktop.\nMenu\t\u00a0\tAdds a responsive menu to the email. After you add a menu to the canvas, edit the following properties:\n\nMenu Items (desktop only)\nTo add menu options:\n1. Click Add New Item.\n2. Enter the menu text.\n3. Select the action type that occurs when you click on the menu option.\n\nStyles: Menu style for font, layout, padding, and more.\n\nGeneral: Properties for the menu container padding.\n\nResponsive Design: Hide the selected menu for desktop or mobile.\nText\t\u00a0\tAdds a text section to the email. Set text properties and include actionable links. Add merge tags to personalize the email with user profile traits.\nAdd blank columns or predefined content blocks\n\nUse the Blocks tool to add both blank columns and pre-existing content blocks to the email. Drag empty column blocks in the canvas to organize the layout, then drag and drop content tools inside the column blocks.\n\nPredefined content blocks allow you to add content such as:\n\nUnsubscribe blocks\nHeading blocks\nImage blocks\nSaved blocks\nEmail body attributes\n\nUse the body tool to apply general style and link attributes to the entire email canvas.\n\nEmail body attributes include:\n\nText and background color\nContent width and alignment\nFont family\nLink color and underline\n\nTo modify style attributes for specific content in the email, select a content block in the canvas and edit attributes in the left sidebar.\n\nAdd an image\n\nUse the images tool to add images to your email. Scroll through available images in the left sidebar or use the search tool.\n\nSelect and drag and image into the canvas, then return to the sidebar to set image properties:\n\nIMAGE PROPERTY\t\u00a0\tDESCRIPTION\nImage\t\u00a0\tUpload a new image (up to 10 MB), add an image url, and adjust the image width. You can also add alternate text to display with the image.\nAction\t\u00a0\tUse the image link drop-down menu to select link actions that occur when a recipient clicks on an image.\nGeneral\t\u00a0\tAdjust container padding, which determines the amount of space between the image and column border.\nResponsive Design\t\u00a0\tHide selected images for an email viewed on either desktop or mobile.\nUpload an image\n\nUse the Uploads tool to upload an image for the email template. Click Upload Image to select an image stored locally or drag and drop images in the sidebar dropzone.\n\nThe maximum image file size you can upload is 10 MB.\n\nLink actions\n\nUse the Action Type drop down menu in the sidebar to select the action that occurs when a recipient clicks on the link, button, or image in the email template.\n\nSelect from the following link actions:\n\nOpen Website: Directs you to a website. Enter a URL and choose to open the website in the same tab or a new one.\nSend Email: Sends an email based on the email recipient, subject, and body you enter.\nCall Phone Number: Makes a phone call to the number you enter.\nSend SMS: Sends an SMS message to the phone number you enter.\nAdd unsubscribe links\n\nIt\u2019s always best practice to include an unsubscribe link in the emails you build. Engage adds an unsubscribe link to templates, which you can edit at any time. For more on email unsubscribe links, view SendGrid\u2019s best practices.\n\nAdd an unsubscribe link as a button:\n\nSelect the button in the email canvas and navigate to Action settings in the left sidebar.\nSet Action Type to Open Website.\nClick Special Links > Unsubscribe.\n\nAdd an unsubscribe link to text:\n\nSelect the text that you want to convert to an unsubscribe link.\nClick the link icon in the text toolbar.\nIn the Insert/Edit link window, set Action Type to Open Website.\nClick Special Links > Unsubscribe, then click Save.\n\nYou can alternatively add a predefined unsubscribe link content block.\n\nAdd a manage preference link\n\nEngage also adds a manage preference link to templates. The manage preference link lets your customers opt in and out of email groups on an individual basis instead of unsubscribing from all your campaigns. For more information, see subscription groups.\n\nPersonalize with merge tags\n\nAdd merge tags in the Drag and Drop Editor to personalize your message with user profile traits.\n\nSelect any heading or body text in the email canvas. From the text toolbar, click Merge Tags.\nSelect the profile traits to include from the drop down menu.\nBased on cursor placement, Engage adds merge tags to your email template.\n\nEngage supports liquid templating to create dynamic content in the email design editor and the SMS editor.\n\nFor example, use {% if %}, {% elseif %}, and {% else %} tags to call a product by name if known, or use a default message:\n\n{% if profile.traits.product_title == \"Sneakers\" %}\n  Hey, view our latest sneakers!\n{% elsif profile.traits.product_title == \"Sandals\" %}\n  Hey, check out these sandals!\n{% else %}\n  Hey, check out our latest footwear.\n{% endif %}\n\n\nTo view more examples related to your use case, visit the LiquidJS docs.\n\nContent validation\n\nFor all content editors in Engage, you\u2019ll see alerts for any issues in your template, such as invalid profile traits or incorrect liquid syntax. Engage both flags template issue(s), and displays recommended next steps. While you can save these templates, you must fix any issues before using them in Engage campaigns.\n\nSave the template\n\nAfter you design the email, click Create Email Template.\n\nNext steps\n\nLearn more about building email templates to include in your Engage campaigns.\n\nYou can learn about the HTML Editor for both code and visual editing capabilities from a single view.\n\nonce you create an email with the Drag and Drop Editor, you can\u2019t modify it with the HTML Editor, and vice versa.\n\nThis page was last modified: 15 Jul 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nLeft sidebar\nEmail canvas\nDesign for desktop or mobile\nAdd content modules\nAdd blank columns or predefined content blocks\nEmail body attributes\nAdd an image\nUpload an image\nLink actions\nAdd unsubscribe links\nAdd a manage preference link\nPersonalize with merge tags\nContent validation\nSave the template\nNext steps\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nUser Subscriptions\n/\nUser Subscription States\nUser Subscription States\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE PREMIER \u2713\n?\n\nEngage Premier entered an End of Sale (EOS) period effective June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:\n\nTwilio Marketing Campaigns\n\nPreferred ISV Partners:\n\nAirship Blog\nBloomreach Blog\nBraze Blog\nInsider Blog\nKlaviyo Blog\nTwilio Engage Foundations Documentation\n\n\nCustomer profiles in your Segment audiences contain contact vectors. A contact vector is a piece of unique, specific contact information associated with a customer, like the customer\u2019s email address or phone number.\n\nSegment associates one of four user subscription states with each contact vector in a customer profile. These subscription states indicate the level of consent customers give you to send them your marketing materials.\n\nA customer profile, then, may have contact vectors with different subscription states. For example, a customer may consent to receive email campaigns but not SMS campaigns. Subscription states, then, describe permissions at the contact vector level, not at the customer level.\n\nUnderstanding the four user subscription states helps you improve campaign deliverability and comply with sending guidelines and legislation. This page explains the four subscription states and how each impacts your sending ability.\n\nSubscription states overview\n\nThe following table displays the four subscription states:\n\nSUBSCRIPTION STATES\tDESCRIPTION\tEXAMPLE\nsubscribed\tA user has given you their contact information and consented to receive marketing campaigns.\tUsers that have signed up for a weekly newsletter\nunsubscribed\tA user has given you their contact information but doesn\u2019t want to receive campaigns.\tUsers that subscribed, then unsubscribed, from a weekly newsletter\ndid-not-subscribe\tA user gave you their contact information but made no decision about receiving marketing campaigns.\tA user provided their email address or phone number in an online transaction, but didn\u2019t sign up to receive your weekly newsletter.\nNo subscription status\tA user did not give you their contact information and made no decision about receiving marketing campaigns.\tSegment collected an email or phone number through identity resolution. No user actively provided the email or phone number.\nUnderstanding subscription states\n\nYou can gain insight into your audience profiles by learning how and why each subscription state is associated with a user\u2019s profile. Below, you\u2019ll find the four states described in detail, along with common scenarios that produce those states.\n\nSubscribed\n\nA subscribed user has, at some point, given you explicit permission to send them your marketing materials.\n\nSubscribed users have intentionally requested to receive your marketing materials and have taken voluntary action to confirm that choice. You may have received this consent from a number of sources, including the following:\n\nA user who opted in to receive marketing campaigns during online checkout\nA user who signed up for your marketing campaigns on your website\u2019s signup form\nA user who signed up for marketing campaigns at an in-person event, like a conference\n\nIt\u2019s your responsibility to ensure that Segment correctly reflects your users\u2019 subscription choices. Failure to do so may put you in violation of legislation like CAN-SPAM, TCPA, or GDPR.\n\nUnsubscribed\n\nAn unsubscribed user has intentionally opted out of receiving your marketing materials. You cannot send Engage campaigns to unsubscribed users.\n\nUsers commonly unsubscribe in the following ways:\n\nBy clicking an unsubscribe link in an email campaign\nBy replying with STOP to an SMS campaign\nBy contacting you in writing to request that you unsubscribe them\n\nYou must include an unsubscribe option in all Engage email and SMS campaigns.\n\nDid not subscribe\n\nUsers with the did-not-subscribe state associated with their email address or phone number gave you their contact information without explicitly agreeing to receive your marketing materials.\n\nThe following scenarios often lead to an email or phone number with the did-not-subscribe subscription state:\n\nA user provides their email or phone number during an online transaction but doesn\u2019t opt in to your marketing materials.\nThe user\u2019s email address was obtained from a support request.\n\nEmails or phone numbers with a did-not-subscribe status won\u2019t receive your marketing campaigns.\n\nNo subscription status\n\nProfiles with no subscription status, or a blank status, indicate that Segment has created a profile for the user, but that the user never actually provided their contact information. Some situations that lead to the no subscription status state include the following:\n\nPublicly available email addresses or phone numbers\nEmail addresses or phone numbers you acquired through other audience lists\nSegment collected the email address or phone number through standard platform tracking methods.\n\nSome contacts within your Segment space may fall into the no subscription status category. Identity resolution, for example, may result in a user profile created from connecting an email address with an anonymous ID. In this case, the profile would exist within your audience despite the fact that the user never had the option to subscribe or unsubscribe.\n\nSetting user subscriptions\n\nYou can set subscription states by either CSV file upload or, programmatically, with the Public API.\n\nUploading contacts with a CSV file works best for initial batches of contacts you\u2019d like to bring into Engage. Syncing programmatically with the Public API is best suited for real-time and ongoing subscription maintenance, like when a user signs up for a form on your site or unsubscribes from your marketing campaigns within their notification center or account settings.\n\nTo learn more about both options, reference the Engage documentation on using the CSV uploader and setting user subscriptions.\n\nSync subscription statuses with SQL\n\nUse SQL to import user subscription states from your data warehouse back to Engage. When you sync with SQL, you can query user subscription data at automated intervals. Pull subscription statuses for each contact vector and use your data warehouse as a single source of truth for subscription data.\n\nThis option is especially useful if you don\u2019t have the ability to set subscription states with CSVs or Segment\u2019s Public API.\n\nView Subscriptions with SQL Traits for more information.\n\nTroubleshooting subscription states\n\nOn occasion, a user\u2019s subscription state may not be up-to-date. For example, a user may have unsuccessfully attempted to unsubscribe from your marketing campaigns.\n\nThe Public API will resolve most subscribe and unsubscribe requests in real time. In some circumstances, however, you\u2019ll need to take action to update a user\u2019s subscription state. The following table lists some situations in which you may find a manual update useful:\n\nISSUE\tCAUSE\tRESOLUTION\nUnsubscribed user still getting marketing campaigns\tPotential API call failure when updating the subscription state\tAsk the user to attempt to unsubscribe again; upload a CSV file with the user\u2019s profile and a state of unsubscribed.\nUser no longer receives desired email campaigns\tUser may have accidentally clicked unsubscribe on an email campaign\tThe user must resubscribe to your campaigns, or you can upload a CSV file with the contact and their corrected state.\nUser no longer receives desired SMS campaigns\tUser may have replied STOP to an SMS campaign\tYou cannot change the state on your own; the user must send START, YES, or UNSTOP to the original campaign number from their own device.\n\nReach out to support with questions you may have about resolving a user\u2019s subscription state.\n\nThis page was last modified: 15 Jul 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSubscription states overview\nUnderstanding subscription states\nSetting user subscriptions\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nContent\n/\nEmail\n/\nHTML Editor\nHTML Editor\n\nEngage Premier entered an End of Sale (EOS) period effective June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:\n\nTwilio Marketing Campaigns\n\nPreferred ISV Partners:\n\nAirship Blog\nBloomreach Blog\nBraze Blog\nInsider Blog\nKlaviyo Blog\nTwilio Engage Foundations Documentation\n\n\nUse the HTML Editor to design your email template with both code and visual editing capabilities. Build your email template with code, copy and paste existing code, or use the Visual Editor for a code free design experience.\n\nOn this page, you\u2019ll learn how to use the HTML Editor to build personalized email templates for your Engage campaigns.\n\nGetting started\n\nYou can navigate to the HTML Editor in two ways:\n\nWhen you build a new email template or edit an existing one.\nFrom a Send Email step in a Journey.\n\nFrom the Select Editor screen, select HTML Editor and click Build Email.\n\nVisual Editor\n\nUse the Visual Editor for a no-code option to design your email. With the Visual Editor, you can:\n\nAdd or modify headings and text\nModify text color, size, and style\nInsert an image\nAdd merge tags and links\nAdd emojis\n\nEngage updates any changes you make in the Visual Editor to the HTML Editor in real-time.\n\nInsert an image\n\nTo insert an image from the Visual Editor:\n\nSelect the image icon in the Visual Editor toolbar.\nAdd the image URL source and alternative text.\nEdit the image width and height.\nYou can also click and drag the corners of the image to resize it in the Visual Editor.\nClick Save.\nPreview for desktop or mobile display\n\nTo preview your email template, click the preview icon in the Visual Editor toolbar.\n\nFrom the preview screen, you can toggle between desktop or mobile to view the email in both displays.\n\nHTML Editor\n\nUse the HTML Editor to maintain your email template with code. Copy and paste existing code or build a new template in the editor.\n\nEngage displays any changes you make in a preview screen to the right of your code. You can preview your email in both desktop and mobile display.\n\nClick Format at any time to properly indent and format your code in the HTML Editor.\n\nWhen you toggle from the HTML Editor to the Visual Editor, Engage may make minor changes to your code formatting. If Engage re-formats your code, it will not affect the email layout.\n\nError flagging and content validation\n\nEngage displays in-line error flags in the code editor to help you debug your code. If there are errors, you might not see content as expected in the preview screen until you\u2019ve debugged your code.\n\nFor all content editors in Engage, you\u2019ll see alerts for any issues in your template, such as invalid profile traits or incorrect liquid syntax. Engage both flags template issue(s), and displays recommended next steps. While you can save these templates, you must fix any issues before using them in Engage campaigns.\n\nPersonalize with merge tags\n\nAdd merge tags to personalize your message with user profile traits.\n\nFrom the text toolbar in the Visual Editor, click the Merge Tags drop-down menu.\nSelect profile traits to add to the merge tags.\nBased on cursor placement, Engage adds merge tags to your template.\n\nYou can also add merge tags to your email right from the code editor.\n\nLiquid templating\n\nEngage supports liquid templating to create dynamic content in the HTML Editor.\n\nFor example, use {% if %}, {% elseif %}, and {% else %} tags to call a product by name if known, or use a default message:\n\n{% if profile.traits.product_title == \"Sneakers\" %}\n  Hey, view our latest sneakers!\n{% elsif profile.traits.product_title == \"Sandals\" %}\n  Hey, check out these sandals!\n{% else %}\n  Hey, check out our latest footwear.\n{% endif %}\n\n\nIf you use liquid templating, be sure to test your email to make sure that everything renders properly.\n\nWhile both the HTML and Visual Editor support liquid templating, Segment recommends using the HTML Editor to write liquid templating.\n\nEngage doesn\u2019t support liquid template syntax that produces partial blocks of HTML.\n\nTo view more examples related to your use case, visit the LiquidJS docs.\n\nAdd unsubscribe links\n\nIt\u2019s always best practice to include an unsubscribe link in the emails you build. Engage adds an unsubscribe link to email templates, which you can edit at any time.\n\nYou can add unsubscribe links from the visual or HTML Editor.\n\nFrom the Visual Editor:\n\nSelect the link icon in the Visual Editor toolbar.\nEnter [unsubscribe] in the URL field.\nEnter the link attributes and text.\nClick Save.\n\nTo add a link from the code editor, use <a href = \"[unsubscribe]\"> </a> in your HTML.\n\nFor more on email unsubscribe links, view SendGrid\u2019s best practices.\n\nToggle between editors\n\nFrom the editor screen, you can click Use HTML Editor or Use Visual Editor to toggle between the two editors.\n\nThe Visual Editor renders your HTML in an editable preview (similar to an email client), so you might need to accept formatting changes to your HTML to use the Visual Editor. In this case, Segment displays a confirmation modal with HTML differences.\n\nPotential HTML changes include formatting, removing attributes with potentially unsuported scripts in your HTML (for example, onclick or onblur), attribute reordering, and adding missing tags.\n\nIf you don\u2019t want to accept the changes required to use the Visual Editor, you can continue editing in the HTML Editor.\n\nFormatting your HTML\n\nIn the HTML Editor, you can use the Format button to properly indent and format your code. Note that the Format button may not implement all changes necessary to use the Visual Editor.\n\nSave the template\n\nAfter you design the email, click Create Email Template. You can navigate to Engage > Content > Templates to view and maintain your email template.\n\nNext steps\n\nLearn more about building email templates to include in your Engage campaigns.\n\nYou can also learn about the Drag and Drop Editor in Engage to build Email templates with drag and drop functionality.\n\nOnce you create an email with the HTML Editor, you can\u2019t modify it with the Drag and Drop Editor, and vice versa.\n\nThis page was last modified: 15 Jul 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nGetting started\nVisual Editor\nHTML Editor\nPersonalize with merge tags\nAdd unsubscribe links\nToggle between editors\nSave the template\nNext steps\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nJourneys\n/\nJourneys Key Terms\nJourneys Key Terms\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nKeep the following terms in mind as you begin to explore Journeys.\n\nGeneral\nTERM\tDEFINITION\nJourney\tA multi-step workflow that progresses users through steps based on time logic, real-time customer interactions, and customer traits.\nJourney list view\tThe Journeys tab shows all Journeys in the selected Engage space.\nJourney builder\tA visual canvas where you can view and edit step definitions and types.\nJourney overview\tA visual canvas where you can view all steps and definitions.\nSteps\nTERM\tDEFINITION\nStep\tAn individual point in the Journey that can be any of the following: Wait for condition, Wait for duration, True/false split, Multi-branch split, Send to destinations.\nBranch\tPaths that lead users away from a step. For example, A True/false split creates one True branch and one False branch.\nEntry condition\tThe first step in the Journey where you define the entry criteria. In this step, you can backfill historical data and preview users before you publish the Journey.\nWait for condition\tA step in which you define one or more conditions which a user must fulfill to move to this step.\nWait for duration\tA step in which you define the amount of time before the user moves to the next step.\nTrue/false split\tA step in which you define a condition to direct a user to A step in which you define a condition to direct user to one of two steps.\n\nUsers who fulfill the condition move to the true branch. Users who do not move to the false branch.\nMulti-branch split\tA step in which you define any number of conditions. Each condition represents a separate branch leading away from the step. Users travel down the branch of the condition they meet.\n\nJourneys does not enforce mutual exclusivity in branch conditions . For more information, see Best Practices.\nSend to destinations\tA step in which you can send track or identify calls to Event destinations, or a list of users to a List destination.\nStep name\tThe name of the step that displays in the Journey builder and overview.\nKey\tName of the Send to Destination step used to identify the step users are on when Journeys sends information to the destination. For Track events, the property name uses this key. For Identify events, the trait name uses this key.\n\nFor more information, see Send data to Destinations.\nStatuses\nTERM\tDEFINITION\nDraft Journey\tA Journey which is not yet computing nor sending data to destinations.\n\nFor more information, see Draft Journeys.\nPublished (live) Journey\tA Journey that is computing and sending data to destinations.\n\nFor more information, see Published Journeys.\nArchived Journey\tA Journey that has been archived.\n\nFor more information, see Archive a Journey.\nFailed (live) Journey\tA Journey that has been published, but failed during the live computations due to an unforeseen error.\n\nContact Segment Support to learn more.\nSteps with Audiences\nSTEP\tAUDIENCE DEFINITION\nEntry condition\tAll users who fulfill the entry condition criteria. \u201cUse Historical Data\u201d evaluates events before Journey publication.\nCondition\tAll users who fulfill condition criteria, at one point fulfilled preceding step criteria, and have met any step wait conditions.\nDestination Sync\tAll users who, at one point, fulfilled parent step criteria and have met any following wait conditions.\nSteps without Audiences\nSTEP\tDETAILS\nDelay\tNo audience. Segment appends the wait duration as a condition to the following step\u2019s audience.\nT/F split\tThe split\u2019s resulting conditions contain two mutually exclusive audiences. The split node itself has no audience.\nMulti-branch splits\tThe split\u2019s resulting conditions contain audiences. The split node itself has no audience.\nAnalytics\nMETRIC\tDEFINITION\nEntry\tWhen a user enters a Journey for the first time or re-enters a Journey after exiting; excludes users who re-enter a Journey without exiting.\n\nThis page was last modified: 06 Feb 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nGeneral\nSteps\nStatuses\nSteps with Audiences\nSteps without Audiences\nAnalytics\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nJourneys\n/\nJourneys Best Practices and FAQ\nJourneys Best Practices and FAQ\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\nBest practices\nEnforce exclusivity in multi-branch splits\n\nWhen you create a multi-branch split, do not create overlapping conditions that might lead a user to qualify for more than one step at a time.\n\nFor example:\n\nIn the case where a multi-branch split is based on the conditions registration form submitted and webinar attended, a user may satisfy both conditions, and therefore is eligible for both paths.\nTo set a priority, branch 2 should then be who performed registration form submitted and did not perform webinar attended to ensure mutual exclusivity\nAdd time windows whenever possible\n\nAdd time windows when defining conditions to enforce funnel constraints in a Journey, rather than using an unbounded event condition which operates on the entire history of the user profile. For example, to check if a user has completed an order since receiving an email triggered 7 days ago, use the condition \u201cOrder Completed at least 1 time within 7 days.\u201d\n\nSuppress targeting with journey lists\n\nUnlike lists associated with Engage Audiences, users who are added to a journey list cannot be subsequently removed. Lists are typically associated with advertising campaigns, and you must take additional steps if you wish to ensure that users do not continue to be targeted with ads after they achieve some goal. A typical implementation pattern is:\n\nUse a send to destination step to add users to the initial targeting list.\nCreate additional journey steps to model the conditions where a user should be removed from targeting. Create a second send to destination step for the removal list.\nWhen configuring targeting conditions in the destination interface, use boolean logic to include only those users who are in the initial list AND NOT in the removal list.\nReview your Journey in drafts first\n\nSave your Journey in a draft state so that you can review before you publish it. Once you publish a Journey, you cannot edit select portions of a Journey and Journeys sends data to destinations.\n\nKnow how to incorporate historical data\n\nAside from the entry condition, all Journey step conditions are triggered by future events and existing trait memberships. Event-based conditions only evaluate events that occur after the Journey is published.\n\nWhen you include historical data in a Journey\u2019s entry condition, Unify identifies users who previously satisfied the entry condition and adds them to entry. For example, to evaluate if a user has ever used a discount code mid-Journey, create and configure a Computed Trait to select for discount_used = true to use in your Journey.\n\nIncluding historical data doesn\u2019t impact any additional Journey steps, however. To include historical data in post-entry conditions, use the following table to identify which conditions will automatically include historical data:\n\nCONDITION TYPE\tAUTOMATIC HISTORICAL DATA INCLUSION\nAudience Reference\tYes\nComputed Trait\tNo\nEvent\tNo\nCustom Trait\tNo\n\nTo include historical data based on custom traits or events that predate the Journey, first build an Audience that includes the targeted data by following these steps:\n\nCreate a standard Engage Audience outside of the Journeys builder.\nAdd conditions that include the historical event or custom trait you want to include in the Journey.\nAfter you\u2019ve created the Audience, return to Journeys and create a Part of an Audience condition that references the audience you created in Step 2.\n\nFor example, to include custom trait = ABC in a Journey, create an Audience called ABC that includes that custom trait, then add the Journey condition Part of Audience ABC.\n\nUsing the Part of Audience condition, Journeys then populates the custom trait as if it were using historical data.\n\nUse dev spaces and data warehouse destinations to test journeys\n\nFollow these best practices to test your journeys:\n\nWhile in the process of configuring a journey, use dev Spaces to model that journey without affecting production data.\nConnect a data warehouse to each step of the journey to test for success or failure of that step.\nFor early version journeys, scaffold Send to Destination steps without connecting to your production advertising or messaging destinations.\nVerify individual users\u2019 progress through the Journey in the Profile explorer view.\nFAQs\nHow often do Journeys run?\n\nJourneys run in real-time, like real-time Audiences in Engage. This means that users will progress through Journeys as Segment receives new events.\n\nCan a user re-enter a Journey?\n\nYes. Users must first exit a Journey, however, before entering it again. To learn more about Journey re-entry, read the Journey re-entry section of the Build a Journey page.\n\nWhat destinations does Journeys support?\n\nJourneys supports all Engage destinations, including Destination Functions. Read more in Send data to destinations .\n\nWhat are the reporting capabilities of Journeys?\n\nWhen building a Journey, if you check Use historical data, you can see the estimated number of users in the initial cohort.\n\nOnce published, Journeys displays the number of users are in each step of the Journey at any given time.\n\nHow are users sent to downstream destinations?\n\nThe data type you send to a destination depends on whether the destination is an Event Destination or a List Destination.\n\nWhich roles can access Journeys?\n\nFor Engage customers, users with either the Engage User or Engage Admin roles can create, edit, and delete journeys. Users with the Engage Read-only role are restricted to view-only access.\n\nWhy am I seeing duplicate entry or exit events?\n\nJourneys triggers audience or trait-related events for each email external_id on a profile. If a profile has two email addresses, you\u2019ll see two Audience Entered and two Audience Exited events for each Journey step. Journeys sends both email addresses to downstream destinations.\n\nHow quickly do user profiles move through Journeys?\n\nIt may take up to five minutes for a user profile to enter each step of a Journey, including the entry condition. For Journey steps that reference a batch audience or SQL trait, Journeys processes user profiles at the same rate as the audience or trait computation. Visit the Engage docs to learn more about compute times.\n\nThis page was last modified: 22 Feb 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nBest practices\nFAQs\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nStorage\n/\nCatalog\n/\nSet Up Segment Data Lakes\nSet Up Segment Data Lakes\n\nSegment Data Lakes provide a way to collect large quantities of data in a format that\u2019s optimized for targeted data science and data analytics workflows. You can read more information about Data Lakes and learn how they differ from Warehouses in Segment\u2019s Data Lakes documentation. Segment supports two type of data-lakes:\n\nAWS Data Lakes\nSegment Data Lakes (Azure)\n\nLake Formation\n\nYou can also set up your Segment Data Lakes using Lake Formation, a fully managed service built on top of the AWS Glue Data Catalog.\n\nSet up Segment Data Lakes (AWS)\n\nTo set up Segment Data Lakes, create your AWS resources, enable the Segment Data Lakes destination in the Segment app, and verify that your Segment data is synced to S3 and Glue.\n\nPrerequisites\n\nBefore you set up Segment Data Lakes, you need the following resources:\n\nAn AWS account\nAn Amazon S3 bucket to receive data and store logs\nA subnet within a VPC for the EMR cluster to run in\nStep 1 - Set up AWS resources\n\nYou can use the open source Terraform module to automate much of the set up work to get Data Lakes up and running. If you\u2019re familiar with Terraform, you can modify the module to meet your organization\u2019s needs, however Segment guarantees support only for the template as provided. The Data Lakes set up uses Terraform v0.12+. To support more versions of Terraform, the AWS provider must use v4, which is included in the example main.tf.\n\nYou can also use Segment\u2019s manual setup instructions to configure these AWS resources, if you prefer.\n\nThe Terraform module and manual setup instructions both provide a base level of permissions to Segment (for example, the correct IAM role to allow Segment to create Glue databases on your behalf). If you want stricter permissions, or other custom configurations, you can customize these manually.\n\nStep 2 - Enable Data Lakes destination\n\nAfter you set up the necessary AWS resources, the next step is to set up the Data Lakes destination within Segment:\n\nIn the Segment App, click Add Destination, then search for and select Data Lakes.\n\nClick Configure Data Lakes and select the source to connect to the Data Lakes destination. Warning: You must add the Workspace ID to the external ID list in the IAM policy, or else the source data cannot be synced to S3.\n\nIn the Settings tab, enter and save the following connection settings:\nAWS Region: The AWS Region where your EMR cluster, S3 Bucket and Glue DB reside, for example: us-west-2\nEMR Cluster ID: The EMR Cluster ID where the Data Lakes jobs will be run.\nGlue Catalog ID: The Glue Catalog ID (this must be the same as your AWS account ID).\nIAM Role ARN: The ARN of the IAM role that Segment will use to connect to Data Lakes, for example: arn:aws:iam::000000000000:role/SegmentDataLakeRole\nS3 Bucket: Name of the S3 bucket used by Data Lakes. The EMR cluster will store logs in this bucket, for example: segment-data-lake\n\nYou must individually connect each source to the Data Lakes destination. However, you can copy the settings from another source by clicking \u2026 (\u201cmore\u201d) (next to the button for \u201cSet up Guide\u201d).\n\n(Optional) Date Partition: Optional advanced setting to change the date partition structure, with a default structure day=<YYYY-MM-DD>/hr=<HH>. To use the default, leave this setting unchanged. To partition the data by a different date structure, choose one of the following options:\nDay/Hour [YYYY-MM-DD/HH] (Default)\nYear/Month/Day/Hour [YYYY/MM/DD/HH]\nYear/Month/Day [YYYY/MM/DD]\nDay [YYYY-MM-DD]\n\n(Optional) Glue Database Name: Optional advanced setting to change the name of the Glue Database which is set to the source slug by default. Each source connected to Data Lakes must have a different Glue Database name otherwise data from different sources will collide in the same database.\n\nEnable the Data Lakes destination by clicking the toggle near the Set up Guide button.\n\nOnce the Data Lakes destination is enabled, the first sync will begin approximately 2 hours later.\n\nStep 3 - Verify data is synced to S3 and Glue\n\nYou will see event data and sync reports populated in S3 and Glue after the first sync successfully completes. However if an insufficient permission or invalid setting is provided during set up, the first data lake sync will fail.\n\nTo receive sync failure alerts by email, subscribe to the Storage Destination Sync Failed activity email notification within the App Settings > User Preferences > Notification Settings.\n\nSync Failed emails are sent on the 1st, 5th, and 20th sync failure. Learn more about the types of errors which can cause sync failures in Segment\u2019s Sync errors docs.\n\n(Optional) Step 4 - Replay historical data\n\nIf you want to add historical data to your data set using a replay of historical data into Data Lakes, contact the Segment Support team to request one.\n\nReplay processing time can vary depending on the volume of data and number of events in each source. If you decide to run a Replay, Segment recommends that you start with data from the last six months to get started, and then replay additional data if you find you need more.\n\nSegment creates a separate EMR cluster to run replays, then destroys it when the replay finishes. This ensures that regular Data Lakes syncs are not interrupted, and helps the replay finish faster.\n\nSet up Segment Data Lakes (Azure)\n\nTo set up Segment Data Lakes (Azure), create your Azure resources and then enable the Data Lakes destination in the Segment app.\n\nPrerequisites\n\nBefore you can configure your Azure resources, you must complete the following prerequisites:\n\nCreate an Azure subscription\nCreate an Azure resource group\nCreate an account with Microsoft.Authorization/roleAssignments/write permissions\nConfigure the Azure Command Line Interface (Azure CLI)\nStep 1 - Create an ALDS-enabled storage account\nSign in to your Azure environment.\nFrom the Azure home page, select Create a resource.\nSearch for and select Storage account.\nOn the Storage account resource page, select the Storage account plan and click Create.\nOn the Basic tab, select an existing subscription and resource group, give your storage account a name, and update any necessary instance details.\nClick Next: Advanced.\nOn the Advanced Settings tab in the Security section, select the following options:\nRequire secure transfer for REST API operations\nEnable storage account key access\nMinimum TLS version: Version 1.2\nIn the Data Lake Storage Gen2 section, select Enable hierarchical namespace. In the Blob storage selection, select the Hot option.\nClick Next: Networking.\nOn the Networking page, select Disable public access and use private access.\nClick Review + create. Take note of your location and storage account name, and review your chosen settings. When you are satisfied with your selections, click Create.\nAfter your resource is deployed, click Go to resource.\nOn the storage account overview page, select the Containers button in the Data storage tab.\nSelect Container. Give your container a name, and select the Private level of public access. Click Create.\n\nBefore continuing, note the Location, Storage account name, and the Azure storage container name: you\u2019ll need this information when configuring the Segment Data Lakes (Azure) destination in the Segment app.\n\nStep 2 - Set up Key Vault\nFrom the home page of your Azure portal, select Create a resource.\nSearch for and select Key Vault.\nOn the Key Vault resource page, select the Key Vault plan and click Create.\nOn the Basic tab, select an existing subscription and resource group, give your Key Vault a name, and update the Days to retain deleted vaults setting, if desired.\nClick Review + create.\nReview your chosen settings. When you are satisfied with your selections, click Review + create.\nAfter your resource is deployed, click Go to resource.\nOn the Key Vault page, select the Access control (IAM) tab.\nClick Add and select Add role assignment.\nOn the Roles tab, select the Key Vault Secrets User role. Click Next.\nOn the Members tab, select a User, group, or service principal.\nClick Select members.\nSearch for and select the Databricks Resource Provider service principal.\nClick Select.\nUnder the Members header, verify that you selected the Databricks Resource Provider. Click Review + assign.\nStep 3 - Set up Azure MySQL database\nFrom the home page of your Azure portal, select Create a resource.\nSearch for and select Azure Database for MySQL.\nOn the Azure Database for MySQL resource page, select the Azure Database for MySQL plan and click Create.\nSelect Single server and click Create.\nOn the Basic tab, select an existing subscription and resource group, enter server details and create an administrator account. Due to the configurations required for the setup, Data Lakes supports MySQL version 5.7 only. Before you proceed, please ensure you have the correct MySQL server version selected.\nClick Review + create.\nReview your chosen settings. When you are satisfied with your selections, click Create.\nAfter your resource is deployed, click Go to resource.\nFrom the resource page, select the Connection security tab.\nUnder the Firewall rules section, select Yes to allow access to Azure services, and click the Allow current client IP address (xx.xxx.xxx.xx) button to allow access from your current IP address.\nClick Save to save the changes you made on the Connection security page, and select the Server parameters tab.\nUpdate the lower_case_table_names value to 2, and click Save.\nSelect the Overview tab and click the Restart button to restart your database. Restarting your database updates the lower_case_table_name setting.\nOnce the server restarts successfully, open your Azure CLI.\nSign into the MySQL server from your command line by entering the following command:\n  mysql --host=/[HOSTNAME] --port=3306 --user=[USERNAME] --password=[PASSWORD]\n\nRun the CREATE DATABASE command to create your Hive Metastore:\n  CREATE DATABASE <name>;\n\n\nBefore continuing, note the MySQL server URL, username and password for the admin account, and your database name: you\u2019ll need this information when configuring the Segment Data Lakes (Azure) destination in the Segment app.\n\nStep 4 - Set up Databricks\n\nDatabricks pricing tier\n\nIf you create a Databricks instance only for Segment Data Lakes (Azure) usage, only the standard pricing tier is required. However, if you use your Databricks instance for other applications, you may require premium pricing.\n\nFrom the home page of your Azure portal, select Create a resource.\nSearch for and select Azure Databricks.\nOn the Azure Database for MySQL resource page, select the Azure Databricks plan and click Create.\nOn the Basic tab, select an existing subscription and resource group, enter a name for your workspace, select the region you\u2019d like to house your Databricks instance in, and select a pricing tier. For those using the Databricks instance only for Segment Data Lakes (Azure), a Standard pricing tier is appropriate. If you plan to use your Databricks instance for more than just Segment Data Lakes (Azure), you may require the premium pricing tier.\nClick Review + create.\nReview your chosen settings. When you are satisfied with your selections, click Create.\nAfter your resource is deployed, click Go to resource.\nOn the Azure Databricks Service overview page, click Launch Workspace.\nOn the Databricks page, select Create a cluster.\nOn the Compute page, select Create Cluster.\nEnter a name for your cluster and select the Standard_DS4_v2 worker type. Set the minimum number of workers to 2, and the maximum number of workers to 8. Segment recommends deselecting the \u201cTerminate after X minutes\u201d setting, as the time it takes to restart a cluster may delay your Data Lake syncs.\nClick Create Cluster.\nOpen your Azure portal and select the Key Vault you created in a previous step.\nOn the Key Vault page, select the JSON View link to view the Resource ID and vaultURI. Take note of these values, as you\u2019ll need them in the next step to configure your Databricks instance.\nOpen https://<databricks-instance>#secrets/createScope and enter the following information to connect your Databricks instance to the Key Vault you created in an earlier step:\nScope Name: Set this value to segment.\nManage Principal: Select All Users.\nDNS Name: Set this value to the Vault URI of your Key Vault instance.\nResource ID: The Resource ID of your Azure Key Vault instance.\nWhen you\u2019ve entered all of your information, click Create.\n\nBefore continuing, note the Cluster ID, Workspace name, Workspace URL, and the Azure Resource Group for your Databricks Workspace: you\u2019ll need this information when configuring the Segment Data Lakes (Azure) destination in the Segment app.\n\nStep 5 - Set up a Service Principal\nOpen the Databricks instance you created in Step 4 - Set up Databricks.\nClick Settings and select User settings.\nOn the Access tokens page, click Generate new token.\nEnter a comment for your token, select the lifetime of your ticket, and click Generate.\nCopy your token, as you\u2019ll use this to add your service principal to your workspace.\nOpen your Azure CLI and create a new service principal using the following commands:\n\naz login\naz ad sp create-for-rbac --name <ServicePrincipalName>\n\nIn your Azure portal, select the Databricks instance you created in Step 4 - Set up Databricks.\nOn the overview page for your Databricks instance, select Access control (IAM).\nClick Add and select Add role assignment.\nOn the Roles tab, select the Managed Application Operator role. Click Next.\nOn the Members tab, select a User, group, or service principal.\nClick Select members.\nSearch for and select the Service Principal you created above.\nClick Select.\nUnder the Members header, verify that you selected your Service Principal. Click Review + assign.\nReturn to the Azure home page. Select your storage account.\nOn the overview page for your storage account, select Access control (IAM).\nClick Add and select Add role assignment.\nOn the Roles tab, select the Storage Blob Data Contributor role. Click Next.\nOn the Members tab, select a User, group, or service principal.\nClick Select members.\nSearch for and select the Service Principal you created above.\nClick Select.\nUnder the Members header, verify that you selected your Service Principal. Click Review + assign.\nOpen your Key Vault. In the sidebar, select Secrets.\nClick Generate/Import.\nOn the Create a secret page, select Manual. Enter the name spsecret for your secret, and enter the name of the secret you created in Databricks in the Value field.\nFrom your Azure CLI, call the Databricks SCIM API to add your service principal to your workspace, replacing <per-workspace-url> with the URL of your Databricks workspace, <personal-access-token> with the access token you created in an earlier step, and <application-id> with the client ID of your service principal:\n\ncurl -X POST 'https://<per-workspace-url>/api/2.0/preview/scim/v2/ServicePrincipals' \\\n  --header 'Content-Type: application/scim+json' \\\n  --header 'Authorization: Bearer <personal-access-token>' \\\n  --data-raw '{\n\"schemas\":[\n  \"urn:ietf:params:scim:schemas:core:2.0:ServicePrincipal\"\n],\n\"applicationId\":\"<application-id>\",\n\"displayName\": \"test-sp\",\n\"entitlements\":[\n  {\n    \"value\":\"allow-cluster-create\"\n  }\n]\n  }'\n\nOpen Databricks and navigate to your cluster. Select Permissions.\nIn the permissions menu, grant your service principal Can Manage permissions.\n\nBefore continuing, note the Client ID and Client Secret for your Service Principal: you\u2019ll need this information when configuring the Segment Data Lakes (Azure) destination in the Segment app.\n\nStep 6 - Configure Databricks Cluster\n\nOptional configuration settings for log4j vulnerability\n\nWhile Databricks released a statement that clusters are likely unaffected by the log4j vulnerability, out of an abundance of caution, Databricks recommends updating to log4j 2.15+ or adding the following options to the Spark configuration:\nspark.driver.extraJavaOptions \"-Dlog4j2.formatMsgNoLookups=true\"\nspark.executor.extraJavaOptions \"-Dlog4j2.formatMsgNoLookups=true\"\n\nConnect to a Hive metastore on your Databricks cluster using the following Spark configuration, replacing the variables (<example_variable>) with information from your workspace:\n\n## Configs so we can read from the storage account\nspark.hadoop.fs.azure.account.oauth.provider.type.<storage_account_name>.dfs.core.windows.net org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\nspark.hadoop.fs.azure.account.oauth2.client.endpoint.<storage_account_name>.dfs.core.windows.net https://login.microsoftonline.com/<azure-tenant-id>/oauth2/token\nspark.hadoop.fs.azure.account.oauth2.client.secret.<storage_account_name>.dfs.core.windows.net <service-principal-secret>\nspark.hadoop.fs.azure.account.auth.type.<storage_account_name>.dfs.core.windows.net OAuth\nspark.hadoop.fs.azure.account.oauth2.client.id.<storage_account_name>.dfs.core.windows.net <service_principal_client_id>\n##\n##\nspark.hadoop.javax.jdo.option.ConnectionDriverName org.mariadb.jdbc.Driver\nspark.hadoop.javax.jdo.option.ConnectionURL jdbc:mysql://<db-host>:<port>/<database-name>?useSSL=true&requireSSL=true&enabledSslProtocolSuites=TLSv1.2\nspark.hadoop.javax.jdo.option.ConnectionUserName <database_user>\nspark.hadoop.javax.jdo.option.ConnectionPassword <database_password>\n##\n##\n##\nspark.hive.mapred.supports.subdirectories true\nspark.sql.storeAssignmentPolicy Legacy\nmapreduce.input.fileinputformat.input.dir.recursive true\nspark.sql.hive.convertMetastoreParquet false\n##\ndatanucleus.autoCreateSchema true\ndatanucleus.autoCreateTables true\nspark.sql.hive.metastore.schema.verification false\ndatanucleus.fixedDatastore false\n##\nspark.sql.hive.metastore.version 2.3.7\nspark.sql.hive.metastore.jars builtin\n\nLog in to your Databricks instance and open your cluster.\nOn the overview page for your cluster, select Edit.\nOpen the Advanced options toggle and paste the Spark config you copied above, replacing the variables (<example_variable>) with information from your workspace.\nSelect Confirm and restart. On the popup window, select Confirm.\nLog in to your Azure MySQL database using the following command:\n\nmysql --host=[HOSTNAME] --port=3306 --user=[USERNAME] --password=[PASSWORD]\n\nOnce you\u2019ve logged in to your MySQL database, run the following commands:\n\nUSE <db-name>\nINSERT INTO VERSION (VER_ID, SCHEMA_VERSION) VALUES (0, '2.3.7');\n\nLog in to your Databricks cluster.\nClick Create and select Notebook.\nGive your cluster a name, select SQL as the default language, and make sure it\u2019s located in the cluster you created in Step 4 - Set up Databricks.\nClick Create.\nOn the overview page for your new notebook, run the following command:\n\nCREATE TABLE test (id string);\n\nOpen your cluster.\nOn the overview page for your cluster, select Edit.\nOpen the Advanced options toggle and paste the following code snippet:\n\ndatanucleus.autoCreateSchema false\ndatanucleus.autoCreateTables false\nspark.sql.hive.metastore.schema.verification true\ndatanucleus.fixedDatastore true\n\nSelect Confirm and restart. On the popup window, select Confirm.\nStep 7 - Enable the Data Lakes destination in the Segment app\n\nAfter you set up the necessary resources in Azure, the next step is to set up the Data Lakes destination in Segment:\n\nIn the Segment App, click Add Destination.\nSearch for and select Segment Data Lakes (Azure).\nClick the Configure Data Lakes button, and select the source you\u2019d like to receive data from. Click Next.\nIn the Connection Settings section, enter the following values:\nAzure Storage Account: The name of the Azure Storage account that you set up in Step 1 - Create an ALDS-enabled storage account.\nAzure Storage Container: The name of the Azure Storage Container you created in Step 1 - Create an ALDS-enabled storage account.\nAzure Subscription ID: The ID of your Azure subscription.\nPlease add it as it is in the Azure portal, in the format ********-****-****-****-************\nAzure Tenant ID: The Tenant ID of your Azure Active directory.\nPlease add it as it is in the Azure portal, in the format ********-****-****-****-************\nDatabricks Cluster ID: The ID of your Databricks cluster.\nDatabricks Instance URL: The ID of your Databricks workspace.\nThe correct format for adding the URL is \u2018adb-0000000000000000.00.azureatabricks.net\u2019\nDatabricks Workspace Name: The name of your Databricks workspace.\nDatabricks Workspace Resource Group: The resource group that hosts your Azure Databricks instance. This is visible in Azure on the overview page for your Databricks instance.\nRegion: The location of the Azure Storage account you set up in Step 1 - Create an ALDS-enabled storage account.\nService Principal Client ID: The Client ID of the Service Principal that you set up in Step 5 - Set up a Service Principal.\nService Principal Client Secret: The Secret for the Service Principal that you set up in Step 5 - Set up a Service Principal.\n(Optional) Set up your Segment Data Lake (Azure) using Terraform\n\nInstead of manually configuring your Data Lake, you can create it using the script in the terraform-segment-data-lakes GitHub repository.\n\nThis script requires Terraform versions 0.12+.\n\nBefore you can run the Terraform script, create a Databricks workspace in the Azure UI using the instructions in Step 4 - Set up Databricks. Note the Workspace URL, as you will need it to run the script.\n\nIn the setup file, set the following local variables:\n\n\nlocals {\nregion         = \"<segment-datlakes-region>\"\nresource_group = \"<segment-datlakes-regource-group>\"\nstorage_account = \"<segment-datalake-storage-account\"\ncontainer_name  = \"<segment-datlakes-container>\"\nkey_vault_name = \"<segment-datlakes-key vault>\"\nserver_name = \"<segment-datlakes-server>\"\ndb_name     = \"<segment-datlakes-db-name>\"\ndb_password = \"<segment-datlakes-db-password>\"\ndb_admin    = \"<segment-datlakes-db-admin>\"\ndatabricks_workspace_url = \"<segment-datlakes-db-worspace-url>\"\ncluster_name   = \"<segment-datlakes-db-cluster>\"\ntenant_id      = \"<tenant-id>\"\n}\n\n\nAfter you\u2019ve configured your local variables, run the following commands:\n\nterraform init\nterraform plan\nterraform apply\n\n\nRunning the plan command gives you an output that creates 19 new objects, unless you are reusing objects in other Azure applications. Running the apply command creates the resources and produces a service principal password you can use to set up the destination.\n\nFAQ\nSegment Data Lakes\nDo I need to create Glue databases?\n\nNo, Data Lakes automatically creates one Glue database per source. This database uses the source slug as its name.\n\nWhat IAM role do I use in the Settings page?\n\nFour roles are created when you set up Data Lakes using Terraform. You add the arn:aws:iam::$ACCOUNT_ID:role/segment-data-lake-iam-role role to the Data Lakes Settings page in the Segment web app.\n\nWhat level of access do the AWS roles have?\n\nThe roles which Data Lakes assigns during set up are:\n\nsegment-datalake-iam-role - This is the role that Segment assumes to access S3, Glue and the EMR cluster. It allows Segment access to:\nGet, create, delete access to the Glue catalog. Note that this does not provide access to Glue ETL or Glue crawlers.\nAccess only to the specific S3 bucket used for Data Lakes.\nEMR access only to the clusters having the vendor=segment tag\n\nsegment_emr_service_role - Restricted role that can only be assumed by the EMR service. This is set up based on AWS best practices.\n\nsegment_emr_instance_profile_role - Role that is assumed by the applications running on the EMR cluster. Based on AWS best practices, it allows Segment access to:\nGet, create, delete access to the Glue catalog. Note that this does not provide access to Glue ETL or Glue crawlers.\nAccess only to the specific S3 bucket used for Data Lakes.\nsegment_emr_autoscaling_role - Restricted role that can only be assumed by EMR and EC2. This is set up based on AWS best practices.\nWhy doesn\u2019t the Data Lakes Terraform module create an S3 bucket?\n\nThe module doesn\u2019t create a new S3 bucket so you can re-use an existing bucket for your Data Lakes.\n\nDoes my S3 bucket need to be in the same region as the other infrastructure?\n\nYes, the S3 bucket and the EMR cluster must be in the same region.\n\nHow do I connect a new source to Data Lakes?\n\nTo connect a new source to Data Lakes:\n\nEnsure that the workspace_id of the Segment workspace is in the list of external ids in the IAM policy. You can either update this from the AWS console, or re-run the Terraform job.\nFrom your Segment workspace, connect the source to the Data Lakes destination.\nCan I configure multiple sources to use the same EMR cluster?\n\nYes, you can configure multiple sources to use the same EMR cluster. Segment recommends that the EMR cluster only be used for Data Lakes to ensure there aren\u2019t interruptions from non-Data Lakes job.\n\nWhy don\u2019t I see any data in S3 or Glue after enabling a source?\n\nIf you don\u2019t see data after enabling a source, check the following:\n\nDoes the IAM role have the Segment account ID and workspace ID as the external ID?\nIs the EMR cluster running?\nIs the correct IAM role and S3 bucket configured in the settings?\n\nIf all of these look correct and you\u2019re still not seeing any data, please contact the Support team.\n\nWhat are \u201cSegment Output\u201d tables in S3?\n\nThe output tables are temporary tables Segment creates when loading data. They are deleted after each sync.\n\nCan I make additional directories in the S3 bucket Data Lakes is using?\n\nYes, you can create new directories in S3 without interfering with Segment data. Do not modify, or create additional directories with the following names:\n\nlogs/\nsegment-stage/\nsegment-data/\nsegment-logs/\nWhat does \u201cpartitioned\u201d mean in the table name?\n\nPartitioned just means that the table has partition columns (day and hour). All tables are partitioned, so you should see this on all table names.\n\nHow can I use AWS Spectrum to access Data Lakes tables in Glue, and join it with Redshift data?\n\nYou can use the following command to create external tables in Spectrum to access tables in Glue and join the data with Redshift:\n\nRun the CREATE EXTERNAL SCHEMA command:\n\ncreate external schema [spectrum_schema_name]\nfrom data catalog\ndatabase [glue_db_name]\niam_role arn:aws:iam::[account_id]:role/MySpectrumRole\ncreate external database if not exists;\n\n\nReplace:\n\n[glue_db_name] = The Glue database created by Data Lakes which is named after the source slug\n[spectrum_schema_name] = The schema name in Redshift you want to map to\nSegment Data Lakes (Azure)\nDoes my ALDS-enabled storage account need to be in the same region as the other infrastructure?\n\nYes, your storage account and Databricks instance should be in the same region.\n\nWhat analytics tools are available to use with my Segment Data Lake (Azure)?\n\nSegment Data Lakes (Azure) supports the following post-processing tools:\n\nPowerBI\nAzure HDInsight\nAzure Synapse Analytics\nDatabricks\nWhat can I do to troubleshoot my Databricks database?\n\nIf you encounter errors related to your Databricks database, try adding the following line to the config:\n\n\nspark.sql.hive.metastore.schema.verification.record.version false\n\n\n\nAfter you\u2019ve added to your config, restart your cluster so that your changes can take effect. If you continue to encounter errors, contact Segment Support.\n\nWhat do I do if I get a \u201cVersion table does not exist\u201d error when setting up the Azure MySQL database?\n\nCheck your Spark configs to ensure that the information you entered about the database is correct, then restart the cluster. The Databricks cluster automatically initializes the Hive Metastore, so an issue with your config file will stop the table from being created. If you continue to encounter errors, contact Segment Support.\n\nThis page was last modified: 15 Aug 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSet up Segment Data Lakes (AWS)\nSet up Segment Data Lakes (Azure)\nFAQ\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSources\n/\nUsing the Source Debugger\nUsing the Source Debugger\n\nThe Source Debugger is a real-time tool that helps you confirm that API calls made from your website, mobile app, or servers arrive to your Segment Source, so you can troubleshoot your Segment set up even quicker. With the Debugger, you can check that calls are sent in the expected format without having to wait for any data processing.\n\nThe Source Debugger\u2019s event order may not reflect how events send downstream or are received by connected destinations. The Debugger primarily confirms incoming data and provides a basic view of its structure. For a reliable record of the data you send to Segment, Segment advises you to attach a raw storage destination to your sources.\n\nThe Debugger is separate from your workspace\u2019s data pipeline and is not an exhaustive view of all the events ever sent to your Segment workspace. The Debugger only shows a sample of the events that the Source receives in real time, with a cap of 500 events. The Debugger is a great way to test specific parts of your implementation to validate that events are being fired successfully and arriving to your Source.\n\nTo see a more complete view of all your events, Segment recommends that you set up a warehouse or an S3 destination.\n\nThe Debugger shows a live stream of sampled events arriving into the Source, but you can also pause the stream from displaying new events by toggling \u201cLive\u201d to \u201cPause\u201d. Events continue to arrive to your Source while you Pause the stream.\n\nYou can search in the Debugger to find a specific payload using any information you know is available in the event\u2019s raw payload. You can also use advanced search options to limit the results to a specific event.\n\nTwo views are available when viewing a payload:\n\nThe Pretty view is an approximate recreation of the API call you made that was sent to Segment. The format shown depends on the library used at the source, and the data displayed may not account for a particular workspace\u2019s unique configuration settings (for example, the region in the API request).\nThe Raw view is the complete JSON object Segment received from the calls you sent. These calls include all the details about what is being tracked: timestamps, properties, traits, ids, and contextual information Segment automatically collects the moment the data is sent.\n\nThis page was last modified: 11 Apr 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nProtocols\n/\nValidate\n/\nForward Violations\nForward Violations\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nPROTOCOLS \u2713\n?\n\nYou can forward Violations (data that does not conform to your Protocols tracking plan) to a Segment Source to enable custom notifications, dashboards and further analysis in any Segment destination that accepts cloud-mode data.\n\nTo set up forwarding, navigate to the settings tab of the Source, then Schema Configuration. Select the source you\u2019ll forward events to from the Forwarding Settings Violations dropdown. Similar to Blocked Event forwarding, Segment recommends that you create a new Source for violations.\n\nViolations are sent to the selected Source as analytics.track() calls. The call payload includes the following properties, along with the context.app and context.library objects to aid in filtering violations.\n\n    {\n      \"context\": {\n        \"app\": null,\n        \"library\": {\n          \"name\": \"analytics-node\",\n          \"version\": \"2.1.0\"\n        }\n      },\n      \"event\": \"Violation Generated\",\n      \"integrations\": {},\n      \"messageId\": \"sch-Vir1JNrorwlBmCHrHWuPJekMx5c59T7c\",\n      \"properties\": {\n        \"appVersion\": \"\",\n        \"eventMessageID\": \"node-YhqA1DK9mgV55INDzOSd542fCFbIkc1o\",\n        \"eventName\": \"Order Completed\",\n        \"eventSentAt\": \"2018-08-23T21:35:02.85860964Z\",\n        \"eventTimestamp\": \"0001-01-01T00:00:00Z\",\n        \"eventType\": \"track\",\n        \"sourceID\": \"dInN1HJ4bi\",\n        \"sourceName\": \"Acme Store\",\n        \"sourceSlug\": \"acme_store_test\",\n        \"trackingPlanID\": \"rs_16KrwVbouFLrYbDkGh4LNslkrNp\",\n        \"trackingPlanName\": \"Acme Ecommerce TP\",\n        \"violationDescription\": \"properties.products is required\",\n        \"violationField\": \"properties.products\",\n        \"violationType\": \"Required\"\n      },\n      \"receivedAt\": \"2018-09-05T23:05:25.862826965Z\",\n      \"timestamp\": \"2018-09-05T23:05:25.862826855Z\",\n      \"type\": \"track\",\n      \"userId\": \"schema-violations\",\n      \"forwardedFromProject\": \"dInN1HJ4bi\"\n    }\n\n\nBilling Note: Enabling Violation forwarding generates one (1) additional MTU in your workspace, total. If you are on an API billing plan, you are charged for the increased API volume generated by the forwarded violations.\n\nSchema and debugger Note:Violation Generated events do not appear in the source\u2019s Schema tab. They do appear as Violation Generated events in the debugger.\n\nThis page was last modified: 03 Aug 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nTrait Activation\n/\nID Sync\nID Sync\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nUse ID Sync to select identifiers and a sync strategy when you send Audience or Journeys data to your destinations or destination functions. Configure how you send identifiers, which provides more control over the data you send downstream.\n\nOn this page, you\u2019ll learn how to configure and begin using ID Sync.\n\nSet up ID Sync\n\nUse the following steps to set up ID Sync with Audiences or Journeys.\n\nSet up ID Sync with Audiences\n\nTo set up ID Sync with Audiences:\n\nNavigate to Engage > Audiences.\nCreate a new Audience. From the Select Destination tab in the Audience builder, select your destination.\nIf you don\u2019t see any destinations to add, you\u2019ll need to add the destination or destination function to your Engage space first.\nFor existing audiences, you\u2019ll find your connected destination on the Audience Overview page.\nIn the Event Settings section, you\u2019ll see two options: Default Setup and Customized Setup. To use ID Sync, select Customized Setup.\nSet up ID Sync with Journeys\n\nYou can configure ID Sync with Journeys as you\u2019re creating or editing your journey in the builder.\n\nFrom a journey step, select the destination you\u2019re going to use with ID Sync.\nOn the Connection Settings tab, select Customized Setup and use the corresponding steps below to customize which identifiers you want to map downstream to your destination.\nDefault setup\n\nDefault setup uses default Segment Destination behavior. To use the default settings, click Save and resume building your audience or journey.\n\nYou can customize additional event settings at any time.\n\nCustomized setup\n\nWith Customized setup, you can choose which identifiers you want to map downstream to your destination.\n\nReview your settings before configuring an ID strategy\n\nIf you want to send ios.idfa as a part of your ID strategy, confirm that you\u2019ve enabled the Send Mobile IDs setting when connecting your destination to an audience or journey.\n\nUsing Customized Setup, click + Add Identifier and add the identifiers:\nSegment: Choose your identifiers from Segment.\nDestination: Choose which identifiers you want to map to from your destination. If the destination doesn\u2019t contain the property, then outgoing events may not be delivered.\nFacebook Custom Audiences and Google Ads Remarketing Lists display a dropdown for you to choose available identifiers.\nAdd an ID strategy.\nThis is a strategy for a particular identifier which sends either the last added, first added, or all identifiers to your destination.\nClick Save, then finish building your audience or journey.\nLimits and best practices\nSegment recommends using ID Sync with new audiences.\nID sync configuration changes apply to new data flowing after about five minutes. Changes don\u2019t apply to active or running syncs.\nID Sync used on existing audience destinations or destination functions won\u2019t resync the entire audience. Only new data flowing into Segment follows your ID Sync configuration.\nSegment doesn\u2019t maintain ID Sync history, which means that any changes are irreversible.\nYou can only select a maximum of three identifiers with an All strategy.\nFAQs\nWhat\u2019s the difference between Trait Enrichment and ID Sync?\n\nTrait Enrichment lets you map the traits data you\u2019ve collected with Engage to use when syncing audiences and Journeys to destinations and destination functions.\n\nID Sync lets you map the identities data gathered for a profile for use when syncing audiences and Journeys to destinations and destination functions.\n\nHow do syncs differ between audiences with ID Sync and audiences without ID Sync?\n\nAudiences without ID Sync aren\u2019t allowed to select any strategy, and by default will send all values of an identifier to the destination. Also, audiences without ID Sync don\u2019t send any custom identifiers that are present in your space.\n\nCan I edit config once the audience has synced?\n\nYes, you can edit configuration in the Destination Settings tab at any time. However, changes will only take place in subsequent audience syncs, or in new audiences connected to the destination.\n\nThis page was last modified: 26 Sep 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSet up ID Sync\nLimits and best practices\nFAQs\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nSegment Glossary\nSegment Glossary\n\nAnalytics.js\n\nThe Segment Javascript library wrapper, which makes it simple to send your data to any tool without having to learn, test or implement the new API for each tool every time.\n\nAPI\n\nApplication Programming Interface. In software, this term describes a way of interacting with software in an automated fashion, without the use of a user-interface designed for humans. In Segment, we call the standard calls (track, page and screen, identify and group) our API. We also provide the Public API, which you can use to configure sources and destinations, and perform other functions in your workspace, without logging in using a web browser.\n\nApp\n\nThe app is what we call the main Segment web application, the part you log in to, and where you configure sources and destinations, see events in the debugger, manage your tracking plan, and so on.\n\nAsynchronous\n\nAsynchronous means occurring without a specific order or sequence. In engineering, it\u2019s most often used for things like when you have to ask a server for a piece of information. If that was a synchronous task, while you were waiting for the server to respond you wouldn\u2019t be able to do anything else, you\u2019d just be stuck waiting. But since it\u2019s asynchronous, you can request information from a server, continue to go do a bunch of other tasks, and then when the server responds you can get back to what you were doing to start. Basically it lets you work on multiple things at once (in parallel) instead of needing to finish everything in the order it comes in. Think of it like one of the ways computers are able to multi-task.\n\nAudience\n\nPersonas Audiences allow you to define cohorts of users or accounts based on their event behavior and traits that Segment then keeps up-to-date over time. Audiences can be built from your core tracking events, traits, or computed traits. These audiences can then be synced to hundreds of destinations or available using the Profile API.\n\nAWS\n\nAmazon Web Services, a large cloud service provider.\n\nCatalog\n\nSegment\u2019s list of available sources, destinations, and warehouses. You can access the catalog from the Segment website, and from inside the Segment app.\n\nCDN\n\nContent Delivery Network. CDNs are a network of servers which make downloading files faster for the user, by placing them all around the world to reduce data transit time. When you have servers in a CDN, they\u2019re much closer on average to the end user. If someone is using Segment in Asia, they don\u2019t have to download our files all the way from California, they can get them from a server much closer to them in the network.\n\nCDP\n\nCustomer Data Platform: a central platform that brokers the flow of customer data through their application infrastructure so that they can build a central understanding of each customer\u2019s interactions, comply with regulations that require them to carefully manage customer data, and ensure that their interactions with the customer are optimized.\n\nCookies\n\nCookies are small pieces of text that are stored by the browser on a website. Cookies have a name and a value. They may also be set from Javascript using document.cookie. Our analytics.js script sets up a cookie called ajs_uid to store the user\u2019s id.\n\nEvery time you make a request to a website, you send along the cookies that you have stored. It\u2019s how the website figures out who you should be logged in as. You can think of it sort of like a passport as a form of ID. When you first enter your username and password, the website generates a secure cookie, and tells your browser to store it. From then on, your browser sends along the cookie as a way of identifying yourself to the website.\n\nClient Side\n\nThis refers to a group of libraries that can send data to Segment and which send Device-mode data. The term \u201cclient side\u201d is usually used in contrast to Server Side.\n\nCloud mode\n\nCloud mode, (or Server-side) libraries send data to Segment, where it\u2019s then sent on to destinations. Usually used in contrast to Device mode libraries, which send the data directly to the destination\u2019s API endpoints.\n\nCloud mode libraries run on a server and are completely invisible to end users. You can find the full list of server-side languages Segment supports in the Sources catalog. These libraries can be used to track a user in realtime as they use your app, but are also able to run batches of calls to update many end users at once. Cloud-mode libraries do not maintain \u201cstate\u201d which means every API call you make must include every detail you want to see inside of Segment.\n\nComputed trait\n\nComputed traits are per-user or per-account traits that you create or \u201ccompute\u201d in Segment. When you build a computed trait, Segment adds it to relevant user profiles.\n\nCustom trait\n\nCustom traits are user or account traits collected from the Identify calls you send to Segment. For example, you can add any trait that you send as a part of your Identify calls (email, first_name, last_name) as custom traits. You can then view them in the Profile explorer, and use them in your audiences, computed traits, and SQL traits.\n\nDebugger\n\nThis is the place in the app where you can see your events flowing through Segment.\n\nDestination\n\nA destination is a target for Segment to forward data to, and represents a tool or storage destination.\n\nDevice mode\n\nDevice mode, (or client-side) libraries are loaded on the on the user\u2019s client (for example their web browser, or mobile device), which means they can collect contextual data about the user. The three main libraries that are considered \u201cdevice mode\u201d are Analytics.js, which is used on websites, our iOS SDK, and our Android SDK. Device mode libraries can maintain a cache of information about each user on the device so we know that device belongs to a consistent anonymousId or userId, and has a consistent list of traits like name, email, etc.\n\nDMP\n\nData management platform\n\nDSP\n\nDemand-side platform\n\nETL\n\nExtract, Transform and Load. Referring to the process of extracting data from a production system, transforming it with enriching data into a new format, and then loading it into a data warehouse for analysis (a separate database from production).\n\nEvent\n\nAn event can refer to either an action by a user, or something a user does which triggers a track call. Events have a name (for example \u201cProduct viewed\u201d) and properties (for example the product name, price, and category), and take place in a single moment in time. (This is in contrast to \u201cobjects\u201d which persist over a period of time, and which have properties that might change.)\n\nIdentify\n\nA call that gathers information about who the user is.\n\nJSON\n\nJSON, or JavaScript Object Notation, is a convenient format for storing structured data.\n\nLibrary\n\nA library is a reusable piece of code which acts as a building block for higher level pieces of code. It\u2019s the software equivalent of a \u2018tool\u2019. For instance, take Instagram filters. In the good old days, to take a picture with an interesting filter, you had to adjust the aperture length, the focal length, the exposure and film speed to get the desired effect. But now, you can just use Instagram, and press a button to get the pre-tuned filter that you want. They\u2019ve packaged those effects in software and made them re-usable.\n\nThe most common use case for this term at Segment is in reference to the libraries people use to send data to our API. A full list of libraries can be found in our docs. Libraries can collect either in \u201cDevice mode\u201d or \u201cCloud mode\u201d.\n\nLookback\n\nA \u201clookback window\u201d limits the period of time in which data is considered when calculating a trait or audience. For example, you might set a lookback window of 7 days on an audience or trait like new_users_7_days, but you would not add a lookback window to a trait that isn\u2019t time-bounded, such as lifetime_value.\n\nMethod\n\nA method is programming speak for \u201can action an object can take\u201d. It\u2019s a specific type of function that is attached to an object. All of our analytics tracking libraries create analytics objects, and then track, identify, page, etc are methods you can invoke on those objects.\n\nMTU\n\nMonthly Tracked Users. You can find more about how they are calculated here but in short, they are calculated by adding the number unique userIds and number of unique anonymousIds that a customer tracks with Segment.\n\nObject\n\nAn object is a type of data that persists over time and can be updated, for example a Business or a User record. Objects have \u201ctraits\u201d which record information about that object, and which can change over time. For example a \u201cuser\u201d object could have a trait of \u201cemail\u201d which doesn\u2019t change often, but could also have a computed trait like logged_in_last_7_days. (Objects are in contrast to \u201cevents\u201d which happen at a single moment in time, and whose properties do not change.)\n\nOTT (Over the Top)\n\nOver the top (OTT) refers to content providers that distribute streaming media as a standalone product directly to viewers over the Internet, bypassing telecommunications, multichannel television, and broadcast television platforms that traditionally act as a controller or distributor of such content.\n\nPage\n\nThe Segment API call records that a user \u201cviewed a page\u201d. It\u2019s recorded by calling analytics.page(). You can optionally pass in a name and a category.\n\nPostgres\n\nAn open-source SQL server.\n\nRedshift\n\nAn analytics data warehouse from Amazon Web Services. Made for loading in tons of event data, and then analyzing it with complex queries. It\u2019s designed to be fast and cheap.\n\nRetry\n\nHow often we attempt to redeliver a payload before marking it as failed.\n\nReplay\n\nThe ability to re-send your user data to new destinations and tools.\n\nSchema\n\nA schema is a word used to refer to the design of a database, including what fields there are, and what data types each one contains. If you were to think of a database as a spreadsheet, the schema would be the top row with the field labels, plus the data types and formatting rules for each column.\n\nIn Segment, you can send data to a Warehouse which has a schema based on the types of data you collect and route through Segment, and which updates as the data you collect changes.\n\nServer Side\n\nRefers to a group of libraries that can send data to Segment, in Cloud mode, meaning through the Segment cloud. \u201cServer side\u201d can could refer to people sending data directly to our HTTP API without using a \u201clibrary\u201d. Usually used in contrast to Client Side.\n\nSDK\n\nSoftware Development Kit. This is a combination of libraries and used mostly in the context of building mobile or native apps.\n\nSQL\n\n(pronounced \u2018sequel\u2019) Structured Query Language. The standard language for retrieving information from a database. It\u2019s often called \u201crelational\u201d because all the queries are related to relations between the data. For instance, find me all users who are named \u201cAlice\u201d would translate to\u2026\n\nselect * from users where first_name equals \"Alice\"\n\nSQL Trait\n\nSQL traits are per-user or per-account traits that you create by running queries against your data warehouse, which can include data not captured using your Segment implementation. Segment imports the results into Personas, and appends these traits to the user profile.\n\nSource\n\nA website, server library, mobile SDK, or cloud application which can send data into Segment.\n\nSpec\n\nShort for \u201cSpecification\u201d, the Segment Spec is our set of recommendations for what data to collect and how to format it.\n\nSSP\n\nSupply-side Platform\n\nThroughput\n\nThe number of API calls and Objects your account makes, per MTU. The limit on this number is defined by your account plan.\n\nTrack\n\nType of API call (method) which records any actions that users perform on a web or mobile app.\n\nTracking Plan\n\nA tracking plan is a tool that our customers use to keep track of their analytics setup. It contains all of the events, pages, traits and properties they want to record with information about how each is laid out. It\u2019s often in a spreadsheet or document, but we\u2019re actually building the functionality into the core product so that people can plan out their tracking setup right from inside Segment.\n\nTraits\n\nTraits are individual pieces of information that we know about an object. Objects persist over time, so traits can change over time.\n\nWarehouses\n\nSegment\u2019s Data warehouse product. All event data can be sent to one of several Warehouses, where your team can use it to perform more complex analysis of the data.\n\nThis page was last modified: 05 Dec 2019\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nReverse Etl\n/\nReverse Etl Source Setup Guides\n/\nAzure Reverse ETL Setup\nAzure Reverse ETL Setup\n\nSet up Azure as your Reverse ETL source.\n\nAt a high level, when you set up Azure dedicated SQL pools for Reverse ETL, the configured user needs read permissions for any resources (databases, schemas, tables) the query needs to access. Segment keeps track of changes to your query results with a managed schema (__SEGMENT_REVERSE_ETL), which requires the configured user to allow write permissions for that schema.\n\nRequired permissions\n\nMake sure the user you use to connect to Segment has permissions to use that warehouse. You can follow the process below to set up a new user with sufficient permissions for Segment\u2019s use.\n\nTo create a login in your master database, run:\n\n  CREATE LOGIN <login name of your choice> WITH PASSWORD = 'Str0ng_password'; -- password of your choice\n\n\nExecute the commands below in the database where your data resides.\n\nTo create a user for Segment, run:\n\n  CREATE USER <user name of your choice> FOR LOGIN <login name of your choice>;\n\n\nTo grant access to the user to read data from all schemas in the database, run:\n\n  EXEC sp_addrolemember 'db_datareader', '<user name of your choice>';\n\n\nTo grant Segment access to read from certain schemas, run:\n\n  CREATE ROLE <role name of your choice>;\n  GRANT SELECT ON SCHEMA::[schema_name] TO <role name of your choice>;\n  EXEC sp_addrolemember '<role name of your choice>', '<user name of your choice>';\n\n\nTo grant Segment access to create a schema to keep track of the running syncs, run:\n\n  GRANT CREATE SCHEMA TO <user name of your choice>;\n\n\nIf you want to create the schema yourself and then give Segment access to it, run:\n\n  CREATE SCHEMA  __segment_reverse_etl;\n  GRANT CONTROL ON SCHEMA::__segment_reverse_etl TO <user name of your choice>;\n  GRANT CREATE TABLE ON DATABASE::[database_name] TO <user name of your choice>;\n\nSet up guide\n\nTo set up Azure as your Reverse ETL source:\n\nLog in to your Azure account.\nNavigate to your dedicated SQL pool. Segment supports both dedicated SQL pool (formerly SQL DW) and dedicated SQL pool in Synapse workspace.\nNavigate to Settings > Connection strings and select the JDBC tab to find the server, port, and database name.\nOpen your Segment workspace.\nNavigate to Connections > Sources and select the Reverse ETL tab.\nClick + Add Reverse ETL source.\nSelect Azure and click Add Source.\nEnter the configuration settings for your Azure source based on the information from Step 3.\nHostname:\nUse xxxxxxx.sql.azuresynapse.net if you\u2019re connecting to a dedicated SQL pool in Synapse workspace.\nUse xxxxxxx.database.windows.net if you\u2019re connecting to a dedicated SQL pool (formerly SQL DW)\nPort: 1433 (default)\nDatabase name: The name of your dedicated SQL pool.\nUsername: The login name you created with CREATE LOGIN in the required permissions section.\nPassword: The password that\u2019s associated with the login name.\nClick Test Connection to see if the connection works. If the connection fails, make sure you have the right permissions and credentials, then try again.\nClick Add source if the test connection is successful.\n\nAfter you\u2019ve successfully added your Azure source, add a model and follow the rest of the steps in the Reverse ETL setup guide.\n\nThis page was last modified: 10 Jun 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nRequired permissions\nSet up guide\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nSegment Documentation\n\nLearn how to use Segment to collect, responsibly manage, and integrate your customer data with hundreds of tools.\n\nGetting started with Segment\n\nLearn about Segment, plan and work through a basic implementation, and explore features and extensions.\n\nHow can Segment help you?\nSimplify data collection\n\nIntegrate the tools you need for analytics, growth, marketing, and more.\n\nProtect data integrity\n\nPrevent data quality issues with a tracking schema and enforcement with Protocols.\n\nPersonalize experiences\n\nBuild audiences and journeys from real-time customer data to personalize experiences on every channel.\n\nRespect users' privacy\n\nKeep customer data private with Segment's data discovery and policy enforcement tools.\n\nGet Data into Segment\n\nThe Segment Spec helps you identify, capture, and format meaningful data for use with Segment libraries and APIs as well as downstream tools.\n\nSegment calls\n\nUse Track, Page, Identify, and other Segment tracking calls.\n\nCommon traits\n\nSave time by letting Segment calls collect information for you.\n\nUse case specs\n\nUse our business-case specs to ensure that your tools get the most from your data.\n\nLearning about Segment\nSegment for Developers\n\nThe basics of your Segment implementation.\n\nHow-To Guides\n\nOver a dozen how-to guides that help you accomplish common tasks.\n\nConnect your app to Segment\nJavaScript\nSwift\nAll other Sources\nAdditional Resources\nTotally new to Analytics?\n\nSegment's Analytics Academy walks you through the wide world of analytics, including best practices, an overview of the most popular tools, and case studies of how other developers have achieved success.\n\nWant more hands-on guidance?\n\nFor a more hands-on tutorial of Segment, check out Segment University. It offers step-by-step instructions, starting with first steps and going through some of our more advanced features.\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account",
        "Log in\nSign Up\nHome\n/\nProtocols\n/\nEnforce\n/\nCustomize your schema controls\nCustomize your schema controls\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nPROTOCOLS \u2713\n?\n\nThe Schema Configuration settings for each source can be used to selectively block events, or omit properties and traits from .track(), .identify() and .group() calls. Segment can permanently drop events that are not included in your Tracking Plan, depending on the settings you select. Segment can also block events with invalid properties or invalid property values.\n\nBlocked events not forwarded to a Source are discarded\n\nBlocking is a serious step that you should only do after you have resolved any violations that appear when you first connect a Tracking Plan to a Source. Any blocked events that are not forwarded to a separate Source are permanently discarded and cannot be recovered.\n\nTo enable blocking, go to the Settings tab for your source and click on Schema Configuration. See below for detailed descriptions for each of the configuration settings.\n\nYou can export your Source Schema as a CSV file to quickly audit events from your Tracking Plan.\n\nArchived events\n\nIf you archive events while your source is connected to a Tracking Plan, and then later disconnect your Tracking Plan from that source, any archived events will remain archived, but will be allowed if the Schema Configuration was previously set to block unplanned events when your Tracking Plan was connected to the source.\n\nTo view all archived events, go to your Source Schema page, click Filter next to the search bar, and select Archived. To unarchive events that have been archived, click Unarchive in the event column.\n\nOrder of Priority in Blocking Options\n\nWhen setting up Schema Configuration, note that Segment prioritizes blocking controls in the following order:\n\nStandard Schema Controls: Segment first evaluates incoming events against these controls and your Tracking Plan. Events, properties, or traits not blocked or omitted in this phase then flow to the next level of controls: the Advanced Blocking Controls/Common JSON Schema.\n\nAdvanced Blocking Controls/Common JSON Schema: These controls act as a secondary layer, evaluating incoming events against the Common JSON schema included in your Tracking Plan.\n\nUsing only the Common JSON Schema to block events\n\nIf your Tracking Plan only has Common JSON Schema rules, you only need to use the Advanced Blocking Controls for your source.\n\nIf you use the Standard Schema Controls and omit properties or traits that do not exist, the Tracking Plan might not generate violations for the Common JSON Schema, as the entire Tracking Plan has nothing and everything is considered to be \u201cunplanned\u201d.\n\nTrack Calls - Unplanned Events\n\nWhen you set this dropdown to Block Event, Segment drops any events that are not defined in your Tracking Plan. Only allowlisted track calls in your Tracking Plan flow through Segment to your Destinations.\n\nFor example, if you include a Subscription Cancelled event in your Tracking Plan, the example track call below would be blocked by Protocols because the event name does not match the event name casing in your Tracking Plan.\n\n    analytics.track('subscription_cancelled')\n\n\nIMPORTANT: Unplanned event blocking is supported across all device-mode and cloud-mode Destinations.\n\nTrack Calls - Unplanned Properties\n\nSetting this dropdown to Omit Properties will ensure that properties not defined in your Tracking Plan are removed from the relevant event.\n\nFor example, if you include a single subscription_id property in the Subscription Cancelled event in your tracking plan, the example track call below would have the subscription_name property omitted by Protocols.\n\n    analytics.track('Subscription Cancelled', {subscription_id: '23r90jfs9ej', subscription_name: 'premium'})\n\n\nIMPORTANT: Unplanned property omission is ONLY supported in cloud-mode Destinations. Unplanned properties will not be omitted when sending to device-mode Destinations.\n\nBlock Track Calls - Common JSON Schema Violations\n\nJSON schema violation event blocking only supports cloud-mode destinations\n\nEvents with invalid properties are not blocked from device-mode destinations.\n\nTo block all Track calls that generate a common JSON schema violation:\n\nIn your Segment workspace, go to Schema Configuration, then click Advanced Blocking Controls and select Block Event from the dropdown.\nEdit the underlying JSON schema and add a rule to the Common JSON Schema definition that you know won\u2019t exist in your Track event.\nTrigger a Track event. Any Track event that generates a common JSON schema violation will be blocked.\n\nSetting the dropdown to Block Event ensures that all Track events with JSON schema violations (for example, missing required properties, incorrect property value data types, or invalid regex patterns) are blocked. A less aggressive option is to select Omit from the dropdown which removes the offending property from the events.\n\nThis is an advanced feature that requires extensive testing and a squeaky clean data set/Tracking Plan to enable. To get a sense of which events will be blocked, or properties omitted, go to the Violations view for a source and note all events with a violation. For example, if you added a subscription_id required property to your Subscription Cancelled event in your Tracking Plan, the below track call would be either blocked by Protocols, or the property would be omitted, depending on your settings.\n\n    analytics.track('Subscription Cancelled', {customer_type: 'enterprise'})\n\n\nIMPORTANT: JSON schema violation event blocking is ONLY supported in cloud-mode Destinations. Events with invalid properties will not be blocked from sending to device-mode Destinations.\n\nIdentify Calls - Unplanned Traits\n\nSetting this dropdown to Omit Traits will ensure that traits not defined in your Tracking Plan are removed from the identify call. For example, if you specify three traits in your Tracking Plan (name, email, join_date), the below identify call would have the first_name property omitted by Protocols.\n\n    analytics.identify('fe923fjid', {email: 'roger@example.com', first_name: 'Roger'})\n\n\nIMPORTANT: Unplanned identify trait blocking is ONLY supported in cloud-mode Destinations. Events with invalid traits will not be blocked from sending to device-mode Destinations.\n\nBlock Identify Calls - Common JSON Schema Violations\n\nJSON schema violation event blocking only supports cloud-mode destinations\n\nEvents with invalid properties are not blocked from device-mode destinations.\n\nTo block all Identify calls that generate a common JSON schema violation:\n\nIn your Segment workspace, go to Schema Configuration, then click Advanced Blocking Controls and select Block Event from the dropdown.\nEdit the underlying JSON schema and add a rule to the Common JSON Schema definition that you know won\u2019t exist in your Identify event.\nTrigger an Identify event. Any Identify event that generates a common JSON schema violation will be blocked. Setting the dropdown to Block Event will ensure that all Identify events with JSON schema violations (for example, missing required traits, incorrect property value data types, or invalid regex patterns) will be blocked. A less aggressive option is to select Omit from the dropdown which will simply remove the offending property from the event.\n\nThis page was last modified: 02 Feb 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nOrder of Priority in Blocking Options\nTrack Calls - Unplanned Events\nTrack Calls - Unplanned Properties\nBlock Track Calls - Common JSON Schema Violations\nIdentify Calls - Unplanned Traits\nBlock Identify Calls - Common JSON Schema Violations\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nIdentity Resolution Overview\nIdentity Resolution Overview\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\nIdentity Graph\n\nIdentity Resolution sits at the core of Segment. The Identity Graph merges the complete history of each customer into a single profile, no matter where they interact with your business. Identity Resolution allows you to understand a user\u2019s interaction across web, mobile, server, and third-party partner touch-points in real time, using an online and offline ID graph with support for cookie IDs, device IDs, emails, and custom external IDs. If you are sending the Group call, you can also understand user behavior at the account-level.\n\nHighlights\nSupports existing data \u2014 no additional code or set up required\nSupports all channels \u2014 stitches web + mobile + server + third party interactions into the same user\nSupports anonymous identity stitching \u2014 by merging child sessions into parent sessions\nSupports user:account relationships - for B2B companies, generates a graph of relationships between users and accounts\nReal-time performance - reliable real-time data stream merges with minimal latency\nTechnical highlights\nSupports custom external IDs - bring your own external IDs\nCustomizable ID Rules \u2014 allows you to enforce uniqueness on select external IDs and customize which external IDs and sources cause associations\nMerge Protection - automatically detects and solves identity issues, like non-unique anonymous IDs and the library problem using the priority trust algorithm\nMaintains persistent ID - multiple external IDs get matched to one persistent ID\nFAQs\nCan I use the Profile API on the client-side?\n\nFor security reasons, Segment requires that the Profile API only be used server-side. The Profile API allows you to look up data about any user given an identifier (for example, email, anonymousId, or userId) and an authorized access secret. While this enables powerful personalization workflows, it could also let your customers\u2019 data fall into the wrong hands if the access secret were exposed on the client.\n\nInstead, by creating an authenticated personalization endpoint server-side backed by the Profile API, you can serve up personalized data to your users without the risk of their information falling into the wrong hands.\n\nThis page was last modified: 22 Nov 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nIdentity Graph\nHighlights\nTechnical highlights\nFAQs\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nSegment App\n/\nIam\n/\nMulti-Factor Authentication (MFA)\nMulti-Factor Authentication (MFA)\nFREE \u2713\nTEAM \u2713\nBUSINESS \u2713\nADD-ON X\n?\n\nMulti-factor Authentication (MFA) provides an additional layer of security when logging into your Segment account. When MFA is enabled, users must enter their username and password, and a one-time use code. Users can either enable MFA for their own account, or workspace owners can require that all users in a workspace use MFA. These security settings are available in the workspace from the \u201cAdvanced Settings\u201d section.\n\nYou can configure your Segment workspace to send a text message code (U.S. and Canada only), or use an authentication app to generate a time-based token (for example Google Authenticator, 1Password, or Authy). You can also log in using a recovery code in case you don\u2019t have your MFA device available. When you configure MFA, be sure to save your recovery code in safe place so you can access your Segment account in the event you lose your MFA device.\n\nWe highly recommend that you choose a strong password and also enable MFA for the email account that you use to log into Segment. If someone is able to gain access to your email, they will be able to access your Segment account even if your Segment account has MFA enabled.\n\nWho can use MFA?\n\nMFA is available to all Segment customers that are not logged in using SSO. If your company uses SSO to sign in to Segment, you should enable MFA at the SSO provider. Contact your company\u2019s IT team if you have questions about your company\u2019s SSO configuration.\n\nEnabling MFA\nLog into Segment and go to your MFA Settings page.\nClick either Authenticator App or Text Message and follow the instructions to set up MFA.\n\nOnce MFA is enabled, Segment prompts you for one of these methods every time you log in.\n\nRecovering MFA\n\nYour recovery code can be used bypass in the event you do not have your MFA device. If you no longer have access to your recovery code, you can choose to send a recovery code to your email to re-access your Account.\n\nEnter your username and password on the login screen.\nClick the authenticating a different way link.\nClick Recovery Code.\nClick Send recovery code to my email.\n\nThis page was last modified: 03 Aug 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nEnabling MFA\nRecovering MFA\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSpec\n/\nSpec: B2B SaaS\nSpec: B2B SaaS\n\nThis guide explains how B2B SaaS companies should send core user and account lifecycle data to Segment.\n\nOverview\n\nMost B2B SaaS companies have a few common, core lifecycle events for users and accounts. We understand that account hierarchies can be unique and complex, but by following this spec you can take advantage of account-based tools on Segment platform, and B2B SaaS data products by Segment.\n\nEvents\n\nThe B2B SaaS category has the following semantic events:\n\nAccount Created\nAccount Deleted\nSigned Up\nSigned In\nSigned Out\nInvite Sent\nAccount Added User\nAccount Removed User\nTrial Started\nTrial Ended\nAccount Created\n\nThis event should be sent when a new account is created.\n\nProperties\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\naccount_name\tString\tThe name of the account being created.\ncontext.groupId\tString\tThe id of the account being created.\nExample\n{\n  \"userId\": \"019mr8mf4r\",\n  \"type\": \"track\",\n  \"event\": \"Account Created\",\n  \"properties\": {\n    \"account_name\": \"Initech\"\n  },\n  \"context\": {\n    \"groupId\": \"acct_123\"\n  }\n}\n\nAccount Deleted\n\nThis event should be sent when an account is deleted.\n\nProperties\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\naccount_name\tString\tThe name of the account being deleted.\ncontext.groupId\tString\tThe id of the account being deleted.\nExample\n{\n  \"userId\": \"019mr8mf4r\",\n  \"type\": \"track\",\n  \"event\": \"Account Deleted\",\n  \"properties\": {\n    \"account_name\": \"Initech\"\n  },\n  \"context\": {\n    \"groupId\": \"acct_123\"\n  }\n}\n\nSigned Up\n\nThis event should be sent when a user signs up for your service.\n\nGood to know: Segment\u2019s best practice is to use an \u201cObject-Action\u201d naming convention, which in this case would be \u201cUser Signed Up\u201d. However, because in the B2B case this may not be a specific user, we omit that noun in our example. You may include or omit it as needed for your implementation.\n\nProperties\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\ntype\tString\tThe type of signup, e.g. invited, organic.\nfirst_name\tString\tThe first name of the user.\nlast_name\tString\tThe last name of the user.\nemail\tString\tThe email of the user.\nphone\tString\tThe phone number of the user.\nusername\tString\tThe username of the user.\ntitle\tString\tThe title of the user.\ncontext.groupId\tString\tThe id of the account the user is joining.\nExample\n{\n  \"userId\": \"019mr8mf4r\",\n  \"type\": \"track\",\n  \"event\": \"Signed Up\",\n  \"properties\": {\n    \"type\": \"organic\",\n    \"first_name\": \"Peter\",\n    \"last_name\": \"Gibbons\",\n    \"email\": \"pgibbons@example.com\",\n    \"phone\": \"410-555-9412\",\n    \"username\": \"pgibbons\",\n    \"title\": \"Mr\"\n  },\n  \"context\": {\n    \"groupId\": \"acct_123\"\n  }\n}\n\nSigned In\n\nThis event should be sent when a user signs in to your service.\n\nGood to know: Segment\u2019s best practice is to use an \u201cObject-Action\u201d naming convention, which in this case would be \u201cUser Signed In\u201d. However, because in the B2B case this may not be a specific user, we omit that noun in our example. You may include or omit it as needed for your implementation.\n\nProperties\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\nusername\tString\tThe username of the user signing in.\ncontext.groupId\tString\tThe id of the account associated with the user signing in.\nExample\n{\n  \"userId\": \"019mr8mf4r\",\n  \"type\": \"track\",\n  \"event\": \"Signed In\",\n  \"properties\": {\n    \"username\": \"pgibbons\"\n  },\n  \"context\": {\n    \"groupId\": \"acct_123\"\n  }\n}\n\nSigned Out\n\nThis event should be sent when a user signs out for your service. You should also call analytics.reset() to refresh the cookie when a Signed Out event occurs.\n\nGood to know: Segment\u2019s best practice is to use an \u201cObject-Action\u201d naming convention, which in this case would be \u201cUser Signed Out\u201d. However, because in the B2B case this may not be a specific user, we omit that noun in our example. You may include or omit it as needed for your implementation.\n\nProperties\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\nusername\tString\tThe username of the user signing out.\ncontext.groupId\tString\tThe id of the account associated with the user signing out.\nExample\n{\n  \"userId\": \"019mr8mf4r\",\n  \"type\": \"track\",\n  \"event\": \"Signed Out\",\n  \"properties\": {\n    \"username\": \"pgibbons\"\n  },\n  \"context\": {\n    \"groupId\": \"acct_123\"\n  }\n}\n\nInvite Sent\n\nThis event should be sent when a user invites another user.\n\nProperties\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\ninvitee_email\tString\tThe email address of the person receiving the invite.\ninvitee_first_name\tString\tThe first name of the person receiving the invite.\ninvitee_last_name\tString\tThe last name of the person receiving the invite.\ninvitee_role\tString\tThe permission group for the person receiving the invite.\ncontext.groupId\tString\tThe id of the account the person is being invited to.\nExample\n{\n  \"userId\": \"019mr8mf4r\",\n  \"type\": \"track\",\n  \"event\": \"Invite Sent\",\n  \"properties\": {\n    \"invitee_email\": \"pgibbons@example.com\",\n    \"invitee_first_name\": \"Peter\",\n    \"invitee_last_name\": \"Gibbons\",\n    \"invitee_role\": \"Owner\"\n  },\n  \"context\": {\n    \"groupId\": \"acct_123\"\n  }\n}\n\nAccount Added User\n\nThis event should be sent when a user is added to a group.\n\nProperties\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\nrole\tString\tThe permission group for this user in this account.\ncontext.groupId\tString\tThe id of the account the user is being added to.\nExample\n{\n  \"userId\": \"019mr8mf4r\",\n  \"type\": \"track\",\n  \"event\": \"Account Added User\",\n  \"properties\": {\n    \"role\": \"Owner\"\n  },\n  \"context\": {\n    \"groupId\": \"acct_123\"\n  }\n}\n\nAccount Removed User\n\nThis event should be sent when a user is removed from a group or account.\n\nProperties\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\ncontext.groupId\tString\tThe id of the account the user is being removed from.\nExample\n{\n  \"userId\": \"019mr8mf4r\",\n  \"type\": \"track\",\n  \"event\": \"Account Removed User\",\n  \"properties\": {},\n  \"context\": {\n    \"groupId\": \"acct_123\"\n  }\n}\n\nTrial Started\n\nThis event should be sent when a trial is started.\n\nProperties\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\ntrial_start_date\tDate\tThe date when the trial starts. It is an ISO-8601 date string.\ntrial_end_date\tDate\tThe date when the trial ends. It is an ISO-8601 date string.\ntrial_plan_name\tString\tThe name of the plan being trialed.\ncontext.groupId\tString\tThe id of the account the trial is associated with.\nExample\n{\n  \"userId\": \"019mr8mf4r\",\n  \"type\": \"track\",\n  \"event\": \"Trial Started\",\n  \"properties\": {\n    \"trial_start_date\": \"2018-08-28T04:09:47Z\",\n    \"trial_end_date\": \"2018-09-20T04:09:47Z\",\n    \"trial_plan_name\": \"Business\"\n  },\n  \"context\": {\n    \"groupId\": \"acct_123\"\n  }\n}\n\nTrial Ended\n\nThis event should be sent when a trial ends.\n\nProperties\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\ntrial_start_date\tDate\tThe date when the trial starts. It is an ISO-8601 date string.\ntrial_end_date\tDate\tThe date when the trial ends. It is an ISO-8601 date string.\ntrial_plan_name\tString\tThe name of the plan being trialed.\ncontext.groupId\tString\tThe id of the account the trial is associated with.\nExample\n{\n  \"userId\": \"019mr8mf4r\",\n  \"type\": \"track\",\n  \"event\": \"Trial Ended\",\n  \"properties\": {\n    \"trial_start_date\": \"2018-08-28T04:09:47Z\",\n    \"trial_end_date\": \"2018-09-20T04:09:47Z\",\n    \"trial_plan_name\": \"Business\"\n  },\n  \"context\": {\n    \"groupId\": \"acct_123\"\n  }\n}\n\n\nThis page was last modified: 19 Jul 2022\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nOverview\nEvents\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nStorage\n/\nData Lakes\n/\nData Lakes Sync Reports and Errors\nData Lakes Sync Reports and Errors\nFREE X\nTEAM X\nBUSINESS \u2713\nADD-ON X\n?\n\nSegment Data Lakes generates reports with operational metrics about each sync to your data lake so you can monitor sync performance. These sync reports are stored in your S3 bucket and Glue Data Catalog. This means you have access to the raw data, so you can query it to answer questions and set up alerting and monitoring tools.\n\nSync Report schema\n\nYour sync_report table stores all of your sync data. You can query it to answer common questions about data synced to your data lake. The table has the following columns in its schema:\n\nSYNC METRIC\tDESCRIPTION\nworkspace_id\tDistinct ID assigned to each Segment workspace and found in the workspace settings.\nsource_id\tDistinct ID assigned to each Segment source, found in the Source Settings > API Keys > Source ID.\ndatabase\tName of the Glue Database used to store sync report tables. Segment automatically creates this database during the Data Lakes set up process.\nemr_cluster_id\tID of the EMR cluster which Data Lakes uses, found in the Data Lakes Settings page.\ns3_bucket\tName of the S3 bucket which Data Lakes uses, found in the Data Lakes Settings page.\nrun_id\tID dynamically generated and assigned to each Data Lakes sync run.\nstart_time\tTime when the sync run started, in UTC.\nfinish_time\tTime when the sync run finished, in UTC.\nduration_mins\tThe length of the sync in minutes, calculated by the difference between the start and finish time.\nstatus\tStatus of the sync. Values can either be finished for a successful sync or failed for a failed sync.\nerror_code\tThe type of error, which can include: insufficient permissions, invalid settings, or a Segment internal error.\nerror\tIf the sync failed, the error that describes the issue, for example \u201cExternal ID is invalid\u201d.\ntable_name\tName of the Segment event synced to S3.\nrow_count\tNumber of rows synced to S3 for a specific run.\npartitions\tPartitions added to the event tables during the sync.\nnew_columns\tNew columns inferred and added to event table during the sync.\nday\tDay on which the sync occurred.\ntype\tDefines whether the run metrics are at the source or event level. If type = source, the run aggregates data for syncs across all events within the source. If type = event, the run shows detailed sync metrics per event.\nreplay\tTrue or false value which indicates whether the sync run was a replay.\nreplay_from\tStart date for the replay, if applicable.\nreplay_to\tFinish date for the replay, if applicable.\n\nThe Glue Database named __segment_datalake stores the schema of the sync_reports table. The __segment_datalake database has the following format:\n\nCOLUMN NAME\tDATA TYPE\tPARTITION KEY\tCOMMENT\ntype\tstring\t\u00a0\t\u00a0\nworkspace_id\tstring\t\u00a0\t\u00a0\nrun_id\tstring\t\u00a0\t\u00a0\nstart_time\ttimestamp\t\u00a0\t\u00a0\nfinish_time\ttimestamp\t\u00a0\t\u00a0\nduration_mins\tbigint\t\u00a0\t\u00a0\nstatus\tstring\t\u00a0\t\u00a0\nerror\tstring\t\u00a0\t\u00a0\nerror_code\tstring\t\u00a0\t\u00a0\ntable_name\tstring\t\u00a0\t\u00a0\ndatabase\tstring\t\u00a0\t\u00a0\npartitions\tarray\t\u00a0\t\u00a0\nnew_columns\tarray\t\u00a0\t\u00a0\nrow_count\tbigint\t\u00a0\t\u00a0\nis_new\tboolean\t\u00a0\t\u00a0\nreplay\tboolean\t\u00a0\t\u00a0\nreplay_from\ttimestamp\t\u00a0\t\u00a0\nreplay_to\ttimestamp\t\u00a0\t\u00a0\nemr_cluster_id\tstring\t\u00a0\t\u00a0\ns3_bucket\tstring\t\u00a0\t\u00a0\nsource_id\tstring\tPartition (0)\t\u00a0\nday\tstring\tPartition (1)\t\u00a0\n\nThe sync_reports table is available in S3 and Glue only once a sync completes. Sync reports are not available for syncs in progress.\n\nData location\n\nData Lakes sync reports are stored in Glue and in S3.\n\nSegment automatically creates a Glue Database and table when you set up Data Lakes to store all sync report tables. The Glue Database is named __segment_datalake, and the table is named sync_reports.\n\nThe S3 structure is: s3://my-bucket/segment-data/reports/day=YYYY-MM-DD/source=$SOURCE_ID/run_id=$RUN_ID/report.json\n\nData format\n\nThe data in the sync reports is stored in JSON format to ensure that it is human-readable and can be processed by other systems.\n\nEach table involved in the sync is a separate JSON object that contains the sync metrics for the data loaded to that table.\n\nThe example below shows the raw JSON object for a successful sync report.\n\n  {\n    \"type\": \"source\",\n    \"workspace_id\": \"P3IMS7SBDH\",\n    \"source_id\": \"9IP56Shn6\",\n    \"run_id\": \"1597581273464733073\",\n    \"start_time\": \"2020-08-19 22:15:59.044084423\",\n    \"finish_time\": \"2020-08-19 22:18:12.891\",\n    \"duration_mins\": 2,\n    \"status\": \"finished\",\n    \"table_name\": \"\",\n    \"database\": \"ios_prod\",\n    \"row_count\": 81020,\n    \"emr_cluster_id\": \"j-3SXSUSDNPIS\",\n    \"s3_bucket\": \"my-segment-datalakes-bucket\"\n  }\n  {\n    \"type\": \"event\",\n    \"workspace_id\": \"P3IMS7SBDH\",\n    \"source_id\": \"9IP56Shn6\",\n    \"run_id\": \"1597581273464733073\",\n    \"start_time\": \"2020-08-19 22:15:59.044084423\",\n    \"finish_time\": \"2020-08-19 22:18:12.891\",\n    \"duration_mins\": 2,\n    \"status\": \"finished\",\n    \"table_name\": \"track_order_completed\",\n    \"database\": \"ios_prod\",\n    \"partitions\": [\n      {\n        \"day\": \"2020-08-16\",\n        \"hr\": \"10\"\n      },\n      {\n        \"day\": \"2020-08-16\",\n        \"hr\": \"11\"\n      }\n    ],\n   \"new_columns\": [\n      {\n        \"name\": \"properties_billing_address\",\n        \"type\": \"string\"\n      }\n    ],\n    \"row_count\": 20020,\n    \"emr_cluster_id\": \"j-3SXSUSDNPIS\",\n    \"s3_bucket\": \"my-segment-datalakes-bucket\"\n  }\n  {\n    \"type\": \"event\",\n    \"workspace_id\": \"P3IMS7SBDH\",\n    \"source_id\": \"9IP56Shn6\",\n    \"run_id\": \"1597581273464733073\",\n    \"start_time\": \"2020-08-19 22:15:59.044084423\",\n    \"finish_time\": \"2020-08-19 22:18:12.891\",\n    \"duration_mins\": 2,\n    \"status\": \"finished\",\n    \"table_name\": \"track_product_added\",\n    \"database\": \"ios_prod\",\n    \"partitions\": [\n      {\n        \"day\": \"2020-08-16\",\n        \"hr\": \"10\"\n      }\n    ],\n    \"row_count\": 20260,\n    \"emr_cluster_id\": \"j-3SXSUSDNPIS\",\n    \"s3_bucket\": \"my-segment-datalakes-bucket\"\n}\n\n\nThe example below shows the raw JSON object for a failed sync report.\n\n{\n    \"type\": \"source\",\n    \"workspace_id\": \"P3IMS7SBDH\",\n    \"source_id\": \"9IP56Shn6\",\n    \"run_id\": \"1597867438900010296\",\n    \"start_time\": \"2020-08-19 20:04:58.368616813\",\n    \"finish_time\": \"2020-08-19 20:49:48.308318686\",\n    \"duration_mins\": 44,\n    \"status\": \"failed\",\n    \"error\": \"Data Lakes Destination has invalid configuration for \\\"AWS Role ARN\\\": field is required.\",\n    \"error_code\": \"Segment.Internal\",\n    \"table_name\": \"\",\n    \"database\": \"ios_prod\",\n    \"emr_cluster_id\": \"j-3SXSUSDNPIS\",\n    \"s3_bucket\": \"segment-datalakes-demo-stage\"\n}\n\nQuerying the Sync Reports table\n\nYou can use SQL to query your Sync Reports table to explore and analyze operational sync metrics. A few helpful and commonly used queries are included below.\n\nReturn row counts per day for a specific event\nSELECT day,sum(row_count)\nFROM \"__segment_datalake\".\"sync_reports\"\nWHERE source_id='9IP56Shn6' and table_name='checkout_started'\nGROUP BY day\nORDER BY day\n\nReturn row counts per day for all events in the source\nSELECT day, table_name,sum(row_count)\nFROM \"__segment_datalake\".\"sync_reports\"\nWHERE source_id='9IP56Shn6' AND type='event'\nGROUP BY day, table_name\nORDER BY day\n\nFind the most recent successful sync\nSELECT max(finish_time)\nFROM \"__segment_datalake\".\"sync_reports\"\nWHERE source_id='9IP56Shn6' AND status='finished' AND date(day) = CURRENT_DATE\nLIMIT 1\n\nFind all failures in the last N days\nSELECT run_id, status, error, error_code\nFROM \"__segment_datalake\".\"sync_reports\"\nWHERE source_id='9IP56Shn6' AND status='failed' AND date(day) >= (CURRENT_DATE - interval '2' day)\n\nSync errors\n\nThe following error types can cause your data lake syncs to fail:\n\nInsufficient permissions - Segment does not have the permissions necessary to perform a critical operation. You must grant Segment additional permissions.\nInvalid settings - The settings are invalid. This could be caused by a missing required field, or a validation check that fails. The invalid setting must be corrected before the sync can succeed.\nInternal error - An error occurred in Segment\u2019s internal systems. This should resolve on its own. Contact the Segment Support team if the sync failure persists.\nInsufficient permissions\n\nIf Data Lakes does not have the correct access permissions for S3, Glue, and EMR, your syncs will fail.\n\nIf permissions are the problem, you might see one of the following permissions-related error messages:\n\n\u201cSegment was unable to upload staging data to your S3 Bucket due to a lack of sufficient permissions\u201d.\n\u201cSegment does not have permissions to download object from S3 Bucket\u201d.\n\u201cSegment does not have permissions to upload object to S3 Bucket\u201d.\n\u201cSegment does not have permissions to delete S3 objects from S3 Bucket\u201d.\n\u201cSegment does not have permissions to submit an EMR job to cluster\u201d.\n\u201cSegment does not have permissions to check the status of EMR Job on EMR Cluster\u201d.\n\u201cSegment does not have permissions to delete table from Glue Catalog\u201d.\n\u201cSegment does not have permissions to fetch schema information from Glue catalog\u201d.\n\nCheck the set up guide to ensure that you set up the required permission configuration for S3, Glue and EMR.\n\nInvalid settings\n\nOne or more settings might be incorrectly configured in the Segment app, preventing your Data Lakes syncs from succeeding.\n\nIf you have invalid settings, you might see one of the error messages below:\n\n\u201cData Lakes Destination has invalid configuration.\u201d\n\u201cThe Table Partitions configuration for this Data Lake is invalid. The field name does not appear to map to the data being processed, which likely means it is misconfigured.\u201d\n\u201cExternal ID is invalid. Please ensure the external ID in the IAM role used to connect to your Data Lake matches the source ID.\u201d\n\u201cExternal ID is not set. Please ensure that the IAM role used to connect to your Data Lake has the source ID in the list of external IDs.\u201d\n\nThe most common error occurs when you do not list all Source IDs in the External ID section of the IAM role. You can find your Source IDs in the Segment workspace, and you must add each one to the list of External IDs in the IAM policy. You can either update the IAM policy from the AWS Console, or re-run the Data Lakes set up Terraform job.\n\nInternal error\n\nInternal errors occur in Segment\u2019s internal systems, and should resolve on their own. If sync failures persist, contact the Segment Support team.\n\nFAQ\nHow are Data Lakes sync reports different from the sync data for Segment Warehouses?\n\nBoth Warehouses and Data Lakes provide similar information about syncs, including the start and finish time, rows synced, and errors.\n\nHowever, Warehouse sync information is only available in the Segment app: on the Sync History page and Warehouse Health pages. With Data Lakes sync reports, the raw sync information is sent directly to your data lake. This means you can query the raw data and answer your own questions about syncs, and use the data to power alerting and monitoring tools.\n\nWhat happens if a sync is partly successful?\n\nSync reports are currently generated only when a sync completes, or when it fails. Partial failure reporting is not currently supported.\n\nThis page was last modified: 03 Aug 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSync Report schema\nData location\nData format\nQuerying the Sync Reports table\nSync errors\nFAQ\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nCampaigns\n/\nEmail Campaigns\nEmail Campaigns\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE PREMIER \u2713\n?\n\nEngage Premier entered an End of Sale (EOS) period effective June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. Segment recommends exploring the following pages in preparation of a migration or future MCM needs:\n\nTwilio Marketing Campaigns\n\nPreferred ISV Partners:\n\nAirship Blog\nBloomreach Blog\nBraze Blog\nInsider Blog\nKlaviyo Blog\nTwilio Engage Foundations Documentation\n\n\nWith Twilio Engage, you can send email and SMS campaigns to users who have opted in to receive your marketing materials. On this page, you\u2019ll learn how to create and send an email campaign.\n\nSome knowledge of the Journeys product will benefit you as you read through this guide. If you\u2019re new to Journeys, the Journeys documentation will bring you up to speed.\n\nHow Engage campaigns work\n\nTwilio Engage uses Journeys to send email and SMS campaigns. With Journeys, you add conditions and steps that trigger actions like sending an email or an SMS.\n\nYou\u2019ll build and then send your campaign in three stages:\n\nCreate a Journey.\nAdd a Journey condition.\nCreate, test, and send your email campaign.\nCreate a Journey\n\nBecause Engage campaigns exist within Journeys, begin by creating a Journey:\n\nIn Engage, select Journeys, then click New Journey.\nName your Journey and select its entry settings.\nClick Build Journey to create the Journey.\nAdd a Journey condition\n\nWith your Journey created, you\u2019ll now create a condition that will trigger your email campaign:\n\nWithin the Journey builder, click + Add Entry Condition.\nIn the Add Entry Condition pane, give the step a name.\nClick + Add Condition, select your desired condition, then click Save.\n\nWith your entry condition added, you\u2019re now ready to create your email.\n\nCreate, test, and publish your email campaign\n\nFollow these steps to create an email campaign:\n\nWithin the Journey builder, click the + node below your new condition.\nFrom the Select a Step window, click Send an email.\nIn the Send Email window, select Build a new email or Use a template to choose an existing email template.\nBuild or edit your design, then click Save Email.\nFill out all Send Email fields relevant to your campaign, select the subscription states or groups that you want to receive your email, (optionally) select an IP pool, then click Save.\n\nSome email campaign fields, like Sender email and Subject, are required. The Send Email window indicates required fields with an asterisk. Refer to the email campaign fields table for a full description of available email fields.\n\nEditing Templates\n\nIf you use a template for your email, Engage creates an editable copy of the original. Editing the template within the Journey won\u2019t alter the original template.\n\nSend an email to all users\n\nAs you create your email campaign, you can set an email to send to all users regardless of their subscription state. This may be useful, for example, when you need to send a marketing transactional email to a user who hasn\u2019t subscribed to your marketing emails.\n\nTo send an email to all users:\n\nIn the email builder, navigate to the Which subscription states should receive this message? field.\nFrom the dropdown menu, select All subscription states including unsubscribed.\n\nWhen you bypass subscription states, be sure to follow local laws and comply with CAN-SPAM guidance.\n\nFor more, view SendGrid\u2019s email deliverability best practices.\n\nTest your email campaign\n\nAt this point, you can send a test email before publishing your campaign. Test emails confirm that your design, unsubscribe links, and merge tags appear as expected.\n\nAs part of the test send, you can enter custom values to populate the profile traits in your message.\n\nFollow these steps to test your campaign:\n\nIn the Send an email pane, navigate to Body, then click Test email.\nIf your template has profile traits, enter a trait value for the test email. This ensures that your merge tags work as expected.\nTo test a default value, leave the profile traits field blank. Default values must be assigned in your merge tags. For example, loyal customer would be the default for the following merge tag: {{profile.traits.first_name | default: \"loyal customer\"}}.\nIn the Recipients field, enter the email address(es) that will receive your test email.\nClick Send test email.\nPublish your email campaign\n\nWith your email designed and tested, you\u2019re now ready to save the campaign and publish your Journey, with the following steps:\n\nVerify that all Send Email fields are correct.\nClick Save.\nIn the Journey builder, click Publish.\n\nYour email campaign is now live. When users trigger the email\u2019s parent Journey condition, they will receive your email campaign.\n\nEmail campaign fields\n\nThe following table contains descriptions of all available fields in the Journeys Send Email builder. Asterisks indicate required fields.\n\nFIELD\tDESCRIPTION\nStep name *\tRequired; the name for the email campaign\u2019s parent Journey step. Email recipients won\u2019t see this field.\nSender email *\tThe email address users will see in the from field of the email campaign.\nSender name *\tThe name users will see next to the sender email.\nReply to email *\tThe email address that will receive any replies users send. You can use different Sender and reply-to email addresses. Email recipients will see this address if they reply to your campaign.\nReply to name *\tThe name users will see next to the reply-to email address.\nBCC\tEmail address that will receive a blind carbon copy of your email campaign.\nPreview text\tA brief message that displays next to the email subject.\nSubject *\tThe email subject.\nBody *\tThe email\u2019s content. Select Build Email Content to create a new campaign, or Use a template to choose an existing template.\nWhich subscription states should receive this message?\tThe subscription state that Engage will send email campaigns to. Defaults to subscribed users only. Select All subscription states including unsubscribed to send emails to all users regardless of subscription state.\nWorking with IP pools\n\nWhen you create an email, you have the option to select an IP pool. An IP pool is a group of IP addresses available to you in SendGrid. You can create and view your IP pools in your Engage-linked SendGrid subuser account by navigating to Settings > IP Addresses > IP Pools.\n\nYour sending reputation is based on a combination of your domain and the IP address you use to send emails. Emails that end up in your recipients\u2019 spam folders could harm your sending reputation. As a result, you may want to keep your marketing and transactional emails on different IP addresses.\n\nKeep the following in mind as you use IP pools:\n\nIf you don\u2019t select an IP pool, Segment will choose one of your SendGrid IP addresses at random.\nIf you select an IP pool during email setup but then delete the IP pool in SendGrid, emails will begin to fail after IP pool deletion.\nSendGrid lets you assign the same IP address to multiple IP pools. If you want to use different IP addresses for different Engage emails, verify within SendGrid that the pools you created have different IP addresses.\nYou can change an IP pool for an email in a live journey by editing the journey, creating a new draft, changing the email\u2019s IP pool, then publishing the new journey version.\n\nFor more information, see SendGrid\u2019s IP pools documentation.\n\nNext steps\n\nUsing Journeys, you can create multi-channel customer engagement with both email and SMS campaigns. Having published an email, learn how Engage SMS campaigns can help you market to customers through text messages.\n\nThis page was last modified: 15 Jul 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nHow Engage campaigns work\nEmail campaign fields\nWorking with IP pools\nNext steps\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nPrivacy\n/\nUser Deletion and Suppression\nUser Deletion and Suppression\n\nIn keeping with Segment\u2019s commitment to GDPR and CCPA readiness, Segment offers the ability to delete and suppress data about end-users when they are identifiable by a userId, should they revoke or alter consent to data collection. For example, if an end-user invokes the Right to Object or Right to Erasure under the GDPR or CCPA, you can use these features to block ongoing data collection about that user and delete all historical data about them from Segment\u2019s systems, connected S3 buckets and warehouses, and supported downstream partners.\n\nContact Support if you need to process more than 110,000 users within a 30 day period.\n\nBusiness Plan Customers\n\nIf you use this feature to delete data, you can not Replay the deleted data. For standard Replay requests, you must wait for any pending deletions to complete, and you cannot submit new deletion requests for the period of time that Segment replays data for you.\n\nThe legacy GraphQL APIs for user deletion and suppression are deprecated. Instead, use the Segment Public API to interact with the User Deletion and Suppression system.\n\nOverview\n\nAll deletion and suppression actions in Segment are asynchronous and categorized as Regulations. Regulations are requests to Segment to control your data flow. You can issue Regulations from:\n\nYour Segment Workspace (Settings > End User Privacy)\nSegment\u2019s Public API\n\nYou can programmatically interact with the User Deletion and Suppression system using the Public API.\n\nWith Regulations, you can issue a single request to delete and suppress data about a user by userId. Segment scopes Regulations to your workspace (which targets all sources within the workspace), to a specific source, or to a cloud source.\n\nThe following regulation types are available:\n\nSUPPRESS_ONLY: Suppress new data without deleting existing data\nUNSUPPRESS: Stop an ongoing suppression\nSUPPRESS_WITH_DELETE: Suppress new data and delete existing data\nDELETE_INTERNAL: Delete data from Segment internals only\nSUPPRESS_WITH_DELETE_INTERNAL: Suppress new data and delete from Segment internals only\nDELETE_ONLY: Delete existing data without suppressing any new data\n\nUsing SUPPRESS_WITH_DELETE or DELETE_ONLY regulation types might lead to additional charges levied by your destination providers.\n\nSuppression Support and the Right to Revoke Consent\n\nSUPPRESS regulations add a user to your suppression list by the userId. Segment blocks suppressed users across all sources; messages you send to Segment with a suppressed userId are blocked at the API. These messages do not appear in the debugger, are not saved in archives and systems, and are not sent to any downstream server-side destinations. However, if you set up a destination in device-mode, the events are sent directly to destinations as well. In this case, Suppression doesn\u2019t suppress the events.\n\nWhen a customer exercises the right to erasure, they expect that you stop collecting data about them. Suppression regulations ensure that regardless of how you\u2019re sending data to Segment, if a user opts out, Segment respects their wishes on an ongoing basis and across applications.\n\nSuppression is not a substitute for gathering affirmative, unambiguous consent about data collection and its uses.\n\nSegment offers suppression tools to help you manage the challenge of users opting-out across different channels and platforms. Segment encourages and expects that you design your systems and applications so you don\u2019t collect or forward data to Segment until you have unambiguous, specific, informed consent or have established another lawful legal basis to do so.\n\nTo remove a user from the suppression list, create an UNSUPPRESS regulation.\n\nDeletion Support and the Right to Be Forgotten\n\nWhen you create a SUPPRESS_WITH_DELETE regulation, the user is actively suppressed, and Segment begins permanently deleting all data associated with this user from your workspace. This includes scanning and removing all messages related to that userId from all storage mediums that don\u2019t automatically expire data within 30 days, including archives, databases, and intermediary stores.\n\nSegment deletes messages with this userId from connected raw data Destinations, including Redshift, BigQuery, Postgres, Snowflake, and Amazon S3. Warehouse deletions occur using a DML run against your cluster or instance, and Segment delete from S3 by \u201crecopying\u201d clean versions of any files in your bucket that included data about that userId.\n\nSegment forwards these deletion requests to a growing list of supported partners.\n\nNote that Segment has a 30-day SLA for submitted deletion requests. Additionally, Segment\u2019s deletion manager can only accommodate 110,000 users within a 30-day period and cannot guarantee a 30-day SLA if there are more than 110,000 deletion requests submitted within those 30 days. You can delete up to 5000 userIds per call via Public API. Contact Support if you need to process more than 110,000 users within a 30 day period.\n\nSegment cannot guarantee that data is deleted from your Destinations.\n\nSegment forwards deletion requests to supported Destinations (such as Braze, Intercom, and Amplitude) but you should confirm that each partner fulfills the request.\n\nYou will also need to contact any unsupported Destinations separately to manage user data deletion.\n\nNote that if you later UNSUPPRESS a user, the deletion functionality does not clean up data sent after removing the user from the suppression list.\n\nSuppressed users\n\nThe Suppressed Users tab in Segment App (Settings > End User Privacy) allows you to create new Suppression requests and also shows an list of userIds which are actively being suppressed. It can take a few hours/days for the suppression to become active, depending on the number of requests that are in the queue for your workspace. Once the request is active, Segment blocks data about these users across all sources.\n\nNote that list only includes SUPPRESS_ONLY regulations. If you created a User Deletion request using UI, you will need to check the Deletion Requests tab, as those are SUPPRESS_WITH_DELETE regulation types.\n\nSuppress a new user\n\nTo create a suppression regulation and add a userId to this list, click Suppress New User, and enter the userId in the field that appears. Then click Request Suppression.\n\nSegment creates a SUPPRESS regulation, and adds the userId to your suppression list, mostly processed within 24 hours. In some cases, the suppression request can take up to 30 days to process. You can suppress up to 5000 userIds per call through the Public API.\n\nRemove a user from the suppression list\n\nTo remove a user from the suppression list, click the ellipses (\u2026) icon on the userId row, and click Remove.\n\nThis creates an UNSUPPRESS regulation, and removes the userId from your suppression list, mostly processed within 24 hours.\n\nDeletion requests\n\nThe deletion requests tab shows a log of all regulations with a deletion element along with status. The deletion requests can take up to 30 days to process.\n\nIn the Segment App (Settings > End User Privacy > Deletion Requests), you can click a userId to view its status in Segment internal systems, and in the connected destinations.\n\nThe deletion request can have one of the following statuses:\n\nFAILED\nFINISHED\nINITIALIZED\nINVALID\nNOT_SUPPORTED\nPARTIAL_SUCCESS\nRUNNING\n\nWhen checking the status of deletion requests using Segment\u2019s API, the deletion will report an overall status of all of the deletion processes. As a result, Segment returns a FAILED status because of a failure on an unsupported destination, even if the deletion from the Segment Internal Systems and supported destinations were completed successfully.\n\nRegulate User from a single Source in a Workspace\n\nRefer to Create Source Regulation in the Public API.\n\nDelete Object from a Cloud Source\n\nRefer to the Create Cloud Source Regulation Public API endpoint.\n\nCloud Sources sync objects to Segment. As a result, Cloud Sources are regulated based on an objectId instead of a userId. Before you delete the object from Segment, you should delete it from the upstream system first.\n\nList Suppressed Users for your Workspace\n\nRefer to List Suppressions method in the Public API.\n\nList Deletion Requests for your Workspace\n\nRefer to the List Regulations from Source Public API method.\n\nData retention\n\nSegment stores a copy of all event data received in Segment\u2019s secure event archives on S3. By default, all workspaces store data for an unlimited period of time, but you can modify the lifecycle policies for the data stored internally. Segment uses this data for data replays and for troubleshooting purposes.\n\nSegment recommends keeping your data for at least 30 days to enable replays of your data.\n\nTo change your data retention settings, navigate to Privacy > Settings > Data Retention in Segment.\n\nWorkspace Default Archive Retention Period\n\nSelect the default retention period for the workspace in this setting. This value applies to all sources in the workspace, unless overridden in the Source-Level Archive Retention Periods setting.\n\nYou can select from the following Archive Retention time periods:\n\n7 days\n30 days\n90 days\n180 days\n365 days\nUnlimited (default)\nSource-Level Archive Retention Periods\n\nOverride the workspace default retention period on a per-source level.\n\nYou can select from the following Archive Retention time periods:\n\nDefault (This is the default value you set in the Workspace Default Archive Retention Period)\n7 days\n30 days\n90 days\n180 days\n365 days\nUnlimited\n\nThis page was last modified: 17 Sep 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nOverview\nSuppression Support and the Right to Revoke Consent\nDeletion Support and the Right to Be Forgotten\nSuppressed users\nDeletion requests\nData retention\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nProtocols\n/\nProtocols Frequently Asked Questions\nProtocols Frequently Asked Questions\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nPROTOCOLS \u2713\n?\nProtocols Notifications\nHow can I subscribe to Protocols notifications?\n\nYou can subscribe to a variety of Protocols specific alerts through the workspace Activity Feed settings. To subscribe, visit your workspace Settings > User Preferences > Activity Notifications > Protocols.\n\nHow can I get notified when someone makes a change to my tracking plan?\n\nYou can forward notifications from Protocols to a new Segment source, which can then send them to notification tools such as Slack webhook.\n\nYou can also forward these Protocols alerts to any (cloud-mode) Segment destination that accepts Track calls, including data warehouses. Most customers record these activity feed events to a data warehouse for analysis.\n\nHow do I get notified when new violations are generated? Can I create custom violation notifications?\n\nYou can enable violation event forwarding to start delivering violations as Track calls to a Segment source. From there, you can forward the events to any Segment destination that accepts Track calls.\n\nYou can also use the Slack Actions destination to set event triggers for context fields, meaning events with violations are sent as Track calls directly from the source.\n\nProtocols Tracking Plan\nWhat is the Segment Consent Preference Updated event, and who added it to my Tracking Plans?\n\nConsent Management users see the Segment Consent Preference Updated event automatically added to all existing Tracking Plans after they create their first consent category, or when they create a new Tracking Plan after configuring Consent Management. Segment recommends that you do not remove this event.\n\nHow do I add Page and Screen events to my Tracking Plan?\n\nTo consolidate the views in the Schema tab, Segment automatically converts page and screen calls into Page Viewed and Screen Viewed events that appear in the Schema Events view. Segment recommends adding a Page Viewed or Screen Viewed event to your Tracking Plan with any properties you want to validate against. At this time, to validate that a specific named page/screen (analytics.page('Homepage') | analytics.screen('Home')) has a specific set of required properties, you will need to use the JSON Schema.\n\nHow can I see who made changes to my Tracking Plan?\n\nEach Tracking Plan includes a Changelog, which shows which changes were made by which users. To view it, open a Tracking Plan, click the \u2026 button (also known as the dot-dot-dot, or ellipses menu) next to the Edit Tracking Plan button, and click View Changelog.\n\nHow many Sources can I connect to a Tracking Plan?\n\nThe Tracking Plan to Source relationship is a one-to-many relationship. This means you can connect as many Sources to a Tracking Plan as you need. However Segment recommends connecting 1-3 Sources per Tracking Plan, because it\u2019s rare to have more than three Sources that share an identical set of events, especially when tracking events across platforms. For example, many Segment mobile SDKs (iOS and Android) automatically collect events that would not make sense to collect in a web app. Segment doesn\u2019t recommend including events in a Tracking Plan that would never be tracked in a Source.\n\nCan I duplicate a Tracking Plan in the Segment UI?\n\nYou can duplicate Tracking Plans in the Segment web app by following the instructions to copy a tracking plan. You can also use the Public API to copy the underlying JSON schema from one Tracking Plan to another.\n\nHow do I handle versioning with mobile apps?\n\nSegment currently supports the ability to create multiple versions of an event in a Tracking Plan. This is ideal for mobile apps, where a breaking change like adding a new required property to an event could cause all previous app versions on user devices to generate violations. You must manually add a context.protocols.event_version property to the specific track call so that Segment can correctly validate the event against the defined version. Learn more in the Tracking Plan event versioning documentation.\n\nHow do I handle null property values?\n\nIn the Tracking Plan editor, click on the data type dropdown for a given property and toggle \u201cAllow Null Values\u201d. Enabling null values means only null values will be accepted for that property.\n\nCan I group specific events in a Tracking Plan?\n\nYes. Tracking Plan Labels are an excellent way to organize events in a Tracking Plan by priority, platform, product, or similar metadata for each event.\n\nHow do I send someone a specific event or group of events to implement?\n\nYou can search in a Tracking Plan to find a specific event, and then copy the URL for the search results page and share it. You can also filter by label to share a group of events. The person you send the URL to must have access to the Workspace and tracking plan to see the results page. (See the Access Management documentation for more details.)\n\nCan I create a master Tracking Plan that supersedes all other Tracking Plans?\n\nYes. Tracking Plan Libraries makes it easy to create groups of events or properties that can be easily imported into multiple Tracking plans.\n\nCan I copy a Tracking Plan into a library?\n\nNo. Unfortunately it\u2019s not yet possible to\u00a0automatically transfer events from a Tracking Plan to Libraries. To import events into a new event library, import them directly from a source.\n\nCan I transfer a Tracking Plan between production and staging environments?\n\nYes. Using the Public API, you can copy the underlying JSON schema from a Tracking Plan in one Workspace to a Tracking Plan in another Workspace.\n\nIf you discarded events as a part of your original Tracking Plan, you must connect to the same Source and configure identical Schema Controls in your other Workspace so that blocked events behave as expected.\n\nCan I connect a Source to more than one Tracking Plan?\n\nUnfortunately, Sources cannot be connected to more than one Tracking Plan. If you were able to connect more than one Tracking Plan to a Source, it could create conflict if events overlapped.\n\nHow do Tracking Plans work?\n\nSegment\u2019s code uses built-in logic to verify if an event exists in the Tracking Plan. If an event does not exist, it will follow the configuration the Schema Configuration settings for each source connected to the Tracking Plan.\n\nWhy are my unplanned properties still getting sent to my destinations even though I\u2019ve set the dropdown to \u201cOmit Properties\u201d?\n\nUnplanned property omission is only supported for cloud-mode destinations. Unplanned properties will not be omitted when they\u2019re sent to device-mode destinations.\n\nWhy do I have two different Tracking Plan IDs?\n\nWhen you access a Tracking Plan, you\u2019ll come across two IDs: tp_ and rs_. Segment uses the two IDs to identify your Tracking Plan in the two APIs you can use to manage your workspace: the Public API and the Config API.\n\nTo view the two IDs for your Tracking Plan, navigate to the Tracking Plan you\u2019d like to view the ID for and select the dropdown next to Tracking Plan ID.\n\nIf you\u2019re using the Public API, you\u2019ll need the ID that starts with tp_.\n\nIf you\u2019re using the Config API, you\u2019ll need the ID that starts with rs.\n\nHow do I import events from a Source Schema into a Tracking Plan?\n\nWhen you first create your Tracking Plan, you can add events from your Source Schema by selecting the Import events from Source button on the Tracking Plan editor page. You can manually add these events after you\u2019ve connected your Source Schema to your Tracking Plan by clicking the (+) next to the event on your Source Schema page.\n\nCan I import events from my Source Schema into a Tracking Plan?\n\nWhen you initially create your Tracking Plan, you can import events into it from a Source Schema. Manually add these events by clicking the the (+) next to the event in your Source Schema page after connecting your Tracking Plan.\n\nCan I recover a Tracking Plan that was deleted?\n\nYou cannot recover a deleted Tracking Plan and Segment cannot recover it on your behalf. Please delete Tracking Plans with caution.\n\nProtocols Validation\nWhat is the difference between Violations Emails and the Violations page in the Segment UI?\n\nViolations Daily Digest The Violations Daily Digest is a great way to keep informed of new violations that might be easy to overlook on the Protocols Violations page. The digest sends one email digest per source, every day at approximately 12AM EST. You cannot currently opt in or out of specific sources.\n\nThe digest contains all violations for that source that are unique in the previous 48 hours. For example, if an event testEvent had violations on the first day of the month, then those violations won\u2019t appear in the digest until the third of the month.\n\nThe email includes information about the violation to help you track down its source and correct it. It includes the event name and property name fields, the violation type, the number of times that specific type of violation was seen, and the last time it was seen.\n\nProtocols Violations Page The Protocols Violations page shows a live count for violations. You can adjust the timeframe to show violations in the last hour, the last 24 hours, or the last seven days.\n\nYou might see a difference between the count on the Violations page and the count in the Violations email digests. This can happen due to differences between the time periods available (24 hours in in the live page, 48 hours in the daily digest email), and the fact that the digest only shows unique violations. The fields displayed on the Violations page are more detailed than those included in the email digest.\n\nWhy do I see root listed on my Violations page?\n\nYou may see violations related to (root). For example:\n\n(root)\nMust validate all the schemas\n// Or\n(root)\nMust validate \"then\" as \"if\" was valid\n\n\nThese violations are related to your common JSON Schema if you\u2019ve applied custom rules. In this instance (root), refers to the top level of the JSON object (Segment event).\n\nProtocols Enforcement\nWhy can\u2019t I use the Schema to filter my events?\n\nThe schema functionality is a reactive way to clean up your data, where the Tracking Plan functionality is a proactive, intentional way to clean and unify all future data. Segment has found that the best data driven companies invest the time required to build strong processes and controls around their data. The investment pays off exponentially.\n\nThat being said, there are plenty of scenarios where the reactive Schema functionality solves immediate needs for customers. Often times, customers will use both Schema Controls and Tracking Plan functionality across their Segment Sources. For smaller volume Sources with less important data, the Schema functionality often works perfectly.\n\nIf I enable blocking, what happens to the blocked events? Are events just blocked from specific Destinations or the entire Segment pipeline?\n\nBlocked events are blocked from sending to all Segment Destinations, including warehouses and streaming Destinations. When an Event is blocked using a Tracking Plan, it does not count towards your MTU limit. They will, however, count toward your MTU limit if you enable blocked event forwarding in your Source settings.\n\nIf I omit unplanned properties or properties that generate JSON schema violations, what happens to them?\n\nSegment doesn\u2019t store unplanned properties and properties omitted due to JSON Schema Violations in Segment logs. Segment drops omitted properties from the events. You can find the omitted properties in the context.violations object of an event payload. If you forward Violations to a new source, then you can also see the omitted properties in the Violation Generated event under violationField in the properties object.\n\nSegment only stores fully blocked events for 30 days.\n\nWhy am I seeing unplanned properties/traits in the payload when violations are triggered, despite using schema controls to omit them?\n\nIf you\u2019re seeing unplanned properties/traits in your payload despite using Schema Controls, you might want to select a new degree of blocking controls.\n\nSegment\u2019s Schema Controls provide three options to omit properties/traits. Select the one that aligns with your requirements:\n\nStandard Schema Controls/\u201dUnplanned Properties/Traits\u201d: Segment checks the names of incoming properties/traits against your Tracking Plan.\nStandard Schema Controls/\u201dJSON Schema Violations\u201d: Segment checks the names and evaluates the values of properties/traits. This is useful if you\u2019ve specified a pattern or a list of acceptable values in the JSON schema for each Track event listed in the Tracking Plan.\nAdvanced Blocking Controls/\u201dCommon JSON Schema Violations\u201d: Segment evaluates incoming events thoroughly, including event names, context field names and values, and the names and values of properties/traits, against the Common JSON schema in your Tracking Plan.\nWhy am I still seeing unplanned properties in my Source Schema when I\u2019ve added the properties to a new version of my Tracking Plan?\n\nThe source schema only validates events against the oldest event version in a Tracking Plan. If, for example, you have a version 1 and version 2 of your Tracking Plan, the schema only checks against version 1 of your Tracking Plan.\n\nDo blocked and discarded events count towards my MTU counts?\n\nBlocking events within a Source Schema or Tracking Plan excludes them from API call and MTU calculations, as the events are discarded before they reach the pipeline that Segment uses for calculations.\n\nDo warehouse connectors use the data type definitions when creating a warehouse schema?\n\nWarehouse connectors don\u2019t use data type definitions for schema creation. The data types for columns are inferred from the first event that comes in from the source.\n\nCan I use schema controls to block events forwarded to my source from another source?\n\nYou can only use schema controls to block events at the point that they are ingested into Segment. When you forward an event that Segment has previously ingested from another source, that event bypasses the pipeline that Segment uses to block events and cannot be blocked a second time.\n\nProtocols Transformations\nDo transformations work with Segment replays?\n\nIf you create a destination scoped transformation and request a replay for that destination, the transformation will transform events into the destination. Segment doesn\u2019t recommended requesting a replay to resend events to a destination as that will likely result in duplicate events in the destination.\n\nWhy can\u2019t I create multiple transformations of the same type for the same event?\n\nTo reduce the risk of creating circular and conflicting transformations, Segment only allows a single transformation to be created for each distinct source, event, destination and type pairing. That means you cannot create two Rename track event transformations for a order_completed event. This eliminates the possibility of different stakeholders creating conflicting transformations to satisfy their own needs. It also simplifies the Transformations list view, making it much easier to sort and filter by source, event, destination, etc.\n\nWhy can\u2019t I select multiple events or destinations in a single transformation?\n\nIn early transformations prototypes, Segment allowed users to select multiple events and destinations for a single transformation rule. Segment realized, however, that this created a structure that was impossible to scale, and likely to generate unintended consequences. For example, if Segment allows multiple track events to be selected for a property name change, it\u2019d be possible to create conflicting changes. Instead, by enforcing a single event, Segment can check to see if a transformation rule exists and smartly link you to that rule using a warning.\n\nWhat permissions are required to create and edit transformations?\n\nOnly workspace admins are allowed to create transformations.\n\nWhat permissions are required to view transformations?\n\nAll users with Protocols admin or read-only permissions can view transformations.\n\nWhy can\u2019t Segment support transformations for device-mode destinations?\n\nTransformations introduce advanced logic that at scale may impact performance of client-side libraries. If you are interested in testing new functionality which supports device-mode destination transformations in analytics.js, contact your account rep.\n\nAre Destination Filters applied before or after my Protocols Transformations?\n\nThat depends. If you are working with source-level Transformations, the Protocols conversion will come first. If you are dealing with a destination scoped transformation (which is set to only impact data going to a specific destination), Destination Filters will be applied prior to Protocols Transformations.\n\nWhy do I need Protocols to use transformations?\n\nTransformations are but one tool among many to help you improve data quality. Segment highly recommends that all customers interested in improving data quality start with a well defined Tracking Plan. The Tracking Plan serves as a roadmap for how you want to collect data. Without a clear roadmap, it\u2019s nearly impossible to build alignment around how transformations should be used to improve data quality, leading to more data quality issues than it solves.\n\nAre transformations applied when using the Event Tester?\n\nTransformations are not applied to events sent through the Event Tester. The Event Tester operates independently from the Segment pipeline, focusing solely on testing specific connections to a destination. For a transformation to take effect, the event must be processed through the Segment pipeline.\n\nWhy am I getting the error \u201crules must contain less than or equal to 200 items\u201d when using the Public API? Can I increase this limit?\n\nThis error occurs because there is a limit of 200 rules per API update. This restriction is by design to ensure stable API performance. Segment is not able to increase this limit on your behalf. To work around this, split your update into smaller batches, each with 200 or fewer rules.\n\nThis page was last modified: 08 Jan 2025\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nProtocols Notifications\nProtocols Tracking Plan\nProtocols Validation\nProtocols Enforcement\nProtocols Transformations\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nSegment App\n/\nIam\n/\nSingle Sign On team management\nSingle Sign On team management\nFREE X\nTEAM X\nBUSINESS \u2713\nADD-ON X\n?\n\nSegment supports Single Sign On for Business Tier accounts. You can use any SAML-based Identity Provider (IdP), for example Okta, Bitium, OneLogin, or Centrify, or use GSuite to serve as your identity provider, delegating access to the application based on rules you create in your central identity management solution.\n\nWith SSO, you have centralized control over your users\u2019 ability to authenticate or not in your IdP. You can also enforce rules like two-factor authentication or password rotation at the IdP level.\n\nYou can configure as many IdP connections to your workspace as needed to support IdP-initiated authentication. This allows seamless migration from one system to a new one, if, for example, your organization switches IdP vendors or switches from GSuite to a dedicated SAML IdP like Okta or OneLogin.\n\nTo enable SSO-based login from the Segment login page (app.segment.com/login), you must first verify that you own the domain, and connect it to your organization\u2019s Segment account. After you have done that, SSO users from your domain can use the Segment login page to access your default Segment workspace.\n\nThe Segment login page can only be connected to one workspace. To use your IdP with multiple workspaces, you will have to initiate login to the other workspaces from the IdP instead of through the login portal.\n\nSet up \u2014 SAML\n\nSegment\u2019s SSO configuration is entirely self-service. Additionally, Segment has prebuilt connections with Okta , OneLogin, and Microsoft Entra ID which can help you get set up faster. Reach out to support if you run into any questions or issues.\n\nTo get started, go to your workspace settings and navigate to Authentication > Connections > Add new Connection. Follow the steps to create a SAML connection.\n\nPrepare your IdP for the connection\n\nSegment officially supports apps for Okta, Microsoft Entra ID, and OneLogin. Next, find Segment in your IdP\u2019s app catalog, and follow the set up instructions they provide.\n\nIf you\u2019re using a different IdP, you must create a custom SAML-based application.\n\nYour provider will ask you for a few things from Segment, which Segment provides in the setup flow:\n\nA few gotchas to look out for:\n\nFor GSuite configurations, make sure the Start URL field in Service Provider Details is left blank.\n\nDifferent IdPs have different names for the Audience URL. Some call it \u201cAudience URI\u201d, some call it \u201cEntity ID\u201d, some call it \u201cService Provider Entity ID.\u201d It\u2019s likely there are only two required fields without correct defaults, and they correspond to the SSO URL and Audience URL values above.\n\nIn all IdPs Segment works with, the default NameID option is the correct one. Make sure it\u2019s using the emailAddress schema.\n\nIn all IdPs Segment works with, the default connection encryption options are the correct ones. (Signed Response & Assertion Signature with SHA256, Unencrypted Assertions).\n\nDifferent IdPs store records of your employees differently. The only attribute mapping Segment requires is to make sure you\u2019re sending email . In Okta this is at user.email. In Duo this is mail.\n\nMake sure you\u2019ve enabled \u201csend all attributes\u201d (not just NameID) if applicable for your IdP.\n\nNo RelayState is required. This is also sometimes called Target.\n\nAfter you create the application in your IdP, you can come back to Segment and click \u201cNext\u201d.\n\nConfigure Segment to Talk to Your IdP\n\nYour IdP provides a URL and x.509 certificate. Copy them into their respective fields in Segment.\n\nThen, click \u201cConfigure Connection.\u201d\n\nYou\u2019re all set.\n\nTest your connection with IdP-initiated SSO\n\nBack at the connections page, make sure your connection is enabled with the switch on the right.\n\nYou can now test using IdP-initiated SSO (by clicking login to Segment from within your IdP) is working correctly. If not, double check the IdP configuration gotchas section above.\n\nRequire SSO\n\nFor most customers, Segment recommends requiring SSO for all users. If you do not require SSO, users can still log in with a username and password. If some members cannot log in using SSO, Segment also supports SSO exceptions.\n\nThese options are off by default, but you can configure them on the Advanced Settings page. Log in using SSO to toggle the Require SSO setting.\n\nSetup \u2014 GSuite\n\nTo configure GSuite for use with Segment, go to your workspace settings and choose the \u201cConnections\u201d tab under \u201cAuthentication\u201d and click \u201cAdd New Connection.\u201d Follow the steps to create a \u201cGoogle Apps For Work\u201d connection.\n\nEnter your domain (or, if you\u2019ve verified it already, choose it from the dropdown) and then click the resulting link to authorize the connection.\n\nEnabling Segment-initiated login\n\nSegment supports SSO on the login page for emails that match your workspace\u2019s domain.\n\nIn order to enable this, you\u2019ll need to verify your domain with Segment. To do that, go to the \u201cDomains\u201d tab under \u201cAuthentication\u201d in the workspace settings page.\n\nEnter your domain and click \u201cAdd Domain.\u201d When you click verify, you\u2019re given two options to verify your domain, either using a meta tag to add to your /index.html at the root, or a DNS text record that you can add through your DNS provider. After you do so and click verify, you can move to the next step.\n\nDomain tokens expire 14 days after they are verified.\n\nConfiguring SSO to access multiple workspaces\n\nTo configure SSO for multiple workspaces, your admin must configure access to each workspace as a separate app in your identity provider. You are unable to use verified domain(s) across multiple workspaces and will encounter the following error if you add a domain that is already verified in another workspace:\n\nWarning: This domain has already been claimed.\n\nAfter your administrator configures separate apps for each workspace in your IdP, the end-users can log in to the IdP and click on the relevant app for the workspace you are trying to access. This is also referred to as IdP-initiated SSO.\n\nOkta setup\n\nThe Okta/Segment SAML integration supports the following features:\n\nIdP-initiated SSO\nSP-initiated SSO\nJIT (Just-in-time) provisioning\n\nFor more information on these features, visit the Okta Glossary.\n\nConfiguration steps\n\nTo set up the Okta/Segment SAML integration, you\u2019ll first carry out several steps in Segment, then finish in Okta.\n\nSegment steps\n\nFollow these steps in Segment to set up the Okta/Segment SAML integration:\n\nLog in to Segment as an administrator.\nNavigate to Settings > Authentication > Connections, then click Add new Connection.\nSelect SAML 2.0, then click Select Connection.\nOn the Configure IDP page, copy your Customer ID, which you\u2019ll find after ?connection= in the Single Sign-on URL field. You\u2019ll need this ID for a later step.\nFor example, if your Single Sign-On URL is https://acme.domen.com/login/callback?connection=a1b2c3d4, your Customer ID is a1b2c3d4.\nClick Next.\nOn the Configure Connection page, enter your SAML 2.0 Endpoint and Public Certificate. You can generate both in your Okta Admin Dashboard. After you\u2019ve entered both, click Next.\n(Optional:) Enter your domain, click Add Domain, then click Verify.\nWhen you click verify, Segment gives you two options to verify your domain: using a meta tag to add to your /index.html file at the root, or a DNS TXT record that you can add through your DNS provider. Domain tokens expire 14 days after they are verified.\nCarry out Step 7 only if you want to enable SP-initiated flow, otherwise click Skip.\nReturn to Settings > Authentication > Connections and toggle the Active switch to enable your SAML configuration.\n\nOkta steps\n\nFinish setting up the Okta/Segment SAML integration by carrying out these steps in Okta:\n\nIn Okta, go to Applications > Catalog > Segment & click \u201cAdd Integration\u201d.\nEnter an Application Label for your integration and click Next.\nSwitch to \u201cSign-On Options\u201d tab and select \u201cSAML 2.0\u201d.\nIn \u201cAdvanced Sign-on Settings\u201d, enter the Customer ID you copied in Step 4 of the Segment steps.\nFor Application username format, select Email.\nClick Save.\n\nYou\u2019ve now completed setup. For SP-initiated SSO, follow these steps:\n\nGo to https://app.segment.com.\nEnter your email, select Single Sign-On, then click Log In.\nSSO Frequently Asked Questions\nDo you support automatic user provisioning?\nDo you support automatic user de-provisioning?\nWill my users lose access to their other workspaces when I enable SSO?\nCan I still invite people outside the organization?\nWhat happens after I configured SSO to access multiple workspaces?\n\nThis page was last modified: 09 Jan 2025\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSet up \u2014 SAML\nPrepare your IdP for the connection\nConfigure Segment to Talk to Your IdP\nTest your connection with IdP-initiated SSO\nRequire SSO\nSetup \u2014 GSuite\nEnabling Segment-initiated login\nConfiguring SSO to access multiple workspaces\nOkta setup\nSSO Frequently Asked Questions\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSources\n/\nVisual Tagger\nVisual Tagger\n\nVisual Tagger entering maintenance mode\n\nVisual Tagger is entering maintenance mode on April 5th, 2021.\n\nYou can continue to use Visual Tagger with sources on which it\u2019s already enabled. However, the feature will no longer be available to new Segment customers and existing customers will not be able to add new Visual Tagger sources.\n\nSegment is committed to enabling customers to collect and deliver high quality customer data to the tools they need to run their businesses. As a CSS-based event tracking method, Visual Tagger has limitations that can prevent detailed data from being consistently collected. For code-based collection best practices, see the Segment Spec.\n\nGoing forward, support for the feature will include:\n\nFree, Team, and Startup customers will receive support with issues that impact multiple customers, but will not receive CSS-selector related troubleshooting\nBusiness customers will continue to receive full support\n\nThe following best practices can make your website more compatible with the feature and eliminate common issues:\n\nAssign unique IDs to all elements you intend to tag with Visual Tagger\nAdhere to HTML standards, such as forms inclosed in a <form> tag, and submitted with an <input type=\"submit\"> button.\n\nVisual Tagger is a tool that enables you to collect data about what your customers are doing on your websites without having to write any code. Specifically, it helps you implement track events by pointing and clicking on parts of your website.\n\nWith Visual Tagger, you can:\n\nCreate track events to start collecting data on actions that your users take, such as button or link clicks and form submissions. Events that you create using Visual Tagger work the same way as track events that you may have implemented in code. You\u2019ll be able to send them to any of the integrations in Segment\u2019s Catalog.\nCollect rich, contextual information in the form of properties to help you understand more about the specific action that the user took.\nTest your events to make sure that they are working as expected before you publish them live.\n\nThe Visual Tagger has two main views: the Visual Tagger Home and the Event Editor, which shows your website in an iframe.\n\nNote: The website you\u2019re tagging must include the Segment analytics.js snippet before you can use the Visual Tagger.\n\nSetting up Visual Tagger\nBefore you begin\n\nGet the following things set up before you use Visual Tagger:\n\nA Segment account and Workspace. You\u2019ll need to have either Workspace Owner or Source Admin-level permissions to create events using Visual Tagger.\nA website. Visual Tagger works best with simple marketing websites, like landing pages or content sites. Visual Tagger does not support mobile apps , but you can tag the mobile-web version of your websites.\n\nNote: Your website must use HTTPS.\n\nA JavaScript (Website) Source in Segment. Once you create a JavaScript Source, you must add the analytics.js snippet to the website for Visual Tagger to work. If you\u2019re having trouble with this step, follow the Analytics.js Quickstart Guide.\nChrome browser. Visual Tagger supports the Chrome browser.\nEnable Visual Tagger\n\nOnce you have all the prerequisites set up, you\u2019re ready to get started:\n\nGo to your Segment Workspace and navigate to your Website Source.\nClick the Visual Tagger tab to go to the Visual Tagger main page. You\u2019ll see an introduction, where you can learn about Visual Tagger and watch a short overview video.\nIf this is your first time on this page, click Get Started. Segment checks that you have the Visual Tagger Chrome extension installed and the correct source type. If either of these items is missing, follow the instructions to install the extension, and enable the Visual Tagger. Click Continue.\nVisual Tagger asks for the website type it can recommend events to track. You can also choose Other and describe your website, to help Segment prioritize which event types to add next.\nThe next screen shows Recommended Events, and you can choose which are relevant to your site. (You can also track any events beyond the recommended!)\nThe UI then loads a page with your website in an internal frame with the option to launch in a popup. Click one of the event type names to select it and begin configuring. For more information about event types and names. see Step 1 below.\n\nAs you click each event, you\u2019re prompted to select the part of the website that should trigger the event, and describe the properties that event should have.\n\nYou are now ready to tag events on your website!\n\nTip: If you ever need to get back to this screen, you can navigate to the JavaScript website source again, click the Visual tagger tab, then click Add Event.\n\nStep 1: Choose the event type and select an element\n\nTo create events, start in the Event Editor on the Build screen. This page shows an iframe with your website.\n\nYou can either choose one of the Recommended Events, or you can click Add Event and create a new one. Recommended Events work the same way, except Segment defines the Event Name, and each event comes with a set of default properties.\n\nWhen you click Add Event, three types of events are available:\n\nButton or Link Click. Select any button and link elements on the page to create events that fire when a user clicks that button or link. Depending on the construction of your website, some elements that look like buttons might not actually be CSS \u201cbutton\u201d elements - use the \u201cAny Element Click\u201d event instead for these elements.\n\nForm Submit. Select this option to highlight any form elements on the page so you can select them. When you choose Form Submit, Segment adds all form fields to your event automatically as properties.\n\nAny Element Click. Selecting this option will allow you to click on any element on your page. This will allow you to create an event for whenever a user on your website clicks on that element.\n\nOnce you choose an event type, mouse over your website in the iframe to highlight elements that you can select to create an event, and click one to start the process.\n\nIf the element on the page has siblings, you can tag them all at the same time, or tag just the one specific element you clicked.\n\nWhen you choose a Recommended Event, it works much in the same way, but the Event Name is pre-set and each event comes with a set of default properties.\n\nThe events you create are not saved until you publish them.\n\nStep 2: Add details to the event\n\nWhen you click on an element on your website, a window appears where you can enter the details for the event.\n\nEvent Name. Edit the event name to be simple yet descriptive. Segment recommends that you use an \u201cObject Action\u201d format (for example, Blog Post Clicked, and use Title Case (capitalize the first letter of each word ) when naming events.\n\nProperties. Add properties to the event to add contextual information about the action that the user took. Properties are optional, but they are very helpful when you analyze events data later.\nUse snake_case for property names (all lowercase, with spaces between words represented as an underscore \u201c_\u201d). For a guide on event naming best practices, check out the Protocols docs.\nCheck the list of properties that are collected by default before you add a property.\nAdvanced. You can also click the </> button to manually edit the CSS selector. If you didn\u2019t select the right element, you can choose the element on the page again by clicking on the finger button.\n\nOnce you\u2019re satisfied with the event name and properties, click Add.\n\nTip! You can create more than one event before you move on to the next step.\n\nMore information on properties\n\nWhen you use Visual Tagger, you can create static properties by entering the value that the property should have. When a property is static its value is hardcoded, and is always the same regardless of any actions from the user.\n\nYou can also create dynamic properties by selecting a piece of text from the website that you want as the value for that property. When a property is dynamic, its value is different depending on what the user clicked.\n\nFor example, if you run an e-commerce web shop and want to create an event that fires every time a user clicks on a product on your search results page, you would create a Product Clicked event.\n\nTo specify where the event was fired from, you would add a property called location to the event. You would hardcode the value of that property as \u201cSearch Results Page\u201d. This is an example of a static property.\n\nTo show which product the user clicked, you can add a property called product_name to the event. You can use the Visual Tagger \u201cselect from page\u201d feature to select the text on the iframe-version of your website that gives the product name. The value of the product_name property would then be different, depending on what users click.\n\nMore information on forms\n\nWhen you tag a form you can track both that the user submitted the form, and also update traits about the user.\n\nThis is useful if the form collects information about the user (such as Name, Email, Country). (This wouldn\u2019t be useful on a Search or Add To Cart form.)\n\nThis is an illustration of how traits and events might appear for a user in a Destination or Engage.\n\nIf you enable the Identify Users feature, you can specify which traits to update in the same way as properties for the event.\n\nMore information on URLs\n\nBy default, events created using the Visual Tagger only fire on the same URL as the one where you tagged the event. This prevents unexpected events if the same CSS selector exists on other pages of your site.\n\nIf you have similar pages with different URLs such as /products/1 and /products/2 and you want to same event to fire on both, you can change the URL Page Targeting to match /products instead of /products/1 or any page on the entire website.\n\nStep 3: Test the event(s)\n\nOnce you finish filling out the event details, click Continue to go to the Test and Publish screen. On this screen you can test your event to confirm that it works as expected before you publish it.\n\nTest your events by clicking around on your website in the iframe, and doing things on the iframe-version of your site that should trigger the event.\n\nWhen the event fires, a green icon appears in the Test Status column.\nIf the event fires, but does not contain any properties you created, the Test Status lists the errors.\nIf the event does not fire, the Test Status remains grey.\n\nIf something doesn\u2019t look right, click Back to return to the Build screen and edit your tags.\n\nIf you\u2019re having trouble validating your events, see Troubleshooting Tips.\n\nStep 4: Publish the event(s)\n\nWhen you\u2019ve finished setting up and testing your events, click Publish.\n\nEvents can take up to 10 minutes appear on your website. Once they are live, events begin sending to Segment when people visit your website and interact with the elements that you created events for. The data from these events appears in the Website Source\u2019s Debugger.\n\nStep 5: Test the event to confirm that it works\n\nAfter you publish your event and wait for ~10 minutes, do a last test to make sure your tags are working expected and that you see your data landing in your Segment Debugger.\n\nIn one window, open up your website where you created the event. In another window (side by side), open the Segment Debugger for your JavaScript Source. The Debugger is a Segment tool that shows a live stream of the data coming from that Source.\n\nGood to know: The Debugger automatically pauses after ~1 minute of idle time. Refresh the page if you leave and come back to it, and don\u2019t see new data.\n\nOn your website, pretend that you are an end-user and try to trigger the event that you created in Visual Tagger. Watch the Debugger in the other window to confirm that the event fired when you took that action.\n\nAll events created using Visual Tagger automatically get a context property that says \"visual_tagger\" : true so that you can distinguish between events that you implemented using Visual Tagger and events that you implemented using code.\n\nManaging and editing Visual Tagger events\n\nOnce you publish your events, they appear on the Visual Tagger Home view in the All Published Events table. From here, you can create click Add Event to create new events, and edit or delete existing ones.\n\nTroubleshooting your events\n\nIf your events are not working as expected, try the following steps to troubleshoot the issue.\n\nEnsure that you toggle Visual Tagger integration on. You find it in the Settings for your Source.\n\nEnsure that your website has analytics.js installed and running. You can confirm this by visiting your website and watching the Segment Debugger. A page call should flow into your Debugger whenever someone visits a page on your website if you have implemented analytics.js. Note that if you have an ad blocker enabled when you visit your website, data will not get fired into Segment when you interact with your website.\n\nConfirm that the CSS selector has not changed in any way since you created the event in the Visual Tagger (for example, a button could change locations or be removed entirely). Because Visual Tagger relies on the CSS selector to tie events to user actions, if the CSS selector changes, the event stops sending. If this happens, edit the event in Visual Tagger and update the CSS selector. Segment does not have a way to alert you when events you created using Visual Tagger begin to fail.\n\nBecause Visual Tagger relies on stable CSS selectors to fire events, it is not compatible with websites that have CSS selectors that are dynamically generated on every page load.\n\nIf your website has any components in iframes (for example, if you embed Typeforms into your site), Visual Tagger cannot create events for those components. Segment recommends that you install analytics.js on the iframed-in site and use Visual Tagger directly on that site.\n\nIf you use the same name for multiple events (whether in code or using Visual Tagger), duplicate events are not created downstream. Instead, Segment merges those events into one event.\n\nIf your events still don\u2019t work as expected, contact Segment Customer Support for help.\n\nFAQs\nAre there situations where Visual Tagger will not work?\n\nYes! Visual Tagger relies on CSS selectors, which involves \u201cclass names\u201d such as \u201chero\u201d or \u201cfooter\u201d. Some web technologies automatically generate these names and change them on a regular basis, which makes the Visual Tagger events stop firing.\n\nSquarespace uses this technique, and you can examine the HTML of your site to see if the \u201cclass names\u201d are randomized letters/numbers.\n\nVisual Tagger also does not support embedded elements, such as a YouTube video player or a Hubspot form.\n\nWhat do I do if my website does not behave correctly inside the VT iframe?\n\nWhen you load your website in the Visual tagger iframe, you might see unexpected or incorrect behavior. This is because browsers load websites differently inside an iframe than in a regular browser window. For example, Google Chrome blocks certain types of cookies when a page is loaded inside an iframe, and this can cause problems with authentication or other functions.\n\nClick the Open in Popup button (above the top right corner of the iframed website) if you experience unexpected behavior when you load your website in the VT iframe, including issues with the login or authentication, or errors with form submissions. This opens the website in a new browser window (outside of an iframe) which is connected to the Visual Tagger.\n\nTip!: You might want to change the width of both the Visual tagger window and your website window so you can view them side by side for easier tagging.\n\nHow can I make my website ideal for Visual Tagger?\n\nThe most stable way for Visual tagger to identify elements is if each one has a unique ID that persists even if the page is reorganized.\n\nWhen should I use Visual Tagger instead of a coded instrumentation?\n\nVisual Tagger is a great way to get started with tracking, but over time you might need to augment with coded instrumentation.\n\nThese are ideal use cases for Visual Tagger:\n\nUnderstanding how users are engaging with your public website. Beyond page tracking, you can learn which CTA\u2019s are most popular, collect information from forms, learn when users engage with interactive content like a carousel.\n\nSimilarly for campaign landing pages, that needs to go live with short turnaround, and require tracking forms, CTA, and other interactive content\n\nUnderstanding your ecommerce funnel, how users browse and filter products, add them to cart before checking out and completing an order\n\nLearn more about how users use your product or service, after logging in. Track key semantic events in your product such as Project Created (project management app), Account Upgraded, Listing Favorited (apartment rental site) etc to learn about adoption, engagement, and retention\n\nThese are examples of when to augment with coded instrumentation:\n\nDepth of instrumentation\nAfter understanding the basic ecommerce funnel, you might want to go deeper and understand the value of orders, or value of products abandoned in the cart. This requires careful formatting of the price, for each product in the cart, which is not supported by Visual Tagger.\nSimilarly if you want to use Google Analytics Ecommerce functionality, some events must have the order_id sent as a property which is not supported by Visual Tagger yet.\nAssociating events with the logged in user requires that you instrument an identify call in code.\nFrequency of change\n\nIf your website or application changes frequently, you will need to keep track of each change and update Visual Tagger events accordingly. In that situation, it can be beneficial for the engineering team to have the event tracking in code and update at the same time as changing how a feature work.\n\nWill using Visual Tagger impact my site or app\u2019s performance?\n\nThe Visual Tagger integration has negligible impact to your site\u2019s performance, because it installs a single event handler that makes an asynchronous call to Segment when a tag\u2019s event is invoked.\n\nHowever, adding a large number of tags to your site could potentially impact your site\u2019s performance. To guard against this we limit the number of tags you can add using Visual Tagger to a maximum of 50.\n\nDoes Visual Tagger work with dynamically generated elements or Single Page Applications?\n\nYes. You can track dynamically generated elements like modals (for example) using the Visual Tagger.\n\nWhy does my form submit event not work?\n\nVisual Tagger binds its event listeners to the document object. Forms using stopPropagation and stopImmediatePropagation methods prevent the event from bubbling up, causing Visual Tagger to not execute the handlers for emitting track/identify calls. Remove any stopPropagation and stopImmediatePropagation method calls from your form handler to allow Visual Tagger to process the event.\n\nDoes Visual Tagger have a data layer so that I can make use of data that\u2019s not rendered on the page?\n\nCurrently, only information that is visually present on the page is available for use in the Visual Tagger.\n\nHow can I tell which events were created using Visual Tagger versus which were implemented using code?\n\nEvents that were added using the Visual Tagger (as opposed to in code) have a context property in the event payload that says \"visual_tagger\": true. Events not implemented using the Visual Tagger do not have this property.\n\nThe old version of Visual Tagger didn\u2019t have support for MFA or SSO. What about the new version?\n\nBecause the new version of Visual Tagger is available in the Segment app, Workspaces that have MFA or SSO enabled are able to access it.\n\nThis page was last modified: 09 Aug 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSetting up Visual Tagger\nEnable Visual Tagger\nStep 1: Choose the event type and select an element\nStep 2: Add details to the event\nStep 3: Test the event(s)\nStep 4: Publish the event(s)\nStep 5: Test the event to confirm that it works\nManaging and editing Visual Tagger events\nTroubleshooting your events\nFAQs\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nStorage\n/\nSegment Data Lakes Overview\nSegment Data Lakes Overview\nFREE X\nTEAM X\nBUSINESS \u2713\nADD-ON X\n?\n\nSegment Data Lakes will enter Limited Access in October 2024\n\nAfter Segment Data Lakes enters Limited Access, new customers will no longer be able to create Segment Data Lake instances. Existing Segment customers with Data Lakes instances will continue to receive data and can create Data Lakes Destinations.\n\nSegment recommends considering alternative solutions, like AWS S3 or Databricks.\n\nA data lake is a centralized cloud storage location that holds structured and unstructured data.\n\nData lakes typically have four layers:\n\nStorage layer: Holds large files and raw data.\nMetadata store: Stores the schema, or the process used to organize the files in the object store.\nQuery layer: Allows you to run SQL queries on the object store.\nCompute layer: Allows you to write to and transform the data in the storage layer.\n\nSegment Data Lakes sends Segment data to a cloud data store, either AWS S3 or Azure Data Lake Storage Gen2 (ADLS), in a format optimized to reduce processing for data analytics and data science workloads. Segment data is great for building machine learning models for personalization and recommendations, and for other large scale advanced analytics. Data Lakes reduces the amount of processing required to get real value out of your data.\n\nSegment Data Lakes deletion policies\n\nSegment Data Lakes (AWS) and Segment Data Lakes (Azure) do not support Segment\u2019s user deletion and suppression capabilities, as you retain your data in systems that you manage.\n\nTo learn more about Segment Data Lakes, check out the Segment blog post Introducing Segment Data Lakes.\n\nHow Data Lakes work\n\nSegment supports Data Lakes hosted on two cloud providers: Amazon Web Services (AWS) and Microsoft Azure. Each cloud provider has a similar system for managing data, but offer different query engines, post-processing systems, and analytics options.\n\nHow Segment Data Lakes (AWS) works\n\nData Lakes store Segment data in S3 in a read-optimized encoding format (Parquet) which makes the data more accessible and actionable. To help you zero-in on the right data, Data Lakes also creates logical data partitions and event tables, and integrates metadata with existing schema management tools, such as the AWS Glue Data Catalog. The resulting data set is optimized for use with systems like Spark, Athena, EMR, or machine learning vendors like DataBricks or DataRobot.\n\nSegment sends data to S3 by orchestrating the processing in an EMR (Elastic MapReduce) cluster within your AWS account using an assumed role. Customers using Data Lakes own and pay AWS directly for these AWS services.\n\nHow Segment Data Lakes (Azure) works\n\nData Lakes store Segment data in ADLS in a read-optimized encoding format (Parquet) which makes the data more accessible and actionable. To help you zero-in on the right data, Data Lakes also creates logical data partitions and event tables, and integrates metadata with existing schema management tools, like the Hive Metastore. The resulting data set is optimized for use with systems like Power BI and Azure HDInsight or machine learning vendors like Azure Databricks or Azure Synapse Analytics.\n\nSet up Segment Data Lakes (Azure)\n\nFor detailed Segment Data Lakes (Azure) setup instructions, see the Data Lakes setup page.\n\nSet up Segment Data Lakes (AWS)\n\nWhen setting up your data lake using the Data Lakes catalog page, be sure to consider the EMR and AWS IAM components listed below.\n\nEMR\n\nData Lakes uses an EMR cluster to run jobs that load events from all sources into Data Lakes. The AWS resources portion of the set up instructions sets up an EMR cluster using the m5.xlarge node type. Data Lakes keeps the cluster always running, however the cluster auto-scales to ensure it\u2019s not always running at full capacity. Check the Terraform module documentation for the EMR specifications.\n\nAWS IAM role\n\nData Lakes uses an IAM role to grant Segment secure access to your AWS account. The required inputs are:\n\nexternal_ids: External IDs are the part of the IAM role which Segment uses to assume the role providing access to your AWS account. You will define the external ID in the IAM role as the Segment Workspace ID in which you want to connect to Data Lakes. The Segment Workspace ID can be retrieved from the Segment app by navigating to Settings > General Settings > ID.\ns3_bucket: Name of the S3 bucket used by the Data Lake.\nSet up Segment Data Lakes (Azure)\n\nTo connect Segment Data Lakes (Azure), you must set up the following components in your Azure environment:\n\nAzure Storage Account: An Azure storage account contains all of your Azure Storage data objects, including blobs, file shares, queues, tables, and disks.\nAzure KeyVault Instance: Azure KeyVault provides a secure store for your keys, secrets, and certificates.\nAzure MySQL Database: The MySQL database is a relational database service based on the MySQL Community Edition, versions 5.6, 5.7, and 8.0.\nDatabricks Instance: Azure Databricks is a data analytics cluster that offers multiple environments (Databricks SQL, Databricks Data Science and Engineering, and Databricks Machine Learning) for you to develop data-intensive applications.\nDatabricks Cluster: The Databricks cluster is a cluster of computation resources that you can use to run data science and analytics workloads.\nService Principal: Service principals are identities used to access specific resources.\n\nFor more information about configuring Segment Data Lakes (Azure), see the Data Lakes setup page.\n\nData Lakes schema\n\nSegment Data Lakes applies a standard schema to make the raw data easier and faster to query. Partitions are applied to the S3 data for granular access to subsets of the data, schema components such as data types are inferred, and a map of the underlying data structure is stored in a Glue Database.\n\nSegment Data Lakes (AWS) schema\nS3 partition structure\n\nSegment partitions the data in S3 by the Segment source, event type, then the day and hour an event was received by Segment, to ensure that the data is actionable and accessible.\n\nThe file path looks like: s3://<top-level-Segment-bucket>/data/<source-id>/segment_type=<event type>/day=<YYYY-MM-DD>/hr=<HH>\n\nHere are a few examples of what events look like: s3:YOUR_BUCKET/segment-data/data/SOURCE_ID/segment_type=identify/day=2020-05-11/hr=11/ s3:YOUR_BUCKET/segment-data/data/SOURCE_ID/segment_type=identify/day=2020-05-11/hr=12/ s3:YOUR_BUCKET/segment-data/data/SOURCE_ID/segment_type=identify/day=2020-05-11/hr=13/\n\ns3:YOUR_BUCKET/segment-data/data/SOURCE_ID/segment_type=page_viewed/day=2020-05-11/hr=11/ s3:YOUR_BUCKET/segment-data/data/SOURCE_ID/segment_type=page_viewed/day=2020-05-11/hr=12/ s3:YOUR_BUCKET/segment-data/data/SOURCE_ID/segment_type=page_viewed/day=2020-05-11/hr=13/\n\nBy default, the date partition structure is day=<YYYY-MM-DD>/hr=<HH> to give you granular access to the S3 data. You can change the partition structure during the set up process, where you can choose from the following options:\n\nDay/Hour [YYYY-MM-DD/HH] (Default)\nYear/Month/Day/Hour [YYYY/MM/DD/HH]\nYear/Month/Day [YYYY/MM/DD]\nDay [YYYY-MM-DD]\nAWS Glue data catalog\n\nData Lakes stores the inferred schema and associated metadata of the S3 data in AWS Glue Data Catalog. This metadata includes the location of the S3 file, data converted into Parquet format, column names inferred from the Segment event, nested properties and traits which are now flattened, and the inferred data type.\n\nNew columns are appended to the end of the table in the Glue Data Catalog as they are detected.\n\nGlue database\n\nThe schema inferred by Segment is stored in a Glue database within Glue Data Catalog. Segment stores the schema for each source in its own Glue database to organize the data so it is easier to query. To make it easier to find, Segment writes the schema to a Glue database named using the source slug by default. The database name can be modified from the Data Lakes settings.\n\nThe recommended IAM role permissions grant Segment access to create the Glue databases on your behalf. If you do not grant Segment these permissions, you must manually create the Glue databases for Segment to write to.\n\nSegment Data Lakes (Azure) schema\n\nSegment Data Lakes (Azure) applies a consistent schema to make raw data accessible for queries. A transformer automatically calculates the desired schema and uploads a schema JSON file for each event type to your Azure Data Lake Storage (ADLS) in the /staging/ directory.\n\nSegment partitions the data in ALDS by the Segment source, event type, then the day and hour an event was received by Segment, to ensure that the data is actionable and accessible.\n\nThe file path looks like this: <storage-account-name>/<container-name>/staging/<source-id>/\n\nData types\n\nData Lakes infers the data type for an event it receives. Groups of events are polled every hour to infer the data type for that each event.\n\nThe data types supported in Segment Data Lakes are:\n\nbigint\nboolean\ndecimal(38,6)\nstring\ntimestamp\nSchema evolution\n\nOnce Data Lakes sets a data type for a column, all subsequent data will attempt to be cast into that data type. If incoming data does not match the data type, Data Lakes tries to cast the column to the target data type.\n\nSize mismatch\n\nIf the data type in Glue is wider than the data type for a column in an on-going sync (for example, a decimal vs integer, or string vs integer), then the column is cast to the wider type in the Glue table. If the column is narrower (for example, integer in the table versus decimal in the data), the data might be dropped if it cannot be cast at all, or in the case of numbers, some data might lose precision. The original data in Segment remains in its original format, so you can fix the types and replay to ensure no data is lost. Learn more about type casting by reading the W3School\u2019s Java Type Casting page.\n\nData mismatch\n\nIf Data Lakes sees a bad data type, for example text in place of a number or an incorrectly formatted date, it attempts a best effort conversion to cast the field to the target data type. Fields that cannot be cast may be dropped. You can also correct the data type in the schema to the desired type and Replay to ensure no data is lost. Contact Segment Support if you find a data type needs to be corrected.\n\nData Lake deduplication\n\nIn addition to Segment\u2019s 99% guarantee of no duplicates for data within a 24 hour look-back window, Data Lakes have another layer of deduplication to ensure clean data in your Data Lake. Segment removes duplicate events at the time your Data Lake ingests data. Data Lakes deduplicate any data synced within the last seven days, based on the messageId field.\n\nUsing a Data Lake with a Data Warehouse\n\nThe Data Lakes and Warehouses products are compatible using a mapping, but do not maintain exact parity with each other. This mapping helps you to identify and manage the differences between the two storage solutions, so you can easily understand how the data in each is related. You can read more about the differences between Data Lakes and Warehouses.\n\nWhen you use Data Lakes, you can either use Data Lakes as your only source of data and query all of your data directly from S3 or ADLS or you can use Data Lakes in addition to a data warehouse.\n\nFAQ\nCan I send all of my Segment data into Data Lakes?\n\nData Lakes supports data from all event sources, including website libraries, mobile, server and event cloud sources. Data Lakes doesn\u2019t support loading object cloud source data, as well as the users and accounts tables from event cloud sources.\n\nAre user deletions and suppression supported?\n\nSegment doesn\u2019t support User deletions in Data Lakes, but supports user suppression.\n\nHow does Data Lakes handle schema evolution?\n\nAs the data schema evolves, both Segment Data Lakes (AWS) and Segment Data Lakes (Azure) can detect new columns and add them to Glue Data Catalog or Azure Data Lake Storage (ADLS). However, Segment can\u2019t update existing data types. To update Segment-created data types, please reach out to AWS Support or Azure Support.\n\nHow does Data Lakes work with Protocols?\n\nData Lakes has no direct integration with Protocols.\n\nAny changes to events at the source level made with Protocols also change the data for all downstream destinations, including Data Lakes.\n\nMutated events - If Protocols mutates an event due to a rule set in the Tracking Plan, then that mutation appears in Segment\u2019s internal archives and reflects in your data lake. For example, if you use Protocols to mutate the event product_id to be productID, then the event appears in both Data Lakes and Warehouses as productID.\n\nBlocked events - If a Protocols Tracking Plan blocks an event, the event isn\u2019t forwarded to any downstream Segment destinations, including Data Lakes. However events which are only marked with a violation are passed to Data Lakes.\n\nData types and labels available in Protocols aren\u2019t supported by Data Lakes.\n\nData Types - Data Lakes infers the data type for each event using its own schema inference systems instead of using a data type set for an event in Protocols. This might lead to the data type set in a data lake being different from the data type in the tracking plan. For example, if you set product_id to be an integer in the Protocols Tracking Plan, but the event is sent into Segment as a string, then Data Lakes may infer this data type as a string in the Glue Data Catalog.\nLabels - Labels set in Protocols aren\u2019t sent to Data Lakes.\nHow frequently does my Data Lake sync?\n\nData Lakes offers 12 syncs in a 24 hour period and doesn\u2019t offer a custom sync schedule or selective sync.\n\nWhat is the cost to use AWS Glue?\n\nYou can find details on Amazon\u2019s pricing for Glue page. For reference, Data Lakes creates 1 table per event type in your source, and adds 1 partition per hour to the event table.\n\nWhat is the cost to use Microsoft Azure?\n\nYou can find details on Microsoft\u2019s pricing for Azure page. For reference, Data Lakes creates 1 table per event type in your source, and adds 1 partition per hour to the event table.\n\nWhat limits does AWS Glue have?\n\nAWS Glue has limits across various factors, such as number of databases per account, tables per account, and so on. See the full list of Glue limits for more information.\n\nThe most common limits to keep in mind are:\n\nDatabases per account: 10,000\nTables per database: 200,000\nCharacters in a column name: 250\n\nSegment stops creating new tables for the events after you exceed this limit. However you can contact your AWS account representative to increase these limits.\n\nYou should also read the additional considerations in Amazon\u2019s documentation when using AWS Glue Data Catalog.\n\nWhat analytics tools are available to use with Segment Data Lakes (Azure)?\n\nSegment Data Lakes (Azure) supports the following analytics tools:\n\nPowerBI\nAzure HDInsight\nAzure Synapse Analytics\nDatabricks\n\nThis page was last modified: 02 Aug 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nHow Data Lakes work\nSet up Segment Data Lakes (Azure)\nData Lakes schema\nFAQ\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nStorage\n/\nWarehouses\n/\nWarehouse Health Dashboard\nWarehouse Health Dashboard\n\nThe Warehouse Health dashboard helps you understand trends in data volume (specifically, rows) synced to your data warehouse over time.\n\nYou can use this feature to answer questions such as:\n\nGrowth patterns - How has the volume of data synced to the warehouse grown over time? How does this growth align to the storage capacity available in my warehouse?\nAnomaly detection - How much data is being synced on a daily basis? Have there been anomalous spikes or dips that may indicate sudden changes in event volume, sync failures, or something else?\nData composition - Which sources are contributing the most (or least) amount of data in my warehouse? Which collections make up the majority of data within a source?\n\nNote: Warehouse Health is available for all Warehouse customers.\n\nThe Warehouse Health dashboards are available at both the warehouse level, and at the warehouse-source connection level, explained below.\n\nData in the dashboards updates in real-time, and covers the previous 30 days. The timezones displayed in the dashboards are converted to the viewer\u2019s local time.\n\nWarehouse dashboards\n\nGo to the Segment App, to the Destinations list, and select the warehouse. On the warehouse\u2019s information page, click the Health tab.\n\nThis dashboard displays aggregate trends from all sources that sync to the specific warehouse.\n\nA warehouse level dashboard\n\nWarehouse-Source dashboards\n\nGo to the Segment App, to the Destinations list, and select the warehouse. On the warehouse\u2019s Overview page, select the Source (schema) you want to see data for, then click the Health tab.\n\nThis dashboard displays trends for each separate source that syncs to a specific warehouse. It also displays aggregations of the collections within that source.\n\nA warehouse-source level dashboard\n\nWarehouse Health Dashboard FAQs\nCan I use the Health Dashboard data for QA and validation?\n\nNo. These dashboards exist to help you understand high-level trends, but not to provide exact numbers about the data synced to the warehouse. The numbers provided in these dashboards are rounded, and are not exact.\n\nThese dashboards will help you understand trends in the data, and use signals to do deeper investigation and QA, as needed.\n\nHow is this similar (or different) than the information available in the Sync History and Overview tabs?\n\nThe Warehouse Overview, Sync History and Health tabs provide different levels of granularity into warehouse syncs.\n\nOverview - Shows which sources (also referred to as schemas) are connected to a warehouse, and information about the most recent sync and upcoming sync for each source. This information includes when the last sync happened, what the status of that sync is, how many events were synced, and when the next sync is scheduled.\nSync History - Shows detailed information about most recent syncs for a specific source connected to a warehouse (warehouse-source level). In this tab you can find information for each sync including sync status, start time, duration, synced rows, and notices about errors and/or warnings.\nHealth - The Health tab provides an aggregate view of syncs to a warehouse over time. You can either look at this at a warehouse level, or warehouse-source level. This shows information about the volume of rows synced over the last 30 days.\nHow often is the data refreshed?\n\nData is refreshed on a real time basis.\n\nWhat timeframe is the data available for?\n\nThe data available shows the last 30 days.\n\nWhat timezone are the dates in?\n\nAll dates and times found within Warehouse Health, Sync History and Warehouse overview pages are in the user\u2019s local time.\n\nThis page was last modified: 21 Apr 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nWarehouse dashboards\nWarehouse-Source dashboards\nWarehouse Health Dashboard FAQs\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nApi\n/\nPublic Api\n/\nDestination Filter Query Language\nDestination Filter Query Language\n\nThe Segment Public API is available\n\nSegment\u2019s Public API is available for Team and Business tier customers to use. You can use the Public API and Config APIs in parallel, but moving forward any API updates will come to the Public API exclusively.\n\nPlease contact your account team or friends@segment.com with any questions.\n\nThis reference provides a comprehensive overview of the Segment Destination Filter query language. For information on the Destination Filters API (including information on migrating from the Config API), visit the Destination Filters API reference.\n\nThe Transformations API uses Filter Query Language (FQL) to filter JSON objects and conditionally apply transformations. You can use FQL statements to:\n\nApply filters that evaluate to true or false based on the contents of each Segment event. If the statement evaluates to true, the transformation is applied, and if it is false the transformation is not applied.\nDefine new properties based on the result of an FQL statement.\n\nIn addition to boolean and equality operators like and and >=, FQL has built-in functions that make it more powerful such as contains( str, substr ) and match( str, pattern ).\n\nExamples\n\nGiven the following JSON object:\n\n{\n  \"event\": \"Button Clicked\",\n  \"type\": \"track\",\n  \"context\": {\n    \"library\": {\n      \"name\": \"analytics.js\",\n      \"version\": \"1.0\"\n    }\n  },\n  \"properties\": {\n    \"features\": [\"discounts\", \"dark-mode\"]\n  }\n}\n\n\nThe following FQL statements will evaluate as follows:\n\nFQL\tRESULT\nevent = 'Button Clicked'\ttrue\nevent = 'Screen Tapped'\tfalse\ncontext.path.path = '/login'\tfalse\ntype = 'identify' or type = 'track'\ttrue\nevent = 'Button Clicked' and type = 'track'\ttrue\nmatch( context.library.version, '1.*' )\ttrue\nmatch( context.library.version, '2.*' )\tfalse\ntype = 'track' and ( event = 'Click' or match( event, 'Button *' ) )\ttrue\n!contains( context.library.name, 'js' )\tfalse\n'dark-mode' in properties.features\ttrue\n'blink' in properties.features\tfalse\nField Paths\n\nFQL statements may refer to any field in the JSON object including top-level properties like userId or event as well as nested properties like context.library.version or properties.title using dot-separated paths. For example, the following fields can be pointed to by the associated field paths:\n\n{\n  \"type\": \"...\",       // type\n  \"event\": \"...\",      // event\n  \"context\": {         // context\n    \"library\": {       // context.library\n      \"name\": \"...\"    // context.library.name\n    },\n    \"page\": {          // context. page\n      \"path\": \"...\",   // context.page.path\n    }\n  }\n}\n\nEscaping Field Paths\n\nIf your field name has a character not in the set of {a-z A-Z 0-9 _ -}, you must escape it using a \\ character. For example, the nested field below can be referred to by properties.product\\ 1.price:\n\n{\n  \"properties\": {\n    \"product 1\": {\n      \"price\": \"19.99\"\n    }\n  }\n}\n\nOperators\nBoolean\nOPERATOR\tLEFT SIDE\tRIGHT SIDE\tRESULT\nand\tbool or null\tbool or null\ttrue if the left and right side are both true, false otherwise.\nor\tbool or null\tbool or null\ttrue if at least one side is true, false if either side is false or null.\nUnary\nOPERATOR\tRIGHT SIDE\tRESULT\n!\tbool\tNegates the right-hand side.\nComparison\nOPERATOR\tLEFT SIDE\tRIGHT SIDE\tRESULT\n=\tstring, number, list, bool, or null\tstring, number, list, bool, or null\ttrue if the left and right side are the same type and are strictly equal, false otherwise.\n!=\tstring, number, list, bool, or null\tstring, number, list, bool, or null\ttrue if the left and right side are different types or if they are not strictly equal, false otherwise.\n>\tnumber\tnumber\ttrue if the left side is greater than the right side.\n>=\tnumber\tnumber\ttrue if the left side is greater than or equal to the right side.\n<\tnumber\tnumber\ttrue if the left side is less than the right side.\n<=\tnumber\tnumber\ttrue if the left side is less than or equal to the right side.\nin\tstring, number, bool, or null\tlist\ttrue if the left side is contained in the list of values.\nSubexpressions\n\nYou can use parentheses to group subexpressions for more complex \u201cand / or\u201d logic as long as the subexpression evaluates to true or false:\n\nFQL\ntype = 'track' and ( event = 'Click' or match( 'Button *', event ) )\n( type = 'track' or type = 'identify' ) and ( properties.enabled or match( traits.email, '*@company.com' ) )\n!( type in ['track', 'identify'] )\nFunctions\nFUNCTION\tRETURN TYPE\tRESULT\ncontains( s string, sub string )\tbool\tReturns true if string s contains string sub.\nlength( list or string )\tnumber\tReturns the number of elements in a list or number of bytes (not necessarily characters) in a string. For example, a is 1 byte and\u30a2 is 3 bytes long. Please note that you can\u2019t use this function with JSON as the argument. Using JSON may result in the function not working.\nlowercase( s string )\tstring\tReturns s with all uppercase characters replaced with their lowercase equivalent.\nuppercase( s string )\tstring\tReturns s with all lowercase characters replaced with their uppercase equivalent.\nsnakecase( s string )\tstring\tReturns s with all space characters replaced by underscores. For example, kebabcase(\"test string\") returns test_string.\nkebabcase( s string )\tstring\tReturns s with all space characters replaced by dashes. For example, kebabcase(\"test string\") returns test-string.\ntitlecase( s string )\tstring\tReturns s with all space characters replaced by dashes. For example, titlecase(\"test string\") returns Test String.\ntypeof( value )\tstring\tReturns the type of the given value: \"string\", \"number\", \"list\", \"bool\", or \"null\".\nmatch( s string, pattern string )\tbool\tReturns true if the glob pattern pattern matches s. See below for more details about glob matching.\nbool( list or string or number or nil )\tbool\tConverts the value to a boolean value.\nstring( list or string or number or nil )\tstring\tConverts the value to a string value.\nnumber( number or string )\tnumber\tConverts the value to a number value.\n\nFunctions handle null with sensible defaults to make writing FQL more concise. For example, you can write length( userId ) > 0 instead of typeof( userId ) =\n'string' and length( userId ) > 0.\n\nFUNCTION\tRESULT\ncontains( null, string )\tfalse\nlength( null )\t0\nlowercase( null )\tnull\ntypeof( null )\t\"null\"\nmatch( null, string )\tfalse\nmatch( string, pattern )\n\nThe match( string, pattern ) function uses \u201cglob\u201d matching to return true if the given string fully matches a given pattern. Glob patterns are case sensitive. If you only need to determine if a string contains another string, you should use contains().\n\nPATTERN\tSUMMARY\n*\tMatches zero or more characters.\n?\tMatches one character.\n[abc]\tMatches one character in the given list. In this case, a, b, or c will be matched.\n[a-z]\tMatches a range of characters. In this case, any lowercase letter will be matched.\n\\x\tMatches the character x literally. This is useful if you need to match *, ? or ] literally. For example, \\*.\nPATTERN\tRESULT\tREASON\nmatch( 'abcd', 'a*d' )\ttrue\t* matches zero or more characters.\nmatch( '', '*' )\ttrue\t* matches zero or more characters.\nmatch( 'abc', 'ab' )\tfalse\tThe pattern must match the full string.\nmatch( 'abcd', 'a??d' )\ttrue\t? matches one character only.\nmatch( 'abcd', '*d' )\ttrue\t* matches one or more characters even at the beginning or end of the string.\nmatch( 'ab*d', 'ab\\*d' )\ttrue\t\\* matches the literal character *.\nmatch( 'abCd', 'ab[cC]d' )\ttrue\t[cC] matches either c or C.\nmatch( 'abcd', 'ab[a-z]d' )\ttrue\t[a-z] matches any character between a and z.\nmatch( 'abcd', 'ab[A-Z]d' )\tfalse\t[A-Z] matches any character between A and Z but c is not in that range because it is lowercase.\nError Handling\n\nIf your FQL statement is invalid (for example userId = oops\"), your Segment event will not be sent on to downstream Destinations. Segment defaults to not sending the event to ensure that invalid FQL doesn\u2019t cause sensitive information like PII to be incorrectly sent to Destinations.\n\nFor this reason, Segment recommends that you use the Destination Filters \u201cPreview\u201d API to test your filters without impacting your production data.\n\nThis page was last modified: 04 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nExamples\nField Paths\nOperators\nSubexpressions\nFunctions\nError Handling\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nCampaigns\n/\nMobile Push Onboarding\nMobile Push Onboarding\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE PREMIER \u2713\n?\n\nEngage Premier entered an End of Sale (EOS) period effective June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:\n\nTwilio Marketing Campaigns\n\nPreferred ISV Partners:\n\nAirship Blog\nBloomreach Blog\nBraze Blog\nInsider Blog\nKlaviyo Blog\nTwilio Engage Foundations Documentation\n\n\nThis page walks you through the process of setting up mobile push notifications using Segment, Twilio, and Firebase/Apple Developer.\n\nPrerequisites\n\nPlease reach out to your CSM or AE prior to trying out this feature. This guide assumes familiarity with Swift and Kotlin and is intended for a developer audience.\n\nOverview\n\nYou\u2019ll set up mobile push in four stages:\n\nSet up analytics for mobile push.\nAdd the Engage SDK plugin.\nConfigure iOS push notifications.\nConfigure Android push notifications.\nConfigure mobile push in Engage.\n1. Set up analytics for mobile push\n\nBefore you can send mobile pushes, you\u2019ll need to set up analytics. In this step, you\u2019ll integrate Segment\u2019s mobile SDK into your app.\n\nAdd the Segment base SDK\n\nThis section outlines the process for adding Segment\u2019s base SDK to your app, including the Analytics Kotlin, Analytics-Swift, and React Native libraries.\n\nKotlin\n\nYou must initialize your Analytics instance in the Application class, otherwise you may experience issues with customization and delivery confirmation.\n\nFollow these steps to integrate Analytics Kotlin:\n\nCreate a source by navigating to Connections > Sources > Add Source.\nSearch for Kotlin (Android), then click Add source.\nAdd the Analytics dependency to your build.gradle file.\nInitialize and configure the client according to your requirements.\nAdd the following permissions to AndroidManifest.xml:\n <uses-permission android:name=\"android.permission.INTERNET\"/>\n <uses-permission android:name=\"android.permission.ACCESS_NETWORK_STATE\"/>\n\n\n\nFor detailed instructions on integrating Analytics Kotlin, follow the steps in the Analytics Kotlin getting started section.\n\nSwift\n\nFollow these steps to integrate Analytics-Swift for iOS & Apple:\n\nCreate a source by navigating to Connections > Sources > Add Source.\nSearch for Apple, then click Add source.\nAdd the Analytics dependency to your application using either Swift package manager or Xcode.\nInitialize and configure the Analytics-Swift client.\n\nFor detailed instructions on integrating Analytics-Swift, follow the steps in the Analytics-Swift getting started section.\n\nReact Native\n\nFollow these steps to integrate the React Native library:\n\nCreate a source by navigating to Connections > Sources > Add Source.\nSearch for React Native, then click Add source.\nUse yarn or npm to install @segment/analytics-react-native, @segment/sovran-react-native, and react-native-get-random-values.\nInitialize and configure the Analytics React Native client.\n\nFor detailed instructions on integrating Analytics for React Native, follow the steps in the Analytics for React Native getting started section.\n\n2. Add the Engage SDK plugin\n\nNext, you\u2019ll add the Engage SDK plugins for both iOS and Android to your application.\n\nInstructions for iOS\n\nNow that you\u2019ve integrated Analytics-Swift, follow the steps in this section to add the Engage Plugin for iOS.\n\n2a. Add the Engage SDK plugin dependency\n\nYou can add the Engage SDK plugin using either Xcode or Package.swift.\n\nInstructions for adding the plugin with Xcode\n\nIn the Xcode File menu, click Add Packages.\n\nIn the Swift packages search dialog, enter the following URL:\n\n https://github.com/segment-integrations/analytics-swift-engage\n\nYou\u2019ll then have the option to pin to a version or a specific branch, as well as to the project in your workspace. Once you\u2019ve made your selections, click Add Package.\n\nInstructions for adding the plugin with Package.swift\n\nOpen the Package.swift file and add the following to the dependencies section:\n.package(\n            name: \"Segment\",\n            url: \"https://github.com/segment-integrations/analytics-swift-engage.git\",\n            from: \"1.1.2\"\n        ),\n\n2b. Import the plugin\n\nImport the plugin in the file where you configure your Analytics instance:\n\n import Segment\n import TwilioEngage // <-- Add this line.\n\n\nAfter your Analytics-Swift library setup, call analytics.add(plugin: ...) to add an instance of the plugin to the Analytics timeline:\n\n let analytics = Analytics(configuration: Configuration(writeKey: \"<YOUR WRITE KEY>\")\n                     .flushAt(3)\n                     .trackApplicationLifecycleEvents(true))\n\n let engage = TwilioEngage { previous, current in\n     print(\"Push Status Changed /(current)\")\n }\n\n analytics.add(plugin: engage)\n\n\nTo start receiving and handling mobile push notifications, add or modify the following methods in your AppDelegate:\n\n   func application(_ application: UIApplication, didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?) -> Bool {\n\n    //Add the following:\n\n        let center  = UNUserNotificationCenter.current()\n        center.delegate = self\n        center.requestAuthorization(options: [.sound, .alert, .badge]) { (granted, error) in\n            guard granted else {\n                Analytics.main.declinedRemoteNotifications()\n                Tab1ViewController.addPush(s: \"User Declined Notifications\")\n                return\n            }\n            DispatchQueue.main.async {\n                UIApplication.shared.registerForRemoteNotifications()\n            }\n        }\n        \n        // The following conditional statement is necessary to handle remote notifications in older versions of iOS.\n        if let notification = launchOptions?[UIApplication.LaunchOptionsKey.remoteNotification] as? [String: Codable] {\n            Tab1ViewController.addPush(s: \"App Launched via Notification \\(notification)\")\n            Analytics.main.receivedRemoteNotification(userInfo: notification)\n        }\n\n        ...\n\n        return true\n}\n\nfunc application(_ application: UIApplication, didRegisterForRemoteNotificationsWithDeviceToken deviceToken: Data) {\n    // Segment event to register for remote notifications\n    Analytics.main.registeredForRemoteNotifications(deviceToken: deviceToken)\n}\n\nfunc application(_ application: UIApplication, didFailToRegisterForRemoteNotificationsWithError error: Error) {\n    // Segment event for failure to register for remote notifications\n    Analytics.main.failedToRegisterForRemoteNotification(error: error)\n}\n\nfunc application(_ application: UIApplication, didReceiveRemoteNotification userInfo: [AnyHashable : Any]) async -> UIBackgroundFetchResult {\n    // Segment event for receiving a remote notification\n    Analytics.main.receivedRemoteNotification(userInfo: userInfo)\n\n    // TODO: Customize notification handling based on the received userInfo.\n    // Implement actions or UI updates based on the notification content.\n\n    return .noData\n}\n\nfunc userNotificationCenter(_ center: UNUserNotificationCenter, didReceive response: UNNotificationResponse) async {\n    let userInfo = response.notification.request.content.userInfo\n    //Segment event for receiving a remote notification\n    Analytics.main.receivedRemoteNotification(userInfo: userInfo)\n\n    // TODO: Customize notification response handling based on the received userInfo.\n    // Implement actions based on the user's response to the notification.\n    // Example: Navigate to a specific screen or perform an action based on the notification.\n\n}\n\n\nThe previous steps are required. For configuration options, including subscription statuses and media handling, visit the getting started section of Segment\u2019s Twilio Engage Plugin documentation on GitHub.\n\nInstructions for Android\n\nNow that you\u2019ve integrated Analytics-Kotlin, follow these steps to add the Engage Plugin for Android:\n\nAdd the following to your Gradle dependencies:\n\n     implementation 'com.segment.analytics.kotlin.destinations:engage:<LATEST_VERSION>'\n\n\nAdd the following service to the application tag of your AndroidManifest.xml file:\n\n     <service\n         android:name=\"com.segment.analytics.kotlin.destinations.engage.EngageFirebaseMessagingService\"\n         android:exported=\"true\">\n         <intent-filter>\n         <action android:name=\"com.google.firebase.INSTANCE_ID_EVENT\"/>\n         <action android:name=\"com.google.firebase.MESSAGING_EVENT\" />\n         </intent-filter>\n     </service>\n\n\nAdd this plugin to your Analytics instance:\n\n     analytics.add(TwilioEngage(applicationContext))\n\n\nThe previous steps are required. For configuration options, including subscription statuses and customized actions, visit the getting started section of Segment\u2019s Twilio Engage Destination documentation on GitHub.\n\nNext, you\u2019ll configure your iOS and Android push credentials for use with Twilio Notify and Twilio Notifications.\n\n3. Configure iOS push notifications\n3a. Set up an App ID\n\nBefore you begin, log into your Apple development account and click on Identifiers under the Certificates, Identifiers & Profiles section. This will show a list of identifiers, including App IDs.\n\nOption 1: Use an existing App ID\nIf your App ID is already on this list, click on it; a list of capabilities will pop up.\nCheck the Push Notifications option.\nIgnore the Configure button for now. Click Save.\nOption 2: Create a new App ID\nIf your App ID isn\u2019t on this list, click the + symbol to add a new App ID.\nChoose App IDs and click the Continue button.\nGive your app a description.\nEnter an Explicit Bundle ID that matches the bundle identifier (such as com.twilio.notify.NotifyQuickstart) of your app in Xcode.\nUnder Capabilities, check Push Notifications.\nClick Continue.\nClick Register to confirm and create your new App ID.\n3b. Create a certificate\n\nNext, you\u2019ll create a push notification certificate, which lets your app receive notifications. You can either make a development certificate or a production certificate. This guide explains how to make a development certificate. Segment recommends that you use Xcode managed certificates.\n\nOption 1: Use an Xcode managed certificate\nIn your Xcode project, go to the General pane of the target for your iOS application.\nIn the Signing section, check Automatically manage signing.\nIf you are using the Quickstart app and see a provisioning error message, you may need to rename the bundle ID to a unique identifier. To do so, give your bundle a new name, then enter your new identifier in the Identity section of the General pane.\nGo to the Capabilities tab and make sure that Push Notifications are enabled.\nVerify that you successfully created your certificates:\nSign in to the Apple developer portal and click on Certificates, IDs & Profile. In the Certificates section, select Development or Production, depending on the type of certificate you want to verify.\nAlternatively, go to Applications > Utilities > Keychain Access and select Certificates. Search for iPhone, and verify that your certificate has a disclosure triangle, which indicates that your private key exists in the keychain.\nOption 2: Manually create a certificate\n\nSegment recommends that you use Xcode managed certificates for your application. If you prefer to create your certificate manually, follow these steps:\n\nAdd a certificate on the Apple Developer Portal.\nUnder Services, select Apple Push Notification service SSL (Sandbox & Production), then click Continue.\nIn the text box, select the App ID you previously created, then click Continue.\nYou\u2019re prompted to create a Certificate Signing Request (CSR) and given instructions on how to do it. Create one.\nOnce you\u2019ve created a CSR, click Continue.\nUpload the CSR, then click Generate to generate your certificate.\n\nYou just created an Apple Development iOS Push Services certificate, which you can now download and double-click to add to your Mac\u2019s keychain.\n\n3c. Create a credential for Twilio\nOn your Mac, go to Applications > Utilities > Keychain Access, then select My Certificates.\nRight-click your new certificate. It should be labeled Apple Development iOS Push Services.\nChoose Export.\nSave your credential file as cred.p12; leave the password blank.\n\nYou\u2019ll extract your certificate key and private key from this file \u2014 you need these two keys to create a Twilio credential. First, run this command in Terminal:\n\nopenssl pkcs12 -in cred.p12 -nokeys -out cert.pem -nodes\n\n\ncert.pem is your certificate key file. Next, run the following command in the terminal:\n\nopenssl pkcs12 -in cred.p12 -nocerts -out key.pem -nodes\n\n\nkey.pem is your private key file. Next, run this command to process this key:\n\nopenssl rsa -in key.pem -out key.pem\n\n\nYou can now paste your credentials into the modal found in the Twilio Console. Make sure that you strip anything outside of the -----BEGIN CERTIFICATE----- and -----END CERTIFICATE----- boundaries and outside of the -----BEGIN RSA PRIVATE KEY----- and -----END RSA PRIVATE KEY----- boundaries before pasting your credentials. Check the Sandbox button if you made a development certificate. Sandbox is synonymous with development mode.\n\nOnce you save a credential, the CERTIFICATE and PRIVATE KEY fields are hidden for security reasons.\n\nAfter you\u2019ve pasted your credentials, click Save. You should see an SID appear on the new page; copy it to your clipboard, as you\u2019ll need it in the next step.\n\n3d. Configure your Twilio Service to use your APNS credentials\n\nTwilio lets you build multiple applications within a single account. To separate those applications, you need to create Service instances that hold all the data and configuration for a given application.\n\nTo do so, you\u2019ll need to configure your Service instance to use the Credential that contains your APNS certificate and private key. You can do that using the Services page in the Console. You\u2019ll need to update your Service with the Twilio Push Credential SID.\n\nIf you\u2019re just getting started, set up the APN credential first, then create your Service by clicking the blue plus button on the Services Console page.\n\n4. Configure Android push notifications\n\nFollow the steps in Twilio\u2019s Configuring Android Push Notifications.\n\nDuring Step 5, Upload your API Key to Twilio, follow these steps:\n\nIn the Firebase console, click the Cloud Messaging tab.\nSelect the three dots menu next to Cloud Messaging API (Legacy) Disabled, then select Manage API in Google Cloud Console. A new window opens.\nIn the new Cloud Messaging window, select Enable.\nReturn to the Firebase Cloud Messaging tab and refresh the page.\nCloud Messaging API (Legacy) is now enabled. Copy the server key; you\u2019ll need it later.\n\nWith your server key copied, finish steps 5 and 6 in the Twilio documentation.\n\n5. Configure mobile push in Engage\n\nFollow these steps to set up mobile push in Twilio Engage.\n\n5a. Set up Twilio credentials\n\nFollow the steps in 5a only if you\u2019re new to Twilio Engage Premier. If you\u2019ve already configured messaging services as part of Twilio Engage Premier onboarding, you can skip to 5b.\n\nIn your Twilio console, select the Account dropdown menu, then API keys & tokens.\nOn the Auth tokens & API keys page, click Create API key.\nEnter a name for the API key in the Friendly name field.\nSet the region to United States (US1) - Default and key type to Main.\nClick Create API Key.\nCopy and save both the SID and Secret field contents.\nReturn to the API keys & tokens page. In the Live credentials section, copy the Account SID credentials.\nReturn to your Segment workspace and navigate to Engage > Engage settings > Channels. Under SMS Service with Twilio, click the Get Started button. The Set up and validate your Twilio account page appears.\nUnder Enter your Twilio API Key information, paste the Account SID, API Key SID, and API Key Secret you copied above into their corresponding fields.\nClick Verify, then select the messaging services you want to use in your space.\nClick Save Twilio Account.\n\nRemoving messaging services\n\nTo remove a messaging service, navigate to Engage > Engage settings > Channels and click the pencil icon under Twilio messaging service. Enter the account credentials by either using the API key secret or creating a new API key. Once you\u2019ve selected the desired services, they will override the existing ones, effectively removing the ones you no longer need.\n\n5b. Create a new push service\n\nComplete mobile push onboarding by creating a new push service:\n\nIn your Segment workspace, navigate to Engage > Engage settings.\nClick the pencil icon next to Messaging services, then click Create new push service.\nIf you don\u2019t see the pencil icon, select Create new push service.\nName the push service, select or create APN and FCM credentials, then click Create Push Service.\nYour new messaging service appears in the Add messaging services dropdown. Select it, then click Save.\nBuild a mobile push template\n\nNow that you\u2019ve completed mobile push setup, you\u2019re ready to build a mobile push template.\n\nThis page was last modified: 13 Aug 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nOverview\n1. Set up analytics for mobile push\n2. Add the Engage SDK plugin\n3. Configure iOS push notifications\n4. Configure Android push notifications\n5. Configure mobile push in Engage\nBuild a mobile push template\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nFunctions\n/\nDestination Insert Functions\nDestination Insert Functions\n\nUse Destination Insert Functions to enrich, transform, or filter your data before it reaches downstream destinations.\n\nImplement advanced data computation: Write custom computation, operations, and business logic on streaming data that you send to downstream destinations.\n\nEnrich your data: Use destination insert functions with Segment\u2019s Profile API or third party sources to add additional context to your data and create personalized customer experiences.\n\nSupport data compliance: Use destination insert functions to support data masking, encryption, decryption, improved PII data handling, and tokenization.\n\nCustomize filtration for your destinations: Create custom logic with nested if-else statements, regex, custom business rules, and more to filter event data.\n\nDestination Insert Functions are not compatible with IP Allowlisting\n\nFor more information, see the IP Allowlisting documentation.\n\nCreate destination insert functions\n\nThere are two ways you can access destination insert functions from your Segment space:\n\nFrom the Connections catalog.\nFrom the Destinations tab.\nUsing the catalog\n\nTo create an insert function from Segment\u2019s catalog:\n\nNavigate to Connections > Catalog > Functions and click New Function.\nFrom the Select Type screen, select Insert and click Next: Build Function.\nWrite and test your function code. Manually enter a sample event and click Run to test.\nClick Next: Configure & Create to add a function name, description, and logo.\nClick Create Function to create your insert function. You\u2019ll see the insert function displayed in the Functions tab.\n\nFor data to flow to your downstream destinations, you\u2019ll need to connect your insert function to a destination:\n\nSelect the insert function you\u2019d like to connect. From the side pane, you can edit, delete, and connect the insert function.\nClick Connect a destination.\nSelect the destination you\u2019d like to connect to and click Connect to destination.\n\nStorage destinations are not compatible with Destination Insert Functions\n\nYou cannot connect an Insert Function to a storage destination at this time.\n\nUsing the Destinations tab\n\nTo access insert functions through the Destinations tab:\n\nNavigate to Connections > Destinations.\nSelect your destination.\nSelect Functions and then select your insert function.\n\nUse this page to edit and manage insert functions in your workspace.\n\nYou can also use this page to enable destination insert functions in your workspace.\n\nCode the destination insert function\n\nTo prevent \u201cUnsupported Event Type\u201d errors, ensure your insert function handles all event types (page, track, identify, alias, group) that are expected to be sent to the destination. It is highly recommended to test the function with each event type to confirm they are being handled as expected.\n\nSegment invokes a separate part of the function (called a \u201chandler\u201d) for each event type that you send to your destination insert function.\n\nIf you\u2019ve configured a destination filter and the event doesn\u2019t pass the filter, then your function isn\u2019t invoked for that event as Segment applies destination filters before insert functions. The same is true for the integrations object). If an event is configured with the integrations object not to go to a particular destination, then the insert function connected to that destination won\u2019t be invoked.\n\nThe default source code template includes handlers for all event types. You don\u2019t need to implement all of them - just use the ones you need, and skip the ones you don\u2019t. For event types that you want to send through the destination, return the event in the respective event handlers.\n\nRemoving the handler for a specific event type results in blocking the events of that type from arriving at their destination. To keep an event type as is but still send it downstream, add a return event inside the event type handler statement.\n\nInsert functions can define handlers for each message type in the Segment spec:\n\nonIdentify\nonTrack\nonPage\nonScreen\nonGroup\nonAlias\nonDelete\nonBatch\n\nEach of the functions above accepts two arguments:\n\nevent - Segment event object, where fields and values depend on the event type. For example, in \u201cIdentify\u201d events, Segment formats the object to match the Identify spec.\nsettings - Set of settings for this function.\n\nThe example below shows a function that listens for \u201cTrack\u201d events, and sends some details about them to an external service.\n\nasync function onTrack(event) {\n  await fetch('https://example-service.com/api', {\n    method: 'POST',\n    headers: {\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n      event_name: event.event,\n      event_properties: event.properties,\n      timestamp: event.timestamp\n    })\n  })\n\n  return event;\n  \n}\n\n\nTo change which event type the handler listens to, you can rename it to the name of the message type. For example, if you rename this function onIdentify, it listens for \u201cIdentify\u201d events instead.\n\nTo ensure the Destination processes an event payload modified by the function, return the event object at the handler\u2019s end.\n\nFunctions\u2019 runtime includes a fetch() polyfill using a node-fetch package. Check out the node-fetch documentation for usage examples.\n\nErrors and error handling\n\nSegment considers a function\u2019s execution successful if it finishes without error. You can throw an error to create a failure on purpose. Use these errors to validate event data before processing it to ensure the function works as expected.\n\nYou can throw the following pre-defined error types to indicate that the function ran as expected, but the data was not deliverable:\n\nEventNotSupported\nInvalidEventPayload\nValidationError\nRetryError\nDropEvent\n\nThe examples show basic uses of these error types.\n\nasync function onGroup(event) {\n  if (!event.traits.company) {\n    throw new InvalidEventPayload('Company name is required')\n  }\n}\n\nasync function onPage(event) {\n  if (!event.properties.pageName) {\n    throw new ValidationError('Page name is required')\n  }\n}\n\nasync function onAlias(event) {\n  throw new EventNotSupported('Alias event is not supported')\n}\n\nasync function onTrack(event) {\n  let res\n  try {\n    res = await fetch('http://example-service.com/api', {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json'\n      },\n      body: JSON.stringify({ event })\n    })\n\n    return event;\n    \n  } catch (err) {\n    // Retry on connection error\n    throw new RetryError(err.message)\n  }\n  if (res.status >= 500 || res.status === 429) {\n    // Retry on 5xx and 429s (ratelimits)\n    throw new RetryError(`HTTP Status ${res.status}`)\n  }\n}\n\nasync function onIdentify(event) {\n  if (event.traits.companyName) {\n    // Drop Event | Do NOT forward event to destination\n    throw new DropEvent('Company name is required')\n  }\n  return event;\n}\n\n\n\nIf you don\u2019t supply a function for an event type, Segment throws an EventNotSupported error by default.\n\nYou can read more about error handling below.\n\nRuntime and dependencies\n\nOn March 26, 2024, Segment is upgrading the Functions runtime environment to Node.js v18, which is the current long-term support (LTS) release.\n\nThis upgrade keeps your runtime current with industry standards. Based on the AWS Lambda and Node.js support schedule, Node.js v16 is no longer in Maintenance LTS. Production applications should only use releases of Node.js that are in Active LTS or Maintenance LTS.\n\nAll new functions will use Node.js v18 starting March 26, 2024.\n\nFor existing functions, this change automatically occurs as you update and deploy an existing function. Segment recommends that you check your function post-deployment to ensure everything\u2019s working. Your function may face issues due to the change in sytax between different Node.js versions and dependency compatibility.\n\nLimited time opt-out option\n\nIf you need more time to prepare, you can opt out of the update before March 19, 2024.\n\nNote that if you opt out:\n- The existing functions will continue working on Node.js v16.\n- You won\u2019t be able to create new functions after July 15, 2024.\n- You won\u2019t be able to update existing functions after August 15, 2024.\n- You won\u2019t receive future bug fixes, enhancements, and dependency updates to the functions runtime.\n\nContact Segment to opt-out or with any questions.\n\nNode.js 18\n\nSegment strongly recommends updating to Node.js v18 to benefit from future runtime updates, the latest security, and performance improvements.\n\nFunctions do not currently support importing dependencies, but you can contact Segment Support to request that one be added.\n\nThe following dependencies are installed in the function environment by default.\n\natob v2.1.2 exposed as atob\naws-sdk v2.488.0 exposed as AWS\nbtoa v1.2.1 exposed as btoa\nfetch-retry exposed as fetchretrylib.fetchretry\nform-data v2.4.0 exposed as FormData\n@google-cloud/automl v2.2.0 exposed as google.cloud.automl\n@google-cloud/bigquery v5.3.0 exposed as google.cloud.bigquery\n@google-cloud/datastore v6.2.0 exposed as google.cloud.datastore\n@google-cloud/firestore v4.4.0 exposed as google.cloud.firestore\n@google-cloud/functions v1.1.0 exposed as google.cloud.functions\n@google-cloud/pubsub v2.6.0 exposed as google.cloud.pubsub\n@google-cloud/storage v5.3.0 exposed as google.cloud.storage\n@google-cloud/tasks v2.6.0 exposed as google.cloud.tasks\nhubspot-api-nodejs exposed as hubspotlib.hubspot\njsforce v1.11.0 exposed as jsforce\njsonwebtoken v8.5.1 exposed as jsonwebtoken\nlibphonenumber-js exposed as libphonenumberjslib.libphonenumberjs\nlodash v4.17.19 exposed as _\nmailchimp marketing exposed as mailchimplib.mailchimp\nmailjet exposed as const mailJet = nodemailjet.nodemailjet;\nmoment-timezone v0.5.31 exposed as moment\nnode-fetch v2.6.0 exposed as fetch\noauth v0.9.15 exposed as OAuth\n@sendgrid/client v7.4.7 exposed as sendgrid.client\n@sendgrid/mail v7.4.7 exposed as sendgrid.mail\nskyflow exposed as skyflowlib.skyflow\nstripe v8.115.0 exposed as stripe\ntwilio v3.68.0 exposed as twilio\nuuidv5 v1.0.0 exposed as uuidv5.uuidv5\nwinston v2.4.6 exposed as const winston = winstonlib.winston\nxml v1.0.1 exposed as xml\nxml2js v0.4.23 exposed as xml2js\n\nzlib v1.0.5 exposed as zlib.zlib\n\n\nuuidv5 is exposed as an object. Use uuidv5.uuidv5 to access its functions. For example:\n\n  async function onRequest(request, settings) {\n       uuidv5 = uuidv5.uuidv5;\n       console.log(typeof uuidv5);\n\n        //Generate a UUID in the default URL namespace\n        var urlUUID = uuidv5('url', 'http://google/com/page');\n        console.log(urlUUID);\n\n        //Default DNS namespace\n        var dnsUUID = uuidv5('dns', 'google.com');\n        console.log(dnsUUID);\n    }\n\n\nzlib\u2019s asynchronous methods inflate and deflate must be used with async or await. For example:\n\nzlib = zlib.zlib;  // Required to access zlib objects and associated functions\nasync function onRequest(request, settings) {\n  const body = request.json();\n\n  const input = 'something';\n\n  // Calling inflateSync method\n  var deflated = zlib.deflateSync(input);\n\n  console.log(deflated.toString('base64'));\n\n  // Calling inflateSync method\n  var inflated = zlib.inflateSync(new Buffer.from(deflated)).toString();\n\n  console.log(inflated);\n\n  console.log('Done');\n  }\n\n\nThe following Node.js modules are available:\n\ncrypto Node.js module exposed as crypto.\nhttps Node.js module exposed as https.\n\nOther built-in Node.js modules aren\u2019t available.\n\nFor more information on using the aws-sdk module, see how to set up functions for calling AWS APIs.\n\nCaching\n\nBasic cache storage is available through the cache object, which has the following methods defined:\n\ncache.load(key: string, ttl: number, fn: async () => any): Promise<any>\nObtains a cached value for the provided key, invoking the callback if the value is missing or has expired. The ttl is the maximum duration in milliseconds the value can be cached. If omitted or set to -1, the value will have no expiry.\ncache.delete(key: string): void\nImmediately remove the value associated with the key.\n\nSome important notes about the cache:\n\nWhen testing functions in the code editor, the cache will be empty because each test temporarily deploys a new instance of the function.\nValues in the cache are not shared between concurrently-running function instances; they are process-local which means that high-volume functions will have many separate caches.\nValues may be expunged at any time, even before the configured TTL is reached. This can happen due to memory pressure or normal scaling activity. Minimizing the size of cached values can improve your hit/miss ratio.\nFunctions that receive a low volume of traffic may be temporarily suspended, during which their caches will be emptied. In general, caches are best used for high-volume functions and with long TTLs. The following example gets a JSON value through the cache, only invoking the callback as needed:\nconst ttl = 5 * 60 * 1000 // 5 minutes\nconst val = await cache.load(\"mycachekey\", ttl, async () => {\n    const res = await fetch(\"http://echo.jsontest.com/key/value/one/two\")\n    const data = await res.json()\n    return data\n})\n\nInsert Functions and Actions destinations\n\nA payload must come into the pipeline with the attributes that allow it to match your mapping triggers. You can\u2019t use an Insert Function to change the event to match your mapping triggers. If an event comes into an Actions destination and already matches a mapping trigger, that mapping subscription will fire. If a payload doesn\u2019t come to the Actions destination matching a mapping trigger, even if an Insert Function is meant to alter the event to allow it to match a trigger, it won\u2019t fire that mapping subscription. Segment sees the mapping trigger first in the pipeline, so a payload won\u2019t make it to the Insert Function at all if it doesn\u2019t come into the pipeline matching a mapping trigger.\n\nUnlike Source Functions and Destination Functions, which return multiple events, an Insert Function only returns one event. When the Insert Function receives an event, it sends the event to be handled by its configured mappings.\n\nIf you would like multiple mappings triggered by the same event:\n\nCreate different types of mappings (Identify, Track, Page, etc) or multiple mappings of the same type.\nConfigure the mapping\u2019s trigger conditions to look for that event name/type or other available field within the payload.\nConfigure the mapped fields to send different data.\n\nYou can also configure the Insert Function to add additional data to the event\u2019s payload before it\u2019s handled by the mappings and configure the mapping\u2019s available fields to reference the payload\u2019s available fields.\n\nYou may want to consider the context object\u2019s available fields when adding new data to the event\u2019s payload.\n\nCreate settings and secrets\n\nSettings allow you to pass configurable variables to your function, which is the best way to pass sensitive information such as security tokens. For example, you might use settings as placeholders to use information such as an API endpoint and API key. This way, you can use the same code with different settings for different purposes. When you deploy a function in your workspace, you are prompted to fill out these settings to configure the function.\n\nFirst, add a setting in Settings tab in the code editor:\n\nClick Add Setting to add your new setting.\n\nYou can configure the details about this setting, which change how it\u2019s displayed to anyone using your function:\n\nLabel - Name of the setting, which users see when configuring the function.\nName - Auto-generated name of the setting to use in function\u2019s source code.\nType - Type of the setting\u2019s value.\nDescription - Optional description, which appears below the setting name.\nRequired - Enable this to ensure that the setting cannot be saved without a value.\nEncrypted - Enable to encrypt the value of this setting. Use this setting for sensitive data, like API keys.\n\nAs you change the values, a preview to the right updates to show how your setting will look and work.\n\nClick Add Setting to save the new setting.\n\nOnce you save a setting, it appears in the Settings tab for the function. You can edit or delete settings from this tab.\n\nNext, fill out this setting\u2019s value in the Test tab, so you can run the function and verify that the correct setting value is passed. (This value is only for testing your function.)\n\nNow that you\u2019ve configured a setting and entered a test value, you can add code to read its value and run the function, as in the example below:\n\nasync function onTrack(request, settings) {\n  const apiKey = settings.apiKey\n  //=> \"super_secret_string\"\n}\n\n\nWhen you deploy your destination insert function in your workspace, you fill out the settings on the destination configuration page, similar to how you would configure a normal destination.\n\nTest the destination insert function\n\nYou can manually test your code from the functions editor:\n\nFrom the Test tab, click customize the event yourself and manually input your own JSON payload.\nIf your test fails, you can check the error details and logs in the Output section.\nError messages display errors surfaced from your function.\nLogs display any messages to console.log() from the function.\n\nThe Event Tester won\u2019t make use of an Insert Function, show how an Insert Function impacts your data, or send data downstream through the Insert Function pipeline. The Event Tester is not impacted by an Insert Function at all. Use the Function tester rather than the Event Tester to see how your Insert Function impacts your data.\n\nSave and deploy the destination insert function\n\nOnce you finish building your insert function, click Next: Configure & Create to name it, then click Create Function to save it.\n\nOnce you do that, you\u2019ll see the insert function from the Functions page in your catalog.\n\nIf you\u2019re editing an existing function, you can save changes without updating the instances of the function that are already deployed and running.\n\nYou can also choose to Save & Deploy to save the changes, then choose which already-deployed functions to update with your changes.\n\nYou may need additional permissions to update existing functions.\n\nEnable the destination insert function\n\nYou need to enable your insert function for it to process your data.\n\nTo enable your insert function:\n\nNavigate to Connections > Destinations.\nSelect your destination, then select the Functions tab.\nSelect the Enable Function toggle, and click Enable on the pop-out window.\n\nTo prevent your insert function from processing data, toggle Enable Function off.\n\nBatching the destination insert function\n\nBatch handlers are an extension of insert functions. When you define an onBatch handler alongside the handler functions for single events (for example, onTrack or onIdentity), you\u2019re telling Segment that the insert function can accept and handle batches of events.\n\nBatching is available for destination and destination insert functions only.\n\nWhen to use batching\n\nConsider creating a batch handler if:\n\nYou have a high-throughput function and want to reduce cost. When you define a batch handler, Segment invokes the function once per batch, rather than once per event. As long as the function\u2019s execution time isn\u2019t adversely affected, the reduction in invocations should lead to a reduction in cost.\n\nYour destination supports batching. When your downstream destination supports sending data downstream in batches you can define a batch handler to avoid throttling. Batching for functions is independent of batch size supported by the destination. Segment automatically handles batch formation for destinations.\n\nIf a batched function receives too low a volume of events (under one event per second) to be worth batching, Segment may not invoke the batch handler.\n\nDefine the batch handler\n\nSegment collects the events over a short period of time and combines them into a batch. The system flushes them when the batch reaches a certain number of events, or when the batch has been waiting for a specified wait time.\n\nTo create a batch handler, define an onBatch function within your destination insert function. You can also use the \u201cDefault Batch\u201d template found in the Functions editor to get started quickly.\n\nasync function onBatch(events, settings){\n  // handle the batch of events\n  return events\n}\n\n\nThe onBatch handler is an optional extension. Destination insert functions must still contain single event handlers as a fallback, in cases where Segment doesn\u2019t receive enough events to execute the batch.\n\nThe handler function receives an array of events. The events can be of any supported type and a single batch may contain more than one event type. Handler functions can also receive function settings. Here is an example of what a batch can look like:\n\n[\n    {\n      \"type\": \"identify\",\n      \"userId\": \"019mr8mf4r\",\n      \"traits\": {\n        \"email\": \"jake@yahoo.com\",\n        \"name\": \"Jake Peterson\",\n        \"age\": 26\n      }\n    },\n    {\n      \"type\": \"track\",\n      \"userId\": \"019mr8mf4r\",\n      \"event\": \"Song Played\",\n      \"properties\": {\n        \"name\": \"Fallin for You\",\n        \"artist\": \"Dierks Bentley\"\n      }\n    },\n    {\n      \"type\": \"track\",\n      \"userId\": \"971mj8mk7p\",\n      \"event\": \"Song Played\",\n      \"properties\": {\n        \"name\": \"Get Right\",\n        \"artist\": \"Jennifer Lopez\"\n      }\n    }\n]\n\nConfigure the event types within a batch\n\nSegment batches together any event of any type that it sees over a short period of time to increase batching efficiency and give you the flexibility to decide how batches are created. If you want to split batches by event type, you can implement this in your functions code by writing a handler.\n\nasync function onBatch(events, settings) {\n  // group events by type\n  const eventsByType = {}\n  for (const event of events) {\n    if (!(event.type in eventsByType)) {\n      eventsByType[event.type] = []\n    }\n    eventsByType[event.type].push(event)\n  }\n\n  // concurrently process sub-batches of a specific event type\n  const promises = Object.entries(eventsByType).map(([type, events]) => {\n    switch (type) {\n    case 'track':\n      return onTrackBatch(events, settings)\n    case 'identify':\n      return onIdentifyBatch(events, settings)\n    // ...handle other event types here...\n    }\n  })\n  try {\n    const results = await Promise.all(promises);\n    const batchResult = [].concat(...results); // Combine arrays into a single array\n    return batchResult;\n  } catch (error) {\n    throw new RetryError(error.message);\n  }\n}\n\nasync function onTrackBatch(events, settings) {\n  // handle a batch of track events\n  return events\n}\n\nasync function onIdentifyBatch(events, settings) {\n  // handle a batch of identify events\n  return events\n}\n\nConfigure your batch parameters\n\nBy default, Functions waits up to 10 seconds to form a batch of 20 events. You can increase the number of events included in each batch (up to 400 events per batch) by contacting Segment support. Segment recommends users who wish to include fewer than 20 events per batch use destination insert functions without the onBatch handler.\n\nTest the batch handler\n\nThe Functions editing environment supports testing batch handlers.\n\nTo test the batch handler:\n\nIn the right panel of the Functions editor, click customize the event yourself to enter Manual Mode.\nAdd events as a JSON array, with one event per element.\nClick Run to preview the batch handler with the specified events.\n\nThe Sample Event option tests single events only. You must use Manual Mode to add more than one event so you can test batch handlers.\n\nThe editor displays logs and request traces from the batch handler.\n\nThe Public API Functions/Preview endpoint also supports testing batch handlers. The payload must be a batch of events as a JSON array.\n\nHandling filtering in a batch\n\nEvents in a batch can be filtered out using custom logic. The filtered events will be surfaced in the Event Delivery page with reason as Filtered at insert function\n\nasync function onBatch(events, settings) {\n  let response = [];\n  try {\n    for (const event of events) {\n      // some business logic to filter event. Here filtering out all the events with name `drop`\n      if (event.properties.name === 'drop') {\n        continue;\n      }\n\n      // some enrichments if needed\n      event.properties.message = \"Enriched from insert function\";\n\n      // Enriched events are pushed to response\n      response.push(event);\n    }\n  } catch (error) {\n    console.log(error)\n    throw new RetryError('Failed function', error);\n  }\n\n  // return a subset of transformed event\n  return response;\n}\n\nHandling batching errors\n\nStandard function error types apply to batch handlers. Segment attempts to retry the batch in the case of Timeout or Retry errors. For all other error types, Segment discards the batch.\n\nDestination insert functions error types\nBad Request - Any error thrown by the function code that is not covered by the other errors.\nInvalid Settings - A configuration error prevented Segment from executing your code. If this error persists for more than an hour, contact Segment Support.\nMessage Rejected - Your code threw InvalidEventPayload or ValidationError due to invalid input.\nUnsupported Event Type - Your code doesn\u2019t implement a specific event type (for example, onTrack()) or threw an EventNotSupported error.\nRetry - Your code threw RetryError indicating that the function should be retried.\n\nSegment only attempts to send the event to your destination insert function again if a Retry error occurs.\n\nYou can view Segment\u2019s list of Integration Error Codes for more information about what might cause an error.\n\nDestination insert functions logs\n\nIf your function throws an error, execution halts immediately. Segment captures the event, any outgoing requests/responses, any logs the function might have printed, as well as the error itself.\n\nSegment then displays the captured error information in the Event Delivery page for your destination. You can use this information to find and fix unexpected errors.\n\nYou can throw an error or a custom error and you can also add helpful context in logs using the console API. For example:\n\nasync function onTrack(event, settings) {\n  const userId = event.userId\n\n  console.log('User ID is', userId)\n\n  if (typeof userId !== 'string' || userId.length < 8) {\n    throw new ValidationError('User ID is invalid')\n  }\n\n  console.log('User ID is valid')\n}\n\n\nDon\u2019t log sensitive data, such as personally-identifying information (PII), authentication tokens, or other secrets. Avoid logging entire request/response payloads. The Function Logs tab may be visible to other workspace members if they have the necessary permissions.\n\nCaching in destination insert functions\n\nFunctions execute only in response to incoming data, but the environments that functions run in are generally long-running. Because of this, you can use global variables to cache small amounts of information between invocations. For example, you can reduce the number of access tokens you generate by caching a token, and regenerating it only after it expires. Segment cannot make any guarantees about the longevity of environments, but by using this strategy, you can improve the performance and reliability of your Functions by reducing the need for redundant API requests.\n\nThis example code fetches an access token from an external API and refreshes it every hour:\n\nconst TOKEN_EXPIRE_MS = 60 * 60 * 1000 // 1 hour\nlet token = null\nasync function getAccessToken () {\n  const now = new Date().getTime()\n  if (!token || now - token.ts > TOKEN_EXPIRE_MS) {\n    const resp = await fetch('https://example.com/tokens', {\n      method: 'POST'\n    }).then(resp => resp.json())\n    token = {\n      ts: now,\n      value: resp.token\n    }\n  }\n  return token.value\n}\n\nManaging destination insert functions\nFunctions permissions\n\nFunctions have specific roles which can be used for access management in your Segment workspace.\n\nAccess to functions is controlled by two permissions roles:\n\nFunctions Admin: Create, edit, and delete all functions, or a subset of specified functions.\nFunctions Read-only: View all functions, or a subset of specified functions.\n\nYou also need additional Source Admin permissions to enable source functions, connect destination functions to a source, or to deploy changes to existing functions.\n\nEditing and deleting functions\n\nIf you are a Workspace Owner or Functions Admin, you can manage your function from the Functions page.\n\nDestination insert functions FAQs\nCan I see who made changes to a function?\n\nYes, Functions access is logged in the Audit Trail, so user activity related to functions appears in the logs.\n\nDoes Segment retry failed function invocations?\n\nYes, Segment retries invocations that throw RetryError or Timeout errors (temporary errors only). Segment\u2019s internal system retries failed functions API calls for four hours with a randomized exponential backoff after each attempt. This substantially improves delivery rates.\n\nRetries work the same for both functions and cloud-mode destinations in Segment.\n\nAre events guaranteed to send data in order?\n\nNo, Segment can\u2019t guarantee the order in which the events are delivered to an endpoint.\n\nDo I Need to specify an endpoint for my Insert function?\n\nNo, specifying an endpoint is not always required for insert functions. If your function is designed to transform or filter data internally\u2014such as adding new properties to events or filtering based on existing properties\u2014you won\u2019t need to specify an external endpoint.\n\nHowever, if your function aims to enrich event data by fetching additional information from an external service, then you must specify the endpoint. This would be the URL of the external service\u2019s API where the enriched or captured data is sent.\n\nCan I use Insert Functions with Device Mode destinations?\n\nNo, Destination Insert Functions are currently available for use with Cloud Mode (server-side) destinations only. Segment is in the early phases of exploration and discovery for supporting customer web plugins for custom Device Mode destinations and other use cases, but this is unsupported today.\n\nCan I use Insert Functions with Storage destinations?\n\nInsert Functions are only supported by Cloud Mode (server-side) destinations and aren\u2019t compatible with Storage destinations.\n\nCan I connect an insert function to multiple destinations?\n\nYes, you can connect an insert function to multiple destinations.\n\nCan I connect multiple insert functions to one destination?\n\nNo, you can only connect one insert function to a destination.\n\nCan I have destination filters and a destination insert function in the same connection?\n\nYes, you can have both destination filters and destination insert functions in the same connection.\n\nAre insert functions invoked before or after Destination Filters are applied?\n\nSegment\u2019s data pipeline applies Destination Filters before invoking Insert Functions.\n\nWhy am I receiving a 500 Internal Error when saving the same of the destination insert function?\n\nThere is an 120-Character limit for the insert function display name.\n\nWhy does the Event Delivery tab show \u201cUnsupported Event Type\u201d errors for events supported by the destination after I enabled an insert function?\n\nThis error occurs because your insert function code might not be handling all event types (Page, Track, Identify, Alias, Group) that your destination supports. When these unlisted events pass through the function, they are rejected with the \u201cUnsupported Event Type\u201d error.\n\nTo resolve this, verify your insert function includes handlers for all expected event types and returns the event object for each. Here\u2019s an example of how you can structure your insert function to handle all event types:\n\nasync function onTrack(event, settings) {\n    //Return event to handle page event OR Your existing code for track event\n    return event;\n}\n\nasync function onPage(event, settings) {\n    //Return event to handle page event OR Your existing code for track event\n    return event;\n}\n\nasync function onIdentify(event, settings) {\n    //Return event to handle page event OR Your existing code for track event\n    return event;\n}\n\nasync function onAlias(event, settings) {\n    //Return event to handle page event OR Your existing code for track event\n    return event;\n}\n\nasync function onGroup(event, settings) {\n  //Return event to handle page event OR Your existing code for track event\n    return event;\n}\n\n// Ensure that all expected event types are included in your function\n\n\nBy including handlers for all the major event types, you ensure that all supported events are processed correctly, preventing the \u201cUnsupported Event Type\u201d error. Always test your updated code before implementing it in production.\n\nWhat is the maximum data size that can be displayed in console.logs() when testing a Function?\n\nThe test function interface has a 4KB console logging limit. Outputs surpassing this limit won\u2019t be visible in the user interface.\n\nThis page was last modified: 18 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nCreate destination insert functions\nCode the destination insert function\nRuntime and dependencies\nInsert Functions and Actions destinations\nCreate settings and secrets\nTest the destination insert function\nSave and deploy the destination insert function\nEnable the destination insert function\nBatching the destination insert function\nCaching in destination insert functions\nManaging destination insert functions\nDestination insert functions FAQs\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nPrivacy\n/\nConsent Management Overview\nConsent Management Overview\nFREE X\nTEAM X\nBUSINESS \u2713\nADDON X\n?\n\nWhen an end user visits your web or mobile app, they set consent preferences, or make decisions about the types of data they want you to collect, use, and share. These consent preferences are typically presented as a set list of categories that describe how your company intends to use that data. Some common categories include personalization, advertising, and site performance.\n\nSegment integrates with your commercial third-party or bespoke consent management platform (CMP) that captures an end user\u2019s consent preferences and enforces those preferences by only routing events to the categories consented to by an end user.\n\nAfter a user sets their consent preferences on your web or mobile app, Segment requires you to add the consent object to all events. If you are using OneTrust, Segment provides a wrapper for your web and mobile libraries that can add the consent object to your events. If you are using another CMP, you are required to add the consent object to your events by either creating your own wrapper or using another mechanism. For more information, see the Configure Consent Management documentation.\n\nThe events, stamped with the consent object, are then sent downstream to any destinations in categories that an end user consented to share data with.\n\nSegment collects consent for both registered users and anonymous users.\n\nFor more information about consent in Segment Connections, see the Consent in Segment Connections documentation.\n\nIf you are a Unify user, you can also see the Consent in Unify for more information about the Segment Consent Preference Updated event, which Segment uses to add consent preference to the Profile.\n\nThis page was last modified: 30 May 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nData Graph\nData Graph\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\n\nThe Data Graph acts as a semantic layer that allows businesses to define relationships between various entity datasets in the warehouse \u2014 such as accounts, subscriptions, households, and products \u2014 with the Segment Profile. It makes these relational datasets easily accessible to business teams for targeted and personalized customer engagements.\n\nLinked Audiences: Empowers marketers to effortlessly create targeted audiences by combining behavioral data from the Segment Profile and warehouse entity data within a self-serve, no-code interface. This tool accelerates audience creation, enabling precise targeting, enhanced customer personalization, and optimized marketing spend without the need for constant data team support.\nLinked Events: Allows data teams to enrich event streams in real time using datasets from data warehouses or lakes, and send these enriched events to any destination. Linked Events is available for both Destination Actions and Functions.\nPrerequisites\n\nTo use the Data Graph, you\u2019ll need the following:\n\nA supported data warehouse with the appropriate Data Graph permissions\nWorkspace Owner or Unify Read-only/Admin and Entities Admin permissions\nFor Linked Audiences, set up Profiles Sync in a Unify space with ready-to-use data models and tables in your warehouse. When setting up selective sync, Segment recommends the following settings:\nUnder Profile materialized tables, select all the tables (user_identifier, user_traits, profile_merges) for faster and more cost-efficient Linked Audiences computations in your data warehouse.\nUnder Track event tables, select Sync all Track Call Tables to enable filtering on event history for Linked Audiences conditions.\nStep 1: Set up Data Graph permissions in your data warehouse\n\nData Graph, Reverse ETL, and Profiles Sync require different warehouse permissions.\n\nData Graph currently only supports workspaces in the United States.\n\nTo get started with the Data Graph, set up the required permissions in your warehouse. Segment supports the following:\n\nLinked Audiences: BigQuery, Databricks, and Snowflake\nLinked Events: BigQuery, Databricks, Redshift, and Snowflake\n\nTo track the data sent to Segment on previous syncs, Segment uses Reverse ETL infrastructure to store diffs in tables within a dedicated schema called _segment_reverse_etl in your data warehouse. You can choose which database or project in your warehouse this data lives in.\n\nStep 2: Connect your warehouse to the Data Graph\n\nTo connect your warehouse to the Data Graph:\n\nNavigate to Unify > Data Graph. This should be a Unify space with Profiles Sync already set up.\nClick Add warehouse.\nSelect your warehouse type.\nEnter your warehouse credentials.\nTest your connection, then click Save.\nStep 3: Build your Data Graph\n\nThe Data Graph is a semantic layer that represents a subset of relevant business data that marketers and business stakeholders can use for audience targeting and personalization in downstream tools. Use the configuration language spec and the following features to build your Data Graph:\n\nUse the Warehouse access tab to view the warehouse tables you\u2019ve granted Segment access to\nBegin typing to autopopulate the configuration spec within the editor, as well as to autocomplete your warehouse schema\nValidate your Data Graph using the Preview tab\nKey steps to build your Data Graph\nFirst, define your entities. An entity corresponds to a table in your warehouse. Segment flexibly supports tables, views and materialized views.\nThen, define the profile block. This is a special class of entity that represents Segment Profiles, which corresponds to the Profiles Sync tables and models. For Linked Audiences, this allows marketers to filter on profile traits, event history, and so on.\nFinally, define how your datasets are related to each other. The Data Graph preserves these relationships and carries this rich context to the destinations to unlock personalization.\n\nDefining Relationships\n\nSimilar to the concept of cardinality in data modeling, the Data Graph supports 3 types of relationships:\n\nProfile-to-entity relationship: This is a relationship between your entity table and the Segment Profiles tables, and is the first level of relationship.\n1:many relationship: For example, an account can have many carts, but each cart can only be associated with one account.\nmany:many relationship: For example, a user can have many carts, and each cart can have many products. However, these products can also belong to many carts.\nThe Data Graph currently supports 6 levels of depth (or nodes) starting from the profile. For example, relating the profile to the accounts table to the carts table is 3 levels of depth. There are no limits on the width of your Data Graph or the number of entities.\nRelationships are nested under the profile. Refer to the example below.\n\nData Graph Example\n\ndata_graph {\n    version =  \"v1.0.0\"\n  \n    # Define entities\n    entity \"account-entity\" {\n      name = \"account\"\n      table_ref = \"PRODUCTION.CUST.ACCOUNT\"\n      primary_key = \"ID\"\n    }\n  \n    entity \"product-entity\" {\n      name = \"product\"\n      table_ref = \"PRODUCTION.PROD.PRODUCT_SKUS\"\n      primary_key = \"SKU\"\n    }\n  \n    entity \"cart-entity\" {\n      name = \"cart\"\n      table_ref = \"PRODUCTION.CUST.CART\"\n      primary_key = \"ID\"\n      enrichment_enabled = true\n    }\n\n    entity \"household-entity\" {\n      name = \"household\"\n      table_ref = \"PRODUCTION.CUST.HOUSEHOLD\"\n      primary_key = \"HOUSEHOLD_ID\"\n    }\n\n    entity \"subscription-entity\" {\n      name = \"subscription\"\n      table_ref = \"PRODUCTION.CUST.SUBSCRIPTION\"\n      primary_key = \"SUB_ID\"\n    }\n  \n    # Define the profile entity, which corresponds to Segment Profiles tables synced via Profiles Sync\n    # Recommend setting up Profiles Sync materialized views to optimize warehouse compute costs\n    profile {\n      profile_folder = \"PRODUCTION.SEGMENT\"\n      type = \"segment:materialized\"\n  \n      # First branch - relate accounts table to the profile\n      # This is a unique type of relationship between an entity and the profile block\n      relationship \"user-accounts\" {\n        name = \"Premium Accounts\"\n        related_entity = \"account-entity\"\n        # Join the profile entity with an identifier (e.g. email) on the related entity table\n        # Option to replace with the traits block below to join with a profile trait on the entity table instead\n        external_id {\n          type = \"email\"\n          join_key = \"EMAIL_ID\"\n        }\n  \n        # Define 1:many relationship between accounts and carts\n        # e.g. an account can be associated with many carts\n        relationship \"user-carts\" {\n          name = \"Shopping Carts\"\n          related_entity = \"cart-entity\"\n          join_on = \"account-entity.ID = cart-entity.ACCOUNT_ID\"\n    \n          # Define many:many relationship between carts and products\n          # e.g. there can be multiple carts, and each cart can be associated with multiple products\n          relationship \"products\" { \n            name = \"Purchased Products\"\n            related_entity = \"product-entity\"\n            junction_table {\n              primary_key = \"ID\"\n              table_ref = \"PRODUCTION.CUSTOMER.CART_PRODUCT\"\n              left_join_on = \"cart-entity.ID = CART_ID\"\n              right_join_on = \"PRODUCT_ID = product-entity.SKU\"\n            }      \n          }\n        }\n      }\n\n      # Second branch - relate households table to the profile by joining with an external ID block\n      relationship \"user-households\" {\n        name = \"Households\"\n        related_entity = \"household-entity\"\n        external_id {\n          type = \"email\"\n          join_key = \"EMAIL_ID\"\n        }\n  \n        # Define 1:many relationship between households and subscriptions\n        # e.g. a household can be associated with multiple subscriptions\n        relationship \"user-subscriptions\" {\n          name = \"Subscriptions\"\n          related_entity = \"subscription-entity\"\n          join_on = \"household-entity.SUB_ID = subscription-entity.HOUSEHOLD_ID\"\n    }\n}\n\n\n3a: Define entities\n\nThe first step in creating a Data Graph is to define your entities. An entity corresponds to a table in the warehouse.\n\nPARAMETERS\tDEFINITION\nentity\tAn immutable slug for the entity, and will be treated as a delete if you make changes. The slug must be in all lowercase, and supports dashes or underscores (e.g account-entity or account_entity).\nname\tA label displayed throughout your Segment space for Linked Events, Linked Audiences, etc. This name can be modified at any time.\ntable_ref\tDefines the fully qualified table reference: [database name].[schema name].[table name]. Segment flexibly supports tables, views and materialized views.\nprimary_key\tThe unique identifier for the given table. Must be a column with unique values per row.\n(If applicable) enrichment_enabled = true\tAdd this if you plan to reference the entity table for Linked Events use cases.\n\nExample:\n\ndata_graph {\n    entity \"account-entity\" {\n      name = \"account\"\n      table_ref = \"PRODUCTION.CUST.ACCOUNT\"\n      primary_key = \"ID\"\n    }\n    \n    entity \"cart-entity\" {\n      name = \"cart\"\n      table_ref = \"PRODUCTION.CUST.CART\"\n      primary_key = \"ID\"\n      enrichment_enabled = true\n    }\n}\n\n3b: Define the profile\n\nSegments recommends that you select materialized views under the Profiles Selective Sync settings to optimize warehouse compute costs.\n\nNext, define the profile. This is a special class of entity that represents Segment Profiles, which corresponds to the Profiles Sync tables and models. For Linked Audiences, this allows marketers to filter on profile traits, event history, etc. There can only be one profile for a Data Graph.\n\nPARAMETERS\tDEFINITION\nprofile_folder\tDefine the fully qualified path of the folder or schema location for the profile tables.\ntype\tIdentify the materialization method of the profile tables defined in your Profiles Sync configuration under Selective Sync settings: segment:unmaterialized or segment:materialized.\n\nExample:\n\n\ndata_graph {\n    # Define entities\n    ...\n  \n    # Define the profile entity, which corresponds to Segment Profiles tables synced via Profiles Sync\n    # Recommend setting up Profiles Sync materialized views to optimize warehouse compute costs\n    profile {\n      profile_folder = \"PRODUCTION.SEGMENT\"\n      type = \"segment:materialized\"\n    }\n}\n\n\n3c: Define relationships\n\nNow define your relationships between your entities. Similar to the concept of cardinality in data modeling, the Data Graph supports 3 types of relationships below. All relationship types require you to define the relationship slug, name, and related entity. Each type of relationship has unique join on conditions.\n\nProfile-to-entity relationship: This is a relationship between your entity table and the Segment Profiles tables, and is the first level of relationship.\n1:many relationship: For example, an account can have many carts, but each cart can only be associated with one account.\nmany:many relationship: For example, a user can have many carts, and each cart can have many products. However, these products can also belong to many carts.\nDefine profile-to-entity relationship\n\nThis is the first level of relationships and a unique type of relationship between the Segment profile entity and a related entity.\n\nPARAMETERS\tDEFINITION\nrelationship\tAn immutable slug for the relationship, and will be treated as a delete if you make changes. The slug must be in all lowercase, and supports dashes or underscores (e.g. user-account or user_account)\nname\tA label displayed throughout your Segment space for Linked Events, Linked Audiences, etc. This name can be modified at any time\nrelated_entity\tReferences your already defined entity\n\nTo define a profile-to-entity relationship, reference your entity table and depending on your table columns, choose to join on one of the following:\n\nOption 1 (Most common) - Join on an external ID: Use the external_id block to join the profile entity with an entity table using external IDs from your Unify ID resolution settings. Typically these identifiers are user_id, email, or phone depending on the column in the entity table that you want to join with.\n\ntype: Represents the external ID type (email, phone, user_id) in your id-res settings. Depending on if you are using materialized or unmaterialized profiles, these correspond to different columns in your Profiles Sync warehouse tables:\nMaterialized (Recommended): This corresponds to the type column in your Profiles Sync user_identifiers table.\nUnmaterialized: This corresponds to the external_id_type column in your Profiles Sync external_id_mapping_updates table.\njoin_key: This is the column on the entity table that you are matching to the external identifier.\n\nOption 2 - Join on a profile trait: Use the traits block to join the profile entity with an entity table using Profile Traits.\n\nname: Represents a trait name in your Unify profiles. Depending on if you are using materialized or unmaterialized profiles, these correspond to different columns in your Profiles Sync warehouse tables:\nMaterialized (Recommended): The trait name corresponds to a unique value of the name column in your Profiles Sync user_traits table.\nUnmaterialized: This corresponds to a column in the Profile Sync profile_trait_updates table.\njoin_key: This is the column on the entity table that you are matching to the trait.\n\nExample:\n\ndata_graph { \n    entity \"account-entity\" {\n      name = \"account\"\n      table_ref = \"PRODUCTION.CUST.ACCOUNT\"\n      primary_key = \"ID\"\n    }\n  \n    # Define additional entities...\n\n    # Note: Relationships are nested\n    profile {\n      profile_folder = \"PRODUCTION.SEGMENT\"\n      type = \"segment:materialized\"\n  \n      # Relate accounts table to the profile \n      relationship \"user-accounts\" {\n        name = \"Premium Accounts\"\n        related_entity = \"account-entity\"\n  \n        # Option 1: Join the profile entity with an identifier (e.g. email) on the related entity table\n        external_id {\n          type = \"email\"\n          join_key = \"EMAIL_ID\"\n        }\n  \n        # Option 2: Join the profile entity with a profile trait on the related entity table\n        trait {\n          name = \"cust_id\"\n          join_key = \"ID\"\n        }\n      }\n    }\n}\n\nDefine a 1:many relationship\n\nFor 1:many relationships, define the join on between the two entity tables using the spec below.\n\nPARAMETERS\tDEFINITION\nrelationship\tAn immutable slug for the relationship, and will be treated as a delete if you make changes. The slug must be in all lowercase, and supports dashes or underscores (e.g. user-account or user_account)\nname\tA label displayed throughout your Segment space for Linked Events, Linked Audiences, and so on. This name can be modified at any time\nrelated_entity\tReferences your already defined entity\njoin_on\tDefines relationship between the two entity tables [lefty entity slug].[column name] = [right entity slug].[column name]. Note that since you\u2019re referencing the entity slug for the join on, you do not need to define the full table reference\n\nExample:\n\ndata_graph { \n    entity \"cart-entity\" {\n      name = \"cart\"\n      table_ref = \"PRODUCTION.CUST.CART\"\n      primary_key = \"ID\"\n    }\n  \n   # Define additional entities...\n\n    # Note: Relationships are nested\n    profile {\n      profile_folder = \"PRODUCTION.SEGMENT\"\n      type = \"segment:materialized\"\n                   \n      relationship \"user-accounts\" {\n        ...\n  \n        # Define 1:many relationship between accounts and carts\n        relationship \"user-carts\" {\n          name = \"Shopping Carts\"\n          related_entity = \"carts-entity\"\n          join_on = \"account-entity.ID = cart-entity.ACCOUNT_ID\"\n        }\n      }\n    }\n}\n\nDefine many:many relationship\n\nFor many:many relationships, define the join on between the two entity tables with the junction_table.\n\nAttributes from a junction table are not referenceable via the Linked Audience builder. If a marketer would like to filter upon a column on the junction table, you must define the junction as an entity and define a relationship.\n\nPARAMETERS\tDEFINITION\nrelationship\tAn immutable slug for the relationship, and will be treated as a delete if you make changes. The slug must be in all lowercase, and supports dashes or underscores (e.g. user-account or user_account)\nname\tA label displayed throughout your Segment space for Linked Events, Linked Audiences, and so on. This name can be modified at any time\nrelated_entity\tReferences your already defined entity\n\nJunction table spec\n\nPARAMETERS\tDEFINITION\ntable_ref\tDefines the fully qualified table reference to the join table: [database name].[schema name].[table name]. Segment flexibly supports tables, views and materialized views\nprimary_key\tThe unique identifier for the given table. Must be a column with unique values per row\nleft_join_on\tDefine the relationship between the left entity table and the junction table: [left entity slug].[column name] = [junction table column name]. Note that schema and table are implied within the junction table column name, so you do not need to define it again\nright_join_on\tDefine the relationship between the junction table and the right entity table: [junction table column name] = [right entity slug].[column name]. Note that schema and table are implied within the junction table column name, so you do not need to define it again\n\nExample:\n\n\ndata_graph { \n    # Define entities\n\n    # Note: Relationships are nested\n    profile {\n      # Define profile\n         \n      relationship \"user-accounts\" {\n        ...\n  \n        relationship \"user-carts\" {\n          ...\n  \n          # Define many:many relationship between carts and products\n          relationship \"products\" {\n            name = \"Purchased Products\"\n            related_entity = \"product-entity\"\n            junction_table {\n              table_ref = \"PRODUCTION.CUSTOMER.CART_PRODUCT\"\n              primary_key = \"ID\"\n              left_join_on = \"cart-entity.ID = CART_ID\"\n              right_join_on = \"PRODUCT_ID = product-entity.SKU\"\n            }\n          }\n        }\n      }\n  }\n}\n         \n\nStep 4: Validate your Data Graph\n\nYou can validate your Data Graph using the preview, then click Save. After you\u2019ve set up your Data Graph, your partner teams can start leveraging these datasets with with Linked Events and Linked Audiences.\n\nEdit and manage your Data Graph\n\nTo edit your Data Graph:\n\nNavigate to Unify > Data Graph.\nSelect the Overview tab, and click Edit Data Graph.\nView Data Graph data consumers\n\nA data consumer refers to a Segment feature like Linked Events and Linked Audiences that are referencing datasets, such as entities and/or relationships, from the Data Graph. You can view a list of data consumers in two places:\n\nUnder Unify > Data Graph, click the Data consumers tab\nUnder Unify > Data Graph > Overview or the Data Graph editor > Preview, click into a node on the Data Graph preview and a side sheet will pop up with the list of data consumers for the respective relationship\nUnderstand changes that may cause breaking and potential breaking changes\n\nUpon editing and saving changes to your Data Graph, a modal will pop up to warn of breaking and/or potential breaking changes to your data consumers. You must acknowledge and click Confirm and save in order to proceed.\n\nDefinite breaking change: Occurs when deleting an entity or relationship that is being referenced by a data consumer. Data consumers affected by breaking changes will fail on the next run. Note: The entity and relationship slug are immutable and treated as a delete if you make changes. You can modify the label.\nPotential breaking change: Some changes such as updating the entity table_ref or primary_key, may lead to errors with data consumers. If there\u2019s a breaking change, the data consumer will fail on the next run. Unaffected data consumers will continue to work.\nDetect warehouse breaking changes\n\nSegment has a service that regularly scans and monitors the Data Graph for changes that occur in your warehouse that may break components of the Data Graph, like when the table being referenced by the Data Graph gets deleted from your warehouse or when the primary key column no longer exists. An alert banner will be displayed on the Data Graph landing page. The banner will be removed once the issues are resolved in your warehouse and/or the Data Graph. You will also have the option to trigger a manual sync of your warehouse schema.\n\nReceive alerts for warehouse breaking changes\n\nConfigure alerts for breaking changes to receive notifications over Slack, email, or in-app notification whenever Segment detects a breaking change in your warehouse.\n\nTo configure alerts for breaking changes:\n\nOpen your workspace and navigate to Settings > User Preferences > Activity Notifications.\nSelect Data Graph.\nSelect one of the following notification methods:\nEmail: Select this to receive notifications at either the email address associated with your account or another email address that you enter into this field.\nSlack: Select this and enter a Slack webhook URL and channel name to send alerts to a channel in your Slack workspace.\nIn-app: Select this to receive notifications in the Segment app. To view your notifications, select the bell next to your user icon in the Segment app.\nClick Save.\n\nThis page was last modified: 05 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nPrerequisites\nStep 1: Set up Data Graph permissions in your data warehouse\nStep 2: Connect your warehouse to the Data Graph\nStep 3: Build your Data Graph\nStep 4: Validate your Data Graph\nEdit and manage your Data Graph\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nHow To Guides\n/\nForecasting LTV with SQL and Excel for E-Commerce\nForecasting LTV with SQL and Excel for E-Commerce\n\nCustomer Lifetime Value (\u201cLTV\u201d) is the amount of money that an individual customer will spend with a given business in the future. It\u2019s often used to value cohorts in your customer base, determine how much to spend in acquiring or retaining new users in a given cohort, rank customers, and measure the success of marketing activities from a baseline LTV forecast.\n\nThe LTV calculation is not straightforward for e-commerce businesses, since future payments are not contractual: at any moment, a customer may never make a single purchase again. Additionally, forecasting future purchases requires statistical modeling that many current LTV formulas lack.\n\nThis guide shows how to calculate forward-looking LTV for non-contractual businesses using SQL and Excel. This analytical approach allows you to accurately rank your highest value customers, as well as predict their future purchase sizes to help focus your marketing efforts.\n\nThis guide assumes you\u2019re using the tracking schema described in How to implement an e-commerce tracking plan and are storing data in a Segment Warehouse.\n\nTalk to a product specialist to learn how companies like Warby Parker and Crate & Barrel use a data warehouse to increase engagement and sales.\n\nCalculating LTV: Buy \u2018Til You Die\n\nIn a non-contractual setting, you can\u2019t use a simple retention rate to determine when customers terminate their relationship. This is because the retention rate is a linear model that doesn\u2019t accurately predict whether a customer has ended her relationship with the company or is merely in the midst of a long hiatus between transactions.\n\nThe most accurate non-contractual LTV model, named \u201cBuy Til You Die\u201d (\u201cBTYD\u201d), focuses on calculating the discounted estimation of future purchases based on recency of last purchase, frequency of purchases, and average purchase value. This model uses non-linear modeling to predict whether or not a user is \u201calive\u201d or \u201cdead\u201d given historic transactions to forecast future probability and size of purchases.\n\nSince LTV is a critical metric for e-commerce companies, it\u2019s important that this model, instead of simpler linear formula that is based on retention rates, is used for it\u2019s calculation.\n\nUse SQL to build the necessary table, which will be exported as a CSV and opened in Google Sheets. Then, use Solver to estimate the predictive model parameters, which ultimately calculates the future purchases of each customer. Finally, the LTV calculation is simply the net present value of each customer\u2019s future purchases. Rank them by LTV, then find behavioral patterns across the top 10 or 50 customers to figure out how best to target or retain this cohort.\n\nRecency, frequency, and average size\n\nAs a growth analyst at the fictitious on-demand artisanal toast company, Toastmates, it\u2019s important to know which customers are worth more to the business than others. Most important, you should understand what similarities these customers all have to help guide the marketing team in their efforts.\n\nThe first step in creating the BTYD model is to get historic purchasing data of at least a month. In your analysis, you can use data from the past six months. The data must include the columns userId (email is fine too), number of purchases within the specified time window, days since last purchase, and days since first purchase.\n\nThen, use this Google Sheet, which provides all of the complex calculations for estimating the model parameters, as well as forecasting the future sales of each customer. This sheet is View Only, so be sure to copy it entirely so you can use it.\n\nTo retrieve a table with the right columns for analysis, use the follow SQL query:\n\n\u00a0 \u00a0 with\n\u00a0 \u00a0 first_transaction as (\n\u00a0 \u00a0 \u00a0 \u00a0 select \u00a0u.email,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 datediff('day', min(oc.received_at)::date, current_date) as first\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 from \u00a0toastmates.order_completed oc\n\u00a0 \u00a0 \u00a0left join \u00a0toastmates.users u\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on \u00a0oc.user_id = u.email\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0where \u00a0oc.received_at > dateadd('month', -6, current_date)\n\u00a0 \u00a0 \u00a0 group by \u00a01\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 frequency as (\n\u00a0 \u00a0 \u00a0 \u00a0 select \u00a0u.email,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 count(distinct oc.checkout_id) as frequency\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 from \u00a0toastmates.order_completed oc\n\u00a0 \u00a0 \u00a0left join \u00a0toastmates.users u\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on \u00a0oc.user_id = u.email\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0where \u00a0oc.received_at > dateadd('month', -6, current_date)\n\u00a0 \u00a0 \u00a0 group by \u00a01\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 last_transaction as (\n\u00a0 \u00a0 \u00a0 \u00a0 select \u00a0u.email,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 datediff('day', max(oc.received_at)::date, current_date) as last\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 from \u00a0toastmates.order_completed oc\n\u00a0 \u00a0 \u00a0left join \u00a0toastmates.users u\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on \u00a0oc.user_id = u.email\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0where \u00a0oc.received_at > dateadd('month', -6, current_date)\n\u00a0 \u00a0 \u00a0 group by \u00a01\n\u00a0 \u00a0 ),\u00a0\n\u00a0 \u00a0 average_transaction_size as (\n\u00a0 \u00a0 \u00a0 \u00a0 select \u00a0u.email,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 avg(oc.total) as avg\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 from \u00a0toastmates.order_completed oc\n\u00a0 \u00a0 \u00a0left join \u00a0toastmates.users u\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on \u00a0oc.user_id = u.email\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0where \u00a0oc.received_at > dateadd('month', -6, current_date)\n\u00a0 \u00a0 \u00a0 group by \u00a01\n\u00a0 \u00a0 \u00a0 order by \u00a02 desc\n\u00a0 \u00a0 )\n\u00a0 \u00a0 \u00a0 \u00a0 select \u00a0distinct\u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 u.email,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvl(f.frequency, 0) as frequency,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvl(z.last, 0) as days_since_last_transaction,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvl(a.first, 0) as days_since_first_transaction,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 t.avg as average_transaction_size\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 from \u00a0toastmates.users u\n\u00a0 \u00a0 \u00a0left join \u00a0first_transaction a\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on \u00a0u.email = a.email\n\u00a0 \u00a0 \u00a0left join \u00a0frequency f\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on \u00a0u.email = f.email\n\u00a0 \u00a0 \u00a0left join \u00a0last_transaction z\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on \u00a0u.email = z.email\n\u00a0 \u00a0 \u00a0left join \u00a0average_transaction_size t\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on \u00a0u.email = t.email\n\u00a0 \u00a0 \u00a0 order by \u00a02 desc\n\n\nThis returns a table where each row is a unique user and the columns are email, number of purchases within the time window, number of discrete time units since last purchase, and average purchase order.\n\nHere is a screenshot of the first twelve rows returned from the query in Mode Analytics.\n\nExport this data to a CSV, then copy and paste it in the first sheet of the Google Sheet where the blue type is in the below screenshot:\n\nAlso be sure to add the total time in days in cell B6. This is important as the second sheet uses this time duration for calculating net present value of future payments.\n\nHow to use the Google Spreadsheet\n\nAfter you paste in the CSV from the table into the first tab of the sheet, the next step is to estimate the model parameters (the variables on the top left of the sheet). In order to do this, we need to use a feature of Microsoft Excel called Solver.\n\nYou can export your Google Sheet as an Excel document. Then, use Excel Solver to minimize the log-likelihood number in cell B5, while keeping the parameters from B1:B4 greater than 0.0001.\n\nAfter Solver runs, cells B1:B4 will be updated to represent the model\u2019s estimates. Now, you can hard code those back into the sheet on Google Sheets. The next sheet relies on these model estimates to calculate the expected purchases per customer.\n\nModel and predict future customer purchases\n\nThe model requires four pieces of information about each customer\u2019s past purchasing history: her \u201crecency\u201d (how many \u201ctime units\u201d her last transaction occurred), \u201cfrequency\u201d (how many transactions she made over the specified time period), the length of time over which we have observed her purchasing behavior, and the average transaction size.\n\nIn the example, you have the purchasing behavior data over the course of six months with each unit of time being a single day.\n\nYou can apply a both a beta-geometric and a negative binomial distribution (\u201cBG/NBD\u201d) to these inputs and then use Excel to estimate the model parameters (an alternative would be the Pareto/NBD model). These probability distributions are used because they accurately reflect the underlying assumptions of the aggregation of realistic individual buying behavior. (Learn more about these models).\n\nAfter estimating the model parameters, you can predict a particular customer\u2019s conditional expected transactions by applying the same historic purchasing data to Bayes\u2019 Theorem, which describes the probability of an event based on prior knowledge of conditions related to the event.\n\nEstimating the model parameters\n\nThe top left part of the first sheet represent the parameters of the BG/NBD model that must be fitted to the historic data you paste in. These four parameters (r, alpha, a, and b) will have \u201cstarting values\u201d of 1.0, since you\u2019ll use Excel Solver to determine their actual values.\n\nThe values in columns F to J represent variables in the BG/NBD model. Column F, in particular, defines a single customer\u2019s contribution to a the overarching function, on which we\u2019ll use Solver to determine the parameters. In statistics, this function is called the likelihood function, which is a function of the parameters of a statistical model.\n\nIn this particular case, this function is the log-likelihood function, which is B5, as calculated as the sum of all cells in column F. Logarithmic functions are easier to work with, since they achieve its maximum value at the same points as the function itself. With Solver, find the maximum value of B5 given the parameters in B1:B4.\n\nWith the new parameter estimates, you can now predict a customer\u2019s future purchases.\n\nPredicting a customer\u2019s future purchases\n\nIn the next sheet, you can apply Bayes\u2019 Theorem to the historic purchasing information to forecast the quantity of transactions in the next period. Multiply the expected quantity with the average transaction size to calculate the expected revenue for that period, which you can extrapolate as an annuity, of which you can find the present discounted value (assuming discount rate is 10%).\n\nCentral to the Bayes\u2019 Theorem formula is the Gaussian hypergeometric function, which is defined by \u201c2F1\u201d in column M. Evaluate the hypergeometric function as if it were a truncated series: by adding terms to the series until each term is small enough that it becomes trivial. In the spreadsheet, we sum the series to it\u2019s 50th term.\n\nThe rest of the variables in Bayes\u2019 Theorem is in columns I through L, which use the inputs from the customer\u2019s historic purchasing information, as well as the model parameter estimates as determined from Solver (cells B1:B4).\n\nThe expected quantity of purchases in the next time period is calculated in column H.\n\nFinally, multiply that with the average transaction size and you can get the expected revenue for the next time period.\n\nRank your customers\n\nThis exercise allows you to rank your customers from most valuable to least by ordering column F in descending order. You can take the userId s of the top several customers and look across their shopping experiences to identify any patterns that they share, to understand what behaviors are leading indicators to becoming high value customers.\n\nBelow is a simple query to get a table of a user\u2019s actions in rows. Just replace the user_idwith the user in question.\n\n\u00a0 \u00a0 with anonymous_ids as (\n\u00a0 \u00a0 \u00a0 \u00a0 select \u00a0anonymous_id from toastmates.tracks\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0where \u00a0user_id = '46X8VF96G6'\n\u00a0 \u00a0 \u00a0 group by \u00a01\n\u00a0 \u00a0 ),\n\n\u00a0 \u00a0 page_views as (\n\u00a0 \u00a0 \u00a0 \u00a0 select \u00a0*\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 from \u00a0toastmates.pages p\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0where \u00a0p.user_id = '46X8VF96G6'\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 or \u00a0anonymous_id in (select anonymous_id from anonymous_ids)\n\u00a0 \u00a0 \u00a0 order by \u00a0p.received_at desc\n\u00a0 \u00a0 ),\n\n\u00a0 \u00a0 track_events as (\n\u00a0 \u00a0 \u00a0 \u00a0 select \u00a0*\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 from \u00a0toastmates.tracks t\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0where \u00a0t.user_id = '46X8VF96G6'\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 or \u00a0anonymous_id in (select anonymous_id from anonymous_ids)\n\u00a0 \u00a0 \u00a0 order by \u00a0t.received_at desc\n\u00a0 \u00a0 )\n\n\u00a0 \u00a0 \u00a0 select \u00a0url,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 received_at\n\u00a0 \u00a0 \u00a0 \u00a0 from \u00a0page_views\n\u00a0 \u00a0 \u00a0 union \u00a0\n\u00a0 \u00a0 \u00a0 select \u00a0event_text,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 received_at\n\u00a0 \u00a0 \u00a0 \u00a0 from \u00a0track_events\n\u00a0 \u00a0 order by \u00a0received_at desc\n\n\nThis above query for user whose user_id is \"46X8VF96G6\" returns the below table:\n\nAt Toastmates, most of the highest forward-looking expected LTV customers share one thing in common: averaging two orders per month with an average purchase size of $20.\n\nWith that in mind, you can define a behavioral cohort in our email tool, Customer.io, as well as create a trigger workflow so we can send an email offer to these customers.\n\nLearn how to use email tools to target this cohort of high value customers.\n\nReward your best customers\n\nThis exercise is useful not only as a forward looking forecasting model for customer LTV, but also as a quality ranking system to see which customers are worth more to your business. Coupled with the ability to glance across the entire shopping experience of a given customer, you can identify broad patterns or specific actions that may be an early signal for a high value shopper. Recognizing these high value shoppers means being proactive in nurturing, rewarding, and retaining them.\n\nAnd this is just the beginning. Having a rich set of raw customer data allows you to create accurate projection models for LTV so you know not only how much you can spend to acquire them, but also how to rank your customers by value. Ultimately, these insights lead to the right actions that can build an engaging shopping experience and drive sales.\n\nTalk to a product specialist to learn how companies like Warby Parker and Crate & Barrel use a data warehouse to increase engagement and sales.\n\nThis page was last modified: 15 Mar 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nCalculating LTV: Buy \u2018Til You Die\nHow to use the Google Spreadsheet\nModel and predict future customer purchases\nRank your customers\nReward your best customers\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGetting Started\n/\nSending data to destinations\nSending data to destinations\n\nOnce you\u2019ve got data flowing into Segment, what do you do with it? The Segment Destination catalog lists all of the places we can send your data.\n\nRouting data to destinations\n\nWhen you enable a destination in the Segment App, you link it to a specific source (or sources). By default, Segment first processes the data from the selected source(s), then translates it and routes it from the Segment servers to the API endpoint for that destination.\n\nThis means that if you previously had loaded code or a snippet for that tool on your website or app, you should remove it once you have Segment implemented so you don\u2019t send duplicate data.\n\nYou might also want to enable tools that need to be loaded on the user\u2019s device (either a computer or mobile device) in order to function properly. For our Analytics.js library, you can make these changes from the Segment App, and the Segment systems then update the bundle of code served when users request the page to include code required by the destination. You can read more about this in our documentation on Connection Modes.\n\nAdding new destinations\n\nAdding a destination is quick and easy from the Segment App. You\u2019ll need a token or API key for the tool, or some way to confirm your account in the tool.\n\nFrom your Segment workspace, click Add destination. You can find this option on the Connections home page, from the Destinations list, or from a Source overview page.\nSearch for the destination in the Catalog, and click the destination\u2019s tile.\nFrom the destination summary page that appears, click Configure.\nChoose which source should send data to this destination, and click Confirm source.\nIn the Connection Settings that appear, enter any required fields. These might be an API key, an account ID, a token, or you might be prompted to log in to the tool.\nIf needed, click the toggle to enable the destination so it begins receiving data.\nRecommended destinations\n\nIf you\u2019re just starting out, we know the catalog can be really overwhelming. How do you choose from all of the available destinations?\n\nWe\u2019ve written a lot about how to choose your tools, but as a start, we recommend that you have one tool from each of the following categories:\n\nAnalytics\nEmail marketing\nLive-chat\n\nIf you\u2019re adding more destinations after you\u2019ve done your Segment instrumentation, you might want to check that the destinations you choose can accept the methods you\u2019re already using, and that they can use the Connection Modes you\u2019re already using.\n\nWe also feel that it\u2019s really important to have a data warehouse, so you can get a clearer view of all of your data for analytics purposes. More on that just below.\n\nAdding a warehouse\n\nWarehouses are a special type of destination which receive streaming data from your Segment sources, and store it in a table schema based on your Segment calls. This allows you to do a lot of interesting analytics work to answer your own questions about what your users are doing and why.\n\nAll customers can connect a data warehouse to Segment. Free and Team customers can connect one, while Business customers can connect as many as needed.\n\nYou should spend a bit of time considering the benefits and tradeoffs of the warehouse options, and then choose one from our warehouse catalog.\n\nWhen you choose a warehouse, you can then use the steps in the documentation to connect it. This may require that you create a new dedicated user (or \u201cservice user\u201d) to allow Segment to access the database.\n\nOnce your warehouse is configured and running, you can connect to it using a Business Intelligence (BI) tool (such as Looker, Mode, Tableau, or others) to analyze your data in-depth.\n\nThere are also a number of Business tier features you can then use with your warehouse, including selective sync and Replay.\n\nSegment University: Warehouses\n\nCheck out our course on warehouses in Segment University. (Must be logged in to access.)\n\nBACK\nA full Segment implementation\n\nTake your plans, and make them real.\n\nNEXT\nTesting and Debugging\n\nTest your implementation and see where your data is and isn't arriving.\n\nThis page was last modified: 09 Aug 2022\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nRouting data to destinations\nAdding new destinations\nAdding a warehouse\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nCampaigns\n/\nBroadcasts\nBroadcasts\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE PREMIER \u2713\n?\n\nEngage Premier entered an End of Sale (EOS) period effective June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. Segment recommends exploring the following pages in preparation of a migration or future MCM needs:\n\nTwilio Marketing Campaigns\n\nPreferred ISV Partners:\n\nAirship Blog\nBloomreach Blog\nBraze Blog\nInsider Blog\nKlaviyo Blog\nTwilio Engage Foundations Documentation\n\n\nBroadcasts are one-time email or SMS campaigns that you can send with Twilio Engage. Use broadcasts for single, one-off occasions like the following:\n\nSpecial events, like webinars or conferences\nOffers, like product discount codes\nNewsletters that you want to send on a specific date\n\nFor more on the different types of Engage campaigns, read Audiences, Journeys, and Broadcasts.\n\nOn this page, you\u2019ll find step by step instructions for how to create a broadcast, as well as information on broadcast best practices and analytics.\n\nCreate and send an email broadcast\n\nFollow these steps to create an email broadcast:\n\nNavigate to Engage > Broadcasts, then click + Create broadcast.\nFrom the New broadcast page, choose Email.\nAdd a name and description, then click Choose recipients.\nClick Add condition to add users who will receive your campaign, then click Build.\nTo send a message to a pre-built audience, choose Part of an Audience, then select the audience.\nTo exclude users from the audience, click Add condition in the And who section. Click And who, then select And not who. Segment will exclude users from the audience you choose.\nClick Preview to estimate the audience size.\nSelect the subscription group that you want to receive your broadcast.\nSelect Build, then choose either Build a new email or select a template.\nFill out the Email settings fields, choose your email editor, then click Continue.\nConfigure your email, then click Continue.\nOn the Review and schedule page, confirm your broadcast\u2019s settings.\nSchedule your broadcast:\nTo send your broadcast immediately, select Send now, then click Send now ->. Confirm a final time by clicking Send in the popup.\nTo send your broadcast later, select Schedule, then enter the date, time, and time zone for your scheduled broadcast. Click Schedule ->, then confirm by clicking Schedule in the Schedule message popup.\n\nSegment recommends sending email broadcasts to users with a subscribed status. However, if you need to send an email broadcast to someone who hasn\u2019t subscribed, you can configure an email to send to all users.\n\nBlind carbon copy\n\nBroadcasts doesn\u2019t support BCC (blind carbon copy). If your use case requires BCC, contact Segment.\n\nCreate and send an SMS broadcast\n\nFollow these steps to create an email broadcast:\n\nNavigate to Engage > Broadcasts, then click + Create broadcast.\nFrom the New broadcast page, choose SMS.\nAdd a name and description, then click Choose recipients.\nClick Add condition to add users who will receive your campaign, then click Build.\nTo send a message to a pre-built audience, choose Part of an Audience, then select the audience.\nTo exclude users from the audience, click Add condition in the And who section. Click And who, then select And not who. Segment will exclude users from the audience you choose.\nClick Preview to estimate the audience size.\nSelect Build.\nChoose an existing template or create a new template.\nYou can edit existing templates. Edited templates won\u2019t be saved in the content tab.\nIf you create a new template, enter a name, then select the language and content type.\nChoose a messaging service, enter your message into the body field, and add any merge tags.\n(Optional:) Test your SMS.\nInclude opt out instructions; your SMS broadcast must contain Reply STOP to unsubscribe.\nTest your SMS, then select Review and schedule.\nOn the Review and schedule page, confirm your broadcast\u2019s settings.\nSchedule your broadcast:\nTo send your broadcast immediately, select Send now, then click Send now ->. Confirm a final time by clicking Send in the popup.\nTo send your broadcast later, select Schedule, then enter the date, time, and time zone for your scheduled broadcast. Click Schedule ->, then confirm by clicking Schedule in the Schedule message popup.\nCancel a scheduled broadcast\n\nFollow these steps to cancel a scheduled broadcast:\n\nNavigate to Engage > Broadcasts > Scheduled.\nSelect the scheduled broadcast you want to cancel.\nFrom the broadcast overview tab, click Unschedule.\nIn the popup, click Unschedule to confirm.\n\nUnscheduled broadcasts revert to draft status and can be found under the Drafts tab of the Broadcasts page.\n\nWorking with broadcasts\n\nKeep the following information in mind as you work with broadcasts.\n\nSMS segments\n\nSMS broadcasts longer than 160 characters are split into segments and then joined together by the recipient\u2019s device. As a result, you can send SMS broadcasts longer than 160 characters, but each 160-character segment is billed individually.\n\nFor more on message segments, view SMS character limits.\n\nEmail template limits\n\nThe total size of your email, including attachments, must be less than 30MB.\n\nTo learn more, view SendGrid\u2019s email limits.\n\nScale and throughput\n\nThe following table lists geographic availability, scale, and speed details for email and SMS broadcasts:\n\nBROADCAST TYPE\tAVAILABILITY\tTHROUGHPUT\nEmail\tUS and EU\t5 million per hour\nSMS short code\tUS, Canada, UK\t360,000 per hour\nSMS long code (10DLC)\tUS, Canada\tTrust-score dependent\n\nLong-code message throughput depends on a number of factors, including your 10DLC trust score.\n\nSegment recommends that you use short code phone numbers for SMS broadcasts sent to more than 5000 recipients.\n\nBroadcast analytics\n\nSegment provides analytics for each broadcast. By selecting a sent broadcast from the broadcasts list, you can view both high-level performance metrics and granular insights on what actions individual recipients have taken on the Broadcast campaign.\n\nEngage powers analytics for both email and SMS broadcasts. For more information on Engage analytics, view Analytics Overview.\n\nReview sent broadcasts\n\nTo view information for a sent broadcast, navigate to Engage > Broadcasts, and select a broadcast from the broadcasts list.\n\nContent tab\n\nThe content tab shows the message content and email or SMS settings that you configured for the broadcast.\n\nRecipients tab\n\nSegment maintains a recipients list for broadcasts. The recipients list lets you filter through several analytics statuses. Selecting an individual profile from the recipients list opens a preview pane with that profile\u2019s details.\n\nSettings tab\n\nThe settings tab shows your broadcast\u2019s setup info, the recipient audience and its subscription status, as well as the broadcast\u2019s scheduled time.\n\nOn the settings tab, you can also find the broadcast\u2019s campaign key, which you can use to reference the broadcast. For example, you can use the campaign key to create an audience for future targeting or to create a suppression list of recipients you don\u2019t want to receive future broadcasts.\n\nThis page was last modified: 15 Jul 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nCreate and send an email broadcast\nCreate and send an SMS broadcast\nWorking with broadcasts\nBroadcast analytics\nReview sent broadcasts\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nCampaigns\n/\nMobile Push\n/\nMobile Push Campaigns\nMobile Push Campaigns\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE PREMIER \u2713\n?\n\nEngage Premier entered an End of Sale (EOS) period effective June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:\n\nTwilio Marketing Campaigns\n\nPreferred ISV Partners:\n\nAirship Blog\nBloomreach Blog\nBraze Blog\nInsider Blog\nKlaviyo Blog\nTwilio Engage Foundations Documentation\n\n\nWith Twilio Engage, you can send campaigns to users who have opted in to receive your marketing materials. On this page, you\u2019ll learn how to create and send a mobile push campaign.\n\nSome knowledge of the Journeys product will benefit you as you read through this guide. If you\u2019re new to Journeys, the Journeys documentation will bring you up to speed.\n\nHow Engage campaigns work\n\nTwilio Engage uses Journeys to send campaigns. With Journeys, you add conditions and steps that trigger actions like sending an email, an SMS, or a mobile push.\n\nYou\u2019ll build and then send your campaign in three stages:\n\nCreate a journey.\nAdd a journey condition.\nCreate, test, and publish your mobile push campaign.\nCreate a journey\n\nBecause Engage campaigns exist within Journeys, begin by creating a journey:\n\nIn Engage, select Journeys, then click New Journey.\nName your journey and select its entry settings.\nClick Build Journey to create the journey.\nAdd a Journey condition\n\nWith your Journey created, you\u2019ll now create a condition that will trigger your campaign:\n\nWithin the Journey builder, click + Add Entry Condition.\nIn the Add Entry Condition pane, give the step a name.\nClick + Add Condition, select your desired condition, then click Save.\n\nWith your entry condition added, you\u2019re now ready to create your mobile push campaign.\n\nCreate, test, and publish your mobile push campaign\n\nFollow these steps to create a mobile push campaign:\n\nWithin the Journey builder, click the + node below your new condition.\nFrom the Add step window, click Send a Push.\nIn the Send a Push window, select the mobile push template you want to use, or click Create new template to build a new template.\nReview your template\u2019s content and click behavior, then click Test or Continue.\nIn the Send a Push modal, give the step a name, choose a messaging service, add any conversion goals, then click Save.\nIn the Journey builder, click Publish.\n\nYour mobile push campaign is now live. Users who trigger the mobile push step\u2019s parent Journey condition will receive your campaign.\n\nTest your mobile push template\n\nPush tokens\n\nPush tokens are unique identifiers Segment associates with each profile. For mobile push, you\u2019ll need to configure identity resolution settings for the push tokens ios.push_token and android.push_token. Using the Profile explorer, you can find a profile\u2019s push tokens by opening a profile and then selecting the Identities tab. You can only send mobile pushes to profiles with push tokens enabled.\n\nFollow these steps to test your mobile push:\n\nChoose a template to test:\nFor new templates, select Test once you\u2019ve finished building a template.\nFor existing templates, navigate to Engage > Content > Push, select the template you want to test, then click Test.\nMobile push templates have a content size limit of 4KB.\nChoose a messaging service and add a recipient.\nYou can add recipients using an email address or user ID.\nClick Send test push.\n\nSegment verifies that the profile you\u2019re sending a test to has a push token, then sends the test. If the test mobile push doesn\u2019t work as expected, confirm that the profile you\u2019re sending to has a push token.\n\nThis page was last modified: 15 Jul 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nHow Engage campaigns work\nTest your mobile push template\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nApi\n/\nConfig Api\n/\nAPI Design\nAPI Design\n\nThe Segment Public API is available\n\nSegment\u2019s Public API is available for Team and Business tier customers to use. You can use the Public API and Config APIs in parallel, but moving forward any API updates will come to the Public API exclusively.\n\nPlease contact your account team or friends@segment.com with any questions.\n\nAPI Evolution: Versioning and Compatibility\n\nSegment strives to maintain a strict contract around the stability of our APIs once they reach maturity.\n\nThe Config API is versioned statically by path prefix. The current version, /v1beta, is stable and suitable for production use, but not 100% mature. We will not ship any backwards incompatible breaking changes after its public launch on December 12, 2018. However, we may release new versions of the API as further /v1betaX releases or as major version releases (such as /v1).\n\nThe reason the path is currently /v1beta is to signify both that Segment\u2019s product model is evolving and that we are looking for feedback from developers on how best to expose that model for public consumption, extension, and automation.\n\nTherefore, /v1beta is subject to deprecation with 3 months of written notice after the release of /v1, should we decide to land any breaking changes in the final /v1 release.\n\nWe\u2019re always actively seeking feedback on the ergonomics and breadth of our APIs. If you have ideas of how we can improve them that you\u2019d like to see as extensions to /v1beta or considered for changes in /v1, don\u2019t hesitate to get in touch.\n\nCommon Methods and Fields\n\nThe Config API is a set of REST APIs for managing Segment resources. The primary Segment resources are:\n\nPersonal Access Tokens\nWorkspaces\nSources\nDestinations\nTracking Plans\n\nYou can manage each resource using standard methods:\n\nMETHOD\tHTTP MAPPING\nList\tGET\nGet\tGET\nCreate\tPOST\nUpdate\tPATCH\nDelete\tDELETE\nErrors\nERROR\tHTTP STATUS\tCODE\tREASONS\nInvalid Argument\t400 Bad Request\t3\tThe request contains invalid JSON or a field contains an invalid value\nUnauthenticated\t401 Unauthorized\t16\tThe access token is missing or invalid\nPermissionDenied\t403 Forbidden\t7\tAn access token with write scope is required for the Create, Update and Delete methods\nNot Found\t404 Not Found\t5\tThe request or resource could not be found. Either the request method or path is incorrect, or the resource does not exist in this workspace (sometimes because of a typo).\nAlready Exists\t409 Conflict\t6\tA resource (e.g. source) already exists with the given name\nResource Exhausted\t429 Too Many Requests\t8\tThe 200 req / min rate limit was exhausted\nInternal\t500 Internal Server Error\t13\tSegment encountered an error processing the request\nUnimplemented\t501 Not Implemented\t12\tThe method is not supported or implemented\nUnavailable\t503 Service Unavailable\t14\tThe API is down\nupstream connect error\nor disconnect/reset before headers\t503 Service Unavailable\tn/a\tThe URL is invalid so the requesst can\u2019t route to an upstream API service\nPagination\n\nThe List method is paginated. Requests take an optional page_size parameter which generally defaults to 10 or 100 items. If there are more than page_size items in a collection, the response includes a next_page_token field. You can then supply this as page_token parameter to the next List request. If there are no more items in the collection, next_page_token will be empty.\n\nUpdate Mask\n\nUpdate methods follow PATCH semantics. In addition to the data to update, the method requires an explicit set of field names to update. This removes all ambiguity around empty values.\n\n$ curl -X PATCH \\\n  -h \"Authorization: Bearer $ACCESS_TOKEN\" \\\n  -d '{\n\t\"destination\": {\n\t\t\"enabled\": true,\n    \"update_mask\": {\n\t\t  \"paths\": [\n  \t\t\t\"destination.enabled\"\n\t  \t]\n    }\n  }'\n  'https://platform.segmentapis.com/v1beta/workspaces/myworkspace/sources/js/destinations/google-analytics'\n\n\nNote that specifying an update path with no value will change the field to an empty value (e.g. \u201c\u201d, 0, or false). For example this will result in enabled false:\n\n$ curl -X PATCH \\\n  -h \"Authorization: Bearer $ACCESS_TOKEN\" \\\n  -d '{\n\t\"destination\": {\n    \"update_mask\": {\n\t\t  \"paths\": [\n  \t\t\t\"destination.enabled\"\n\t  \t]\n    }\n  }'\n  'https://platform.segmentapis.com/v1beta/workspaces/myworkspace/sources/js/destinations/google-analytics'\n\n\nAnd note that specifying data but no paths will not change the field value. For example this has no effect:\n\n$ curl -X PATCH \\\n  -h \"Authorization: Bearer $ACCESS_TOKEN\" \\\n  -d '{\n\t\"destination\": {\n    \"enabled\": true\n  }'\n  'https://platform.segmentapis.com/v1beta/workspaces/myworkspace/sources/js/destinations/google-analytics'\n\nResource Names\n\nResources are named entities exposed by our services and APIs, and resource names are their identifiers. Each resource has its own unique resource name made up of the ID or slug of the resource itself and the IDs or slugs of any parent resources.\n\nSo a given Source resource might look like:\n\nworkspaces/<customer-workspace-slug>/sources/<customer-source-slug>\n\nSegment APIs use scheme-less URIs for resource names. This allows us to generally follow REST URL conventions and provides a machine and human-legible standard for our identifiers.\n\nSegment favors \u201cslugs\u201d over opaque IDs when the resource is customer- or Segment-named and unlikely to change, such as sources and destinations. For resources whose name is subject to change frequently, such as Tracking Plans, we will autogenerate prefixed IDs for you upon creation and use that in the fully qualified resource name.\n\nWhile full resource names resemble normal URLs, they are not the same thing. A single resource may be exposed by different API versions, API protocols, or API network endpoints as Segment\u2019s APIs evolve.\n\nThis page was last modified: 07 Sep 2022\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nAPI Evolution: Versioning and Compatibility\nCommon Methods and Fields\nErrors\nPagination\nUpdate Mask\nResource Names\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nReverse ETL\nReverse ETL\n\nReverse ETL (Extract, Transform, Load) extracts data from a warehouse using a query you provide and syncs this warehouse data to your third party destinations.\n\nUse Reverse ETL when you want to:\n\nElevate marketing campaigns: Sync audiences and other data built in the warehouse to multi-channel marketing tools, like Braze, Hubspot, or Salesforce Marketing Cloud, to personalize marketing campaigns.\nEnrich your customer profiles: Sync enriched data to destinations like Mixpanel for a more complete view of the customer, or enrich Segment Profiles with data from your warehouse.\nActivate data in Twilio Engage: Send data in the warehouse back into Segment as events that can be activated in all supported destinations, including Twilio Engage destinations.\nStrengthen your conversion events: Pass offline or enriched data to conversion APIs like Facebook, Google Ads, TikTok, or Snapchat.\nMake warehouse data accessible to business teams: Connect destinations like Google Sheets to a view in the warehouse to allow business teams to access up-to-date reports.\n\nReverse ETL supports event and object data\n\nEvent and object data includes customer profile data, subscriptions, product tables, shopping cart tables, and more.\n\nGet started with Reverse ETL\nSet up Reverse ETL\n\nSet up the infrastructure you need to sync data from your warehouse to your downstream destinations.\n\nManage Reverse ETL Syncs\n\nView your sync history, reset your syncs, or subscribe to alerts.\n\nLearn more\n\nLearn more about the system that powers Reverse ETL, supported destinations, and frequently asked questions.\n\nReverse ETL System\n\nReference material about system limits and how Segment detects data changes.\n\nDestination catalog\n\nView the destinations you can connect to your Reverse ETL sources.\n\nReverse ETL FAQ\n\nFrequently asked questions about Reverse ETL.\n\nMore Reverse ETL resources\nWhat is Reverse ETL? A complete guide\n\nIn this blog from Segment, learn how Reverse ETL helps businesses activate their data to drive better decision-making and greater operational efficiency.\n\nCustomer story: MongoDB\n\nLearn how MongoDB used Reverse ETL to connect the work of analytics teams to downstream marketing and sales tools to deliver just-in-time communications that increased customer satisfaction and engagement.\n\nThis page was last modified: 10 Sep 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nData Storage overview\nData Storage overview\n\nOff-the-shelf analytics tools (like Google Analytics and Mixpanel) offer quick and easy insights about common business questions, and often meet the needs of marketing teams and product managers. However, data analysts and data scientists need access to an organization\u2019s raw data to derive deeper and more customized insights to support their organization.\n\nOnly users with Business or Team plans can add Warehouse destinations.\n\nSegment offers several Data Storage Destinations to help you store your raw Segment data, including:\n\nData Warehouses (available to Team and Business Tier customers)\nAWS S3 (available to all users)\nGoogle Cloud Storage (available to all users)\nSegment Data Lakes (available to Business Tier customers only)\nAnalytics Academy: Why you should own your data\n\nAlthough the sharing economy is eroding the idea of \u201cownership,\u201d when it comes to analytics data, we strongly believe that you should own it.\n\nThis page was last modified: 30 Mar 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSources\n/\nCatalog\n/\nLibraries\n/\nWebsite\n/\nAnalytics.js Source\nAnalytics.js Source\nCOMMUNITY X\nMAINTENANCE X\nFLAGSHIP \u2713\n?\n\nAnalytics.js enables you to send your data to hundreds of destination tools without having to learn, test, or use a new API every time.\n\nSegment\u2019s Analytics.js library is fully open-source and can be viewed on GitHub.\n\nGetting started\n\nUse the Analytics.js QuickStart Guide to learn how to add Analytics.js to your site. Once you\u2019ve installed the library, read on for the detailed API reference.\n\nBenefits of Analytics.js\n\nAnalytics.js provides two key benefits over the previous version.\n\nPerformance\n\nAnalytics.js reduces page load time and improves site performance. Its package size is ~70% smaller than its predecessor, the classic version of Analytics.js.\n\nMany factors impact page load time, including network conditions, hosting locations, and page weight. Page weight for each customer integration varies based on the number of device-mode destinations that are enabled for each source. The more device-mode destinations that are enabled, the more data gets added to the library, which will impact the weight of the library.\n\nDeveloper experience\n\nAnalytics.js improves developer experience by introducing new ways for developers to augment events throughout the event timeline. For example, developers can augment events either before or after an event occurs, or while the event is in-flight.\n\nFor example, you can use Analytics.js to build features that:\n\nEnsure you have user consent to track before an event fires\nEnrich events with customer or page context while in-flight with middleware\nCheck an event for errors after the event is sent to Segment\nBasic tracking methods\n\nThe basic tracking methods below serve as the building blocks of your Segment tracking. They include Identify, Track, Page, Group, and Alias.\n\nThese methods correspond with those used in the Segment Spec. The documentation on this page explains how to use these methods in Analytics.js.\n\nGood to know\n\nFor any of the methods described in this page, you can replace the properties in the code samples with variables that represent the data collected.\n\nIdentify\n\nUse the identify method to link your users and their actions, to a recognizable userId and traits. You can see an identify example in the Quickstart guide or find details on the identify method payload.\n\n`identify` and anonymous visitors\n\nSegment recommends against using identify for anonymous visitors to your site. Analytics.js automatically retrieves an anonymousId from localStorage or assigns one for new visitors, and then attaches it to all page and track events both before and after an identify.\n\nThe Identify method follows the format below:\n\nanalytics.identify([userId], [traits], [options], [callback]);\n\n\nThe Identify call has the following fields:\n\nFIELD\t\u00a0\tTYPE\tDESCRIPTION\nuserId\toptional\tString\tThe database ID for the user. If you don\u2019t know who the user is yet, you can omit the userId and just record traits. You can read more about identities in the identify reference.\ntraits\toptional\tObject\tA dictionary of traits you know about the user, like email or name. You can read more about traits in the identify reference.\noptions\toptional\tObject\tA dictionary of options. For example, enable or disable specific destinations for the call. Note: If you do not pass a traits object, pass an empty object (as an \u2018{}\u2019) before options.\ncallback\toptional\tFunction\tA function executed after a timeout of 300 ms, giving the browser time to make outbound requests first.\n\nBy default, Analytics.js caches traits in the browser\u2019s localStorage and attaches them to each Identify call.\n\nFor example, you might call Identify when someone signs up for a newsletter but hasn\u2019t yet created an account on your site. The example below shows an Identify call (using hard-coded traits) that you might send in this case.\n\nanalytics.identify({\n  nickname: 'Amazing Grace',\n  favoriteCompiler: 'A-0',\n  industry: 'Computer Science'\n});\n\n\nThen, when the user completes the sign up process, you might see the following:\n\nanalytics.identify('12091906-01011992', {\n  name: 'Grace Hopper',\n  email: 'grace@usnavy.gov'\n});\n\n\nThe traits object for the second call also includes nickname, favoriteCompiler, and industry.\n\nYou may omit both traits and options, and pass the callback as the second argument.\n\nanalytics.identify('12091906-01011992', function(){\n  // Do something after the identify request has been sent\n  // Note: site-critical functionality should not depend on your analytics provider\n});\n\nTrack\n\nThe Track method lets you record actions your users perform. You can see a track example in the Quickstart guide or find details on the track method payload.\n\nThe Track method follows the format below:\n\nanalytics.track(event, [properties], [options], [callback]);\n\n\nThe track call has the following fields:\n\nFIELD\tTYPE\tDESCRIPTION\nevent\tString\tThe name of the event you\u2019re tracking. You can read more about the track method and recommended event names.\nproperties\tObject\tOptional. A dictionary of properties for the event. If the event was 'Added to Cart', it might have properties like price and productType.\noptions\tObject\tOptional. A dictionary of options. For example, enable or disable specific destinations for the call. Note: If you do not pass a properties object, pass an empty object (like \u2018{}\u2019) before options.\ncallback\tFunction\tOptional. A function that runs after a timeout of 300 ms, giving the browser time to make outbound requests first.\n\nThe only required argument in Analytics.js is an event name string. You can read more about how Segment recommends you name events.\n\nExample Track call:\n\nanalytics.track('Article Completed', {\n  title: 'How to Create a Tracking Plan',\n  course: 'Intro to Analytics',\n});\n\n\nFor more information about choosing which events to track, event naming, and more, check out Analytics Academy.\n\nThe only required argument on Track calls in Analytics.js is an event name string. Read more about how Segment recommends naming your events.\n\nTrack link\n\ntrackLink is a helper method that attaches the track call as a handler to a link. With trackLink, Analytics.js inserts a timeout of 300 ms to give the track call more time. This is useful when a page would redirect before the track method could complete all requests.\n\nThe trackLink method follows the format below.\n\nanalytics.trackLink(element, event, [properties])\n\nFIELD\t\u00a0\tTYPE\tDESCRIPTION\nelement(s)\t\u00a0\tElement or Array\tDOM element to bind with track method. You may pass an array of elements or jQuery objects. Note: This must be an element, not a CSS selector.\nevent\t\u00a0\tString or Function\tThe name of the event, passed to the track method. Or a function that returns a string to use as the name of the track event.\nproperties\toptional\tObject or Function\tA dictionary of properties to pass with the track method or a function that returns an object to use as the properties of the event.\n\nExample:\n\nvar link = document.getElementById('free-trial-link');\n\nanalytics.trackLink(link, 'Clicked Free-Trial Link', {\n  plan: 'Enterprise'\n});\n\nTrack form\n\ntrackForm is a helper method that binds a track call to a form submission. The trackForm method inserts a timeout of 300 ms to give the track call more time to complete. This is useful to prevent a page from redirecting before the track method could complete all requests.\n\nThe trackForm method follows the format below.\n\nanalytics.trackForm(form, event, [properties])\n\nFIELD\t\u00a0\tTYPE\tDESCRIPTION\nform(s)\t\u00a0\tElement or Array\tThe form element to track or an array of form elements or jQuery objects. Note: trackForm takes an element, not a CSS selector. Segment recommends that you wait until the DOM loads before passing the form element.\nevent\t\u00a0\tString or Function\tThe name of the event, passed to the track method. Or a function that returns a string to use as the name of the track event.\nproperties\toptional\tObject or Function\tA dictionary of properties to pass with the track method. Or a function that returns an object to use as the properties of the event.\n\nExample:\n\nvar form = document.getElementById('signup-form');\n\nanalytics.trackForm(form, 'Signed Up', {\n  plan: 'Premium',\n  revenue: 99.00\n});\n\nPage\n\nThe Page method lets you record page views on your website, along with optional extra information about the page viewed by the user.\n\nBecause some Destinations require a page call to instantiate their libraries, you must call page at least once per page load. You can call it more than once if needed, for example, on virtual page changes in a single page app.\n\nSee the implementation guide for more information about calling the Page method.\n\nAnalytics.js includes a Page call by default as the final line in the Analytics.js snippet. You can update this page call within the guidelines below.\n\nThe page method follows the format below.\n\nanalytics.page([category], [name], [properties], [options], [callback]);\n\n\nThe page call has the following fields:\n\nFIELD\t\u00a0\tTYPE\tDESCRIPTION\ncategory\toptional\tString\tThe category of the page. Useful for cases like ecommerce where many pages might live under a single category. Note: if you pass only one string to page it is assumed to be name. You must include a name to send a category.\nname\toptional\tString\tThe name of the page.\nproperties\toptional\tObject\tA dictionary of properties of the page. Note: Analytics.js collects url, title, referrer and path are automatically. This defaults to a canonical url, if available, and falls back to document.location.href.\noptions\toptional\tObject\tA dictionary of options. For example, enable or disable specific destinations for the call. Note: If you do not pass a properties object, pass an empty object (like \u2018{}\u2019) before options.\ncallback\toptional\tFunction\tA function that runs after a timeout of 300 ms, giving the browser time to make outbound requests first. However, this function might not execute if one of the device-mode libraries has been blocked from loading.\nDefault page properties\n\nAnalytics.js adds properties to each page call.\n\nanalytics.page('Pricing');\n\n\nSegment adds the following information:\n\nanalytics.page('Pricing', {\n  title: 'Segment Pricing',\n  url: 'https://segment.com/pricing',\n  path: '/pricing',\n  referrer: 'https://segment.com/warehouses'\n});\n\n\nYou can override these values by explicitly setting them in your calls. For example:\n\nanalytics.page('Pricing', {\n  title: 'My Overridden Title',\n  path: '/pricing/view'\n});\n\n\nTranslates to:\n\nanalytics.page('Pricing', {\n  title: 'My Overridden Title',\n  url: 'https://segment.com/pricing',\n  path: '/pricing/view',\n  referrer: 'https://segment.com/warehouses'\n});\n\n\nSegment sets the path and url property to the value of the canonical element on your page. If a canonical element is not set, the values will be set from the browser.\n\nGroup\n\nThe Group method associates an identified user with a company, organization, project, workspace, team, tribe, platoon, assemblage, cluster, troop, gang, party, society or any other collective noun you come up with for the same concept.\n\nThis is useful for tools like Intercom, Preact, and Totango, as it ties the user to a group of other users.\n\nThe Group method follows the format below.\n\nanalytics.group(groupId, [traits], [options], [callback]);\n\n\nThe Group call has the following fields:\n\nFIELD\t\u00a0\tTYPE\tDESCRIPTION\ngroupId\t\u00a0\tString\tThe Group ID to associate with the current user.\ntraits\toptional\tObject\tA dictionary of traits for the group. Example traits for a group include address, website, and employees.\noptions\toptional\tObject\tA dictionary of options. For example, enable or disable specific destinations for the call. Note: If you do not pass a properties object, pass an empty object (like \u2018{}\u2019) before options.\ncallback\toptional\tFunction\tA function that runs after a timeout of 300 ms, giving the browser time to make outbound requests first.\n\nExample group call:\n\nanalytics.group('UNIVAC Working Group', {\n  principles: ['Eckert', 'Mauchly'],\n  site: 'Eckert\u2013Mauchly Computer Corporation',\n  statedGoals: 'Develop the first commercial computer',\n  industry: 'Technology'\n});\n\n\nBy default, Analytics.js caches group traits in the browser\u2019s local storage and attaches them to each group call, similar to how the identify method works.\n\nFind more details about group, including the group payload, in the Group Spec.\n\nAlias\n\nThe Alias method combines two unassociated user identities. Segment usually handles aliasing automatically when you call identify on a user, however some tools require an explicit alias call.\n\nThis is an advanced method, but it\u2019s required to manage user identities successfully in some Segment destinations like Kissmetrics and Mixpanel.\n\nThe Alias method follows the format below:\n\nanalytics.alias(userId, [previousId], [options], [callback]);\n\n\nThe Alias call has the following fields:\n\nFIELD\t\u00a0\tTYPE\tDESCRIPTION\nuserId\t\u00a0\tString\tThe new user ID you want to associate with the user.\npreviousId\toptional\tString\tThe previous ID that the user was recognized by. This defaults to the currently identified user\u2019s ID.\noptions\toptional\tObject\tA dictionary of options. For example, enable or disable specific destinations for the call.\ncallback\toptional\tFunction\tA function that is executed after a timeout of 300 ms, giving the browser time to make outbound requests first.\n\nFor more details about Alias, including the alias call payload, check out the Segment Spec.\n\nUtility methods\n\nThe Analytics.js utility methods help you change how Segment loads on your page. They include:\n\nLoad\nReady\nDebug\nOn (Emitter)\nTimeout\nReset (Logout)\nKeepalive\nLoad\n\nThe load method is also available when you load analytics.js through the NPM package.\n\nYou can load a buffered version of analytics.js that requires you to call load explicitly before analytics.js initiates any network activity. This is useful if you want to, for example, wait for user consent before you fetch tracking destinations or send buffered events to Segment.\n\nCall load one time only.\n\nexport const analytics = new AnalyticsBrowser()\n\nanalytics.identify(\"hello world\")\n\nif (userConsentsToBeingTracked) {\n    analytics.load({ writeKey: '<YOUR_WRITE_KEY>' }) // destinations loaded, enqueued events are flushed\n}\n\n\nYou can also use load if you fetch some settings asynchronously.\n\nconst analytics = new AnalyticsBrowser()\nfetchWriteKey().then(writeKey => analytics.load({ writeKey }))\n\nanalytics.identify(\"hello world\")\n\nReady\n\nThe ready method lets you pass in a method that gets called after Analytics.js finishes initializing and after all enabled device-mode destinations load. It\u2019s like jQuery\u2019s ready method, except for Destinations. Because it doesn\u2019t fire until all enabled device-mode destinations are loaded, it can\u2019t be used to change configuration options for downstream SDKs. That can only be done if the SDK is loaded natively.\n\nThe ready method isn\u2019t invoked if any Destination throws an error (for example, for an expired API key, incorrect settings configuration, or when a Destination is blocked by the browser) during initialization. If you want to check when Analytics.js has loaded, you can look at the value of window.analytics.initialized. When it\u2019s true, the library has successfully initialized, even if some destinations are blocked.\n\nNote: window.analytics.initialized is a simple boolean, not an event or a pub/sub system. This means you can\u2019t subscribe to changes in its value. If you need to detect when it changes from false to true, you must set up a polling mechanism to monitor the value.\n\nThe code in the ready function only executes after ready is emitted.\n\nIf you want to access end-tool library methods that do not match any Analytics.js methods, like adding an extra setting to Mixpanel, you can use a ready callback so that you\u2019re guaranteed to have access to the Mixpanel object, like so:\n\nanalytics.ready(function() {\n  window.mixpanel.set_config({ verbose: true });\n});\n\n\nThe ready method uses the following format:\n\nanalytics.ready(callback);\n\n\nThe ready method has the following fields:\n\nFIELD\tTYPE\tDESCRIPTION\ncallback\tFunction\tA function to be executed after all enabled destinations have loaded.\nDebug\n\nCalling the debug method turns on debug mode, which logs helpful messages to the console. Subsequent Segment events generate messages in the developer console after you invoke debug.\n\nEnable:\n\nanalytics.debug(true);\n\n\nDisable:\n\nanalytics.debug(false);\n\nEmitter\n\nThe global analytics object emits events whenever you call alias, group, identify, track, or page.\n\nUse the on method to set listeners for these events and run your own custom code. This can be useful if you want to send data to a service for which Segment doesn\u2019t have a destination.\n\nanalytics.on(method, callback);\n\nFIELD\tTYPE\tDESCRIPTION\nmethod\tString\tName of the method to listen for.\ncallback\tFunction\tA function to execute after each emitted method, taking three arguments: event, properties, options.\n\nExample:\n\nanalytics.on('track', function(event, properties, options) {\n\n  bigdataTool.push(['recordEvent', event]);\n\n});\n\n\nThis method emits events before they are processed by the Segment integration, and may not include some of the normalization Segment performs on the client before sending the data to the Segment servers.\n\nNote\n\nPage event properties are stored in the options object.\n\nExtending timeout\n\nThe timeout method sets the length (in milliseconds) of callbacks and helper functions. This is useful if you have multiple scripts that need to fire in your callback or trackLink, trackForm helper function.\n\nThe example below sets the timeout to 500 ms.\n\nanalytics.timeout(500);\n\n\nIf you\u2019re triggering ad network conversion pixels, Segment recommends extending timeout to 500 ms to account for slow load times.\n\nReset or log out\n\nCalling reset resets the id, including anonymousId, and clears traits for the currently identified user and group.\n\nanalytics.reset();\n\n\nThe reset method only clears the cookies and localStorage created by Segment. It doesn\u2019t clear data from other integrated tools, as those native libraries might set their own cookies to manage user tracking, sessions, and manage state. To completely clear out the user session, see the documentation provided by those tools.\n\nSegment doesn\u2019t share localStorage across subdomains. If you use Segment tracking on multiple subdomains, you must call analytics.reset() for each subdomain to completely clear out the user session.\n\nKeepalive\n\nYou can utilize this in instances where an API call fires on a hard redirect, and are missed from getting captured in Segment. If you set this flag to true, it enables firing the event before the redirect. This is available for all events. You can read more about this in the Github PR.\n\nManaging data flow with the Integrations object\n\nTip: You can change how your data flows in several different ways without having to change your code. See Filtering Data to learn more.\n\nYou can pass an integrations object in the options of Alias, Group, Identify, Page, and Track methods to send data to only the selected destinations. By default, all Destinations are enabled.\n\nThe example below sends a message only to Intercom and Google Analytics.\n\nanalytics.identify('user_123', {\n  email: 'jane.kim@example.com',\n  name: 'Jane Kim'\n}, {\n  integrations: {\n    'All': false,\n    'Intercom': true,\n    'Google Analytics': true\n  }\n});\n\n\n'All': false tells Segment not to send data to any Destinations by default, unless they\u2019re explicitly listed as true in the next lines.\n\nAs an opposite example, the snippet below sends a message to all integrations except Intercom and Google Analytics.\n\nanalytics.identify('user_123', {\n  email: 'jane.kim@example.com',\n  name: 'Jane Kim'\n}, {\n  integrations: {\n    'Intercom': false,\n    'Google Analytics': false\n  }\n});\n\n\nYou don\u2019t need to include 'All': true in this call because it\u2019s implied as the default behavior. Instead, only list the destinations that you want to exclude, with a false flag for each.\n\nDestination flags are case sensitive and match the destination\u2019s name in the docs (for example, \u201cAdLearn Open Platform\u201d, \u201cawe.sm\u201d, \u201cMailchimp\u201d, etc). If a Destination has more than one acceptable name, this appears in the documentation for that destination.\n\nBusiness tier customers can filter Track calls from the Source Schema page in the Segment UI. Segment recommends that you use the UI to simplify filter management and make updates without changing your site\u2019s code.\n\nLoad options\n\nNote: To use this feature, you must be on snippet version 4.1.0 or later. You can get the latest version of the snippet from the Analytics.js Quickstart.\n\nYou can modify the .load method in Analytics.js (the second line of the snippet) to take a second argument. If you pass an object with an integrations dictionary, then Segment only loads the integrations in that dictionary that are marked as enabled with the boolean value true.\n\nYou can only call .load on page load, or reload (refresh). If you modify the .load method between page loads, it doesn\u2019t have any effect until the page is reloaded.\n\nFor example:\n\nanalytics.load('writekey', { integrations: { All: false, 'Google Analytics': true, 'Segment.io': true } })\n\n\nThis way, you can conditionally load integrations based on what customers opt into on your site. The example below shows how you might load only the tools that the user agreed to use.\n\nonConsentDialogClosed(function(consentedTools){\n  analytics.load('writekey', { integrations: consentedTools })\n})\n\nBundle obfuscation\n\nYou can also add an obfuscate property to the object in the second parameter, which obscures the URL from which your integrations and destination actions are loaded. This helps prevent words that are flagged by ad blockers to not be detected in your URL, enabling the integration to properly load.\n\nFor example:\n\nanalytics.load('writekey', { obfuscate: true })\n\n\nThe obfuscate value is false by default.\n\nISO string conversion\n\nBy default, the Analytics.js library will convert ISO8061 strings to a Date object before passing it to downstream device-mode integrations. If you would like to disable this functionality and send those strings as they are passed to the event, you can use the load method to pass in the disableAutoISOConversion option.\n\nFor example:\n\nanalytics.load('writekey', { disableAutoISOConversion: true })\n\nClient hints\n\nSome userAgent strings are frozen and contain less information. If you would like to request more information when it\u2019s available, you can pass an array of strings with fields you would like to request to the highEntropyValuesClientHints option. The example array below contains all possible values.\n\nFor example:\n\nanalytics.load('writekey', { highEntropyValuesClientHints: ['architecture', 'bitness', 'model', 'platformVersion', 'uaFullVersion', 'fullVersionList', 'wow64'] })\n\nDisabling\n\nFor testing or staging environments, it can be useful to disable your SDK to ensure no events send.\n\nIf disable: true is passed, all analytics method calls will be a no-op, and no network calls will be initiated.\n\nanalytics.load('writekey', { disable: true })\n\n\nFor wrapper/plugin authors: if you have a use case where you need special access to the CDN Settings (for example, consent management), you can also pass a function. This API waits for cdnSettings to be fetched. Keep in mind that cdnSettings is an unstable object.\n\nanalytics.load('writekey', { disable: (cdnSettings) => true })\n\nRetries\n\nWhen enabled, Analytics.js automatically retries network and server errors. With persistent retries, Analytics.js can:\n\nSupport offline tracking. Analytics.js queues your events and delivers them when the user comes back online.\nBetter handle network issues. When your application can\u2019t connect to the Segment API, Segment continues to store the events on the browser to prevent data loss.\n\nAnalytics.js stores events in localStorage and falls back to in-memory storage when localStorage is unavailable. It retries up to 10 times with an incrementally increasing back-off time between each retry. Analytics.js queues up to 100 events at a time to avoid using too much of the device\u2019s local storage. See the destination Retries documentation to learn more.\n\nBatching\n\nBatching is the ability to group multiple requests or calls into one request or API call. All requests sent within the same batch have the same receivedAt time. With Analytics.js, you can send events to Segment in batches. Sending events in batches enables you to have:\n\nDelivery of multiple events with fewer API calls\nFewer errors if a connection is lost because an entire batch will retry at once rather than multiple calls retrying at random times.\nSetup\n\nYou can start batching by changing the strategy to \"batching\" and the parameters for size and timeout within the load method in the analytics object. Batching requires both parameters.\n\nanalytics.load(\"<write_key>\", {\n    integrations: {\n      \"Segment.io\": {\n        deliveryStrategy: {\n          strategy: \"batching\",\n          config: {\n            size: 10,\n            timeout: 5000\n          }\n        }\n      }\n    }\n  });\n\n\nYou can check to see if batching works by checking your source\u2019s debugger in Sources > Debugger. When you select an event and view the Raw code, the receivedAt time of all the events in the batch should be the same.\n\nBatch size\n\nThe batch size is the threshold that forces all batched events to be sent once it\u2019s reached. For example, size: 10 means that after triggering 10 events, Analytics.js sends those 10 events together as a batch to Segment.\n\nYour total batched events can\u2019t exceed the maximum payload size of 500 KB, with a limit of 32 KB for each event in the batch. If the 500 KB limit is reached, the batch will be split.\n\nTimeout\n\ntimeout is the number of milliseconds that forces all events queued for batching to be sent, regardless of the batch size, once it\u2019s reached. For example, timeout: 5000 sends every event in the batch to Segment once 5 seconds passes.\n\nBatching FAQs\nWill Analytics.js deliver events that are in the queue when a user closes the browser?\n\nAnalytics.js does its best to deliver the queued events before the browser closes, but the delivery isn\u2019t guaranteed.\n\nUpon receiving the beforeunload browser event, Analytics.js attempts to flush the queue using fetch requests with keepalive set to true. Since the max size of keepalive payloads is limited to 64 KB, if the queue size is bigger than 64 KB at the time the browser closes, then there is a chance of losing a subset of the queued events. Reducing the batch size or timeout will alleviate this issue, but that will be a trade-off decision.\n\nCan other destinations receive batched events?\n\nNo, this batching only impacts events sent to Segment. Once the batch reaches Segment, it\u2019s split up and follows the normal path of an event.\n\nWill batching impact billing or throughput?\n\nNo, batching won\u2019t impact billing or throughput.\n\nCan I use batching with partner integrations?\n\nPartner integrations don\u2019t support batching as all other partner integrations run one event at a time. Only Segment.io events support batched delivery.\n\nDoes batching work on all browsers?\n\nBatching won\u2019t work on Internet Explorer.\n\nIf a source has retry enabled, does the retry behavior change when using batching?\n\nBatching delays retries, as events that are queued for batching aren\u2019t retried until a batch delivery fails.\n\nWhen using Middlewares as a source and destination, is there a change in behavior when using batching?\n\nNo, there is no change in behavior to Middlewares.\n\nWhen using Segment features (Schema filtering, integrations object, Protocols) to filter events from going to destinations (device and cloud-mode), will batching impact the filtering of events?\n\nNo, there is no impact to how events filter.\n\nPlugins and source middleware\n\nWhen you develop against Analytics 2.0, the plugins you write can augment functionality, enrich data, and control the flow and delivery of events. From modifying event payloads to changing analytics functionality, plugins and middleware help to speed up the process of getting things done.\n\nPlugins and source middleware accomplish the same thing, but plugins are significantly more powerful (but more verbose to implement).\n\nFor basic use cases like adding event fields or dropping specific events, use source middleware. If you need more granular control of the lifecycle, or want to be able to abort the Segment initialization, you should use plugins.\n\nSource Middleware\n\nSource middleware runs before any other plugins. You can use this to enrich or drop an event.\n\nExample usage of addSourceMiddleware\n\nHere are some examples of using addSourceMiddleware for enrichment and validation.\n\nEnrichment\n  analytics.addSourceMiddleware(({ payload, next }) => {\n     const { event } = payload.obj.context\n     if (event.type === 'track') {\n        event.event.toLowerCase()\n     }\n     next(payload)\n  });\n\nValidation\n  analytics.addSourceMiddleware(({ payload, next }) => {\n    const { event } = payload.obj.context\n    if (!isValid(event)) {\n      return null // event is dropped\n    }\n    next(payload)\n  });\n\nAdvanced Plugin API\n\nFor advanced modification to the event pipeline.\n\nTYPE\tDETAILS\nbefore\tExecutes before event processing begins. These are plugins that run before any other plugins run. Thrown errors here can block the event pipeline. Source middleware added via addSourceMiddleware is treated as a before plugin.\nenrichment\tExecutes as the first level of event processing. These plugins modify an event. Thrown errors here can block the event pipeline.\ndestination\tExecutes as events begin to pass off to destinations. Segment.io is implemented as a destination plugin. Thrown errors here will not block the event pipeline.\nafter\tExecutes after all event processing completes. You can use this to perform cleanup operations.\nutility\tExecutes only once during the analytics.js bootstrap. Gives you access to the analytics instance via the plugin\u2019s load() method. This doesn\u2019t allow you to modify events.\nExample plugins\n\nHere\u2019s an example of a plugin that converts all track event names to lowercase before the event goes through the rest of the pipeline:\n\nexport const lowercase: Plugin = {\n  name: 'Lowercase events',\n  type: 'enrichment',\n  version: '1.0.0',\n\n  isLoaded: () => true,\n  load: () => Promise.resolve(),\n\n  track: (ctx) => {\n    ctx.updateEvent('event', ctx.event.event.toLowerCase())\n    return ctx\n  }\n}\n\nconst identityStitching = () => {\n  let user\n\n  const identity = {\n    // Identifies your plugin in the Plugins stack.\n    // Access `window.analytics.queue.plugins` to see the full list of plugins\n    name: 'Identity Stitching',\n    // Defines where in the event timeline a plugin should run\n    type: 'enrichment',\n    version: '0.1.0',\n\n    // use the `load` hook to bootstrap your plugin\n    // The load hook will receive a context object as its first argument\n    // followed by a reference to the analytics.js instance from the page\n    load: async (_ctx, ajs) => {\n      user = ajs.user()\n    },\n\n    // Used to signal that a plugin has been property loaded\n    isLoaded: () => user !== undefined,\n\n    // Applies the plugin code to every `identify` call in Analytics.js\n    // You can override any of the existing types in the Segment Spec.\n    async identify(ctx) {\n      // Request some extra info to enrich your `identify` events from\n      // an external API.\n      const req = await fetch(\n        `https://jsonplaceholder.typicode.com/users/${ctx.event.userId}`\n      )\n      const userReq = await req.json()\n\n      // ctx.updateEvent can be used to update deeply nested properties\n      // in your events. It's a safe way to change events as it'll\n      //  create any missing objects and properties you may require.\n      ctx.updateEvent('traits.custom', userReq)\n      user.traits(userReq)\n\n      // Every plugin must return a `ctx` object, so that the event\n      // timeline can continue processing.\n      return ctx\n    },\n  }\n\n  return identity\n}\n\n// Registers Segment's new plugin into Analytics.js\nawait window.analytics.register(identityStitching())\n\n\nHere\u2019s an example of a utility plugin that allows you to change the format of the anonymous_id cookie:\n\n\nwindow.analytics.ready(() => {\n      window.analytics.register({\n        name: 'Cookie Compatibility',\n        version: '0.1.0',\n        type: 'utility',\n        load: (_ctx, ajs) => {\n          const user = ajs.user()\n          const cookieJar = user.cookies\n          const cookieSetter = cookieJar.set.bind(cookieJar)\n\n          // blindly convert any values into JSON strings\n          cookieJar.set = (key, value, opts) => cookieSetter(key, JSON.stringify(value), opts)\n\n          // stringify any existing IDs\n          user.anonymousId(user.anonymousId())\n          user.id(user.id())\n        },\n        isLoaded: () => true\n      })\n    })\n\n\nYou can view Segment\u2019s existing plugins to see more examples.\n\nRegister a plugin\n\nRegistering plugins enable you to modify your analytics implementation to best fit your needs. You can register a plugin using this:\n\n// A promise will resolve once the plugins have been successfully loaded into Analytics.js\n// You can register multiple plugins at once by using the variable args interface in Analytics.js\nawait window.analytics.register(pluginA, pluginB, pluginN)\n\nVideo player plugins\n\nSegment offers video player \u2018plugins\u2019 so you can quickly collect video events using Analytics.js. See the specific documentation below to learn more:\n\nYouTube\nVimeo\nCross-subdomain analytics\n\nAnalytics.js tracks across subdomains out of the box. All Segment destinations fully support this feature.\n\nTo track activity on your subdomains, include the Segment Analytics.js snippet on each subdomain. Segment sets users\u2019 anonymousId on the top-level domain, so that users are tracked across any subdomain.\n\nBecause Segment tracks across subdomains, you can either use the same Segment source, or use separate sources for each subdomain. What you decide depends on your team\u2019s goals for tracking each subdomain.\n\nSegment doesn\u2019t offer tracking across top-level domains out of the box. If you want to track across top-level domains, you can utilize Segment\u2019s Querystring API to pass the anonymousId from Website A to Website B in the query string. When a user moves from Website A to Website B with the anonymousId in the query string, Analytics.js reads that value and sets the anonymousId to it, rather than generating a new one.\n\nUTM Tracking\n\nUTM parameters are only used when linking to your site from outside your domain. When a visitor arrives using a link containing UTM parameters, Segment\u2019s analytics.js library will parse the URL query string and add the information to the event payload. For more information about UTM tracking, see the Tracking Customers Across Channels and Devices documentation.\n\nUTM parameters contain three essential components (utm_source, utm_medium, utm_campaign) and two optional (utm_content, utm_term). For example, if you include the following three parameters in your URL: ?utm_source=mysource&utm_medium=email&utm_campaign=mytestcampaign, once a visitor arrives using a link containing the above, Segment automatically grabs the UTM parameters and subsequent events will contain these parameters within the \u2018context\u2019 object (visible in the raw view of your Source Debugger.)\n\nSo, for example, if somebody follows the link with above query string to your site, the subsequent \u2018page\u2019 call in your Debugger should contain the below and will be passed to any enabled destinations:\n\n\"context\": {\n \"campaign\": {\n \"medium\": \"email\",\n \"name\": \"mytestcampaign\",\n \"source\": \"mysource\",\n },\n\n\nWhenever the UTM parameters are no longer a part of the URL, Segment no longer includes them. For example, if the user goes to a new page within your website which does not contain these parameters, they will not be included in subsequent events. UTM parameters are non-persistent by default as they could potentially cause data accuracy problems. Here\u2019s an example of why: Say a user clicks on an ad and lands on your site. He navigates around and bookmarks an internal page - or maybe shares a link with a friend, who shares it with another friend. All those links would then point back to the same test utm_source as the initial referrer for any purchase.\n\nSegment doesn\u2019t validate UTM parameter names. This design supports the flexibility to track both standard parameters (for example, utm_source, utm_medium) and custom parameters defined by users. As a result, all parameters present in the URL collected as is, and are added to the context field without checks for naming conventions or validity.\n\nIf you want to ensure that only standard UTM parameters (such as, utm_source, utm_medium, utm_campaign, utm_content, utm_term) are included in the context.campaign object, you can implement Source middleware in your Analytics.js setup.\n\nFor example:\n\nwindow.analytics.addSourceMiddleware(({ payload, next }) => {\n  if (payload.obj.context?.campaign) {\n    const allowedFields = [\"source\", \"medium\", \"term\", \"campaign\", \"content\"];\n    const campaign = payload.obj.context.campaign;\n    Object.keys(campaign).forEach(key => {\n      if (!allowedFields.includes(key)) {\n        delete campaign[key];\n      }\n    });\n  }\n  next(payload);\n});\n\n\nThis middleware filters out any non-standard parameters from the context.campaign object before they\u2019re sent to Segment or forwarded to your enabled destinations.\n\nAnalytics.js performance\n\nThe Analytics.js library and all Destination libraries are loaded with the HTML script async tag. This also means that Segment fires methods asynchronously, so you should adjust your code accordingly if you require that events be sent from the browser in a specific order.\n\nWhile many tools require access to the DOM or cookies, for the Zendesk, Salesforce, and Mailchimp destinations, Segment doesn\u2019t need to load a native JavaScript library. Instead, Segment\u2019s servers send data to the end-tools.\n\nSegment loads the libraries required for your enabled Destinations. When you disable a destination, the custom version of Analytics.js loaded on your site stops requesting that library.\n\nUsing Analytics.js doesn\u2019t offer a large performance benefit, but is more performant than installing each of the destinations individually. And as more destinations move to accept data directly from Segment, you\u2019ll receive more performance benefits automatically.\n\nOne option, if you don\u2019t want to use any bundled third-party tools, is to use the Analytics-Node package.\n\nAnalytics.js doesn\u2019t set third-party cookies and only sets first-party cookies.\n\nBundle size\n\nSegment\u2019s Analytics.js JavaScript snippet increases the page size by about 1.1KB.\n\nThe snippet asynchronously requests and loads a customized JavaScript bundle (analytics.min.js), which contains the code and settings needed to load your device-mode destinations. The size of this file changes depending on the number of and which destinations you enable.\n\nWithout any destinations enabled, the analytics.min.js file is about 62KB. Each time you enable a destination, the file\u2019s size may increase slightly.\n\nCookies set by Analytics.js\n\nSegment sets three cookies in general:\n\nCOOKIE\tDESCRIPTION\najs_anonymous_id\tAn anonymous ID generated by Analytics.js, used for Segment calls.\najs_group_id\tA group ID that can be specified by making a group() call with Analytics.js.\najs_user_id\tA user ID that can be specified by making an identify() call with Analytics.js.\n\nFor Google Chrome, these cookies expire by default one year after the date created. Other supported browsers might have a different expiration time.\n\nSome user/group traits are also stored in localStorage:\n\nCOOKIE\tDESCRIPTION\najs_user_traits\tThe traits that are passed in an identify() call.\najs_group_properties\tThe properties that are passed in a group() call.\n\nNote that localStorage variables don\u2019t expire because the browser defines that functionality.\n\nLocal storage cookies used by Analytics.js\n\nAnalytics.js uses localstorage cookies if you have retries enabled, to keep track of retry timing.\n\nThe ack cookie is a timer used to see if another tab should claim the retry queue.\nThe reclaimStart and reclaimEnd cookies determine if a tab takes over the queue from another tab.\nThe inProgress and queue cookies track events in progress, and events queued for retry.\n\nFor more information, visit the Segment localstorage-retry library.\n\nYou can set the debug cookie to analytics.js to log debug messages from Analytics.js to the console.\n\nTracking Blockers and Browser Privacy Settings\n\nSegment does not endorse bypassing tracking blockers or browser privacy settings for client-side tracking. Your users have control over what gets loaded on their pages and can use plugins or browser settings to block third-party scripts, including Segment. To minimize client-side data loss, Segment recommends you choose from the following routes:\n\nRespect the user\u2019s decision to implement tracking blockers or use privacy settings, knowing that, unfortunately, some data will be lost.\nAsk the customer to disable the tracking blockers or adjust their privacy settings (for example, in the case of large, corporate customers).\nMove as many events and tracking actions as possible to a server-side library, which won\u2019t encounter the same limitations.\n\nTo minimize client-side data loss, Segment provides a few workarounds. However, it\u2019s important to note that Segment cannot guarantee their effectiveness.\n\nUse the bundle obfuscation feature. You can add an obfuscate property to the object in the second parameter, which obscures the URL from which your integrations and destination actions are loaded. This helps prevent words that are flagged by ad blockers to not be detected in your URL, enabling the integration to properly load.\n\nCreate a custom proxy. This changes the URL that Segment loads from (cdn.segment.com) and the outgoing requests generated when events are triggered (api.segment.io).\n\nConsider implementing the Segment Edge SDK. The Segment Edge SDK leverages Cloudflare Workers to facilitate first-party data collection and real-time user profiling for app personalization. It integrates Segment\u2019s library into web apps, manages user identity via HttpOnly cookies, and employs an internal router for efficient data processing and user experience customization. This innovative approach simplifies tracking and personalization for Segment customers. More information is available in the Edge SDK README.\n\nConsider using one of Segment\u2019s server-side libraries. Using a server-side library eliminates concerns about tracking blockers and privacy browsers that can prevent Segment from loading. This option may require additional code to track actions like a Page call, as you now need to manually pass contextual information that would have been automatically collected by Analytics.js, like url, path, and referrer. Note that some destinations are device-mode only.\n\nInstalling the library under a custom global namespace\n\nWhen you load Analytics.js through snippet code, by default, the SDK installs on window.analytics global variable. If this causes a conflict with another library on your page, you can change the global variable used by Analytics.js if you use snippet version 5.2.1 or later.\n\nChange the global variable in the beginning of your snippet code as shown below. In this case, Analytics.js uses window.custom_key to load instead of window.analytics.\n\n  - !function(){var i=\"analytics\", ...\n  + !function(){var i=\"custom_key\", ...\n\nAdd destinations from npm\n\nBundle the destinations you want loaded from npm instead of having them loaded from a remote CDN. This enables you to have fewer network requests when adding destinations.\n\nTo add actions-based destinations from npm:\n\nimport vwo from '@segment/analytics-browser-actions-vwo'\nimport braze from '@segment/analytics-browser-actions-braze'\n\nconst analytics = AnalyticsBrowser.load({\n  writeKey: '<WRITE_KEY>',\n  plugins: [vwo, braze],\n})\n\n\nPass in the destination plugin to the added config option called plugins. A list of all action destination packages can be found on GitHub in the @segmentio/action-destinations repository.\n\nTo add classic destinations from npm:\n\nimport { AnalyticsBrowser } from '@segment/analytics-next'\nimport GoogleAnalyticsIntegration from '@segment/analytics.js-integration-google-analytics'\n\n// The following example assumes configuration for Google Analytics will be available in the fetched settings\nconst analytics = AnalyticsBrowser.load({\n  writeKey: '<WRITE_KEY>',\n  classicIntegrations: [ GoogleAnalyticsIntegration ]\n}),\n\nSegment Inspector\n\nThe Segment Inspector is a Chrome web extension that enables you to debug your Segment integration on web applications instrumented with Analytics.js. Analytics.js sends data to the extension so that you can see how events change before they\u2019re sent to your destinations and so that you can verify that the event details are correct. The Segment Inspector also lets you analyze and confirm that API calls made from your website arrive to your Analytics.js source.\n\nFor the Segment inspector to work, you must enable the Analytics.js source.\n\nTo add the Segment Inspector as a Chrome extension:\n\nGo to the Segment Inspector in the Chrome web store.\nClick Add to Chrome.\nClick Add Extension in the pop-up window.\n\nOnce installed, use the Inspect Elements developer tool in Chrome to use the Segment Inspector. To access the Inspector, go to the top menu bar of Chrome and navigate to View > Developer > Developer Tools and go to the Segment tab. On the Segment tab, you can:\n\nFilter the different calls by type\nSearch based off of the content in the calls\nIdentify users\nComponents of the Segment Inspector\n\nThe Segment Inspector is composed of these three components:\n\nThe Diagnostics tab\nThis tab shows the library versions and the list of active integrations that are running.\nWhen you select an integration, you can see the options that passed while the integration loads. If you made any local overrides within the integration or on the page itself, they appear highlighted in the code.\nThe Events tab\nThis tab enables you to select an event and see the specific details of the event. You can view the time the event occurred, the status of the event (whether it sent or failed), what plugins were added, and how the context object changed. Any changes made to the payload appear highlighted.\nSelect the double-checked icon to see the payload at the delivery stage.\nSelect the fx icon to see the payloads after plugins ran.\nSelect the single-checked icon to see the payload as it was when the event triggered.\nThe Identity tab\nThis tab enables you to see the information of a user if you\u2019re using the identify feature. You can associate the data to an individual and measure their activity across multiple sessions and devices. This tab only shows the user\u2019s traits that are on the client.\nIf you\u2019re not using the identify feature, the user remains anonymous.\nExample uses\n\nHere are some examples of using Analytics.js. Note that the examples assume Analytics.js is installed through npm.\n\nNext.js\nwith-segment-analytics\nwith-segment-analytics-pages-router\nVanilla React, Vue\nSee Usage in Common Frameworks & SPAs\nExternal dependencies\n\nAnalytics.js production dependencies are listed under the dependencies key.\n\nThis page was last modified: 17 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nGetting started\nBenefits of Analytics.js\nBasic tracking methods\nUtility methods\nManaging data flow with the Integrations object\nRetries\nBatching\nPlugins and source middleware\nVideo player plugins\nCross-subdomain analytics\nUTM Tracking\nAnalytics.js performance\nTracking Blockers and Browser Privacy Settings\nInstalling the library under a custom global namespace\nAdd destinations from npm\nSegment Inspector\nExample uses\nExternal dependencies\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nData Graph\n/\nSetup Guides\n/\nRedshift Data Graph Setup\nRedshift Data Graph Setup\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\n\nRedshift for Data Graph is in beta and Segment is actively working on this feature. Some functionality may change before it becomes generally available. This feature is governed by Twilio Segment\u2019s First Access and Beta Preview Terms.\n\nSet up your Redshift data warehouse to Segment for the Data Graph.\n\nPrerequisite\n\nTo use Linked Audiences with Redshift, the Data Graph only supports materialized views.\n\nIf you\u2019re setting up Profiles Sync for the first time in the Unify space, go through the setup flow for Selective sync. If Profiles Sync is already set up for your Unify space, follow these steps to configure Profiles Sync for your Unify space:\n\nNavigate to Unify > Profile Sync.\nSelect the Settings tab and select Selective sync.\nSelect all the tables under Profile raw tables. These include, external_id_mapping_updates, id_graph_updates, profile_traits_updates. Linked Audiences require Profile Sync to be configured such that both the Profile raw tables and the Profile materialized tables are synchronized with your Redshift instance.\nSelect all of the tables under Profile materialized tables. These include profile_merges, user_traits, user_identifiers. This allows faster and more cost-efficient Linked Audiences computations in your data warehouse.\nSelect Sync all Track Call Tables under Track event tables to enable filtering on event history for Linked Audiences conditions.\nGetting started\n\nYou need to be an AWS Redshift account admin to set up the Segment Redshift connector as well as write permissions for the __segment_reverse_etl dataset.\n\nTo get started with Redshift:\n\nLog in to Redshift and select the Redshift cluster you want to connect.\nFollow the networking instructions to configure network and security settings.\nStep 1: Roles and permissions\n\nSegment recommends you to create a new Redshift user and role with only the required permissions.\n\nCreate a new role and user for the Segment Data Graph. This new role will only have access to the datasets you provide access to for the Data Graph. Run the SQL commands in your Redshift cluster:\n\n  -- Create a user with role for the Data Graph\n  CREATE ROLE SEGMENT_LINKED_ROLE;\n  CREATE USER SEGMENT_LINKED_USER PASSWORD \"your_password\";\n  GRANT ROLE SEGMENT_LINKED_ROLE TO SEGMENT_LINKED_USER;\n\nStep 2: Create a database for Segment to store checkpoint tables\n\nSegment recommends you to create a new database for the Data Graph. If you choose to use an existing database that has also been used for Segment Reverse ETL, you must follow the additional instructions to update user access for the Segment Reverse ETL schema.\n\nProvide write access to the database as Segment requires this in order to create a schema for internal bookkeeping and to store checkpoint tables for the queries that are executed. Segment recommends you to create a new database for this purpose. This is also the database you\u2019ll be required to specify for the Database Name when connecting Redshift with the Segment app.\n\nRun the following SQL commands in your Redshift cluster:\n\n-- Create and Grant access to a Segment internal DB used for bookkeeping \n\nCREATE DATABASE SEGMENT_LINKED_PROFILES_DB;\nGRANT CREATE ON DATABASE SEGMENT_LINKED_PROFILES_DB TO ROLE SEGMENT_LINKED_ROLE;\n\nStep 3: Grant read-only access for the Data Graph\n\nGrant the Segment role read-only access to additional schemas you want to use for the Data Graph including the Profiles Sync database.\n\nTo locate the Profile Sync database, navigate to Unify > Profiles Sync > Settings > Connection Settings. You will see the database and schema name.\n\nSchemas\n\nGrant schema permissions based on customer need. See Amazon\u2019s docs to view schema permissions and example commands that you can use to grant permissions. Repeat the following SQL query for each schema you want to use for the Data Graph.\n\n-- ********** REPEAT THE SQL QUERY BELOW FOR EACH SCHEMA YOU WANT TO USE FOR THE DATA GRAPH **********\n\nGRANT USAGE ON SCHEMA \"the_schema_name\" TO ROLE SEGMENT_LINKED_ROLE;\n\nTable\n\nGrant table permissions based on your needs. Learn more about Amazon\u2019s table permissions.\n\nTable permissions can either be handled in bulk:\n\n-- query data from all tables in a schema\nGRANT SELECT ON ALL TABLES IN SCHEMA \"the_schema_name\" TO ROLE SEGMENT_LINKED_ROLE;\n\n\nOr in a more granular fashion if needed:\n\n-- query data from a specific table in a schema\nGRANT SELECT ON TABLE <schema-name>.<table-name> TO ROLE segment_linked_role;\n\nStep 4: Validate permissions\n\nTo verify you have set up the right permissions for a specific table, log in with the username and password you created for SEGMENT_LINKED_USER and run the following command to verify the role you created has the correct permissions. If this command succeeds, you should be able to view the respective table.\n\nSHOW SCHEMAS FROM DATABASE \"THE_READ_ONLY_DB\";\nSELECT * FROM \"THE_READ_ONLY_DB.A_SCHEMA.SOME_TABLE\" LIMIT 10;\n\nStep 5: Connect your warehouse to Segment\n\nTo connect your warehouse to Segment:\n\nNavigate to Unify > Data Graph. This should be a Unify space with Profiles Sync already set up.\nClick Connect warehouse.\nSelect Redshift as your warehouse type.\nEnter your warehouse credentials. Segment requires the following settings to connect to your Redshift warehouse:\nHost Name: The Redshift URL\nPort: The Redshift connection port\nDatabase: The only database that Segment requires write access to in order to create tables for internal bookkeeping. This database is referred to as segment_linked_profiles_db in the SQL above.\nUsername: The Redshift user that Segment uses to run SQL in your warehouse. This user is referred to as segment_linked_user in the SQL above.\nPassword: The password of the user above\nTest your connection, then click Save.\nUpdate user access for Segment Reverse ETL dataset\n\nIf Segment Reverse ETL ran in the project you are configuring as the Segment connection project, a Segment-managed dataset is already created, and you need to provide the new Segment user access to the existing dataset. Run the following SQL if you run into an error on the Segment app indicating that the user doesn\u2019t have sufficient privileges on an existing __segment_reverse_etl:\n\n-- If you want to use an existing database that already has Segment Reverse ETL schemas, you\u2019ll need to run some additional steps below to grant the role access to the existing schemas.\n\nGRANT USAGE, CREATE, DROP ON SCHEMA segment_connection_db.__segment_reverse_etl TO ROLE SEGMENT_LINKED_ROLE;\nGRANT SELECT,INSERT,UPDATE,DELETE,DROP ON ALL TABLES IN SCHEMA segment_connection_db.__segment_reverse_etl TO ROLE SEGMENT_LINKED_ROLE;\n\n\nThis page was last modified: 10 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nPrerequisite\nGetting started\nStep 1: Roles and permissions\nStep 2: Create a database for Segment to store checkpoint tables\nStep 3: Grant read-only access for the Data Graph\nStep 4: Validate permissions\nStep 5: Connect your warehouse to Segment\nUpdate user access for Segment Reverse ETL dataset\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nStorage\n/\nWarehouses\n/\nRedshift cluster and Redshift connector limitations\nRedshift cluster and Redshift connector limitations\n\n\u201cAre there limitations of Redshift clusters and our Redshift connector?\u201d\n\nWhile Redshift clusters are incredibly scalable and efficient, limitations are imposed to ensure that clusters maintain performance.\n\nReserved words\n\nRedshift does not allow you to create tables or columns using reserved words. To avoid naming convention issues, we prepend a\u00a0_\u00a0to any reserved word names. If you\u2019re having trouble finding a column or table, you can check the list of\u00a0Redshift reserved words\u00a0or search for the table with a prepended underscore like\u00a0_open.\n\nTable count limitations\n\nRedshift sets the maximum number of tables you can create in a cluster to 9,900 including temporary tables. While it\u2019s rare to reach that limit, we recommend keeping an eye on the number of tables our warehouse connector is creating in your cluster. Keep in mind that a new table is created for each unique event you send to Segment, which becomes an issue if events are being dynamically generated.\n\nCluster node limitations\n\nWhen setting up your Redshift cluster, you can select between dense storage (ds2) and dense compute (dc1) cluster types. Dense compute nodes are SSD based which allocates only 200GB per node, but results in faster queries. Dense storage nodes are hard disk based which allocates 2TB of space per node, but result in slower queries. When scaling up your cluster by adding nodes, it\u2019s important to remember that adding more nodes will not add space linearly. As you add more dc1 nodes, the amount of preallocated space for each table increases. For example, if you have a table with 10 columns, Redshift will preallocate 20mb of space (10 columns X 2 slices) per node. That means that the same table will preallocate 20mb of space in a single ds2 cluster, and 200mb in a 10 node dc1 cluster.\n\nColumn type changes\n\nLike with most data warehouses, column data types (string, integer, float, etc.) must be defined at the time the column is created. Unlike most data warehouses, Redshift does not allow for easy column type changes after the column has been created. Additionally, we store a record of what the tables and column types should be set to in a local database, and validate the structure on each connector run. Currently, column type changes (i.e. change an integer column to float) are only available to our business tier customers on an ad-hoc basis.\n\nVARCHAR size limits\n\nAll Segment-managed schemas have a default VARCHAR size of 512 in order to keep performance high. If you wish to increase the VARCHAR size, you can run the following query.\n\n  ALTER TABLE table_name ALTER COLUMN column_name column_type;\n\n\nExample:\n\n  ALTER TABLE segment_prod.identifies ALTER COLUMN account_id TYPE VARCHAR(1024);\n\n\nIncreasing the default size can impact query performance as it needs to process more data to accomodate the increased column size. See Amazon\u2019s Redshift Documentation for more details.\n\nBlocklisted track call properties\n\nWhile almost all event properties are valid, we are unable to pass through properties that have naming conflicts with the default key/value pairs included in a standard raw JSON call. For example, if you send through a property in a track call named \u201ctimestamp\u201d or \u201cevent\u201d, it will cause a conflict and you likely wont see it appear in your warehouse. To be more specific, if you send the following track call, {\u2018event\u2019:\u2019birthday\u2019} will likely be dropped when syncing the data to your data warehouse.\n\nanalytics.track('selected gift', {'event':'birthday', 'type':'cake'})\n\nThis page was last modified: 11 Mar 2021\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nReserved words\nTable count limitations\nCluster node limitations\nColumn type changes\nVARCHAR size limits\nBlocklisted track call properties\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nSegment App\n/\nIam\n/\nAccess Management Concepts\nAccess Management Concepts\nFREE X\nTEAM X\nBUSINESS \u2713\nADD-ON X\n?\nTeam Members\n\nA Segment Team Member is an individual with access to a workspace. A Segment user can be associated with one or more workspaces, either as an owner or member of each. Check out the roles documentation for a complete list of roles.\n\nThe user session for a Segment Team Member is 7 days. Team Members in a HIPAA eligible workspace have a 15 minute user session across all workspaces.\n\nIf you are a Team Member in a HIPAA eligible workspace and want to access a non-HIPAA eligible workspace with a 7 day user session, you can create an alias (for example name+workspace@gmail.com).\n\nUser Groups\n\nA User Group is a set of Team Members with a set of shared policies. A Segment Team Member can be a member of one or many Groups. All roles in the Segment App are additive, which means that group membership can be assigned in addition to individual roles for a single team member. For example, a single user could inherit roles from a Group definition AND have access to additional resources through individually assigned roles.\n\nTokens\n\nYou can generate tokens to programmatically access Segment resources using the Segment Public API.\n\nResources\n\nResources are the building blocks of Segment, and represent the different parts of your Segment deployment to which you can grant access. These include:\n\nWorkspaces\nSources\nDestinations\nWarehouses\nSpaces\nProtocols Tracking Plans\nLabels\n\nWorkspace owners can use Labels to grant users access to groups of resources. When you add a Label to a Source or Spaces, any users who are granted access to that Label gain access to those resources.\n\nTo create or configure labels, go to the Labels tab in your workspace settings. Only workspace Owners can manage labels for the entire workspace.\n\nQuick Links:\nLabel-based Access Management\nLabels Best Practices\n\nThis page was last modified: 23 Oct 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nTeam Members\nUser Groups\nTokens\nResources\nLabels\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nDeveloper Center Overview\nDeveloper Center Overview\n\nWelcome! Here are the steps you\u2019ll follow to build an integration on Dev Center 2.0, launch your destination to Private Beta so customers can test it, and then launch it as Public in the Segment catalog.\n\nBuild on Segment\n\nOver 19,000 companies use Segment as their central hub for collecting and synthesizing first-party customer data. Customers use Segment sources to collect data across all their properties (for example, web, mobile, CRMs, or email) and send this data into destinations (SaaS tools, internal databases or queues, or a data warehouse) to perform analytics, run marketing campaigns and much more.\n\nIntegration types\n\nSegment provides two different integration types to support bringing your data into Segment, and sending your data downstream to other third-party tools.\n\nSources\n\nSources bring users\u2019 first-party data into Segment. While there are several types of sources (for example, web or server libraries, mobile integrations, and Cloud), the Developer Center enables you to build your own Cloud Event sources. These sources enable users to import data directly from your application into Segment.\n\nDestinations\n\nDestinations send data to other tools for processing or analysis. For example, a Segment user may want to send their data to your advertising platform or analytics tool. To accomplish this, they\u2019ll connect your Segment destination to their workspace.\n\nAll new Segment Destinations are built on the Actions framework, which enables a simplified build experience for you and a more straightforward configuration experience for your users.\n\nDevelopment process\n\nTo develop your integration in the Developer Center, complete the following steps:\n\nBecome a Segment Partner\nUnderstand Segment\u2019s conceptual model and Spec\nFollow Segment\u2019s security guidance\nRequest access to the Segment Developer Center\nCreate your integration\nWrite your integration\u2019s documentation\nBecome a Segment Partner\n\nSign up for the Segment Select Partner Program. During the sign-up process, you\u2019ll agree to the Segment Partner Program Agreement and Privacy Policy.\n\nUnderstand Segment\u2019s conceptual model and Spec\n\nSegment\u2019s Conceptual Model is a high-level overview of how Segment works and explains how your integration fits into the Segment catalog.\n\nThe Segment Spec provides best practices for the specific data you should capture and the best way to format that data based on your use case. The Spec outlines the semantic definition of the customer data that Segment captures across all its libraries and APIs, and will be a main building block for your integration.\n\nFollow Segment\u2019s security guidance\n\nSecurity for both customers and partners is a priority at Segment. Before you start building on the Developer Center, review the Acceptable Use Policy and ensure you\u2019re following these guidelines:\n\nFollow a secure software-development lifecycle, which enables you to create code that is safe for Segment customers and their end users, and that enables you to maintain and raise the security of that code over time\nIf you or your code comes into contact with Segment customer- or end-user data for any reason, protect it with commercially reasonable methods throughout its data lifecycle, including creation, handling, transporting, storing and destruction.\nIf you suspect a security event, incident or breach while working on this project or afterward, contact Segment Security for assistance with your investigation and communications\nPractice modern and common-sense security for any scenario that is not explicitly stated.\nRequest access to the Segment Developer Center\n\nSegment provides access to the Developer Portal on request. Open the Developer Portal page and click Sign up to request access. A Segment account is required for this step.\n\nSegment receives a large volume of requests so please include a valid company website and email address, answer all questions with details about integration\u2019s use case as well as highlighting specific customer requests to expedite the approval process.\n\nCreate your integration\n\nFollow the steps to build your source or destination.\n\nWrite your integration\u2019s documentation\n\nDocumentation is integral to enabling Segment\u2019s users to self-serve and onboard with your integration. Segment\u2019s documentation team will work with you during this part of the process to ensure your documentation matches the Segment style and is as instructive as possible.\n\nSource Documentation Instructions\nDestination Documentation Instructions\nSubmit your integration for review\n\nBefore users can go hands on with your integration, a review by Segment engineers is required to ensure the integration meets security and usability standards.\n\nDestinations\n\nTo submit your destination for review, follow the destination-specific instructions in the Submit a pull request docs.\n\nSources\n\nTo submit your source for review, complete the steps described in the Developer Portal and click Submit for review.\n\nThis page was last modified: 12 Aug 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nBuild on Segment\nDevelopment process\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSources\n/\nCatalog\n/\nLibraries\n/\nMobile\n/\nAnalytics-Swift for iOS & Apple\nAnalytics-Swift for iOS & Apple\nCOMMUNITY X\nMAINTENANCE X\nFLAGSHIP \u2713\n?\n\nWith Analytics-Swift, you can send data from iOS, tvOS, iPadOS, WatchOS, macOS and Linux applications to any analytics or marketing tool without having to learn, test, or implement a new API every time. Analytics-Swift is compatible with both Swift and Objective-C applications.\n\nIf you\u2019re migrating to Analytics-Swift from Analytics iOS (Classic), you can skip to the migration guide.\n\nBenefits of Analytics-Swift\n\nAnalytics-Swift provides several key benefits including improvements in stability, performance, and developer experience when compared to Analytics iOS (Classic).\n\nPerformance\n\nAnalytics-Swift offers improved performance when compared to Analytics iOS. For a more detailed overview, you can reference the blog post.\n\nFaster event processing and delivery\nSignificantly lower CPU usage\nSmall memory & disk usage footprint\nDeveloper Experience\n\nAnalytics-Swift adds several improvements to the overall experience of using the core SDK, as well as improvements to the overall Plugin Architecture.\n\nAbility to use Type Safe data structures rather than just dictionaries.\nSimpler syntax and more developer friendly overall.\nMore customization options than ever before.\nDevice Mode Transformations & Filtering\n\nFor the first time ever, developers can filter and transform their users\u2019 events even before the events leave the mobile device. What\u2019s more, these Filters & transformations can be applied dynamically (either through the Segment Dashboard, or Javascript uploaded to the workspace) and do not require any app updates.\n\nLearn more about Destination Filters on Mobile, and Edge Functions on Mobile.\n\nGetting started\n\nMultiple Instances\n\nMultiple Instances are supported as part of the Analytics-Swift mobile library. However, each instance must have a unique writeKey defined, or malformed JSON may be sent to our API resulting in 400 errors.\n\nTo get started with the Analytics-Swift mobile library:\n\nCreate a Source in Segment.\nGo to Connections > Sources > Add Source.\nSearch for Apple and click Add source.\nAdd the Analytics dependency to your application. Add the Swift package, git@github.com:segmentio/analytics-swift.git as a dependency through either of these 2 options:\nYour package.swift file\nXcode\nXcode 12: File > Swift Packages > Add Package Dependency\nXcode 13: File > Add Packages\u2026\n\nAfter installing the package, you can reference Analytics-Swift by importing Segment\u2019s Analytics package with import Segment.\n\nInitialize and configure the Analytics-Swift client. For example, in a lifecycle method such as didFinishLaunchingWithOptions in iOS:\nSwift\nObjective-C\n    var analytics: Analytics? = nil\n\n    func application(_ application: UIApplication, didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?) -> Bool {\n            // Override point for customization after application launch.\n            let configuration = Configuration(writeKey: \"WRITE_KEY\")\n                .trackApplicationLifecycleEvents(true)\n                .flushInterval(10)\n\n            analytics = Analytics(configuration: configuration)\n    }\n\n\nThese are the options you can apply to configure the client:\n\nOPTION NAME\tDESCRIPTION\nwriteKey required\tThis is your Segment write key.\napiHost\tThe default is set to api.segment.io/v1.\nThis sets a default API Host to which Segment sends event.\nautoAddSegmentDestination\tThe default is set to true.\nThis automatically adds the Segment Destination plugin. Set to false if you want to add plugins to the Segment Destination.\ncdnHost\tThe default is set to cdn-settings.segment.com/v1.\nThis sets a default CDN Host from which Segment retrieves settings.\ndefaultSettings\tThe default is set to {}.\nThis is the settings object used as fallback in case of network failure.\nflushAt\tThe default is set to 20.\nThe count of events at which Segment flushes events.\nflushInterval\tThe default is set to 30 (seconds).\nThe interval in seconds at which Segment flushes events.\ntrackApplicationLifecycleEvents\tThe default is set to true.\nThis automatically tracks lifecycle events. Set to false to stop tracking lifecycle events.\n\nAppClip Tracking\n\nIf you are tracking App Clips using iOS or Swift libraries, you may encounter zeros in your device ID. Segment recommends that you set your own device ID in these instances to avoid this issue.\n\nCore tracking methods\n\nOnce you\u2019ve installed the Analytics-Swift library, you can start collecting data through Segment\u2019s tracking methods:\n\nTrack\nIdentify\nScreen\nGroup\nAlias\nDestinations\n\nDestinations are the business tools or apps that Segment forwards your data to. Adding Destinations allow you to act on your data and learn more about your customers in real time.\n\nSee Segment\u2019s documentation for device-mode destinations for a full list of supported device-mode plugins.\n\nSee Segment\u2019s cloud-mode destinations for a full list of available cloud-mode destinations that Swift supports.\n\n\nSegment offers support for two different types of destination connection modes: Cloud-mode and Device-mode. learn more about the differences between the two in the Segment Destination docs.\n\nCloud-mode Destinations\n\nDestinations that can be enabled from your Segment workspace and require no additional app setup.\n\nDevice-mode Destinations\n\nDestinations that require additional app setup, and limit certain Segment functionality.\n\nTools and extensions\n\nAnalytics-Swift is built with extensibility in mind. Use the tools list below to improve data collection.\n\nPlugin architecture\nTypewriter\nDestination Filters\nCode samples\nProxying events\n\nIf you proxy your events through the apiHost config option, you must forward the batched events to https://api.segment.io/v1/b. The https://api.segment.io/v1/batch endpoint is reserved for events arriving from server-side sending, and proxying to that endpoint for your mobile events may result in unexpected behavior.\n\nIf you are using the Analytics iOS (Classic) SDK, you can find the documentation here. Many of the features available in the Analytics-Swift SDK are not available in the Analytics iOS (Classic) SDK.\n\nTelemetry\n\nThe Analytics-Swift SDK collects telemetry data on configuration and usage by default. This includes basic information on SDK setup, plugins and event types used, and basic error details. Segment downsamples the data to minimize traffic and doesn\u2019t collect any personally identifiable information (PII) or event data.\n\nYou can disable telemetry at any time by setting Telemetry.shared.enable = false.\n\nWhen internal errors or errors from plugins occur, the write key may be included with error data to help Segment identify the issue(s). You can disable this by setting Telemetry.shared.sendWriteKeyOnError = false.\n\nTimestamps in Swift\n\nDue to efficiency updates made to Segment\u2019s Swift library, Segment now adds the sentAt timestamp to an event when the batch is complete and initially tried to the Segment API. This can impact the value of the timestamp field calculated by Segment if users are operating in an offline mode. More details on this change can be seen in Segment\u2019s timestamp documentation.\n\nThis page was last modified: 14 Nov 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nBenefits of Analytics-Swift\nGetting started\nDestinations\nTools and extensions\nProxying events\nTelemetry\nTimestamps in Swift\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nPrivacy\n/\nComplying with the GDPR\nComplying with the GDPR\n\nOn May 25, 2018 businesses faced the greatest regulatory change in data privacy policy since the 1995 EU Data Protection Directive was enacted: the\u00a0EU General Data Protection Regulation\u00a0(GDPR). The European Union began enforcing the GDPR on May 25, 2018 in an effort to strengthen the security and protection of personal data of EU residents.\n\nIn keeping with Segment\u2019s ongoing commitment to privacy and security, Segment updated its practices to be GDPR compliant before the May 25, 2018, enforcement date. But that\u2019s not all. As the central record for your customer data, Segment is also committed to making it easier for you to comply with the GDPR.\n\nSpecifically, here is how Segment supports its customers:\n\nAn updated Data Protection Addendum (DPA) to reflect the requirements of the GDPR and to ensure compliant data transfer with storage outside the EU. Existing customers can enter into the updated Data Protection Addendum using the opt-in process.\n\nNew product capabilities to help you be compliant when users request you delete or suppress their data.\n\nCheck out Segment\u2019s GDPR blog post\u00a0to learn about Segment\u2019s plan for GDPR readiness.\n\nHow does the GDPR impact your business?\n\nThe GDPR has different requirements depending on how your business interacts with personal data. Companies can be data controllers, data processors, or, in some cases, both a controller and a processor. Data controllers are businesses that collect their end users\u2019 data and decide why and how that data is processed. On Segment\u2019s marketing website, for example, Segment is considered a\u00a0data controller. As a vendor, however, the more meaningful way Segment is impacted by the GDPR is as a\u00a0data processor, as Segment is a company that helps its customers with the processing of their customer data.\n\nIn addition to damaging your customers\u2019 trust, failure to comply with the GDPR can result in fines of\u00a0\u20ac20 million or 4% of global annual turnover for the previous year (whichever is greater).\n\nWhat are your responsibilities as a data controller?\n\nIf you collect data about EU residents and decide why and how those data are collected and processed, you may be considered a\u00a0data controller\u00a0under the GDPR. Data controllers are responsible for implementing adequate technical, organizational, and operational measures to ensure and demonstrate that all data collection and processing is performed in accordance with the GDPR, including entering into a relevant data processing agreement. Moreover, you must fulfill data subjects\u2019 rights with respect to their data along the following principles:\n\nLawfulness, fairness and transparency\nPurpose limitation\nData minimization\nAccuracy\nStorage limitation\nIntegrity and confidentiality (security)\nAccountability\n\nSegment recommends reading the full text of the\u00a0GDPR\u00a0to better understand these rights and seeking independent legal advice regarding your obligations under the GDPR. You can also check out publications by data privacy associations such as the International Association of Privacy Professionals (IAPP) for the latest news.\u00a0\n\nThings you can do to address GDPR\n\nIn addition to seeking independent legal advice regarding your obligations under the GDPR, here are some tips to get you started:\n\nEducate yourself on the provisions of the\u00a0GDPR\u00a0to understand how they may differ from your existing data protection obligations and practices.\n\nIf you don\u2019t have dedicated data privacy or security personnel in-house, consider appointing a directly responsible individual (DRI) or small team to manage your company\u2019s GDPR compliance efforts.\n\nCreate an up-to-date inventory of personal data that you collect and manage. -\n\nFor data flowing through Segment, you can start with the Overview page in your workspace to understand where you are collecting (Sources) and routing (Destinations) customer data. Next, visit the Schema page within each of your Sources to understand the type of data you\u2019re sending to Segment.\n\nBe sure to consider the data that is not flowing through Segment. You\u2019ll need to make sure the same bar for compliance is met across your organization.\n\nCreate a list of vendors who you send data to (analytics tools, CRMs, email tools, etc.), and understand whether they are a controller or a processor. Then, determine what their obligations are, and make sure they have a plan to be ready for the GDPR.\n\nDevelop a plan for obtaining and managing\u00a0consent\u00a0in accordance with the GDPR or establish other lawful grounds for using personal data.\n\nDetermine if your company needs to appoint a\u00a0Data Protection Officer\u00a0(DPO). If you will be appointing a DPO, begin searching for the best person for the role.\n\nBecoming GDPR compliant takes time, and will require you to rethink how you collect and manage customer data. If you have any questions about the GDPR or want to learn how Segment can help you prepare,\u00a0let us know!\n\nOpting into the Data Protection Addendum and Standard Contractual Clauses\n\nSegment offers a Data Protection Addendum (DPA) and Standard Contractual (SCCs) as a means of meeting contractual requirements of applicable data privacy laws and regulations, such as GDPR, and to address international data transfers. Segment\u2019s online Data Protection Addendum (DPA) is already part of and incorporated into the Terms of Service. If you have a separate written agreement with Segment that does not include a Data Protection Addendum (DPA) or you would like to replace the existing Data Protection Addendum (DPA) that is attached to your separate written agreement with Segment\u2019s latest Data Protection Addendum (DPA), please contact your account team or customer support.\n\nSegment offers a Data Protection Addendum (DPA) and Standard Contractual Clauses (SCCs) as a means of meeting the regulatory contractual requirements of GDPR in its role as processor and also to address international data transfers.\n\nNote on Schrems II: Despite the CJEU\u2019s July 2020 ruling invalidating Privacy Shield as a means of validly transferring data to the USA from the EU, these developments are not expected to disrupt Segment\u2019s ability to provide services to its EU customers as the European Court of Justice has reaffirmed that the Standard Contractual Clauses (SCC) remain valid as a method of transfer. Segment\u2019s standard Data Protection Addendum includes a provision whereby should Privacy Shield ever be invalidated (as is the case now) then the SCCs will automatically apply.\n\nThis page was last modified: 14 Dec 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nHow does the GDPR impact your business?\nThings you can do to address GDPR\nOpting into the Data Protection Addendum and Standard Contractual Clauses\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nHow To Guides\n/\nHow do we set up event-triggered notifications or alerts?\nHow do we set up event-triggered notifications or alerts?\n\nBelow you\u2019ll find a\u00a0bunch of ways to set up notifications for yourself based on the data you\u2019re sending through Segment.\u00a0\n\nConnections Alerting\n\nConnections Alerting allows Segment users to receive in-app, email, and Slack notifications related to the performance and throughput of an event-streaming connection.\n\nConnections Alerting allows you to create two different alerts:\n\nSource volume alerts: These alerts notify you if your source ingests an abnormally small or large amount of data. For example, if you set a change percentage of 4%, you would be notified when your source ingests less than 96% or more than 104% of the typical event volume.\nSuccessful delivery rate alerts: These alerts notify you if your destination\u2019s successful delivery rate falls outside of a percentage that you set. For example, if you set a percentage of 99%, you would be notified if you destination had a successful delivery rate of 98% or below.\n\nFor more information about Connections Alerting, see the Connections Alerting documentation.\n\nGoogle Analytics custom alerts\n\nYou can use Google Analytics Custom Alerts to send yourself emails whenever a specific traffic segment drops below (or above) a threshold you set.\u00a0\n\nLearn how to set up email alerts in Google\u2019s documentation.\n\nAnalytics\u00a0email summaries\n\nWith tools like Amplitude, Kissmetrics, and Mixpanel,\u00a0you can set up email reports delivered to you on a daily basis. They\u00a0are completely customizable, so you can keep an eye on as many events or other metrics you\u2019d like.\u00a0\n\nMixpanel email reports\nAmplitude email alerts\nRealtime traffic monitoring\n\nChartbeat and GoSquared both\u00a0offer awesome real-time dashboards to see what\u2019s happening right now on your site. They both\u00a0include\u00a0the option to get notified when your\u00a0traffic hits a certain threshold. For example, if your on-site visitors is less than 100 people, or more than 1,000.\n\nChartbeat Spike Alerts\nGoSquared Traffic Spike Alerts\n\nGoSquared also offers in-depth historical and user analysis. Chartbeat sticks to realtime anonymous traffic, but offers some sweet features for publishers.\n\nWebhook-based alerts\n\nThe last option Segment recommends is to use a monitoring tool like\u00a0PagerDuty\u00a0or\u00a0Datadog\u00a0and point Segment\u2019s\u00a0webhooks\u00a0destination at them. That way you can set up custom alerts in their system.\n\nEvent-triggered emails\n\nThe last option for alerting based off of Segment events is to use one of the email tools available on the Segment platform that offers event-triggered emails. Your options there are Customer.io, Vero, Autopilot, Outbound, Klaviyo, or Threads.\n\nThis page was last modified: 30 May 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nConnections Alerting\nGoogle Analytics custom alerts\nAnalytics\u00a0email summaries\nRealtime traffic monitoring\nWebhook-based alerts\nEvent-triggered emails\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nJourneys\n/\nJourney Context\nJourney Context\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nEvent-Triggered Journeys redefine how you orchestrate and personalize customer experiences.\n\nThis page explains Journey context, which can help you dynamically adapt each journey to individual user interactions, creating highly relevant, real-time workflows.\n\nPublic Beta\n\nEvent-Triggered Journeys is in public beta, and Segment is actively working on this feature. Some functionality may change before it becomes generally available. Event-Triggered Journeys is not currently HIPAA eligible.\n\nOverview\n\nUnlike traditional audience-based journeys, which rely solely on user progress through predefined steps, event-triggered journeys capture and store the details of user-triggered events. This shift allows you to access the data that caused users to reach a specific step and use it to make more precise decisions throughout the journey.\n\nWith journey context, you can:\n\nPersonalize customer experiences using real-time event data.\nEnable advanced use cases like abandonment recovery, dynamic delays, and more.\n\nFor example:\n\nWhen a user cancels an appointment, send a message that includes the time and location of the appointment they just canceled.\nWhen a user abandons a cart, send a message that includes the current contents of their cart.\nWhat is Journey context?\n\nJourney context is a flexible data structure that captures key details about the events and conditions that shape a customer\u2019s journey. Journey context provides a point-in-time snapshot of event properties, making accurate and reliable data available throughout the journey.\n\nJourney context stores event property information tied to specific user actions, like Appointment ID or Order ID.\n\nJourney context doesn\u2019t store:\n\nProfile traits, which may change over time.\nAudience memberships, which can evolve dynamically.\n\nHowever, the up-to-date values of profile traits and audience membership can be added in a payload sent to a destination.\n\nThis focused approach ensures journey decisions are always based on static, reliable data points.\n\nExamples of stored context\n\nEvent properties are the foundation of Journey context. Examples of event properties include:\n\nAppointment Scheduled:\nAppointment ID\nAppointment Start Time\nAppointment End Time\nAssigned Provider Name\nOrder Completed:\nCart ID\nOrder ID\nAn array of cart contents\n\nSegment captures each event\u2019s properties as a point-in-time snapshot when the event occurs, ensuring that the data remains consistent for use in personalization.\n\nUsing Journey context in Event-Triggered Journeys\n\nJourney context provides the framework for capturing and referencing data about events and conditions within a journey. It allows Event-Triggered Journeys to dynamically respond to user behavior by making event-specific data available for decisions and actions at each step.\n\nThis is useful for scenarios like:\n\nAbandonment recovery: Checking whether a user completed a follow-up action, like a purchase.\nCustomizing messages: Using event properties to include relevant details in communications.\n\nBy incorporating event-specific data at each step, journey context helps workflows remain relevant and adaptable to user actions.\n\nJourney steps that use context\n\nJourney context gets referenced and updated at various steps in an event-triggered journey. Each step plays a specific role in adapting the journey to user behavior or conditions.\n\nHold Until split\n\nThis step checks whether a user performs a specific event within a given time window. If the event occurs, Segment adds its details to journey context for use in later steps.\n\nFor example, a journey may wait to see if a checkout_completed event occurs within two hours of a user starting checkout. If the event happens, its properties are added to context and the workflow can proceed; otherwise, it may take an alternate path. The data captured includes event properties (like Order ID).\n\nIf a Hold Until branch is set to send profiles back to the beginning of the step when the event is performed, those events are also captured in context. Because they may or may not be performed during a journey, they will show as available in future steps but will not be guaranteed for every user\u2019s progression through the journey.\n\nSend to destination\n\nThe send to destination step allows journey context data to be included in payloads sent to external tools, like messaging platforms or analytics systems.\n\nFor example, a payload sent to a messaging platform might include Order ID and Cart Contents to personalize the message. Users can select which parts of journey context to include in the payload.\n\nContext structure\n\nThe structure of journey context organizes event-specific data gets and makes it accessible throughout the journey workflow. By standardizing how data is stored, Segment makes it easier to reference, use, and send this information at different stages of a journey.\n\nJourney context is organized as a collection of key-value pairs, where each key represents a data point or category, and its value holds the associated data.\n\nFor example, when a user triggers an event like Appointment Scheduled, Segment stores its properties (like Appointment ID, Appointment Start Time) as key-value pairs. You can then reference these values in later journey steps or include them in external payloads.\n\nThe following example shows how journey context might look during a workflow. In this case, the user scheduled an appointment, and the workflow added related event data to the context:\n\n{\n  \"journey_context\": {\n    \"appointment_scheduled\": {\n      \"appointment_id\": 12345,\n      \"start_time\": \"2024-12-06T10:00:00Z\",\n     \"end_time\": \"2024-12-06T11:00:00Z\",\n      \"provider_name\": \"Dr. Smith\"\n    },\n    \"appointment_rescheduled\": {\n      \"appointment_id\": 12345,\n      \"start_time\": \"2024-12-07T10:00:00Z\",\n      \"end_time\": \"2024-12-07T11:00:00Z\",\n      \"provider_name\": \"Dr. Jameson\"\n    }\n  }\n}\n\n\nThis payload contains:\n\nEntry Event properties: Captured under the appointment_scheduled key.\nHold Until Event properties: Captured under the appointment_rescheduled key.\nJourney context and Event-Triggered Journeys\n\nJourney context underpins the flexibility and precision of Event-Triggered Journeys. By capturing key details about events and decisions as they happen, journey context lets workflows respond dynamically to user actions and conditions.\n\nWhether you\u2019re orchestrating real-time abandonment recovery or personalizing messages with event-specific data, journey context provides the tools to make your workflows more relevant and effective.\n\nTo learn more about how Event-Triggered Journeys work, visit the Event-Triggered Journeys documentation.\n\nThis page was last modified: 19 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nOverview\nWhat is Journey context?\nUsing Journey context in Event-Triggered Journeys\nContext structure\nJourney context and Event-Triggered Journeys\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nHow To Guides\n/\nMigrating Code From Other Analytics Tools\nMigrating Code From Other Analytics Tools\n\nSwitching from your current client-side JavaScript event tracking to Segment is easy. Below you can find migration guides for the following tools:\n\nGoogle Analytics\nMixpanel\n\nIf you\u2019d like us to add more tools or mobile/server-side examples to this guide\u00a0let us know!\n\nGoogle Analytics\nCustom Events\n\nGoogle Analytics Custom Events are simple to record in Segment. You\u2019ll record them with our track method and use the same properties you would when sending to Google Analytics directly.\n\nThe only mapping exception is the Event Action. That will automatically be populated by the Event Name you include in the track call.\n\nHere\u2019s an example:\n\nga('send', {\n  'hitType': 'event',\n  'eventCategory': 'Account',\n  'eventAction': 'Signed Up',\n  'eventLabel': 'Premium',\n  'eventValue': 4\n});\n\n\nBecomes:\n\nanalytics.track('Signed Up', {\n  category: 'Account',\n  label: 'Premium',\n  value: 4\n});\n\n\nSince Event Category is required we\u2019ll populate it with All if you don\u2019t specify one. You can read more about this in\u00a0our Google Analytics docs.\n\nEcommerce\n\nSegment has full support for the Google Analytics E-Commere API and the\u00a0Enhanced E-Commerce API\u00a0as well. Make sure you follow\u00a0our e-commerce tracking plan\u00a0to make sure you\u2019ll be able to use all e-commerce features in the tools we support.\n\nFor an e-commerce transaction to appear in Google Analytics you\u2019ll need to enable e-commerce for your Google Analytics view and send an Order Completed event to Segment. This simplifies things a lot compared to the direct Google Analytics code.\n\nHere\u2019s an example:\n\nga('require', 'ecommerce');\n\nga('ecommerce:addTransaction', {\n'id': '93745',\n'revenue': '30',\n'shipping': '3',\n'tax': '2',\n'currency': USD\n});\n\nga('ecommerce:addItem', {\n'id': '23423',\n'name': 'Monopoly: 3rd Edition',\n'sku': 'J90-32',\n'category': 'Games',\n'price': '19.00',\n'quantity': '1'\n});\n\nga('ecommerce:addItem', {\n'id': '22744',\n'name': 'Uno Card Game',\n'sku': 'Q93-32',\n'category': 'Cards',\n'price': '3.00',\n'quantity': '2'\n});\n\nga('ecommerce:send');\n\n\nBecomes:\n\nanalytics.track('Order Completed', {\n  order_id: '93745',\n  total: 46,\n  shipping: 3,\n  tax: 2,\n  currency: USD,\n  products: [{\n    id: '23423',\n    name: 'Monopoly: 3rd Edition',\n    sku: 'J90-32',\n    category: 'Games',\n    price: 19,\n    quantity: 1\n  }, {\n    id: '22744',\n    name: 'Uno Card Game',\n    sku: 'Q93-32',\n    category: 'Cards',\n    price: 3,\n    quantity: 2\n  }]\n})\n\n\nAt the very minimum you must include an orderId for each Order and for each product inside that order you must include an id and name. All other properties are optional.\n\nCustom Dimensions\n\nThrough Segment you can record user-scope custom dimensions using our identify, page, or track methods.\n\nA full explanation can be found in\u00a0our Google Analytics docs\u00a0page, but here\u2019s a quick example:\n\nga('set', 'dimension5', 'Male');\nga('send', 'pageview');\n\n\nBecomes:\n\nanalytics.identify({\n  gender: 'Male'\n});\nanalytics.page();\n\n\n(This example assumes you have already mapped Gender to the correct dimension in your Segment source settings for Google Analytics.)\n\nEverything Else\n\nTo see a full list of Google Analytics features and how they work through Segment read our Google Analytics docs page.\n\nMixpanel\nEvent Tracking\n\nEvent tracking is Mixpanel\u2019s bread and butter. Below are all the relevant Mixpanel functions and how you can map them to Segment functions.\n\nSwitching your event tracking from Mixpanel to Segment couldn\u2019t be easier. Our trackmethod maps directly to Mixpanel\u2019s. The event name is the first argument and the event properties are the second argument.\n\nmixpanel.track('Registered',{\n  type: 'Referral'\n});\n\n\nBecomes:\n\nanalytics.track('Registered', {\n  type: 'Referral'\n});\n\n\nThe identify method in Mixpanel is used to merge together events from multiple environments so your unique events number is accurate and your funnels don\u2019t break.\n\nSince mixpanel.identify only takes a single argument (a userID) it maps directly to our identify method:\n\nmixpanel.identify('123');\n\n\nBecomes:\n\nanalytics.identify('123');\n\n\nMixpanel has the idea of Super Properties, which are user traits that get attached to every event that the user does. In Segment you can set Mixpanel Super Properties using our identify method. Super properties are only supported in client-side libraries Analytics.js,\u00a0iOS,\u00a0Android.\n\nHere\u2019s an example:\n\nmixpanel.register({\n  \"gender\": \"male\",\n  \"hairColor\": \"brown\"\n});\n\n\nBecomes:\n\nanalytics.identify({\n  gender: 'male',\n  hairColor: 'brown'\n});\n\n\nThis also works when you include a userId argument in your identify call.\n\nAlias\n\nAlias is necessary in Mixpanel to tie together an anonymous visitor with an identified one. The Mixpanel and Segment alias methods both work the same.\n\nIn client-side javascript passing a single argument will alias the current anonymous or identified visitor distinct_id to the userId you pass into it:\n\nmixpanel.alias('1234');\n\n\nBecomes:\n\nanalytics.alias('1234');\n\nTrack Links\n\nIf you are tracking links with Mixpanel\u2019s track_links helper you can switch that code to the Segment trackLink helper function in Analytics.js.\n\nAnd here\u2019s an example:\n\n// track click for link id #nav\nmixpanel.track_links(\"#free-trial-link\", \"Clicked Free-Trial Link\", {\n\u00a0 plan: 'Enterprise'\n})\n\n\nBecomes:\n\nvar link = document.getElementById('free-trial-link');\nanalytics.trackLink(link, 'Clicked Free-Trial Link', {\n\u00a0 plan: 'Enterprise'\n});\n\nTrack Forms\n\nIf you are tracking forms with Mixpanel\u2019s track_forms helper you can switch that code tothe Segment trackForm helper function in Analytics.js.\n\nAnd here\u2019s an example:\n\n// track submission for form id \"register\"\nmixpanel.track_forms(\"#register\", \"Created Account\",\n\u00a0 plan: 'Premium'\n});\n\n\nBecomes:\n\nvar form = document.getElementById('register');\nanalytics.trackForm(form, 'Created Account',\n\u00a0 plan: 'Premium'\n});\n\nPeople Tracking\n\nMixpanel people tracking is a separate database from the event tracking outlined above. For that reason there are separate API methods to record data to Mixpanel People.\n\nThis method sets people properties in Mixpanel People. In Segment you will use ouridentify method to accomplish this.\n\nHere\u2019s an example:\n\nmixpanel.people.set({\n\u00a0 \"$email\": \"jake.peterson@example.com\",\n\u00a0 \"$name\": \"Jake Peterson\"\n});\n\n\nBecomes:\n\nanalytics.identify({\n\u00a0 email: 'jake.peterson@example.com',\n\u00a0 name: 'Jake Peterson'\n});\n\n\nThis also works when you include a userId argument in your identify call.\n\nAs you can see Segment also recognizes special traits like email and name and translates them to the keys that Mixpanel expects (we automatically add the dollar sign).\n\nFor more information check out our Mixpanel docs.\n\nIncrement\n\nTo use Mixpanel increment through Segment you won\u2019t event need anything in your code! All you have to do is list the events you\u2019d like to increment automatically in your Mixpanel destination settings.\n\nRead more in our Mixpanel Increment Docs.\n\nRevenue\n\nMixpanel\u2019s Revenue report requires the use of a special function called track_charge. In Segment that special function becomes a simple track call. By using the event name Order Completed we\u2019ll also use that event for any tools you use that recognize our ecommerce spec.\n\nmixpanel.people.track_charge(30.50,\n\u00a0 'orderId': 'F9274'\n});\n\n\nBecomes:\n\nanalytics.track('Order Completed',\n\u00a0 revenue: 30.50,\n\u00a0 orderId: 'F9274'\n});\n\n\nThis page was last modified: 07 Nov 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nGoogle Analytics\nMixpanel\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nPrivacy\n/\nPrivacy Frequently Asked Questions\nPrivacy Frequently Asked Questions\nPrivacy Portal questions\nWhy aren\u2019t fields from my Cloud Object Sources (like Salesforce and Zendesk) showing up in the Privacy Portal Inbox and Inventory?\n\nThe Privacy Portal doesn\u2019t doesn\u2019t support fields from Cloud Object Sources like Salesforce or Zendesk.\n\nWhy does Segment suggest classifying my fields as Yellow or Red?\n\nSegment provides suggested classifications based on default PII matchers. These suggestions include exact and fuzzy matches for potential PII. You can update these classifications by following the instructions to change a recommended classification.\n\nWho can access the Privacy Portal?\n\nOnly Workspace Owners can access the portal.\n\nWhich Segment plan types include access to the Privacy Portal?\n\nAll Segment plans include access to the Privacy Portal. Data privacy is a fundamental Segment feature, not an add-on.\n\nIf I block data at the source level, can I reverse it or recover the data using Segment\u2019s Data Replay feature?\n\nWhen you block data at the source level using Privacy Controls, the data never enters Segment. As a result, Segment can\u2019t replay the data. Segment recommends exercising caution when blocking data at the source level.\n\nThe Privacy Portal classified my property as Yellow, but my destinations require it to function. What should I do?\n\nSegment classifications are recommendations. If a destination requires a field classified as Yellow, you can override the recommended classification to ensure the field gets sent downstream.\n\nUser deletion and suppression questions\nHow can I find a specific userId?\n\nTo locate a specific userId, query your Segment data warehouse for the users table. Use other known details about the user, like their email address, to identify the correct row and retrieve the userId.\n\nHow many deletion requests can I send?\n\nYou can send batches of up to 5,000 userIds, or 4 MB, per payload. Segment processes these batches asynchronously. Contact Segment if you need to process more than 110,000 users within a 30-day period.\n\nWhich destinations can I send deletion requests to?\n\nIn addition to your Raw Data destinations (Amazon S3 and data warehouses), Segment can forward requests to the following streaming destinations:\n\nAmplitude\nIterable\nBraze\nIntercom\nWebhooks\ntray.io\nAppcues\nVero\nGoogle Analytics\nCustomer.io\nOptimizely Full Stack\nGoogle Cloud PubSub\nFriendbuy (Cloud Destination)\n\nSegment forwards deletion requests but cannot guarantee that data is deleted from downstream destinations. You must contact these destinations to confirm that they executed the request.\n\nWhich destinations require additional configuration to process deletion requests?\nAmplitude\n\nTo process deletion requests in Amplitude, add your Amplitude secret key to the destination settings under \u201cSecret Key.\u201d You can find this key in your Amplitude project\u2019s General Settings.\n\nGoogle Analytics\n\nTo send deletion requests to Google Analytics, authenticate your account with Segment using OAuth. Go to the User Deletion settings in your Segment Google Analytics destination and use your email and password to complete authentication.\n\nWhat regulation types does Segment support?\n\nSegment supports the following regulation types:\n\nSUPPRESS_ONLY: Suppresses new data for a userId without deleting existing data in your workspace or downstream destinations.\nUNSUPPRESS: Stops ongoing suppression of a userId.\nSUPPRESS_WITH_DELETE: Suppresses new data for a userId and deletes all existing data for that ID in your workspace and Segment\u2019s internal archives. Segment forwards the deletion request to downstream destinations but can\u2019t guarantee deletion in third-party tools.\nDELETE_INTERNAL: Deletes user data only from Segment archives, without affecting downstream destinations.\nDELETE_ONLY: Deletes user data from Segment and your connected warehouses. Also sends a deletion request to your downstream destinations.\n\nUsing SUPPRESS_WITH_DELETE or DELETE_ONLY regulation types might lead to additional charges levied by your destination providers.\n\nThis page was last modified: 26 Nov 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nPrivacy Portal questions\nUser deletion and suppression questions\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nThe Segment Web App\nThe Segment Web App\n\nWhen you first log in, you go to your workspace. (If you\u2019re a member of several workspaces, you get to choose which one to go to.) Workspaces organize sets of sources and destinations into a central location.\n\nSegment University: Segment App Overview\n\nWant a video tour of the Segment workspace? Head over to Segment University! (Must be logged in to access.)\n\nWhat\u2019s a Workspace?\n\nA workspace is a group of sources that can be administered and billed together. Workspaces help companies manage access for multiple users and data sources. Workspaces let you collaborate with team members, add permissions, and share sources across your whole team using a shared billing account.\n\nWhen you first log in to your Segment account, you can create a new workspace, or choose to log into an existing workspace if your account is part of an existing organization.\n\nWorkspace Overview page\n\nThis is a Segment workspace.\n\nThe first thing you see is a graph of the sources and destinations you have connected to Segment. Sources send data to your workspace: these are your mobile apps, server sources, and website-based sources. Destinations are tools which get the data, and can also include warehouses, which just store large amounts of data for later reuse and analysis.\n\nThe graph on this overview page includes lines which can show you which sources send data to which destinations. If this is the first time you\u2019re looking at your workspace and you haven\u2019t set it up yet, it won\u2019t look quite like this.\n\nIn the left navigation bar, you see the main parts of the Segment application: Sources, Destinations, Privacy, Engage, and Protocols, if your subscription includes them.\n\nYou can also find the Catalog in the left navigation, which lists the sources you can collect data from, and the destinations you can send data to.\n\nYou can always click the Segment logo in the top left corner to get back to the Overview page.\n\nSources\n\nThe Sources tab lists everything that is sending data to your Segment workspace. Sources are organized by type: website, mobile, sever, or by cloud-app type, like CRM or payments.\n\nEach source has a status and a list of destinations. A source\u2019s status tells you whether or not the source is sending data to Segment, and how long it\u2019s been since Segment last saw data from the source. The source\u2019s destinations list shows you which destinations are receiving data from that source. You can expand them for more detail.\n\nDestinations\n\nThe Destination tab lists all of the destinations connected to your workspace. These are sorted into categories like analytics, email marketing, and other tool types. The list also shows whether or not Segment is sending data to that tool, or if the tool is enabled or disabled.\n\nThe Segment Integration Catalog\n\nNext up is the Catalog. The catalog includes a list of all sources and destinations available in Segment. You can search either by category or name. When you click on a catalog tile, the tile shows instructions on how to connect the tool to your Segment workspace.\n\nThe Catalog is always growing, so check out the \u201cNew and Noteworthy\u201d section from time to time to see what\u2019s new.\n\nEngage and Protocols\n\nIf you have Engage or Protocols enabled in your workspace, you\u2019ll see sections for those too. Engage helps you use your Segment data to build audiences and better understand your users, and Protocols helps you structure and maintain the format of the data you send through Segment.\n\nThese features are fairly advanced, but you can learn more about them by requesting a demo, or reading more in the Engage documentation, and the Protocols documentation.\n\nSegment Settings\n\nThe Workspace Settings tab shows more information about your workspace, including your team settings, GDPR requests, and so on. You might not have access to edit these settings.\n\nThe User Preferences tab shows your individual account settings, including Notification settings.\n\nThe Activity Notifications feature in Notification settings provides alerts for specific workspace activities when enabled. These alerts keep you updated on actions taken by other workspace users, excluding activities you initiate. This ensures you\u2019re only alerted to actions you\u2019re not directly involved in.\n\nThe Usage tab shows how many API calls or Monthly Tracked Users (MTUs) your workspace has used this month - which can be important for keeping an eye on your Segment bill.\n\nHealth\n\nThe Health tab lists any repeated or consistent errors, which can help alert you to misconfigurations or data issues which you can correct.\n\nIssues on the Health tab are sorted by Sources, Destinations, Warehouses, and again by type.\n\nIf errors are present, they\u2019re sorted by type and include information about how long ago they were last seen and how many times they\u2019ve occurred. You can click the wrench icon on an individual error line to view the Event Delivery tool and see the erroring payload and response. You can also disable or delete the erroring integration from this menu.\n\nPrivacy Portal\n\nThe Privacy Portal allows you to inspect data coming into your Segment account, check it for Personally Identifying Information (PII), classify it based on how sensitive the information is, and then determine which categories of data to send to different destinations. Read more about these tools in the Privacy Portal documentation.\n\nThis page was last modified: 22 Feb 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nWorkspace Overview page\nSources\nDestinations\nThe Segment Integration Catalog\nEngage and Protocols\nSegment Settings\nHealth\nPrivacy Portal\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nSegment App\n/\nExtensions\nExtensions\n\nExtensions let you integrate third-party tools into your existing Segment workspace, helping you automate tasks, manage data flows, and maintain version control.\n\nSegment offers the following extensions:\n\ndbt models and dbt Cloud: Sync your dbt Labs models with Segment to streamline model management, versioning, and CI checks. This extension lets you securely connect Segment to a Git repository, making it easier to integrate and manage dbt models across different environments like testing, staging, and production.\nGit Sync: Manage versioning and track changes by syncing your Segment workspace a Git repository. The Git Sync extension helps maintain a clear and organized relationship between your workspace and its corresponding Git repository, ensuring that your resources are consistently managed and versioned across your environments.\n\nSegment built Extensions to help you get the most out of your Segment workspace, allowing you to keep your projects organized, efficient, and aligned with best practices for data management and version control.\n\nThis page was last modified: 16 Aug 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nEvent Tester\nEvent Tester\n\nSegment has an Event Tester that enables you to test your connections between Segment and your destination. You can access the Event Tester from your Source Debugger, or from your destination settings. \u00a0\u00a0\n\nAvailable for server-side event streaming destinations only\n\nThis feature is only available for server-side integrations (also known as cloud-mode destinations). You can\u2019t use this for client-side integrations (also known as device-mode destinations).\n\nUse Cases\n\nThere are two scenarios where you might want to use the Event Tester:\n\nensuring an event is successfully making it to a specific destination\nensuring your new destination is configured correctly\nEnsuring an event is successfully making it to a specific destination\n\n1. Choose an event from the Source Debugger that you want to debug and select \u201cValidate\u201d\n\nGo to your Source Debugger, select an event and in the top right hand side of the debugger view, select \u201cValidate\u201d.\n\n2. Choose the destination you want to test with\n\nSelect the destination that you want to test this event with. At this time, you can only use the Event Tester for cloud-mode (server side) destinations.\n\n3. Send event to destination\n\nThe event payload from your debugger that you just selected will automatically load in the JSON view. You have the option to edit the payload if you want. Assuming it looks good, select \u201cSend Event\u201d at the bottom right of the screen.\u00a0\n\n4. Ensure you\u2019re happy to send the test event to the destination\n\nThis is a real event that will appear in your end tool alongside your existing data. If you\u2019re not comfortable with this, then select \u201cCancel\u201d and do not send the event.\u00a0\n\n5. View the Partner API response\n\nOn the right hand side of the Event Tester you will see the response from the partner API. At the top, Segment provide of summary of the response. Below is the raw response payload Segment received that you can use for further debugging if necessary.\u00a0\n\nIf you are receiving an error and are unsure how to fix the issue, visit the partner docs (for example https://developers.google.com/analytics/devguides/reporting/core/v3/errors) or contact the partner support team.\u00a0\n\nFAQ\nWhy can\u2019t I see the Event Tester when I log into my workspace?\n\nThe Event Tester is only accessible to users with write access in their Segment workspace (read-only users will not see the Event Tester in their workspace).\u00a0\n\nThe Event Tester experienced an error when sending my event. Why did this happen?\n\nIf you experience an error, let Segment know and the Segment team will help you troubleshoot the issue.\n\nIs this feature available for Data Lakes?\n\nThe Event Tester is not available for Data Lakes.\n\nWhy are my destination filters being ignored?\n\nEvents passed into the Event Tester bypass destination filters. Destination filters are applied to events as they are sent to specific destinations. However, the Event Tester is designed to help you troubleshoot your Sources, their configuration, and their downstream destinations by showing a sample of the data available. It allows you to check that data is being sent, and that it\u2019s in the correct format without the filters being applied. This means that when you use the Event Tester, you\u2019re seeing the data before any destination filters or other processing rules are applied, providing a clear view of the raw event data as it comes from the source.\n\nThis page was last modified: 13 Jun 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nUse Cases\nEnsuring an event is successfully making it to a specific destination\nFAQ\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nTraits\n/\nPredictions\n/\nUsing Predictions\nUsing Predictions\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY PLUS \u2713\n?\nWorking with Predictions in Segment\n\nPredictions are stored as computed traits in user profiles, with scores represented as percentage cohorts. For example, a score of 0.8 indicates the user is in the 80th percentile, or the top 20% of the cohort.\n\nAfter selecting a cohort, use Predictions with the following Segment features:\n\nAudiences, build new audiences using Predictions as a base. Segment also provides prebuilt Suggested Predictive Audiences as part of Engage..\nJourneys; use Predictions in Journeys to trigger Engage marketing campaigns when users enter a high-percentage cohort, or send promotional material if a customer shows interest and has a high propensity to buy.\nDestinations; send your Predictions downstream to Warehouses, support systems, and ad platforms.\nPrediction tab\n\nYou can access generated Predictions in the Prediction tab of your Trait. The Prediction tab gives you actionable insight into your prediction.\n\nThe Explore your prediction section of the Prediction tab visualizes prediction data and lets you create Audiences to target. An interactive chart displays a percentile cohort score that indicates the likelihood of users in each group to convert on your chosen goal. You can choose the top 20%, bottom 80%, or create custom ranges for specific use cases.\n\nYou can then create an Audience from the group you\u2019ve selected, letting you send efficient, targeted marketing campaigns within Journeys. You can also send your prediction data to downstream destinations.\n\nModel monitoring\n\nPredictions rank your customers by their likelihood to perform a specific conversion event, from most to least likely.\n\nFor each custom prediction, Segment monitors the percentile cohort where customers were ranked when they performed the predicted conversion event. After around 7 days, Segment creates a graph data visualization, allowing you to evaluate the prediction\u2019s accuracy based on real workspace data.\n\nFor example, suppose you\u2019re predicting the likelihood of customers completing an order_completed event. The graph shows that:\n\nCustomers in the 91\u2013100% cohort performed the event about 6,700 times.\nCustomers in the 81\u201390% cohort performed the event about 3,900 times.\nCustomers in the 71\u201380% cohort performed the event about 3,000 times.\n\nThis pattern shows that the prediction was extremely accurate in identifying customers most likely to convert. Ideally, most graphs will show a similar trend, where the highest-ranked cohorts have the most conversion activity.\n\nHowever, this pattern can change depending on how you use Predictions. For example, if you run a marketing campaign targeting the bottom 10% cohort, you might see an increase in conversions for that group instead.\n\nLike any AI or machine learning tool, Predictions may not always be perfect. Start small, test your predictions, and refine your approach as needed. Model monitoring makes it easier to measure and improve the accuracy of your predictions.\n\nModel statistics\n\nThe Predictions tab\u2019s Understand your prediction section provides insights into the performance of the underlying predictive model. This information helps you understand the data points that contribute to the prediction results.\n\nThe Understand your prediction dashboard displays the following model metrics:\n\nAUC, or Area under the ROC curve; AUC values range from 0 to 1, with 1 indicating a perfect prediction and 0 indicating the opposite. Higher AUC indicates better predictions.\nLift Quality, which measures the effectiveness of a predictive model. Segment calculates lift quality as the ratio between the results obtained with and without the predictive model. Higher lift quality indicates better predictions.\nLog Loss; the more a predicted probability diverges from the actual value, the higher the log-loss value will be. Lower log loss indicates better predictions.\nTop contributing events; this graph visually describes the events factored into the model, as well as the associated weights used to create the prediction.\nPredictions use cases\n\nPredictions offer more value in some situations than others. This sections covers common scenarios where predictions have high impact, as well as others where alternative approaches may be more appropriate.\n\nMarketing opportunities\nImprove ad targeting; build targeted audience segments based on predictive behavior.\nOptimize campaign performance; reduce customer acquisition costs (CAC), and improve customer lifetime value (LTV) by building campaigns that target customers most likely to purchase or perform another desired action.\nPower more personalization; With Predictions, you can deliver the right message at the right time. You can create targeted customer Journeys with personalized offers and recommendations that boost conversion and promote upsell and cross sell.\nWin back unengaged customers; Predictions let you identify unengaged customers you can re-engage with personalized winback campaigns.\nData science use cases\nModel improvement; You can extract Predictions from Segment and use them to improve proprietary machine learning models.\nTesting experiences; data teams can validate and strengthen existing machine learning models by testing proprietary models against Segment\u2019s out-of-the-box models.\nSave time on predictive modeling; data science teams can use Segment\u2019s predictive models, freeing up time to building other in-house models like inventory management and fraud alerting.\nWhen to use a prediction\n\nPredictions are most effective in the following situations:\n\nWhen your desired outcome is difficult to measure and not clearly defined, like activation, retention, engagement, or long-term value Journeys.\nWhen your product has more than 100,000 average monthly users; smaller sample sizes lead to less accurate statistical conclusions.\nWhen you need to save time building cohorts; Predictions lets marketers access and take action on predictive data without the help of data science teams, while also giving data teams out-of-the-box machine learning models they can use in downstream tools.\nWhen other approaches work better\n\nPredictions may not be as beneficial in the following situations:\n\nWhen you sell limited but highly-priced items, like enterprise software, complex medical machines, and so on; this also applies if you\u2019re in the B2B sector.\nWhen you don\u2019t yet have enough data; your model could produce errors if, for example, your target is too new and lacks sufficient data. Waiting a month could allow Segment to gather more predictive data.\nFAQs\nWhat type of machine learning model does Segment use?\n\nSegment uses a binary classification model that uses decision trees.\n\nWhat level of confidence can I have in my predictions?\n\nOnce Segment creates your prediction, you can check the model statistics page, where Segments shows you how the model was created. Segment also maintains automated systems that monitor model performance and will alert you if your model is not predictive.\n\nHow long do predictions take to create?\n\nTrait creation depends on the amount of data, but Segment expects predictions to be completed in around 24 hours. For larger customers, however, this could take 48 hours. Predictions shows a status of In Progress while computing; Segment updates this status when customers are scored.\n\nWhat are AUC, log loss, and lift quality?\n\nThese data science statistics measure the effectiveness of Segment\u2019s predictions when tested against historical data. For more information, refer to ROC Curve and AUC, The Lift Curve in Machine Learning, and Intuition behind log-loss score.\n\nWhat is the Prediction Quality Score?\n\nThe Prediction Quality Score factors AUC, log loss, and lift quality to determine whether Segment recommends using the prediction. A model can have a score of Poor, Fair, Good, or Excellent.\n\nHow does Segment store trait values?\n\nThe created trait value represents the user\u2019s percentile cohort. This value will refresh when we re score the customers based on your refresh cadence. If you see 0.85 on a user\u2019s profile, this means the user is in the 85th percentile, or the top 15% for the prediction.\n\nHow frequently do you re-train the model?\n\nSegment rebuilds the machine learning model every 30 days.\n\nHow frequently do you update trait values?\n\nBy default, Segment refreshes scores every 7 days. However, you can request that trait values update daily. Reach out to your CSM to determine your eligibility.\n\nCan I update Predictive Traits and Predictive Audiences?\n\nPredictive Traits can\u2019t be updated, but Predictive Audiences can. To modify a Predictive Trait, you\u2019ll need to recreate it.\n\nHow many predictions can I have?\n\nYou get five predictions as part of Engage Foundations or Unify Plus. To purchase more predictions, reach out to your CSM.\n\nPredictive Audiences contribute to the Engage limit of 100 audiences. Whether you create the audience manually or with predictive modeling, the audience counts towards the 100-audience limit.\n\nIs Predictions HIPAA eligible?\n\nYes.\n\nAre there any known Predictions limitations?\n\nYes. Keep the following in mind when you work with Predictions:\n\nPredictions made for more than 100 million users will fail. Segment recommends making predictions only for non-anonymous users, or, as an alternative, use the Starting Cohort to narrow down the audience for which you want to make a prediction.\nPredictions will not work as intended if you track more than 5,000 unique events in your workspace.\nPrediction is failing with error \u201cWe weren\u2019t able to create this prediction because your requested prediction event is not being tracked anymore. Please choose a different prediction event and try again.\u201d Predictions are computed based on the available data and the conditions specified for the trait. A gap in tracking events for seven continuous days could potentially affect the computation of the prediction. Nevertheless, once data tracking resumes and there is enough data, the prediction should be recomputed.\nHow is the average calculated?\n\nThe probabilities for all users are added together and then divided by the total number of users. If a user\u2019s score in \u201cLikelier to convert than average\u201d is below 1, it means they are less likely than the average user to convert.\n\nThis page was last modified: 26 Nov 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nWorking with Predictions in Segment\nPredictions use cases\nFAQs\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nCampaigns Overview\nCampaigns Overview\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE PREMIER \u2713\n?\n\nEngage Premier entered an End of Sale (EOS) period effective June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:\n\nTwilio Marketing Campaigns\n\nPreferred ISV Partners:\n\nAirship Blog\nBloomreach Blog\nBraze Blog\nInsider Blog\nKlaviyo Blog\nTwilio Engage Foundations Documentation\n\n\nWith Engage, you can build email and SMS marketing campaigns within Journeys.\n\nUse real-time data and unified customer profiles to send personalized messages to subscribed users. Build and send email and SMS campaigns for multi-channel customer engagement.\n\nVisit Segment\u2019s Journeys documentation for more information on how to build a Journey.\n\nSend personalized campaigns\n\nUse Engage to send email and SMS campaigns in Journeys:\n\nSend email and SMS messages to subscribed users as a step in a Journey.\nInsert real-time profile traits from merge tags to personalize messages.\nAdd emojis to customize and add creativity to your campaigns.\nTest your email and SMS before you include them in campaigns.\nBuild email and SMS templates\n\nWith Engage, you can build email and SMS templates to use throughout your campaigns. Build an email template using a drag and drop or a visual HTML editor. Personalize templates with merge tags and test your messages before you send them in campaigns.\n\nEngage saves the message templates for you to preview, maintain, and reuse throughout your campaigns.\n\nThis page was last modified: 15 Jul 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSend personalized campaigns\nBuild email and SMS templates\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nTraits\n/\nRecommended Items\nRecommended Items\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY PLUS \u2713\n?\n\nWith Recommended Items, you can add personalized item recommendations as a computed trait to each user profile.\n\nBased on a user\u2019s past interactions, this trait generates a list of up to 5 items, like products, articles, or songs, that each user is most likely to engage with.\n\nSegment designed Recommended Items for cases where you want to personalize experiences, like email content, in-app recommendations, or website suggestions, to fit each user\u2019s unique preferences.\n\nOn this page, you\u2019ll learn how Recommended Items works, how to create a Recommended Item trait, and best practices to get the most out of your recommendations.\n\n.\n\nHow Recommended Items works\n\nRecommended Items uses your interaction events (like order_completed, product_added, and product_searched) along with event metadata to generate personalized recommendations for each user. Here\u2019s an overview of the process:\n\nData collection: Segment captures user interactions from your chosen events.\nPattern analysis: Machine learning models analyze these interactions to recognize patterns and user preferences.\nItem ranking: Based on this analysis, Segment generates an ordered list of recommended items for each user, ranked from most to least likely to engage.\nProfile storage: Segment then saves these recommendations as an array on each eligible user profile.\n\nOnce Segment attaches the recommendation array to a profile, you can use it to:\n\nPersonalize experiences with the Profile API\nSend Recommended Items traits to downstream destinations\nBuild further segments based on Recommended Items\nTrigger customized campaigns and experiences tailored to individual users\nCreate a Recommended Items trait\n\nBefore you begin\n\nBefore you create Recommended Item traits, you\u2019ll first need to set up a Recommendation Catalog. The catalog setup process involves mapping your interaction events and providing product metadata to support recommendations. If you haven\u2019t yet set up your Recommendation Catalog, follow the steps in the Product Based Audiences documentation.\n\nTo create a Recommended Item trait:\n\nIn your Segment workspace, navigate to Unify > Traits > + Create computed trait.\nIn the New Computed Trait builder, click Recommendation, then click Next.\nIn Select users, click + Add condition to choose the users who should receive recommendations.\nYou can create recommendations for up to 2 million non-anonymous customers.\nIn Define recommended items, choose the item type you want to recommend.\nThis is based on your product catalog.\nChoose how many item types you want to return onto each profile.\nYou can select up to 5 item types.\nClick Calculate to get a preview of the number of users who will receive your recommendations, then click Next.\n(Optional) Select destinations you want to sync the trait to, then click Next.\nGive your trait a name, then click Create Trait.\n\nSegment begins creating your new trait. This process could take up to 48 hours.\n\nExample use case: personalized album recommendations\n\nSuppose you\u2019re managing a music streaming app and want to give each user personalized music recommendations based on their listening habits.\n\nHere\u2019s how you could configure this trait:\n\nSTEP\tCONFIGURATION\nSelect users\tUse an audience based on up to 2 million active, non-anonymous listeners who played at least one song in the past month.\nItem type\tSelect Albums as the item type to recommend. Because you have an extensive catalog of music, this lets each listener receive recommendations tailored to their interests.\nNumber of item types\tYou decide to return a maximum of 5 albums for each profile, keeping the recommendations relevant and concise.\nCalculate\tClicking Calculate gives you an overview of how many users will receive the album recommendations. Use it to ensure your conditions and catalog mapping meet your criteria.\nSync to destinations\tThis optional step lets you sync the trait to third-party destinations to deliver album recommendations over email, in-app messaging, or push notifications.\nTrait naming\tName your trait Personalized Album Recommendations, making it easy to identify for future campaigns.\n\nBy setting up a trait like this, each user profile now includes personalized recommendations that reflect individual tastes. You can use these recommendations across a range of touchpoints, like in-app sections, personalized email content, or targeted messaging, to create a more engaging and customized user experience.\n\nBest practices\n\nKeep the following in mind as you work with Recommended Items:\n\nLimit recommendations to key items: Start with 5-7 items per profile. This keeps recommendations concise and tailored to each user\u2019s preferences.\nConsider audience size: Larger audiences can dilute engagement rates for each recommended item. Focusing on the top 20% of users keeps recommendations relevant and impactful.\nGive the system time to build the trait: Recommended Item traits can take up to 48 hours to build, depending on data volume and complexity. Segment recommends waiting until 48 hours have passed before using the trait in campaigns.\n\nThis page was last modified: 31 Oct 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nHow Recommended Items works\nCreate a Recommended Items trait\nExample use case: personalized album recommendations\nBest practices\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nSegment App\n/\nProd and Testing Environments in Segment\nProd and Testing Environments in Segment\n\nWe recommend that instead of setting up separate workspaces for different environments (local/development/prod), you set up one workspace and make each of these environments a different source.\u00a0\n\nWe bill per workspace, and on our Team or Business plans you can create as many sources as you need. Each Segment source will have its own Write Key, so you can easily keep things separate.\n\nFor each source, you also get to choose which integrations you want it to send data to.\n\nThis page was last modified: 22 Jan 2020\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSpec\n/\nSpec: Alias\nSpec: Alias\n\nThe Alias method is an advanced method used to merge 2 unassociated user identities, effectively connecting 2 sets of user data in one profile.\n\nAlias and Unify\n\nAlias calls can\u2019t be used to merge profiles in Unify. For more information on how Unify merges user profiles, view the Identity Resolution documentation.\n\nAlias is an advanced method\n\nThe Alias method allows you to explicitly change the ID of a tracked user. This should only be done when it\u2019s required for downstream destination compatibility. See the Best Practices for Identifying Users docs for more information.\n\nSyntax\n\nThe Alias call has the following fields:\n\nFIELD\t\u00a0\tTYPE\tDESCRIPTION\nuserId\t\u00a0\tString\tThe userId is a string that will be the user\u2019s new identity, or an existing identity that you wish to merge with the previousId. See the User ID docs for more detail.\npreviousId\toptional\tString\tThe previousId is the existing ID you\u2019ve referred to the user by. It might be an Anonymous ID assigned to that user or a User ID you previously identified them with using Segment\u2019s Identify call.\noptions\toptional\tObject\tA dictionary of options. For example, enable or disable specific destinations for the call.\ncallback\toptional\tFunction\tA function that is executed after a timeout of 300 ms, giving the browser time to make outbound requests first.\n\nThe Alias method follows the format below:\n\nanalytics.alias(userId, [previousId], [options], [callback]);\n\n\nHere\u2019s the payload of a basic Alias call that will associate this user\u2019s existing id (email address) with a new one (a database ID), with most common fields removed:\n\n{\n  \"type\": \"alias\",\n  \"previousId\": \"jen@email.com\",\n  \"userId\": \"507f191e81\"\n}\n\n\nHere\u2019s the corresponding JavaScript event that would generate the above payload. If you\u2019re using Segment\u2019s JavaScript library, Segment automatically passes in the user\u2019s anonymousId as previousId for you:\n\nanalytics.alias(\"507f191e81\");\n\n\nIf you\u2019re instrumenting a website, the Anonymous ID is generated in the browser so you must call Alias from the client-side. If you\u2019re using a server-side session ID as the Anonymous ID, then you must call Alias from the server-side.\n\nBased on the library you use, the syntax in the examples might be different. You can find library-specific documentation on the Sources Overview page.\n\nExamples\n\nHere\u2019s a complete example of an Alias call:\n\n{\n  \"anonymousId\": \"507f191e810c19729de860ea\",\n  \"channel\": \"browser\",\n  \"context\": {\n    \"ip\": \"8.8.8.8\",\n    \"userAgent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36\"\n  },\n  \"integrations\": {\n    \"All\": true,\n    \"Mixpanel\": false,\n    \"Salesforce\": false\n  },\n  \"messageId\": \"022bb90c-bbac-11e4-8dfc-aa07a5b093db\",\n  \"previousId\": \"39239-239239-239239-23923\",\n  \"receivedAt\": \"2015-02-23T22:28:55.387Z\",\n  \"sentAt\": \"2015-02-23T22:28:55.111Z\",\n  \"timestamp\": \"2015-02-23T22:28:55.111Z\",\n  \"type\": \"alias\",\n  \"userId\": \"507f191e81\",\n  \"version\": \"1.1\"\n}\n\n\nThis page was last modified: 24 Jan 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSyntax\nExamples\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nSegment for Developers\nSegment for Developers\n\nThis guide explains all you need to know to get started with your Segment implementation, and directs you to more resources depending on your specific needs.\n\nIf you haven\u2019t already, you should read the detailed explanation of Segment on the previous page!\n\nSegment University: Segment in Action\n\nSee a quick example of Segment working on an ecommerce website. (Must be logged in to access.)\n\nWhat does Segment do?\n\nSegment sends messages about activities in your mobile apps, websites or servers, receives those messages, and translates and forwards the message content to Destination tools. It also can send the contents of those messages to a bulk storage destination for archiving. In more complicated implementations, Segment can serve as a wrapper to trigger messages directly to other APIs, and can inspect, correct, classify and block the message contents.\n\nTypes of Segment messages\n\nSegment\u2019s libraries generate and send messages to our tracking API in JSON format, and provide a standard structure for the basic API calls. We also provide recommended JSON structure (also known as a schema, or \u2018Spec\u2019) that helps keep the most important parts of your data consistent, while allowing great flexibility in what other information you collect and where.\n\nThere are six calls in the basic tracking API, which answer specific questions:\n\nIdentify: Who is the user?\nTrack: What are they doing?\nPage: What web page are they on?\nScreen: What app screen are they on?\nGroup: What account or organization are they part of?\nAlias: What was their past identity?\n\nAmong these calls, you can think of Identify, Group, and Alias as similar types of calls, all to do with updating our understanding of the user who is triggering Segment messages. You can think of these calls as adding information to, or updating an object record in a database. Objects are described using \u201ctraits\u201d, which you can collect as part of your calls.\n\nThe other three, Track, Page, and Screen, can be considered as increasingly specific types of events. Events can occur multiple times, but generate separate records which append to a list, instead of being updated over time.\n\nA Track call is the most basic type of call, and can represent any type of event. Page and Screen are similar and are triggered by a user viewing a page or screen, however Page calls can come from both web and mobile-web views, while Screen calls only occur on mobile devices. Because of the difference in platform, the context information collected is very different between the two types of calls.\n\nTip! Segment recommends that you always use the Page and Screen calls when recording a page-view, rather than creating a \u201cPage Viewed\u201d event, because the Page/Screen calls automatically collect much better context information.\n\nAnatomy of a Segment message\n\nThe most basic Segment message requires only a userID or anonymousID; all other fields are optional to allow for maximum flexibility. However, a normal Segment message has three main parts: the common fields, the \u201ccontext\u201d object, and the properties (if it\u2019s an event) or traits (if it\u2019s an object).\n\nThe common fields include information specific to how the call was generated, like the timestamp and library name and version. The fields in the context object are usually generated by the library, and include information about the environment in which the call was generated: page path, user agent, OS, locale settings, etc. The properties and traits are optional and are where you customize the information you want to collect for your implementation.\n\nAnother common part of a Segment message is the integrations object, which you can use to explicitly filter which destinations the call is forwarded to. However this object is optional, and is often omitted in favor of non-code based filtering options.\n\nMessage schemas, Blocks, and Specs\n\nThe Segment \u201cSpecs\u201d provide recommended message schemas - the information we recommend that you collect - for each type of call. These are recommendations not requirements, but if you follow these schema guidelines the Segment servers can more easily identify parts of your messages, and translate them to downstream tools.\n\nIn addition to the recommended message schemas, Segment also provides \u201cblocks\u201d: recommendations on what information to collect and how to format it, for different industries and use cases. These are recommendations only, but by collecting all of the information in these blocks, you can ensure that common tools used in that use-case have the information they need to function.\n\nA third section of the Spec is the \u201cindustry specs\u201d which provide recommendations that include an explicit translation or mapping in the Segment servers, to best power the downstream Destinations commonly used in these industries.\n\nSources and Destinations\n\nWhen you start out, you create a Workspace, which serves as a container for all of your Sources and Destinations.\n\nSegment has Sources and Destinations. Sources send data into Segment, while Destinations receive data from Segment.\n\nSegment has five types of sources: Web (Analytics.js), Mobile, Server, and Cloud App, plus a fifth type: User-created Source Functions. Web, Mobile, and Server sources send first-party data from your digital properties. Cloud-app sources send data about your users from your connected web apps, for example a ticketing system such as Zendesk, a payments system such as Stripe, or a marketing tool like Braze.\n\nConnection modes\n\nSegment has several types of sources, and many destinations can accept data from all of them. However, some are only compatible with specific source types (for example, web only, or server only). To find out which source types a specific destination can accept data from, check the documentation for that destination for a \u201cSupported Sources and Connection Modes\u201d section.\n\nSegment\u2019s web source (Analytics.js), and native client-side libraries (iOS, Android, React-native) allow you to choose how you send data to Segment from your website or app. There are two ways to send data:\n\nCloud-mode: The sources send data directly to the Segment servers, which then translate it for each connected downstream destination, and send it on. Translation is done on the Segment servers, keeping your page size, method count, and load time small.\n\nHealthcare and Life Sciences (HLS) customers can encrypt data flowing into their destinations\n\nHLS customers with a HIPAA eligible workspace can encrypt data in fields marked as Yellow in the Privacy Portal before they flow into an event stream, cloud-mode destination.\nTo learn more about data encryption, see the HIPAA Eligible Segment documentation\n\nDevice-mode: You include additional code on your website or mobile app which allows Segment to use the data you collect on the device to make calls directly to the destination tool\u2019s API, without sending it to the Segment servers first. (You still send your data to the Segment servers, but this occurs asynchronously.) This is also called wrapping or bundling, and it might be required when the source has to be loaded on the page to work, or loaded directly on the device to function correctly. When you use Analytics.js, you can change the device-mode destinations that a specific source sends from within the Segment web app, without touching any code.\n\nIf you use Server source libraries, they only send data directly to Segment in Cloud-mode. Server library implementations operate in the server backend, and can't load additional destination SDKs.\n\nTo learn more about connection modes and when you should use each, see the details in the Destinations docs.\n\nPlanning your Segment implementation\n\nThe journey of a thousand miles begins, ideally, with a plan. Regardless of if you\u2019re a new company just implementing analytics for the first time, or a multi\u2013national corporation modernizing your analytics stack, it\u2019s a great idea to start with a Tracking Plan. For new implementations, this can be as simple as a document where you write down these four things for each item you track:\n\nWhat am I tracking? (What is the event name or type?)\nWhy am I tracking it? (What questions does this data answer?)\nFor whom am I tracking it? (Who owns this question, tool, or business area?)\nWhere (which destination tools) do I want to send this data to?\n\nIf you\u2019re a large or long-established organization and you\u2019re replacing existing tools, you\u2019ll want to spend more time on this to maintain analytic parity and continuity of tooling. We highly recommend reading up on tracking plans and schemas for Protocols, our tool for managing and sharing tracking plans and enforcing schemas.\n\nRegardless of your organization\u2019s size or age, you\u2019ll want to take an inventory of the destination tools you\u2019ll be using with Segment, and make a list of the connection modes each one accepts. This makes it easier to check off when you\u2019ve implemented each one, so you\u2019re not missing anything.\n\nHow do I test if it\u2019s working?\n\nThere are several ways to check if your data is flowing. One is the Debugger tab in each Source in the Segment web app, where you can see data coming from a source into Segment. Another is the Event Delivery tool which shows which data is arriving at specific destinations.\n\nFor monitoring purposes, you\u2019ll also see alerts in the Workspace Health tool if your sources or destinations produce repeated errors.\n\nHow do I filter my data?\n\nThere are several different ways to ensure that you can collect your data once, but filter it out of specific destinations. See Filtering Data for a list of the available methods and descriptions.\n\nTroubleshooting\n\nIf you\u2019re seeing errors thrown by your destinations, you might have an implementation issue. See the Integration Error Codes list or contact our Success engineering team for help.\n\nHave suggestions for things to add to this guide? Drop us a line.\n\nSegment Terraform Provider\n\nSegment has a Terraform provider, powered by the Public API, that you can use to manage Segment resources, automate cloud deployments, and change control. Take a look at the Segment provider documentation on Terraform to see what\u2019s supported.\n\nThis page was last modified: 09 Apr 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nWhat does Segment do?\nTypes of Segment messages\nAnatomy of a Segment message\nMessage schemas, Blocks, and Specs\nSources and Destinations\nConnection modes\nPlanning your Segment implementation\nHow do I test if it\u2019s working?\nHow do I filter my data?\nTroubleshooting\nSegment Terraform Provider\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify Overview\nUnify Overview\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\n\nUse Segment Unify, formerly known as Profiles, for a complete view of your customers.\n\nWith Identity Resolution, track every interaction across the entire user journey to create unified, real-time customer identities. View user profiles in one place through the Profile explorer in the Segment app. Use the Profile API to programmatically query user profiles, traits, and events.\n\nYou can then use this interaction data with customer engagement tools, such as Engage, to deliver personalized, omnichannel experiences.\n\nIf you need to troubleshoot or learn about your profile data, use Profiles Insights for a transparent view of your Unify profiles.\n\nGetting started\n\nUnify is an add-on to Segment Connections Business Tier. It\u2019s also a required add-on for Twilio Engage. To use Computed Traits and Audiences with Unify, you must have access to Engage.\n\nTo set up and get data flowing through Unify, visit Segment\u2019s Onboarding Guide.\n\nIdentity Resolution\n\nSet Identity Resolution rules to take event data from across devices and channels and intelligently merge it into complete user- or account-level profiles. This enables you to understand customer behavior as it evolves in real-time across multiple touchpoints.\n\nWith Identity Resolution:\n\nUnderstand behaviors that lead a user from an anonymous window shopper to a loyal customer.\nTrack customer activity across multiple devices and apps.\nLearn how a user interacts with your brand through different channels and departments.\n\nVisit Segment\u2019s Identity Resolution docs to learn more.\n\nProfile explorer\n\nUse the Profile explorer to view all user data, including their event history, traits, and identifiers.\n\nWith the Profile explorer, you have a complete view of your customers.\n\nVisualize unified profiles: Explore profiles from a single location in Segment to understand who\u2019s using your product.\nEnsure quality data: Be sure that the data you receive is the data you expect.\nProvide sales and support context: Look up a user profile to understand where they are on their journey with your business or product.\n\nIf you\u2019re using Engage, use the Profile explorer to view audiences, traits, journey membership, and subscription states for email and phone numbers.\n\nEnrich profiles with traits\n\nWith Unify Plus, you can add detail to user profiles with new traits and use them to power personalized marketing campaigns. Add new traits to your user or account profiles using:\n\nComputed Traits: Use the Unify drag-and-drop interface to build per-user (B2C) or per-account (B2B) metrics on user profiles (for example, \u201clifetime value\u201d or \u201clead score\u201d).\nSQL Traits: Run custom queries on your data warehouse using the Unify SQL editor, and import the results into Segment. With SQL Traits, you can pull rich, uncaptured user data back into Segment.\nPredictions: Predict the likelihood that users will perform custom events tracked in Segment, like LTV, churn, and purchase.\nProfile API\n\nUse Segment\u2019s Profile API to programmatically access all traits stored for a user. This includes the external_ids, traits, and events that make up a customer\u2019s journey with your product.\n\nUse the Profile API to help your organization:\n\nBuild in-app recommendations.\nEmpower your sales and support teams with complete customer context.\nCreate personalized marketing campaigns.\nQualify leads faster.\n\nVisit Segment\u2019s Profile API doc for more information.\n\nProfiles Insights\n\nUse Profiles Insights to troubleshoot your event data with a transparent view of your Unify profiles.\n\nLearn about your events and identifiers on your profiles and answer questions such as why two profiles didn\u2019t merge, why an event wasn\u2019t resolved to a profile, or why an external ID isn\u2019t present.\n\nVisit the Profiles Insights doc to learn more.\n\nProfiles Sync\n\nUse Profiles Sync to connect identity-resolved customer profiles to a data warehouse of your choice.\n\nWith a continual flow of synced profiles, teams can enrich and use these data sets as the basis for new audiences and models. Profiles Sync addresses a number of use cases, with applications for identity graph monitoring, attribution analysis, machine learning, and more.\n\nVisit the Profiles Sync Setup doc to learn more.\n\nNext steps: activate your profiles with Engage\n\nFor Engage users, after you set up your identity rules and have data flowing through Unify, you can activate profiles to deliver personalized engagement experiences. Visit the Engage docs to learn more.\n\nThis page was last modified: 26 Jun 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nGetting started\nIdentity Resolution\nProfile explorer\nEnrich profiles with traits\nProfile API\nProfiles Insights\nProfiles Sync\nNext steps: activate your profiles with Engage\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nJourneys Overview\nJourneys Overview\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nJourneys, a feature of Engage, provides a way for marketers to personalize experiences through planning how and when to engage customers with the right campaigns and messages.\n\nJourneys enable you to define steps in a user\u2019s journey based on event behavior and traits. You can build Journeys from your tracking events, traits, computed traits, or audiences. At each step of a journey, you can send your list of users to any Engage-compatible destination.\n\nGet started\n\nStart with the visual builder to define entrance criteria, build out conditional branching logic, then focus messaging to drive conversion. Repeat purchase campaigns, trial conversions, and onboarding flows are great examples to get started from. For more information, see Build a Journey.\n\nSend data to your destinations\n\nConnect destinations to your Journey to send events or user lists when users reach the corresponding step in the Journey. For more information, see Send Journeys data to a Destination.\n\nBest practices and FAQ\n\nFor information about best practices for getting started with Journeys, and to view frequently asked questions about Journeys, see Best Practices and FAQ.\n\nJourneys use cases\n\nSee Examples Journeys Use Cases for examples of ways you can use Journeys in your marketing workflow.\n\nJourneys glossary\n\nFor a list of key terms related to Journeys, see Journeys Key Terms.\n\nJourneys Product Limits\n\nFor information about Product Limits related to Journeys, see Product Limits - Journeys.\n\nThis page was last modified: 27 Sep 2022\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nGet started\nSend data to your destinations\nBest practices and FAQ\nJourneys use cases\nJourneys glossary\nJourneys Product Limits\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nSegment Documentation\n\nLearn how to use Segment to collect, responsibly manage, and integrate your customer data with hundreds of tools.\n\nGetting started with Segment\n\nLearn about Segment, plan and work through a basic implementation, and explore features and extensions.\n\nHow can Segment help you?\nSimplify data collection\n\nIntegrate the tools you need for analytics, growth, marketing, and more.\n\nProtect data integrity\n\nPrevent data quality issues with a tracking schema and enforcement with Protocols.\n\nPersonalize experiences\n\nBuild audiences and journeys from real-time customer data to personalize experiences on every channel.\n\nRespect users' privacy\n\nKeep customer data private with Segment's data discovery and policy enforcement tools.\n\nGet Data into Segment\n\nThe Segment Spec helps you identify, capture, and format meaningful data for use with Segment libraries and APIs as well as downstream tools.\n\nSegment calls\n\nUse Track, Page, Identify, and other Segment tracking calls.\n\nCommon traits\n\nSave time by letting Segment calls collect information for you.\n\nUse case specs\n\nUse our business-case specs to ensure that your tools get the most from your data.\n\nLearning about Segment\nSegment for Developers\n\nThe basics of your Segment implementation.\n\nHow-To Guides\n\nOver a dozen how-to guides that help you accomplish common tasks.\n\nConnect your app to Segment\nJavaScript\nSwift\nAll other Sources\nAdditional Resources\nTotally new to Analytics?\n\nSegment's Analytics Academy walks you through the wide world of analytics, including best practices, an overview of the most popular tools, and case studies of how other developers have achieved success.\n\nWant more hands-on guidance?\n\nFor a more hands-on tutorial of Segment, check out Segment University. It offers step-by-step instructions, starting with first steps and going through some of our more advanced features.\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nJourneys\n/\nEvent-Triggered Journeys\nEvent-Triggered Journeys\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nWith Event-Triggered Journeys, you can build real-time, event-based marketing workflows to automate and personalize customer journeys.\n\nUnlike traditional audience-based journeys that rely on pre-defined user segments, event-triggered journeys start automatically when users perform specific actions on your website or app.\n\nOn this page, you\u2019ll learn how to create an event-triggered journey, configure entry conditions, and work with published event-triggered journeys.\n\nPublic Beta\n\nEvent-Triggered Journeys is in public beta, and Segment is actively working on this feature. Some functionality may change before it becomes generally available. Event-Triggered Journeys is not currently HIPAA eligible.\n\nOverview\n\nEvent-triggered journeys help you create a responsive approach for time-sensitive use cases, like cart abandonment campaigns and transactional messages.\n\nWhere audience-based journeys activate based on aggregated conditions, event-triggered journeys respond instantly to individual events, delivering personalized experiences based on the full context of each event.\n\nOpt for an event-triggered journey in situations like these:\n\nWhen campaigns require real-time action in response to user behavior.\nFor transactional messages (like receipts and confirmations) that require specific event-based triggers.\nIn abandonment campaigns where a follow-up is needed if a corresponding completion event doesn\u2019t occur.\nBuild an event-triggered journey\n\nBefore you begin\n\nBefore you start building an event-triggered journey, make sure that you\u2019ve enabled all destinations you plan to send data to and that the events you want to use as triggers are already available in your Segment workspace.\n\nTo set up an event-triggered journey:\n\nIn your Segment workspace, navigate to Engage > Journeys, then click + Create journey.\nOn the Create journey page, select User performs an event, then click Next.\nGive your new journey a name and, optionally, a description.\nSelect entry event:\nChoose the event that will trigger user entry into the journey.\n(Optional) Use an audience filter to restrict entry to users who are already part of a specific audience when they perform the triggering event.\n(Optional) Apply filters based on event property values to refine entry conditions. For example, enter only if {property} = value A, value B, or value C.\nConfigure entry rules:\nRe-enter every time event occurs (default): Users enter the journey each time they trigger the specified event.\nEnter one time: Users enter the journey once only, regardless of repeated event triggers.\nIf you chose Re-enter every time event occurs in Step 5, select a unique identifier.\nBuild your journey using logical operators.\nConfigure event delivery to destinations by selecting a destination or setting up a custom destination function.\nPreview the contextual payload that Segment will send to your destination(s).\nAfter you\u2019ve finished setting up your journey, click Publish, then click Publish again in the popup.\nSend data to downstream destinations\n\nWhen a journey instance reaches a Send to Destination step, you can configure how data is sent to your desired destination. This step allows you to define where the data goes, what actions are performed, and how information is mapped, giving you control over the integration. Event-Triggered Journeys currently supports all Actions Destinations.\n\nFor other destinations or more complex logic, you can use Destination Functions.\n\nConfigure the Destination Send Step\n\nSelect a Destination\nChoose the destination where you want to send data. Currently, only Actions Destinations and Destination Functions are supported.\n\nChoose an Action\nSpecify the action to take within the selected destination. For example, you might update a user profile, trigger an email, or log an event.\n\nDefine the Event Name\nAdd a descriptive event name to send to your destination.\n\nDefine the Payload Attributes\nThe journey context provides a set of attributes from the entry event or events used in the Hold Until operator that can be included in the payload.\nYou may also add a user\u2019s profile traits to the destination payload.\nReview the available attributes and decide which ones to include in your data send.\nMap Attributes to Destination Keys\nUse the mapping interface to link payload attributes to the appropriate keys required by the destination.\nFor example, map user_email from the journey context to the email field expected by the destination.\nTest the Integration\nSend a test event to validate the configuration.\nEnsure that the data is received correctly by the destination and mapped as expected.\n\nWhen a journey reaches this step, the Segment prepares and sends the payload based on your configuration. The integration ensures compatibility with the selected destination\u2019s API, allowing seamless data transfer and execution of the specified action.\n\nJourney setup configuration options\n\nEvent-Triggered Journeys includes advanced options to help you tailor journey behavior and customize data delivery to downstream destinations.\n\nUnique identifiers\n\nUnique identifiers in event-triggered journeys help you manage multiple journey instances when a user triggers the same event more than once.\n\nWhen you select Re-enter every time event occurs when you create an event-triggered journey, you can choose an event property as a unique identifier. Selecting this option does two things:\n\nIt creates a separate journey instance for each unique identifier value, allowing multiple instances to run in parallel for the same user.\nIt ensures that any follow-up events link back to the right journey instance, preserving context for tracking and personalization.\n\nFor example, in an abandonment journey, suppose a user starts two applications (like application_started), each with a different application_id. By setting application_id as the unique identifier, Segment can match follow-up events (like application_completed) to the correct application journey. As a result, each journey instance only receives the completion event for its specific application.\n\nNotes and limitations\nSupported destinations: Only Actions Destinations in the Segment catalog are supported.\nData mapping: Ensure all required keys for the destination are properly mapped to avoid errors.\nBest practices\n\nFollow the best practices in this table to optimize your event-triggered journeys:\n\nRECOMMENDATION\tDETAILS\nUse specific event filters\tWhen you configure entry events, apply precise filters based on event property values to refine which users enter the journey. This helps target specific user actions and improves the journey\u2019s relevance.\nUse unique identifiers\tIf a journey allows users to enter multiple times, set a unique identifier to track each instance accurately. Using an identifier like application_id ensures that follow-up events stay associated with the right journey instance.\nPreview payloads before publishing\tReview the journey payload to verify that it includes all necessary context from the triggering event. This helps confirm that the data reaching destinations matches your campaign needs.\nTest journey after publishing\tConsider setting up a live test right after publishing to confirm that the journey behaves as expected and that data flows correctly to destinations.\nWorking with Event-Triggered Journeys\n\nSegment built Event-Triggered Journeys to respond instantly to events, offering real-time capabilities with a few considerations in mind.\n\nEntry event requirements: The entry event you use must already exist in your Segment workspace for it to appear as a selection in journey setup. Make sure that you\u2019ve already created the event before setting up your journey.\nEvent property filters: You can filter event properties using the equals or equals any of operators. When you apply multiple conditions, filters operate with AND logic, meaning all conditions must be true for the event to trigger entry into the journey.\nAudience filtering: You can only use active, pre-existing audience records as filters. For more complex filtering, like specific profile traits or multiple audiences, first create the audience in Engage > Audiences, then apply it as a filter once it\u2019s live.\nDestination options: While Event-Triggered Journeys support all actions-based destinations and Destination Functions, you can only add one destination per Send to Destination step. If you need to send to multiple destinations, you can use multiple Send to Destination steps.\nEvent payload structure: Each payload sent to a destination includes a unique key to identify the specific send step within the journey, rather than the journey instance itself. You can also set a custom event name to make it easier to identify the specific event instance you want to track in your destination.\nEditing and versioning: After you publish an event-triggered journey, you won\u2019t be able to edit it. To modify a journey, create a new journey.\nReal-time delivery: Event-Triggered Journeys aim for an expected delivery time of under 5 minutes from the moment an event is performed to when the payload reaches the destination, assuming there is no delay step in the journey. However, external factors outside of Segment\u2019s control may occasionally introduce latency.\nUse Cases\n\nEvent-Triggered Journeys can power a variety of real-time, personalized experiences. This section details some common scenarios to help you see how they might work in practice.\n\nReal-time event forwarding\n\nSuppose you want to instantly send a personalized message whenever a user completes a specific action on your site, like filling out a form or subscribing to a service. With Event-Triggered Journeys, you can configure the journey to trigger each time this entry event occurs. Segment will forward the event data, including all relevant details, to your connected destination in real-time.\n\nReal-time abandonment Campaigns\n\nImagine you\u2019re running an e-commerce site and want to follow up with users who start the checkout process but don\u2019t complete it within a certain timeframe. You can create an event-triggered Journey to watch for abandonment cases like these.\n\nStart by setting the checkout_started event as the trigger and specify a unique identifier like session_id to track each user\u2019s journey instance. Then, configure the journey to check for the purchase_completed event within a defined window (for example, 1 hour). If the user doesn\u2019t complete the purchase, the journey can automatically send a nudge to encourage them to finish their order.\n\nPersonalized follow-up Messages\n\nSay you want to follow up with users after they engage with specific content, like downloading an e-book or watching a demo video. Event-Triggered Journeys can help you send timely, personalized messages based on these interactions.\n\nTo do this, set the entry event to content_downloaded or video_watched and configure the journey to send a follow-up email. You could even personalize the email with details from the triggering event, like the content title or timestamp, by configuring your destination payload to enrich the message with event-specific context.\n\nThis page was last modified: 19 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nOverview\nBuild an event-triggered journey\nBest practices\nWorking with Event-Triggered Journeys\nUse Cases\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nFunctions\n/\nThe Functions Editing Environment\nThe Functions Editing Environment\n\nSegment Functions create reusable code that can be run in your Segment workspace either as sources to format incoming events, or as destinations, to handle specific event types.\n\nWhen you create a function, write code for it, and save it, the function appears in the Catalog in your workspace only. You can then deploy that function in your workspace just as you would a conventional source or destination.\n\nAccess to Functions is controlled by specific access management roles. You may need additional access to create and deploy functions.\n\nCreating functions\n\nOnly Functions admins can create or edit functions.\n\nFrom your workspace, go to the Catalog and click the Functions tab.\nClick Create function.\n\nSelect the type of function you want to build, and click Build.\n\nWhen you click Build, a code editor appears. Different template code is available depending on which type of function you created.\n\nUse the editor to write the code for your function, configure settings, and test the function\u2019s behavior.\nOnce you finish writing your function, click Configure to give it a name.\nClick Create Function to save your work and make this function available in your workspace.\n\nAfter you click Create Function, the function appears on the Functions catalog page in your workspace.\n\nEditing a function\n\nIf you are a Workspace Owner or Functions Admin, you can manage your function from the Functions catalog page.\n\nIf you\u2019re editing an existing function, you can Save changes without changing the behavior of existing instances of the function.\n\nYou can also choose to Save & Deploy to push changes to all, or specific functions in your workspace that are already deployed. You might need additional permissions to deploy these changes.\n\nTesting a function\n\nYou have the option to test your functions code with either a sample event or by loading a default event that you can customize yourself.\n\nSample event: When you click Test with custom event you can select a sample event from any of your workspace sources to test this function.\nCustomize the event yourself: When you click customize the event yourself a default event payload loads which you can modify with the desired data. You have the option to paste in a JSON event or click Manual Mode and type in the fields manually. If you\u2019d like to locate a recent event from a source that\u2019s not available by following the sample event instruction:\nNavigate to the source debugger.\nClick the event you want to test and copy the raw JSON payload.\nPaste the raw JSON payload into your Function Editor.\n\nOnce the payload you want to test is ready, click Run.\n\nIf you create settings in your function, then you need to fill in the setting values before clicking Run.\n\nDeploying source functions\n\nYou must be a Workspace Owner or Source Admin to connect an instance of your function in your workspace.\n\nFrom the Functions tab, locate the source function you want to deploy.\nClick Connect Source and follow the prompts to configure the source. (You can access these settings later by navigating to the Source Settings page for your source function.)\nLocate the webhook URL for the source, either on the Overview or Settings \u2192 Endpoint page.\nCopy this URL and paste it into the upstream tool or service.\nDeploying destination functions\n\nIf you\u2019re editing an existing function, you can Save changes without changing the behavior of your deployed function. You can also choose to Save & Deploy to push changes to all, or specific functions in your workspace that are already deployed.\n\nWhen you deploy your destination function in your workspace, you fill out the settings on the destination configuration page, similar to how you would configure a normal destination.\n\nFunctions Versioning\n\nWith Functions Versioning, you can access a complete change history for each source or destination function. View version history and creation details, then use a unified or split display to compare code and restore previous versions of a function.\n\nView and compare version history\n\nTo view the version history of a function:\n\nNavigate to Connections > Catalog > Functions.\nSelect your source or destination function.\nSelect Edit Function, then click Version history.\n\nSelect previous versions to compare code using a unified or split view. With the split view, Segment displays the latest version on the left and the version you\u2019ve selected on the right.\n\nUnified and split compare screens are read-only. While you can copy code, you can\u2019t make changes directly from these screens.\n\nLATEST and DEPLOYED versions\n\nIn the Version History panel, Segment displays LATEST and DEPLOYED labels that represent a function version state. You\u2019ll see the LATEST version at the top.\n\nSegment labels a version as the LATEST when:\n\nYou save a change to the function source code, but don\u2019t deploy the function at the same time.\nYou restore a previous version from your function\u2019s version history.\n\nThe DEPLOYED version is the function version that\u2019s currently deployed.\n\nRestore a previous version\n\nTo restore a previous function version:\n\nSelect the function you want to restore.\nClick Restore this version.\nSegment creates a duplicate of the selected version and labels it as the LATEST version.\nClick Restore on the confirmation screen.\nTo deploy the restored version, click Save and Deploy on the Source Code screen.\nUse Versioning with Segment\u2019s Public API\n\nYou can use Functions Versioning with Segment\u2019s Public API to retrieve version history records and source code, as well as to restore previous versions.\n\nHere are some Public API use case examples:\n\nGet Version history: Use the /versions endpoint to retrieve a list of version records and metadata of a certain page size. You can also use this endpoint to get version source code for a given version ID.\n\nRestore a previous version: Use the /restore endpoint to restore a previous function version. This creates a new version with the same source as the version you are restoring.\n\nCreate or update versions: Create or update a function to add a version record and save the source code.\n\nDeploy a function: Use the Public API to deploy a function. After you deploy, Segment marks the function version as DEPLOYED. Learn more about function version states in the Latest and deployed versions section.\n\nView Segment\u2019s Public API docs for more information on how to use Functions Versioning with the Public API.\n\nFunctions permissions\n\nFunctions have specific roles which can be used for access management in your Segment workspace.\n\nAccess to functions is controlled by two permissions roles:\n\nFunctions Admin: Create, edit, and delete all functions, or a subset of specified functions.\nFunctions Read-only: View all functions, or a subset of specified functions.\n\nYou also need additional Source Admin permissions to enable source functions, connect destination functions to a source, or to deploy changes to existing functions.\n\n\ufe0fSettings and secrets\n\nSettings allow you to pass configurable variables to your function, which is the best way to pass sensitive information such as security tokens. For example, you might use settings as placeholders to use information such as an API endpoint and API key. This way, you can use the same code with different settings for different purposes. When you deploy a function in your workspace, you are prompted to fill out these settings to configure the function.\n\nFirst, add a setting in Settings tab in the code editor:\n\nClick Add Setting to add your new setting.\n\nYou can configure the details about this setting, which change how it\u2019s displayed to anyone using your function:\n\nLabel - Name of the setting, which users see when configuring the function.\nName - Auto-generated name of the setting to use in function\u2019s source code.\nType - Type of the setting\u2019s value.\nDescription - Optional description, which appears below the setting name.\nRequired - Enable this to ensure that the setting cannot be saved without a value.\nEncrypted - Enable to encrypt the value of this setting. Use this setting for sensitive data, like API keys.\n\nAs you change the values, a preview to the right updates to show how your setting will look and work.\n\nClick Add Setting to save the new setting.\n\nOnce you save a setting, it appears in the Settings tab for the function. You can edit or delete settings from this tab.\n\nRuntime and dependencies\n\nOn March 26, 2024, Segment is upgrading the Functions runtime environment to Node.js v18, which is the current long-term support (LTS) release.\n\nThis upgrade keeps your runtime current with industry standards. Based on the AWS Lambda and Node.js support schedule, Node.js v16 is no longer in Maintenance LTS. Production applications should only use releases of Node.js that are in Active LTS or Maintenance LTS.\n\nAll new functions will use Node.js v18 starting March 26, 2024.\n\nFor existing functions, this change automatically occurs as you update and deploy an existing function. Segment recommends that you check your function post-deployment to ensure everything\u2019s working. Your function may face issues due to the change in sytax between different Node.js versions and dependency compatibility.\n\nLimited time opt-out option\n\nIf you need more time to prepare, you can opt out of the update before March 19, 2024.\n\nNote that if you opt out:\n- The existing functions will continue working on Node.js v16.\n- You won\u2019t be able to create new functions after July 15, 2024.\n- You won\u2019t be able to update existing functions after August 15, 2024.\n- You won\u2019t receive future bug fixes, enhancements, and dependency updates to the functions runtime.\n\nContact Segment to opt-out or with any questions.\n\nNode.js 18\n\nSegment strongly recommends updating to Node.js v18 to benefit from future runtime updates, the latest security, and performance improvements.\n\nFunctions do not currently support importing dependencies, but you can contact Segment Support to request that one be added.\n\nThe following dependencies are installed in the function environment by default.\n\natob v2.1.2 exposed as atob\naws-sdk v2.488.0 exposed as AWS\nbtoa v1.2.1 exposed as btoa\nfetch-retry exposed as fetchretrylib.fetchretry\nform-data v2.4.0 exposed as FormData\n@google-cloud/automl v2.2.0 exposed as google.cloud.automl\n@google-cloud/bigquery v5.3.0 exposed as google.cloud.bigquery\n@google-cloud/datastore v6.2.0 exposed as google.cloud.datastore\n@google-cloud/firestore v4.4.0 exposed as google.cloud.firestore\n@google-cloud/functions v1.1.0 exposed as google.cloud.functions\n@google-cloud/pubsub v2.6.0 exposed as google.cloud.pubsub\n@google-cloud/storage v5.3.0 exposed as google.cloud.storage\n@google-cloud/tasks v2.6.0 exposed as google.cloud.tasks\nhubspot-api-nodejs exposed as hubspotlib.hubspot\njsforce v1.11.0 exposed as jsforce\njsonwebtoken v8.5.1 exposed as jsonwebtoken\nlibphonenumber-js exposed as libphonenumberjslib.libphonenumberjs\nlodash v4.17.19 exposed as _\nmailchimp marketing exposed as mailchimplib.mailchimp\nmailjet exposed as const mailJet = nodemailjet.nodemailjet;\nmoment-timezone v0.5.31 exposed as moment\nnode-fetch v2.6.0 exposed as fetch\noauth v0.9.15 exposed as OAuth\n@sendgrid/client v7.4.7 exposed as sendgrid.client\n@sendgrid/mail v7.4.7 exposed as sendgrid.mail\nskyflow exposed as skyflowlib.skyflow\nstripe v8.115.0 exposed as stripe\ntwilio v3.68.0 exposed as twilio\nuuidv5 v1.0.0 exposed as uuidv5.uuidv5\nwinston v2.4.6 exposed as const winston = winstonlib.winston\nxml v1.0.1 exposed as xml\nxml2js v0.4.23 exposed as xml2js\n\nzlib v1.0.5 exposed as zlib.zlib\n\n\nuuidv5 is exposed as an object. Use uuidv5.uuidv5 to access its functions. For example:\n\n  async function onRequest(request, settings) {\n       uuidv5 = uuidv5.uuidv5;\n       console.log(typeof uuidv5);\n\n        //Generate a UUID in the default URL namespace\n        var urlUUID = uuidv5('url', 'http://google/com/page');\n        console.log(urlUUID);\n\n        //Default DNS namespace\n        var dnsUUID = uuidv5('dns', 'google.com');\n        console.log(dnsUUID);\n    }\n\n\nzlib\u2019s asynchronous methods inflate and deflate must be used with async or await. For example:\n\nzlib = zlib.zlib;  // Required to access zlib objects and associated functions\nasync function onRequest(request, settings) {\n  const body = request.json();\n\n  const input = 'something';\n\n  // Calling inflateSync method\n  var deflated = zlib.deflateSync(input);\n\n  console.log(deflated.toString('base64'));\n\n  // Calling inflateSync method\n  var inflated = zlib.inflateSync(new Buffer.from(deflated)).toString();\n\n  console.log(inflated);\n\n  console.log('Done');\n  }\n\n\nThe following Node.js modules are available:\n\ncrypto Node.js module exposed as crypto.\nhttps Node.js module exposed as https.\n\nOther built-in Node.js modules aren\u2019t available.\n\nFor more information on using the aws-sdk module, see how to set up functions for calling AWS APIs.\n\nCaching\n\nBasic cache storage is available through the cache object, which has the following methods defined:\n\ncache.load(key: string, ttl: number, fn: async () => any): Promise<any>\nObtains a cached value for the provided key, invoking the callback if the value is missing or has expired. The ttl is the maximum duration in milliseconds the value can be cached. If omitted or set to -1, the value will have no expiry.\ncache.delete(key: string): void\nImmediately remove the value associated with the key.\n\nSome important notes about the cache:\n\nWhen testing functions in the code editor, the cache will be empty because each test temporarily deploys a new instance of the function.\nValues in the cache are not shared between concurrently-running function instances; they are process-local which means that high-volume functions will have many separate caches.\nValues may be expunged at any time, even before the configured TTL is reached. This can happen due to memory pressure or normal scaling activity. Minimizing the size of cached values can improve your hit/miss ratio.\nFunctions that receive a low volume of traffic may be temporarily suspended, during which their caches will be emptied. In general, caches are best used for high-volume functions and with long TTLs. The following example gets a JSON value through the cache, only invoking the callback as needed:\nconst ttl = 5 * 60 * 1000 // 5 minutes\nconst val = await cache.load(\"mycachekey\", ttl, async () => {\n    const res = await fetch(\"http://echo.jsontest.com/key/value/one/two\")\n    const data = await res.json()\n    return data\n})\n\n\nThis page was last modified: 09 Aug 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nCreating functions\nEditing a function\nTesting a function\nDeploying source functions\nDeploying destination functions\nFunctions Versioning\nFunctions permissions\n\ufe0fSettings and secrets\nRuntime and dependencies\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nUnify and GDPR\nUnify and GDPR\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\n\nAll Segment GDPR features apply to Unify.\n\nSegment never shares or sells user data. Unify inherits Segment\u2019s holistic approach to security and privacy, using 256-bit AES standard encryption to safeguard data stores both at rest and in transit.\n\nUser Rights\n\nEnd-user privacy and the GDPR principles informed the design of Unify, a product powered by first-party data. Unify integrates Segment\u2019s existing end-user privacy features with several user rights:\n\nRight to Erasure\nRight to Object\nRight to Rectification\nRights to Access and Portability\n\nBelow, learn how each of these rights protects the integrity of users and their data.\n\nRight to Erasure\n\nUsing Segment\u2019s platform, you can manage user deletion across all Segment products and supported Destinations. User deletion requests remove user data from all internal Segment archives and environments, including Engage audiences, within 30 days.\n\nRight to Object\n\nWith one-click suppression, you can block data collection for specific users. Segment discontinues profile building around suppressed users and prevents them from joining future audiences.\n\nRight to Rectification\n\nWhen Segment receives new information, the platform updates user profiles and traits in both Segment and its downstream tools. Use the Profile API to confirm that an update has been processed.\n\nRights to Access and Portability\n\nIdentity Resolution connects information you\u2019ve gathered about a customer into a single profile. Using the Profile API, you can provide end users with this data. You can also enable raw data integrations and warehouses to share a user\u2019s data in a structured format.\n\nNext Steps\n\nVisit the Segment site to learn how Segment products simplify GDPR compliance, and reference Segment\u2019s complying with the GDPR documentation to incorporate GDPR best practices into your workflow.\n\nThis page was last modified: 28 Mar 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nUser Rights\nNext Steps\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSources\n/\nCloud Sources\nCloud Sources\n\nCloud-App Sources (often shortened to Cloud Sources) allow you to pull in data from third-party tools so you can use it in Segment. There are two types of Cloud Apps: Object and Event sources.\n\nAs in the basic tracking API, objects usually contain information about a person or group which is updated over time, while event data happens once, and is appended to a list.\n\nEvent Cloud-App Sources\n\nEvent Cloud Sources can export their data both into Segment warehouses, and into other enabled Segment integrations that work with event data.\n\nObject Cloud-App Sources\n\nObject Cloud App Sources can export data and import it directly into a Segment warehouse. You must have a Segment warehouse enabled before you enable these. From the warehouse, you can analyze your data with SQL, use Reverse ETL to extract data, or use Engage SQL Traits to build audiences. Some examples of Object Cloud sources are Salesforce (account information), Zendesk (support cases), and Stripe (payments information).\n\nIn the app, data from website, mobile, and server sources can go to a warehouse\u00a0or\u00a0to destinations. Object Cloud-App Source data can\u00a0only\u00a0go to Warehouses.\n\nHow do cloud sources work?\n\nSources are functionally comprised of either one or both of the following components: a \u201csync\u201d component and a \u201cstreaming\u201d component. They work together to populate logical collections of data based on upstream resource availability and following data normalization best practices. These collections may be either events (append only data streams, akin to \u201cfacts\u201d in data warehousing parlance) or objects (dimensional values that may be updated based on changes in state upstream).\n\nSync frequency\n\nYou enable a cloud source from the Segment web app, and grant Segment access by pasting an API key or authenticating with OAuth. Segment then starts a scheduled job on your behalf which makes requests to the downstream tool, normalizes and transforms the data, and forwards that data to the Segment API.\n\nCloud sources attempt to use as few API calls as possible, and (where possible) only fetch data that changed since the last sync. The syncs might take a long time (especially on the first sync), so the cloud source syncs have robust retry and rate limiting logic.\n\nContact Segment Product Support\u00a0if you\u2019d like to change the cadence of your source\u2019s sync frequency.\n\nAPI call use and collection selection\n\nWe make an effort to be respectful of your API call allotments and limits. For example, in the case of Salesforce, we issue only one query per collection per run, using the absolute minimum number of API calls possible (typically about 350/day).\n\nMoreover, we\u2019re deliberate about which collections we pull, striking a balance between allowing you to get a full picture of your users and reducing extraneous data (like administrative and metadata tables).\n\nSoon, we\u2019ll allow you to specify which collections you care about during the source set up phase, so if you need to cut down on calls, you\u2019ll be able to just deselect collections.\n\nStreaming\n\nStreaming components are used to listen in real time to webhooks from downstream cloud sources, normalize and transform the data, and forward it to our APIs.\n\nBoth sync and streaming components can forward data to our event tracking and objects upsertion API processing layers, but generally sync components are used to fetch objects and streaming components listen for events.\n\nSet up a cloud source\n\nTo use cloud sources, we suggest going through the following steps.\n\nGet cloud source credentials\nGet warehouse credentials\nChoose your preferred sync time\n\nBefore you connect a source, check out the\u00a0sources documentation. See what kind of credentials you will need to enable the source. Different sources require different levels of permissioning.\n\nNext, you\u2019ll also need to get the credentials for your\u00a0warehouse.\n\nOnce you have the necessary credentials (or are logged in to OAuth for your cloud source), you should be ready to go!\n\nGo to the \u201csources catalog\u201d in the Segment web app.\nChoose a cloud source, and click Configure.\nEnter your credentials or log in using OAuth.\nGo to the \u201cwarehouses\u201d tab and enter the credentials for your warehouse if you don\u2019t already have one connected to Segment.\n\nBased on your plan, you can schedule a certain number of syncs per day. We suggest setting these up so your dashboards and reports are fresh for reporting, but not at the same time of day that a lot of people are querying your database.\n\nTroubleshooting cloud sources\n\nThe most common reason cloud sources have trouble because of authentication or permission issues. When the issue is related to authentication, you\u2019ll see an \u201caccess denied\u201d connection error in your source details. When this happens, Segment quits the process early and does not make any further attempts on any collections.\n\nWhen you successfully authenticate, but your user lacks the required permissions (for example, if you use an agent login instead of an administrator for Zendesk), Segment attempts to pull each collection and reports errors on a per-collection basis. This helps you troubleshoot why source runs fail, because sometimes permission-based denials are scoped to specific resources from the upstream tool.\n\nSegment attempts to make the errors displayed in the UI clear enough so we don\u2019t need to document all of them. However, if it\u2019s not clear what to do to fix an error you encounter, contact support\u00a0and let them know.\n\nSometimes, when the sync job fails due to an unhandled error or is mysteriously hanging for too long, we\u2019ll kill the job and report a failure with instructions to contact support. When this happens, our support and engineering teams have already been notified of the failure and have the complete set of logs to set about debugging and remediating the issue, but don\u2019t hesitate to get in touch so they can keep you in the loop!\n\nUsing Cloud Source data\nWhat kind of data does Segment pull from each source?\n\nIn general, we\u2019ve focused on pulling all of the collections directly related to the customer experience. We do not automatically pull all collections available from a partner API, since many of them aren\u2019t relevant to the customer journey. You can see a list of the collections we pull in the docs\u00a0for each cloud source. Each collection reflects a table in your database.\n\nContact Segment Product Support\u00a0if you need additional data collected, or to change the schema to do the analysis you want. We\u2019d love to know what analysis you\u2019re trying to run, what additional data you need, and we\u2019ll share with the product team to evaluate.\n\nWhat questions can you answer with data from cloud, web, and mobile sources combined in a single warehouse?\nWhat content drives people forward in our sales funnel?\nWhat are the top pages viewed before a support ticket is sent?\nDo people who opt into text messages engage more than people who only get emails?\nDo customers that interact with our support team activate faster? - Retain more overtime?\nWhat are all of the communications across marketing, success, and sales, this account has had in the last 2 months?\nQuerying source data\n\nGenerally, you need intermediate- to advanced SQL experience to explore and analyze cloud source data in a warehouse. The following resources can help you get up and running more quickly!\n\nJoining IDs\u00a0As you start to get into joining across different types of sources, you\u2019ll need a way to join user IDs. This\u00a0help article\u00a0explains how to do this in detail.\n\nPartner Dashboards\u00a0Our BI partners at Mode, Looker, BIME, Periscope, and Chartio have created out of the box dashboards that work on top of our source schemas.\n\nThis page was last modified: 22 Oct 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nHow do cloud sources work?\nSet up a cloud source\nTroubleshooting cloud sources\nUsing Cloud Source data\nQuerying source data\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nSegment App\n/\nExtensions\n/\ndbt Extension\ndbt Extension\n\nSegment\u2019s dbt extension lets you use Reverse ETL with your existing dbt labs models and syncs to help centralize model management and versioning, reduce redundancies, and run CI checks to prevent breaking changes.\n\nWith Segment\u2019s dbt extension, you can:\n\nSecurely connect Segment to a Git repository that stores your dbt models.\nUse centralized dbt models to set up Reverse ETL.\nTrigger Reverse ETL syncs from dbt jobs.\n\nThis page explains how to set up a dbt Model and then use the model with Reverse ETL.\n\nBefore you begin\n\nKeep the following in mind as you set up the dbt extension:\n\nThe extension supports dbt Core v1.7.\nYou can use Snowflake, Databricks, Redshift, Postgres, and BigQuery as Reverse ETL sources.\ndbt models aren\u2019t synchronized from the dbt cloud. The model sync connects to a Git repository that loads models into Segment for use with Reverse ETL.\nYou can connect to GitHub using a GitHub App, token, or SSH.\nFor GitLab and Bitbucket, use SSH to connect.\nSet up Git dbt Models and dbt Cloud\n\nTo set up the dbt extension, you\u2019ll need:\n\nan existing dbt account with a Git repository\nfor job syncs, dbt cloud with jobs already created\nGit repository and dbt Models setup\n\nFollow these steps to connect the Git repository that stores your dbt Models:\n\nIn your Segment workspace, navigate to Settings > Extensions.\nClick Set up Git sync.\nOn the Configure service credentials page, select a service and protocol, add your GitHub App, SSH private key or GitHub token, then click Next.\nIn the Connect source window, select an existing Reverse ETL warehouse source from the dropdown, then click Save.\n\nAfter you\u2019ve saved your setup, you can configure your Git repository\u2019s settings to your needs by changing the repository, branch, dbt version, default schema, and project path.\n\ndbt Cloud setup\n\nYou can also use dbt Cloud to schedule Reverse ETL syncs after a dbt Cloud job successfully runs.\n\nTo set up dbt Cloud:\n\nIn your Segment workspace, navigate to Settings > Extensions.\nClick Manage dbt Cloud.\nAdd your dbt Cloud API key or dbt Personal Access Token and an optional custom subdomain, then click Save.\n\nAdding a custom subdomain\n\nBy default, dbt sets the subdomain to cloud. To identify your custom subdomain, open your URL and copy the portion before .getdbt.com. For example, if your domain was https://subdomain.getdbt.com/, your subdomain would be subdomain.\n\ndbt Cloud Webhooks\n\nThe dbt Cloud integration allows you to schedule Reverse ETL syncs based on a dbt Cloud job. When a dbt Cloud job is selected under the Reverse ETL scheduling section, Segment creates a webhook in the dbt Cloud account that will initiate to run the Reverse ETL sync when the job is scheduled.\n\nIn order to create the webhook, ensure that you have webhook permissions associated with the dbt Cloud token in the previous step.\n\nModel syncs\n\nAfter you set up dbt, Segment runs an initial sync to load models from your connected Git repository. This initial sync lets you use the most recent models when you set up Reverse ETL. In addition to Segment\u2019s initial dbt sync, you can also trigger manual dbt model syncs.\n\nUse a model with Reverse ETL\n\nAfter you\u2019ve successfully set up dbt with a warehouse and connected to your Git repository, you can select dbt models for use with Reverse ETL by following these steps:\n\nIn your Segment workspace, navigate to Connections > Sources and select the Reverse ETL tab.\nClick +Add Reverse ETL source , select your source, then click Add Model.\nClick dbt Models as your modeling method, then select and preview a model from the dbt model dropdown.\nAdd a primary key, then click Preview your model.\nClick Next.\nEnter your Model Name, then click Create Model.\n\nTo change a connected model, ensure that you\u2019ve removed it from all active Reverse ETL syncs.\n\nGit Connections\n\nGit Connections enable Segment to sync data with your preferred Git repository through supported like SSH and token-based authentication.\n\nGit Sync and the dbt integration operate independently. You don\u2019t need to set up Git Sync to use dbt, and dbt Cloud can trigger its own syncs without relying on Git Sync.\n\nSupported connection types\n\nSegment supports the following credential types for setting up a Git Connection:\n\nSSH: Compatible with GitHub, GitLab, and Bitbucket, SSH provides a secure method for connecting to your repository.\nGit token: Git tokens are supported across GitHub, GitLab, and Bitbucket, enabling token-based authentication for added flexibility.\nGitHub App: For GitHub users, GitHub App integrations offer enhanced security and functionality. This method is exclusive to GitHub and supports additional features, like CI checks.\nReusing Git Connections\n\nSegment lets you set up multiple Git Connections, allowing you to reuse credentials across both dbt and Git Sync. You can either use the same credential for multiple configurations or create separate Git Connections for each product and environment as needed.\n\nIf you plan to reuse a Git token across both dbt and Git Sync, ensure it has the necessary read and write permissions for both integrations.\n\nSetting Up CI checks\n\nCI check availability\n\nCI checks are available only with the GitHub App connection.\n\nCI checks in Segment help prevent breaking changes to active dbt models. Avoid changing dbt models currently in use with an active Reverse ETL sync, since changes could disrupt existing mappings and active syncs.\n\nWhen CI checks are enabled, Segment monitors model changes in your Git repository. If a model already linked to an active Reverse ETL sync gets modified, Segment automatically rejects the change to maintain data integrity.\n\nTo enable CI Checks, authorize a GitHub App credential for your Git connection. Once connected, you can enable CI Checks in the dbt model sync configuration section.\n\nTroubleshooting dbt Extensions\n\nThe following table lists common dbt Extension errors, as well as their solutions:\n\nERROR\tERROR MESSAGE\tSOLUTION\nFailed sync\tSync Failed: Incorrect dbt Project File Path: dbt project file not found\tVerify that the path to your dbt_project.yml file is relative to the repository root, excluding the root branch.\nFor example, use project/dbt_project.yml instead of main/project/dbt_project.yml.\nFailed sync\tSync Failed: remote: Write access to repository not granted\tVerify that the account associated with the token has a write role in the repository settings. Fine-grained tokens may require specific roles, depending on your Git provider.\n\nThis page was last modified: 11 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nBefore you begin\nSet up Git dbt Models and dbt Cloud\nGit Connections\nSetting Up CI checks\nTroubleshooting dbt Extensions\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nConnections Alerting\nConnections Alerting\n\nConnections Alerting allows Segment users to receive in-app, email, and Slack notifications related to the performance and throughput of an event-streaming connection.\n\nTo access Connections Alerting, select an event-streaming connection (like a web library source or cloud mode destination) and click the Alerts tab.\n\nOn the Alerts tab, you can create alerts and view all active alerts for this connection. You can only edit or delete the alerts that you create.\n\nSource volume alerts\n\nYou can create an alert that notifies you when the volume of events received by your source in the last 24 hours changes beyond a percentage you set. For example, if you set a change percentage of 4% and your source received 100 events over the first 24 hours, Segment would notify you the following day if your source ingested fewer than 96 or more than 104 events.\n\nTo receive a source volume alert in a Slack channel, you must first create a Slack webhook. For more information about Slack webhooks, see the Sending messages using incoming webhooks documentation.\n\nTo create a source volume alert:\n\nIn your workspace, navigate to Connections, select Sources, and select the Event streams tab.\nSelect the event streams source you\u2019d like to configure alerts for.\nSelect the Alerts tab and click Create alert.\nOn the Create alert sidesheet, enter a percentage of source volume change that you\u2019d like to be notified for.\nSelect one or more of the following alert channels:\nEmail: Select this to receive notifications at the provided email address.\nSlack: Select this to send alerts to one or more channels in your workspace.\nIn-app: Select this to receive notifications in the Segment app. To view your notifications, select the bell next to your user icon in the Segment app.\nClick Save.\n\nTo make changes to a source volume alert, select the icon in the Actions column for the alert and click Edit.\n\nTo delete a source volume alert, select the icon in the Actions column for the alert and click Delete.\n\nDeleting alerts created by other users requires Workspace Owner permissions\n\nAll users can delete source volume alerts that they created, but only those with Workspace Owner permissions can delete alerts created by other users.\n\nSuccessful delivery rate alerts\n\nYou can create an alert that notifies you when the volume of events successfully received by your destination in the last 24 hours falls below a percentage you set. For example, if you set a percentage of 99%, Segment notifies you if your destination had a successful delivery rate of 98% or below.\n\nTo receive a successful delivery rate alert in a Slack channel, you must first create a Slack webhook. For more information about Slack webhooks, see the Sending messages using incoming webhooks documentation.\n\nTo create a successful delivery rate alert:\n\nNavigate to the cloud-mode destinations you\u2019d like to configure alerts for.\nSelect the Alerts tab and click Create alert.\nOn the Create alert sidesheet, enter a percentage. You will receive events if your successful delivery rate falls below this percentage.\nSelect one of the following alert channels:\nEmail: Select this to receive notifications at either the email address associated with your account or another email address that you enter into this field.\nSlack: Select this and enter a Slack webhook URL and channel name to send alerts to a channel in your Slack workspace.\nIn-app: Select this to receive notifications in the Segment app. To view your notifications, select the bell next to your user icon in the Segment app.\nClick Save.\n\nTo make changes to a successful delivery rate alert, select the icon in the Actions column for the alert and click Edit.\n\nTo delete a successful delivery rate alert, select the icon in the Actions column for the alert and click Delete.\n\nDeleting alerts created by other users requires Workspace Owner permissions\n\nAll users can delete successful delivery alerts that they created, but only those with Workspace Owner permissions can delete alerts created by other users.\n\nSegment generates delivery alerts for failed deliveries and successful deliveries, which are the last two stages of the delivery pipeline. As a result, alerts are based on Segment\u2019s attempts to send qualified events to your destination, excluding those filtered out by business rules (like protocols, destination filters, or mappings).\n\nThis page was last modified: 02 Jul 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSource volume alerts\nSuccessful delivery rate alerts\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nSegment App\n/\nExtensions\n/\nGit Sync Extension\nGit Sync Extension\n\nSegment\u2019s Git extension lets you manage versioning by syncing changes you make in your Segment workspace to a Git repository.\n\nGit Sync supports one-way synchronization from Segment to Git. This sync captures the current state of your workspace through a full sync and includes all new records and changes for supported resources.\n\nSegment doesn\u2019t support syncing changes from Git back to Segment.\n\nSet up Git Sync\n\nFollow these steps to set up Git Sync:\n\nIn your Segment workspace, navigate to Settings > Extensions.\nClick Set up Git sync.\nOn the Configure service credentials page, select a service and protocol, add your GitHub App, SSH private key, or GitHub token, then click Next.\nTo connect to GitLab or Bitbucket, use your SSH private key.\nWorking with Git Sync\n\nThe Git sync extension syncs the following resources from Segment to your Git repository:\n\nSources and Destinations\nWarehouses\nDestination Filters and Mappings for Connections\nTracking Plans\nFunctions\nTransformations\nReverse ETL\nUsers and User groups\nLabels\n\nThe Git sync extension doesn\u2019t support the following resources:\n\nSpaces\nAudiences and Journeys\nData Graph\nMappings for Linked Audiences\n\nReach out to Segment support to request support for additional Git Sync resources.\n\nAfter you set up the Git sync extension for the first time, Segment performs an initial sync that sends the current state of your Segment workspace to the Git repository you connected. Segment automatically tracks all following workspace updates.\n\nYou can manually trigger syncs at any time by clicking Full Sync on the Git Sync page. To disable Git Sync from the Git Sync page, switch the Enabled toggle to off.\n\nGit Sync architecture and data model\n\nBecause a Segment workspace can represent a distinct environment (testing, staging, production), each workspace is mapped directly to a single Git repository. This direct mapping ensures a clear and organized relationship between workspace resources and a Git repository.\n\nSegment uses its Terraform provider to manage key functions like tracking changes and retrieving information about those changes in Segment. Segment stores changes in HashiCorp Configuration Language (HCL), the format used by Terraform. To learn more about HCL and how it compares to JSON or YAML, visit HashiCorp\u2019s HCL repository on GitHub.\n\nUsing HCL makes it easier to document Segment\u2019s data model, especially for users managing versioning and Git Sync with Terraform. It also helps manage Segment configurations directly from Git. For more details on the Git Sync data model, read Segment\u2019s Terraform provider documentation.\n\nManaging your Segment workspace with Terraform and Git Sync\n\nSegment supports one-way synchronization from Segment to Git, but you can set up two-way synchronization using the Segment Terraform provider.\n\nTerraform offers an open-source way to manage Segment resources through a Git repository as an alternative to a fully managed two-way sync. This method requires third-party tools like Atlantis for CI integration.\n\nTo manage Segment resources using Git and Terraform, follow these steps:\n\nCopy the generated Terraform configuration for the resources you want to manage into a separate Git repository dedicated to Terraform.\n\nInclude the following provider configuration blocks:\n\n # providers.tf\n\n terraform {\n required_providers {\n     segment = {\n     source  = \"segmentio/segment\"\n     version = \"1.0.4\"\n     }\n }\n }\n\n provider \"segment\" {\n # Provide the token directly or load it from an environment variable\n }\n\nApply configuration changes by running Terraform locally or using a tool like Atlantis to run it directly from your Git provider.\n\nFor more information on using Terraform, visit Terraform\u2019s documentation.\n\nGit Connections\n\nGit Connections enable Segment to sync data with your preferred Git repository through supported like SSH and token-based authentication.\n\nGit Sync and the dbt integration operate independently. You don\u2019t need to set up Git Sync to use dbt, and dbt Cloud can trigger its own syncs without relying on Git Sync.\n\nSupported connection types\n\nSegment supports the following credential types for setting up a Git Connection:\n\nSSH: Compatible with GitHub, GitLab, and Bitbucket, SSH provides a secure method for connecting to your repository.\nGit token: Git tokens are also supported across GitHub, GitLab, and Bitbucket, enabling token-based authentication..\nGitHub App: For GitHub users, GitHub App integrations offer enhanced security and functionality. This method is exclusive to GitHub and supports additional features, like CI checks.\nReusing Git Connections\n\nSegment lets you set up multiple Git Connections, allowing you to reuse credentials across both dbt and Git Sync. You can either use the same credential for multiple configurations or create separate Git Connections for each product and environment as needed.\n\nIf you plan to reuse a Git token across both dbt and Git Sync, ensure it has the necessary read and write permissions for both integrations.\n\nTroubleshooting Git Sync\n\nWhen setting up Git Sync, you may run into an access error with the following message: \u201cUnable to create Git Sync due to Git connection issues. Please check your configuration and try again.\n\nThis error can occur if there are issues with your Git connection settings or permissions. To resolve the error, verify that:\n\nYour credentials have write access to the Git repository, as Segment requires this to sync changes.\nYour repository is hosted by GitHub, GitLab, or Bitbucket (Segment doesn\u2019t support self-hosted repositories).\nBranch protections are disabled on the repository.\n\nThis page was last modified: 07 Nov 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSet up Git Sync\nWorking with Git Sync\nGit Sync architecture and data model\nManaging your Segment workspace with Terraform and Git Sync\nGit Connections\nTroubleshooting Git Sync\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nAudiences\n/\nGenerative Audiences Nutrition Facts Label\nGenerative Audiences Nutrition Facts Label\n\nTwilio\u2019s AI Nutrition Facts provide an overview of the AI feature you\u2019re using, so you can better understand how the AI is working with your data. Twilio outlines AI qualities in Generative Audiences in the Nutrition Facts label below. For more information, including the AI Nutrition Facts label glossary, refer to the AI Nutrition Facts page.\n\nAI Nutrition Facts\n\nGenerative Audiences\n\n\n\n\nDescription\n\nGenerate user audiences from text instructions\n\n\n\n\nPrivacy Ladder Level\n1\n\n\n\n\nFeature is Optional\nYes\n\n\n\n\nModel Type\nGenerative\n\n\n\n\nBase Model\nOpenAI - GPT-4\n\n\n\n\nTrust Ingredients\n\n\n\n\nBase Model Trained with Customer Data\nNo\n\n\n\n\nCustomer Data is Shared with Model Vendor\nNo\n\n\n\n\nTraining Data Anonymized \u00a0\nN/A\n\n\n\n\nData Deletion\nYes\n\n\n\n\nHuman in the Loop\nYes\n\n\n\n\nData Retention\n30 days\n\n\nCompliance \u00a0 \u00a0\nLogging & Auditing\nNo\n\nGuardrails\nYes\n\n\n\nInput/Output Consistency\nNo\n\n\n\n\nOther Resources\n\nThis page was last modified: 04 Mar 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nApi\n/\nConfig API Overview\nConfig API Overview\n\nThe Segment Public API is available\n\nSegment\u2019s Public API is available for Team and Business tier customers to use. You can use the Public API and Config APIs in parallel, but moving forward any API updates will come to the Public API exclusively.\n\nPlease contact your account team or friends@segment.com with any questions.\n\nThe Config API enables you to programmatically manage Segment workspaces, sources, destinations and more. With the API you can:\n\nList all your workspace Sources and Destinations to see how data flows through Segment\nCreate new Destinations - or delete them - with a few lines of code\nCreate new users and assign them to scoped roles\nConfigure, disable, or view Sources and manage connected Destinations\nGet a complete view of all the Sources and Destinations available in Segment\u2019s catalog\nConfigure a Tracking Plan to see how data conforms to your expected schema\nQuery Event Delivery metrics to build custom dashboards and alerts to monitor delivery of your events to destinations\nFilter entire events or individual fields from reaching specific destinations\n\nThe Config API is a set of REST services under segmentapis.com:\n\nSERVICE\tDESCRIPTION\nAccess Tokens\tManage access tokens\nSource Catalog\tGet info about all event and cloud sources\nDestination Catalog\tGet info about all destinations\nWorkspaces\tGet info about workspaces\nSources\tManage workspace sources\nDestinations\tManage workspace destinations\nTracking Plans\tManage workspace tracking plans\nEvent Delivery Metrics\tGet event delivery metrics for cloud-mode destinations\nDestination Filters\tManage destination filters\nIAM\tManage workspace users and roles\nFunctions\tManage Functions\n\nTo see all the API methods and models see the Segment Config API Reference.\n\nAt this time there are no language-specific clients. However the API Reference also contains example code snippets for cURL, Go, Node, Python and more.\n\nQuick Start\n\nYou can interact with the API from the command line. First install the curl tool.\n\n$ brew install curl\n\nAccess Tokens\n\nYou can use the Config API with an access token to programmatically access Segment resources that the token can access. Access tokens are created by workspace owners using the Access Management page, and can only access resources that the token has permission to.\n\nThese are currently only suitable for first party, trusted applications, such as your personal local scripts and server side programs. Partners should not prompt Segment users for their username and password and save an access token as a way to delegate access. See the Authentication doc for more information.\n\nWhen you create an access token, you\u2019ll give it a description, a workspace, and determine whether it has workspace owner or member access.\n\nSecret Token\n\nYou can not retrieve the plain-text token later, so you should save it in a secret manager. If you lose the token you can generate a new one.\n\ninfo As of February 1, 2024, new Config API tokens cannot be created in the app as Segment moves toward exclusive support for the Public API. Migrate your implementation to the Public API to access the latest features and available endpoints. To create a new Config API token, reach out to friends@segment.com for support.\n\nAPI Requests\n\nNow that you have an access token, you can use this token to access the rest of the Config API by setting it in the Authorization header of your requests, for example:\n\n$ ACCESS_TOKEN=qiTgISif4zprgBb_5j4hXfp3qhDbxrntWwwOaHgAMr8.gg9ok4Bk7sWlP67rFyXeH3ABBsXyWqNuoXbXZPv1y2g\n\n$ curl \\\n  -X GET \\\n  -H \"Authorization: Bearer $ACCESS_TOKEN\" \\\n  https://platform.segmentapis.com/v1beta/workspaces\n\n\nExample response:\n\n{\n  \"workspaces\": [\n    {\n      \"name\": \"workspaces/myworkspace\",\n      \"display_name\": \"My Space\",\n      \"id\": \"e5bdb0902b\",\n      \"create_time\": \"2018-08-08T13:24:02.651Z\"\n    }\n  ],\n  \"next_page_token\": \"\"\n}\n\nReference\n\nFor an overview of the API\u2019s common design patterns and important information about versioning and compatibility, see the API Design document.\n\nTo see all the API methods and models see the Segment Config API Reference.\n\nThis page was last modified: 01 Feb 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nQuick Start\nReference\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nSegment App\n/\nWorkspace Home\nWorkspace Home\n\nSegment\u2019s Workspace Home serves as a dashboard that gives you a single consolidated view of the workspace, its health and status, and metrics for specific integrations over time.\n\nAvailability\n\nThe Workspace Home is visible to users who have the Workspace Owner role. If you have access to the Home page, it is the first screen you see when you log in to your Segment Workspace.\n\nThe Home page shows a summary of errors in the workspace\u2019s sources and destinations, a list of \u201cfavorite\u201d integrations that you can configure, and information about recent audit logged events, and your billing plan usage.\n\nWorkspace Home reporting period\n\nThe Workspace Home page shows when the data it represents was last updated. You can click the Updated line to refresh the display.\n\nYou can also use the drop-down menu at the right to choose between a 24-hour and 7-day rolling view of the data. The page automatically updates the graphs and statistics when you change your selection.\n\nSources - Violations\n\nThe Sources section of the Home pages shows a summary of the event volume flowing through Segment.\n\nIf your Segment plan includes Protocols, the dashboard shows Event Violations occurring in the workspace, including a graph over time. You can click into the violations section to see a list of sources ordered from highest number of violations, to lowest. You can click into individual sources to see more details and go to their individual source pages, or go to the Violations page.\n\nSources - Events Received\n\nIf your Segment plan does not include Protocols, an Events Received chart is shown to reflect the number of events received across sources.\n\nDestinations - Event Delivery\n\nThe Destinations section of the Home page shows a summary of the Event Delivery for the workspace, including a graph over time. You can click the destinations link to see a list of destinations with delivery problems, ordered from highest to lowest error rate. You can click a destination to see more details about the delivery failures, and from the details panel click the destination name to go directly to its configuration page.\n\nFavorite integrations\n\nIf you have access to the Workspace Home page, you can customize it for yourself by bookmarking or saving \u201cfavorite\u201d integrations. These could be sources, destinations, storage destinations, and functions that you work with regularly, or want to monitor closely.\n\nThese Home page favorites are specific to your user account in the workspace. The are not shared among administrators in the same workspace, and are not accessible from other workspaces you might be an administrator for.\n\nTo create a favorite, click Add Favorite or the plus icon, and select the integration(s) to bookmark. The list that appears displays a status icon for each integration, so you can tell which ones are healthy (green) and unhealthy (red), and which ones are inactive (gray).\n\nOnce you create a favorite, the section displays your favorites in tabs so you can view sources only, destinations only (including storage destinations), or all of them at once. Each favorite displays important configuration details and summary statistics for that integration. You can click the more (\u2026) menu to jump directly to one of the configuration or detail pages for that integration.\n\nTo delete a favorite, click the more (\u2026) menu and select Remove favorite.\n\nRecent Activity\n\nFor users with Business Tier workspaces, the recent activity section displays the most recent items logged to the Segment Audit trail. This includes workspace membership changes and requests, changes to the configuration of different Segment features (including sources, destinations, and to Engage and Protocols configurations), and data storage sync failures.\n\nUsage\n\nThe Usage section shows a summary of the workspace\u2019s plan utilization for the current billing period. This includes billing information for all parts of your Segment plan. This includes MTUs or API call volume (as applicable), Functions usage time (if applicable), and Engage details (if applicable). Click view all to go to the workspace\u2019s billing page for more detailed statistics.\n\nThis page was last modified: 21 Apr 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSources - Violations\nSources - Events Received\nDestinations - Event Delivery\nFavorite integrations\nRecent Activity\nUsage\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSpec\n/\nBest Practices for Identifying Users\nBest Practices for Identifying Users\n\nThe most important calls you make with Segment are the identify and track calls. When you use these calls together, you can attribute actions on your site or app to individuals, and gain a better understanding of their activities, identity, and use patterns over time. Tracking users with the identify and track calls reduces the number of Monthly Tracked Users you are billed for.\n\nIdentifying users\n\nThe Identify call specifies a customer identity that you can reference across the customer\u2019s lifetime. There are instances where you want to record information about a user that isn\u2019t already known to you. An example of this might be, a user that visits your site and doesn\u2019t register, but they do give you their email address through a newsletter email sign-up form. In this instance, you would record that email address as a trait, and for the identifier (ID), you would use anonymous ID.\n\nWhen you make an identify call using Segment\u2019s Analytics.js library, Segment saves the userId to the browser cookie, and writes all the user traits in localStorage. If you\u2019re using one of the Segment mobile libraries, the userId and traits are stored in the device\u2019s memory. This makes it possible to append the user\u2019s data to all subsequent page calls or track calls for the user, so you can properly attribute those actions.\n\nIf a user returns to your site after the cookie expires, Analytics.js looks for an old ID in the user\u2019s localStorage, and if one is found, sets it as the user\u2019s ID again in a new cookie. If the user clears their cookies and localStorage, all of the IDs are removed and the user gets a completely new anonymousId when they next visit the page.\n\nWhenever possible, follow the Identify call with a Track event that records what caused the user to be identified.\n\nAnonymousId generation\n\nIf you\u2019re using Segment\u2019s browser or mobile libraries, the Segment SDK generates and sets a UUID as anonymousID at the user\u2019s first visit to your site. That anonymousId is saved in the user\u2019s cookie, as well as localStorage, and will stick with that user until the cache is cleared or a reset call is triggered.\n\nYou can use the anonymousId to link events performed by the user as they navigate around your website. When you track the anonymousId, you can attribute activities over multiple days to the same user by collecting all of the activities with that ID. If a user chooses to register for your site, or log in to your app, you can Identify them, and still include their anonymousId in the event payload along with the new userId.\n\nIf you use Segment\u2019s server libraries, you must generate an anonymousId manually. It can be any pseudo-unique identifier, for example, you might use a sessionId from a backend server.\n\nBest options for userIds\n\nSegment recommends that you use a unique user identifier (UUID) that won\u2019t change for your userId. A userId should be a robust, static, unique identifier that you recognize a user by in your own database systems. Because these IDs are consistent across a customer\u2019s lifetime, you should include a userId in Identify calls as often as you can. If you don\u2019t have a userId, you need to include an anonymousId in your Identify call in order to record identifying information about your user.\n\nIdeally, the userId could be a database ID. For example, if you\u2019re using MongoDB it might be a row identifier and look something like 507f191e810c19729de860ea. These can also be UUIDs that you generate somewhere in your application. You can also use identifiers that you get from other tools - such as Shopify or Braze - however this approach can lead to extra complexity in your systems.\n\nSegment does not recommend using simple email addresses or usernames as a User ID, as these can change over time. Segment recommends that you use static IDs instead, so the IDs never change. When you use a static ID, you can still recognize the user in your analytics tools, even if the user changes their email address. And even better, you can link your analytics data with your own internal database.\n\nTip! Even though Segment doesn\u2019t recommend using an email address or a username as a User ID, you can still send that identifying information in your Identify call as traits.\n\nWhen to call Identify\n\nYou should make an Identify call in the following situations:\n\nWhen the user provides any identifying information (such as a newsletter sign-up with email and name)\nWhen first you create a user (and so it is assigned a userId)\nWhen a user changes information in their profile\nWhen a user logs in\n(Optional) Upon loading any pages that are accessible by a logged in user\nSoft User Registration\n\nAn anonymous user visits the site for the very first time. The home page has the analytics.js tracking snippet loaded in its header. When the page loads, this sets off the default Page call to Segment. The Segment SDK generates and sets anonymousId.\n\nanalytics.page({\n  path: '/',\n  title: 'Home Page',\n  url: 'https://somesite.com/',\n})\n\n\nYou can see in this full page event, the anonymousId is populated, and the userId is null.\n\n{\n  \"anonymousId\": \"bd077b70-816b-448b-ae79-2f5f7d856513\"\n  \"context\": {\n    \"ip\": \"0.0.0.0\",\n    \"library\": {\n      \"name\": \"analytics.js\",\n      \"version\": \"3.11.4\"\n    },\n    \"locale\": \"en-US\",\n    \"page\":{\n      \"path\":\"/\"\n      \"referrer\": \"\",\n      \"search\": \"\",\n      \"title\": \"Home Page\",\n      \"url\": \"https://somesite.com\"\n      },\n    \"userAgent\": \"Mozilla/5.0\"\n    },\n    \"integrations\": {},\n    \"messageId\": \"ajs-84d32beb4273e661a2257bfef41c4964\",\n    \"originalTimestamp\": \"2020-04-23T22:38:48.55Z\",\n    \"properties\":{\n      \"path\": \"/\",\n      \"referrer\": \"\",\n      \"search\": \"\",\n      \"title\": \"Home Page\",\n      \"url\": \"https://somesite.com\"\n    },\n  \"receivedAt\": \"2020-04-23T22:38:48.55Z\",\n  \"sentAt\": \"2020-04-23T22:38:48.55Z\",\n  \"timestamp\": \"2020-04-23T22:38:48.55Z\",\n  \"type\": \"page\",\n  \"userId\": null\n}\n\n\nThe user signs up for an email newsletter and fills out the form giving you their first and last name, as well as their email address. At this point, you will fire off an Identify call. You won\u2019t yet assign them a user ID in this example, but you can still grab these traits about them.\n\nanalytics.identify({\n  firstName: 'Joe',\n  lastName: 'Visitor',\n  email: 'jvisitor@thissite.com'\n});\n\n\nYou\u2019ll notice the Identify call contains no userId. These traits will be associated to the anonymousId that is available in the user\u2019s cookie and localStorage.\n\n{\n  \"anonymousId\": \"bd077b70-816b-448b-ae79-2f5f7d856513\"\n  \"context\": {\n    \"ip\": \"0.0.0.0\",\n    \"library\": {\n      \"name\": \"analytics.js\",\n      \"version\": \"3.11.4\"\n    },\n    \"locale\": \"en-US\",\n    \"page\":{\n      \"path\":\"/\"\n      \"referrer\": \"\",\n      \"search\": \"\",\n      \"title\": \"Email Signup\",\n      \"url\": \"https://somesite.email\"\n      },\n    \"userAgent\": \"Mozilla/5.0\"\n    },\n    \"integrations\": {},\n    \"messageId\": \"ajs-84d32beb4273e661a2257bfef41c4964\",\n    \"originalTimestamp\": \"2020-04-23T22:38:48.55Z\",\n    \"properties\":{\n      \"path\": \"/\",\n      \"referrer\": \"\",\n      \"search\": \"\",\n      \"title\": \"Home Page\",\n      \"url\": \"https://somesite.com\"\n    },\n  \"receivedAt\": \"2020-04-23T22:38:48.55Z\",\n  \"sentAt\": \"2020-04-23T22:38:48.55Z\",\n  \"timestamp\": \"2020-04-23T22:38:48.55Z\",\n  \"traits\"{\n    \"email\": \"jvisitor@thissite.com\",\n    \"first_name\": \"Joe\"\n    \"last_name\": \"Visitor\"\n  },\n  \"type\": \"page\",\n  \"userId\": null\n}\n\nFull User Registration\n\nAn anonymous visitor registers for an account and becomes a known user. The account creation process allows you to assign a userId from your production database and capture additional traits. For this example, the userId that is assigned is \u201c123abc\u201d. This is when you\u2019ll want to fire an Identify call with this user\u2019s newly assigned userId and additional traits.\n\nanalytics.identify(`123abc`,{\n  phone: '555-555-5555',\n  address: {\n    street: '6th Street',\n    city: 'San Fransisco',\n    state: 'CA',\n    postalCode: '94103',\n    country: 'US',\n  }\n});\n\n\nAfter you fire the Identify call with the userId, you\u2019ll notice that the payload now has both a userId and an anonymousId attributed to the user.\n\n{\n  \"anonymousId\": \"bd077b70-816b-448b-ae79-2f5f7d856513\"\n  \"context\": {\n    \"ip\": \"0.0.0.0\",\n    \"library\": {\n      \"name\": \"analytics.js\",\n      \"version\": \"3.11.4\"\n    },\n    \"locale\": \"en-US\",\n    \"page\":{\n      \"path\":\"/\"\n      \"referrer\": \"\",\n      \"search\": \"\",\n      \"title\": \"Email Signup\",\n      \"url\": \"https://somesite.email\"\n      },\n    \"userAgent\": \"Mozilla/5.0\"\n    },\n    \"integrations\": {},\n    \"messageId\": \"ajs-84d32beb4273e661a2257bfef41c4964\",\n    \"originalTimestamp\": \"2020-04-23T22:38:48.55Z\",\n    \"properties\":{\n      \"path\": \"/\",\n      \"referrer\": \"\",\n      \"search\": \"\",\n      \"title\": \"Home Page\",\n      \"url\": \"https://somesite.com\"\n    },\n  \"receivedAt\": \"2020-04-23T22:38:48.55Z\",\n  \"sentAt\": \"2020-04-23T22:38:48.55Z\",\n  \"timestamp\": \"2020-04-23T22:38:48.55Z\",\n  \"traits\"{\n    \"phone\": '555-555-5555',\n    \"address\": {\n    \"street\": '6th Street',\n    \"city\": 'San Fransisco',\n    \"state\": 'CA',\n    \"postalCode\": '94103',\n    \"country\": 'US',\n  }  \n  },\n  \"type\": \"page\",\n  \"userId\": \"123abc\"\n}\n\nMerging Identified and Anonymous user profiles\n\nThe illustration below shows a timeline with a user\u2019s interactions on a website, including sample API calls above that show Segment calls, and the user\u2019s anonymousId and userId.\n\nWhen the user first visits a page, Analytics.js automatically assigns the user an anonymousId and saves it to the user\u2019s localStorage. As the user interacts with the site, for example clicking around to different pages, Analytics.js includes this anonymousId and some contextual information with each Page and Track call. The contextual information might be the user\u2019s IP address, browser, and more.\n\nWhen a user signs up to create an account on the website, the .identify(\"userId\") and .track(\u201cSigned Up\u201d) events fire, in that order. You pull the userId unique to the user from your systems, and send it to the Segment library so you can label that user\u2019s later events with their ID. The later Track call (\u201cSigned Up\u201d) contains both the userId and the automatically-collected anonymousId for the user, and any other information you capture about them - such as their first name, last name, and email address.\n\nThe example below shows an Identify call including user traits. It uses a database ID (97980cfea0067) as the userId.\n\nanalytics.identify(\"97980cfea0067\", {\n  name: \"Peter Gibbons\", //user trait\n  email: \"peter@example.com\", //user trait\n  plan: \"premium\" //user trait\n});\n\n\nFor a Track call, information about this event is stored either in the context field or in the event properties. The example below shows a Track call including properties that tell you about the user.\n\nanalytics.track(\"Signed Up\", {\n  userId: \"97980cfea0067\", //event property\n  name: \"Peter Gibbons\", //event property\n  email: \"peter@example.com\", //event property\n  plan: \"premium\" //event property\n});\n\n\nAdditionally, Analytics.js adds a message_id and four timestamps to the call.\n\nNow, as the user interacts with your site and different buttons or links that you track using Segment, their userId and anonymousId are sent with each subsequent tracking API call.\n\nUserId merge examples\n\nLet\u2019s go through some more scenarios to explain how an anonymousId is assigned and how it might be merged with a userId.\n\nScenario #1 - Multi-day, single device\n\nIf a user clicks on an ad and is directed to a webpage, they are assigned an anonymousId. While this user is anonymous, they navigate to different pages and click around on the website. Say they come back two days later from the same device, sign up, and are assigned a userId from your database.\n\nFor simplicity, we\u2019re assuming that the user has not cleared their cookies or localStorage, where the original anonymousId is stored. If they had, they\u2019d be assigned a new anonymousId when they visited the website, and the userId they got when they register on the website would not be attached to the activities tracked with the old anonymousId.\n\nScenario #2 - Multi-day, multi-device, single login\n\nIn this scenario, the person uses both a web browser, and a mobile application to interact with your site. In each case, they are assigned a different anonymousId. In this scenario, the user signs up on the web browser, so Segment assigns their web session a userId. However, because they do not log in on the mobile application, Segment cannot tie the mobile activity to this specific user. Their mobile application activity remains anonymous unless they log in on the mobile application.\n\nScenario #3 - Multi-day, multi-device, multiple logins\n\nSimilar to the previous scenario, the user accessed both your website and mobile application, and also logged in on both. In this case, both sessions on the web and mobile app receive the user\u2019s userId, so Segment can tie the anonymous activity on both web and mobile to this user.\n\nUser profiles in warehouses\n\nYour data warehouse has a schema for each of your Segment sources. User information is stored in two tables in your source schemas - the identifies and users table.\n\nThe identifies table contains all of your identify events, and the timestamps for these events. Every time you make an Identify call, Segment adds the userId, anonymousId, any updated or added user traits from the call, as well as the timestamp of when the call was made. Your identifies table is your first stop when you have questions about users and their traits.\n\nThe users table contains only unique Identify method calls, and is a collation of the identifies table. The users table is the single source of truth for a user\u2019s most up-to-date traits.\n\nThese tables only contain information about a user once they have been identified. However, you can still find information about an anonymous user on the pages, screens, and tracks tables, as well as the individual track event tables.\n\nID expiration and overwriting\n\nThe Segment ID cookie is set with a one year expiration. However, there are some ways an ID can be reset or overwritten:\n\nIf you call reset during a user\u2019s browser session, it removes both their userId and anonymousId, which means the user generates a new anonymousId on the next visit.\nIf the user manually clears their cookies and local storage, they generate a new anonymousId on the next visit.\nIf you invoke any call before you set an anonymousId, Segment automatically sets the anonymousId first. This means if you explicitly set an anonymousId, you might give the user two anonymousIds or overwrite an existing one.\nIf you fetch the anonymousId using analytics.user().anonymousId() before one is set, Segment generates and sets an anonymousId rather than returning null.\nIf you call analytics.identify() with a userId that is different from the currently cached userId, this can overwrite the existing one and cause attribution problems.\nIf you generate a new anonymousId on a server library, and pass it from the server to the browser, this could overwrite the user\u2019s existing anonymousId.\n\nRemember, if a user has multiple devices, they can have different anonymousIds on each different device.\n\nLinking server and client generated Ids\n\nIf you\u2019re tracking on the client and on the server, the anonymousId can be retrieved from localStorage on the client and passed to the server. You can access a user\u2019s anonymousId using the following call:\n\nanalytics.user().anonymousId()\n\n\nIf you\u2019re identifying on the server, then you will want to pass the user ID from the server to the client using an Identify call with the anonymousId. That will allow the userId to be aliased with the existing anonymousId and stored in the cookie in localStorage. With that, all previous anonymous activity and all subsequent activity is associated to the newly generated userId, as well as existing anonymousIds.\n\nThere are some advantages to sending details about your users directly from your server once the user registers. Server library\u00a0Identify calls are invisible to the end user, making them more secure, and much more reliable. Or, if you want to send user data that is sensitive or which you don\u2019t want to expose to the client, then you can make an\u00a0Identify\u00a0call from the server with all the traits you know about the user. More about collecting data on the client or server in Segment\u2019s documentation.\n\nAliasing from a server library\n\nIf you plan to track anonymous visitors from the browser and only make Identify calls from your server libraries, Kissmetrics and Mixpanel might require that you make an Alias call\u00a0to link the records. The Alias call links client-side anonymous visitors with server-identified users. This isn\u2019t recommended, but if you do this, read the\u00a0Kissmetrics and Mixpanel\u00a0specific\u00a0alias\u00a0docs.\n\nCommon questions\n\nThere are a few things that might cause your numbers to be off.\n\nMissing sign-ups\n\nThe most common problem people run into when tracking new user signups client-side is that only a portion of their new users are showing up in reports.\n\nThis is usually caused by the page redirecting or reloading before the tracking calls get a chance to run. Segment recommends that you make those calls from a welcome page after the user registers, rather than trying to squeeze in the tracking calls on the sign-up page itself.\n\nAnonymous history is lost\n\nThis is usually only an issue in\u00a0Mixpanel, since it\u2019s the only destination that requires a call to\u00a0alias\u00a0in the browser to link anonymous browsing history to a new identified user.\n\nRemember that for destinations that require aliasing,\u00a0you must make the Alias call before\u00a0you make the Identify call\u00a0for that user. Even if you make an Identify call\u00a0from a server library, it can\u2019t happen before the client-side\u00a0alias.\n\nCan you update a userId?\n\nUnfortunately, there is no way to change an existing userId within Segment. Historical data with an existing userId remains the same, and a new userId will not replace the existing userId in Segment event call logs. For downstream destinations, consult the corresponding docs about user profile behaviors when using a new userId.\n\nChanging a userId is incredibly hard to do, as that is a fundamental part of analytics. While some downstream analytics tools let you change a userId once set, others don\u2019t and the process will be different for each tool.\n\nThis page was last modified: 15 Mar 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nIdentifying users\nAnonymousId generation\nBest options for userIds\nWhen to call Identify\nSoft User Registration\nFull User Registration\nMerging Identified and Anonymous user profiles\nUser profiles in warehouses\nID expiration and overwriting\nLinking server and client generated Ids\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nTraits\n/\nComputed Traits\nComputed Traits\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY PLUS \u2713\n?\n\nBeginning August 18, 2023, new Unify Plus users can access Computed Traits in Unify.\n\nComputed Traits allow you to quickly create user or account-level calculations that Segment keeps up-to-date over time. These can be computations like the total_num_orders a customer has completed, the lifetime_revenue of a customer, the most_frequent_user to determine which user is most active in an account, or the unique_visitors_count to assess how many visitors from a single domain. These computations are based on your events and event properties that you are sending through Segment on the page and track calls.\n\nComparing trait types\n\nView the table below to better understand how Segment collects custom, computed, and SQL traits.\n\nYou can use the Profile explorer (Unify > Profile explorer) to view traits attached to a profile.\n\nTRAIT TYPE\tDESCRIPTION\nCustom traits\tTraits created from source events you pass into Segment. From your sources, send custom traits as pieces of information that you know about a user in an Identify call.\nComputed traits\tTraits collected from computations off of event and event property data from your sources. Create user or account-level calculations like most_viewed_page or total_num_orders for a customer. Learn more by viewing types of computed traits.\nSQL traits\tTraits created by running SQL queries on data in your warehouse. SQL traits are a type of computed trait. SQL traits help you import traits from your data warehouse back into Segment to build audiences or enhance data that you send to other destinations.\nTypes of Computed Traits\n\nSegment currently supports the following types of computed traits:\n\nTypes of Computed Traits\nEvent Counter\nAggregation\nMost Frequent\nFirst\nLast\nUnique List\nUnique List Count\nPredictions\nRecommended Items\nConditions\nConnecting your Computed Trait to a Destination\nEditing Realtime Traits\nAccessing your Computed Traits using the Profiles API\nDownloading your Computed Trait as a CSV file\n\nEvent Properties per Computed Trait limit\n\nSegment limits the number of Event Properties on each Computed trait to 10,000. If your Computed Trait exceeds this limit, Segment will not persist any new Event Properties and will drop new trait keys and corresponding values.\n\nEvent Counter\n\nAn Event Counter trait stores a count of an event over a period of time. For example, you can create a trait called number_logins_90_days based on a User Logged In event. You can also use event properties to only specific types of events.\n\nUser-level examples:\n\nOrders Completed Last 30 Days\nPricing Page Views Last 30 Days\n\nAccount-level examples:\n\nTotal Logins by Account 30 Days\nEmails Opened by Account 90 Days\n\nAggregation\n\nAn aggregation computes a sum, average, minimum, or maximum of a numeric event property. A good example is a sum_cosmetics_revenue_90_days if you\u2019re sending an Order Completed event with a revenue property. In the example we\u2019re refining the revenue even further based on another event property: category = 'cosmetics'. Note that you can only compute an aggregation trait for event properties that have a numeric value.\n\nUser-level examples:\n\nOrder Revenue Last 14 Days\nMax Ride Distance Last 60 Days\n\nAccount-level use cases\n\nTotal Minutes Watched 30 Days\nAvg Order Size Last 180 Days\n\nMost Frequent\n\nA most frequent user-level computed trait will return the most common value for an event property. This is helpful to create traits like preferred_product_viewed or most_commonly_viewed_category that tell you what a user\u2019s preferred product, or content category might be. Note that the most frequent computed trait requires the event property to have been tracked at least twice. In the case of a tie, Segment returns the first alphabetical value. For account-level computed traits, you can also return the most frequent user trait. This is helpful when you want to determine which user has performed an event the most frequently. For example, you might to return the email of the user in an account most actively viewing your app.\n\nUser-level examples:\n\nFavorite Blog Post\nTop Purchase Category\n\nAccount-level examples:\n\nMost frequent product viewed\nMost active user\n\nFirst\n\nThe first user-level trait returns the first event property value Segment has seen. This is common for creating traits like first_page_visited based on the page name. For accounts, the first computed trait could also return a trait like first_user_signup, to calculate the first user to use your product.\n\nUser-level examples:\n\nFirst seen timestamp\nFirst utm parameter\n\nAccount-level examples:\n\nFirst email opened\nFirst user signup\n\nLast\n\nThe last trait returns the last event property value Segment has seen. This is common for creating traits like last_utm_campaign to help you calculate last-touch attribution for paid advertising.\n\nUser-level examples:\n\nLast seen at\nLast utm parameter\n\nAccount-level examples:\n\nLast unsubscribe timestamp\nLast user active\n\nUnique List\n\nUnique list computed traits will output a list of unique values in alphabetical order for an event property. This is helpful to understand the different types of products or content that a customer or users in an account have interacted with or purchased. Customers are creating traits like unique_product_categories_viewed and sending them to email marketing tools and accessing them through the Profiles API for in-app personalization.\n\nExample use cases:\n\nUnique products purchased\nUnique categories\nUnique games played\n\nUnique List Count\n\nUnique list count computed traits will output a count of the unique list of values for an event property. Customers are creating traits like unique_product_categories_viewed_count to understand the variety of products that a customer is viewing. At the account-level, customers are creating traits like unique_visitors_count to calculate the number of unique visitors by ip address.\n\nUser-level examples:\n\nUnique products viewed count\nUnique categories count\n\nAccount-level examples:\n\nUnique products viewed\nUnique visitors count\n\nConditions\n\nAll computed trait types support a common \u201cAdd Conditions\u201d section. Conditions defined here restrict the messages considered when calculating the final value of the computed trait by looking at a property of the events. For example, you could limits events to only those where \u201cprice\u201d is greater than 30.00 or where \u201cpage.url\u201d contains \u201cpricing\u201d.\n\nThe following operators are available.\n\nequals\nnot equals\nless than\ngreater than\nless than or equal\ngreater than or equal\ncontains\ndoes not contain\nstarts with\nends with\nexists\nnot exists\nbefore date\nafter date\nequals one of\ncontains one of\nConnecting your Computed Trait to a Destination\n\nSegment sends user-level computed Traits to destinations using the Identify call for user traits, or using the Track call for event properties. Segment includes the trait value and property in the identify and track calls.\n\nFor example, the name of a computed trait is added to the user profile as a trait, and the trait\u2019s value is set to the value of the computed trait. Segment sends an identify or track call when the trait is computed, depending on the destination configuration. If a computed trait counts the number of times a user visits your pricing page, and the user visits your pricing page five times, Segment sends an identify call with the property pricing_page_visits: 5.\n\nLearn more about Computed trait generated events here. The trait name corresponds to the snake cased name that you see in the trait settings, for example most_viewed_page_category. See the list of Engage-compatible destinations\n\nFor account-level computed traits, you have the option to send either a group call and/or identify call. Group calls will send one event per account, whereas identify calls will send an identify call for each user in the account. This means that even if a user hasn\u2019t performed an event, Segment will still set the account-level computed trait on that user. Because most marketing tools are still based at the user level, it is often important to map this account-level trait onto each user within an account. See Account-level Audiences for more information.\n\nView compute status\n\nAfter you create a computed trait, use the Overview page to view a compute progress bar, current status, number of users with the trait, connected destinations, and more. For real-time traits, click Refresh Trait to update the current number of users with the trait.\n\nViewing compute progress\n\nWhen you create a real-time computed trait, you\u2019ll see a progress bar, computed percentage, and status updates. For existing traits that you edit, Segment displays the compute status but not the progress bar or percentage.\n\nEditing Realtime Traits\n\nSegment supports the editing of real-time Traits, which allows you to make nuanced changes to existing Traits in situations where cloning or building from scratch may not suit your use case.\n\nTo edit a real-time Trait, follow these steps:\n\nIn your Unify or Engage space, select the Computed Traits tab.\nSelect the realtime Trait you want to edit.\nSelect the Builder tab and make your edits.\nPreview the results, then select Save Computed Trait to confirm your edits.\n\nSegment then processes your Trait edits. While the edit task runs, the trait remains locked and you can\u2019t make further changes. Once Segment incorporates your changes, you\u2019ll be able to access your updated Trait.\n\nIt is not possible to edit a trait to convert it from real-time to batch, or vice-versa. If the computation type needs to be changed, you will need to recreate the trait with the appropriate conditions.\n\nAccessing your Computed Traits using the Profiles API\n\nYou can access your computed traits using the Profile API by querying the /traits endpoint. For example, you can query for the emails_opened_last_30_days with the following GET request:\n\nhttps://profiles.segment.com/v1/spaces/<workspace_id>/collections/users/profiles/email:john.doe@segment.com/traits?include=emails_opened_last_30_days\n\n\nreturns:\n\n    {\n        \"traits\": {\n            \"emails_opened_last_30_days\": 255\n        },\n        \"cursor\": {\n            \"url\": \"\",\n            \"has_more\": false,\n            \"next\": \"\",\n            \"limit\": 100\n        }\n    }\n\n\nTraits You can query a user\u2019s traits (such as first_name, last_name, and more):\n\nhttps://profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles/<external_id>/traits\n\nBy default, the response includes 20 traits. You can return up to 200 traits by appending ?limit=200 to the querystring. If you wish to return a specific trait, append ?include={trait} to the querystring (for example, ?include=age). You can also use the ?class=audience\u200b or ?class=computed_trait\u200b URL parameters to retrieve audiences or computed traits specifically.\n\nYou can read the full Profile API docs to learn more.\n\nDownloading your Computed Trait as a CSV file\n\nYou can download a copy of your trait by visiting the the computed trait overview page.\nComputed Trait CSVs are generated on demand. Before you can download the CSV, you will need to generate it. There are three different options for formatting:\n\nUnformatted: Contains three columns. The first contains the user or account key, the second contains the trait value and the third is a JSON object containing the external IDs. Generating this CSV is by far the fastest of the three options. Download example unformatted CSV\nDistinct columns for unique external IDs (with indexed columns for ID types with multiple values): Contains the same first three columns as the unformatted CSV. Additional columns are added for each distinct external ID type. When a single row has more than one value for a given external ID type, for example a user with three email addresses, additional columns with indexed headers are added, (email, email_1, email_2). Download example formatted CSV with indexed columns\nDistinct columns for unique external IDs (with additional rows for ID types with multiple values): Contains the same first three columns as the unformatted CSV. Additional columns are added for each distinct external ID type. When a single row has more than one value for a given external ID type, for example a user with two email addresses, additional rows are added with the first three columns repeated (user or account key, trait value and external IDs JSON). Download example formatted CSV with additional rows\n\tGenerating a CSV can take a substantial amount of time for large traits (around 30 seconds for a formatted CSV with 1 million rows). For CSVs that are expected to take over 20 seconds, the Segment app displays an estimated generation time. After clicking Generate, it is recommended that you leave the modal and page open while the CSV is created. (If the trait recalculates between when you click Generate and when you download the file, you might want to regenerate the file. The CSV is a snapshot from when you clicked Generate, and could be outdated.)\n\nYou can\u2019t add account traits and identifiers using the CSV downloader with account level audiences. This is because every row listed in the CSV file is a user, and since account traits and identifiers only exist on accounts, they wouldn\u2019t exist as a user\u2019s custom trait and appear on the CSV.\n\nThis page was last modified: 30 Oct 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nComparing trait types\nTypes of Computed Traits\nConditions\nConnecting your Computed Trait to a Destination\nView compute status\nEditing Realtime Traits\nAccessing your Computed Traits using the Profiles API\nDownloading your Computed Trait as a CSV file\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nDestinations\n/\nDestination Actions\nDestination Actions\nFREE \u2713\nTEAM \u2713\nBUSINESS \u2713\nADD-ON X\n?\n\nThe Destination Actions framework improves on classic destinations by enabling you to see and control how Segment sends the event data it receives from your sources, to actions-based destinations. Each Action in a destination lists the event data it requires, and the event data that is optional.\n\nYou can also choose which event types, event names, or event property values trigger an Action. These Triggers and mappings make it possible to send different versions of the Action, depending on the context from which it is triggered.\n\nEach Actions-framework Destination you see in the Segment catalog represents a feature or capability of the destination which can consume data from your Segment source. The Action clearly lists which data from the events it requires, and which data is optional. For example, Amplitude requires that you always send a LogEvent , or Slack always requires a PostMessage. Each Action also includes a default mapping which you can modify.\n\nBenefits of Destination Actions\nEasier setup: Users see fewer initial settings which can decrease the time spent configuring the destination.\nIncreased transparency: Users can see the exact data that is sent to the destination, and when Segment sends it. For example, users can see exactly when Segment sends an IP address to FullStory or an AnonymousId to Amplitude.\nImproved customization: Users can determine how the events their sources trigger and map to actions supported by the destination. For example, define the exact events that are considered purchases by Braze.\nPartner ownership: Partners can own and contribute to any Actions-based destination that use cloud and device mode (web).\nAvailable Actions-based Destinations\n\nThe following Actions-based Destinations are available:\n\nDESTINATION\tSTATUS\n1Flow Web (Actions)\tBeta\nABsmartly (Actions)\tGA\nAccoil Analytics\tGA\nAcoustic (Actions)\tGA\nActable Predictive\tBeta\nActions Pipedrive\tBeta\nAdobe Target Cloud Mode\tGA\nAdobe Target Web\tGA\nAggregations.io (Actions)\tBeta\nAirship (Actions)\tGA\nAlgolia Insights (Actions)\tGA\nAmazon Ads DSP and AMC\tBeta\nAmplitude (Actions)\tGA\nAngler AI\tBeta\nAppFit\tBeta\nAttio (Actions)\tBeta\nAvo\tGA\nBlackbaud Raiser's Edge NXT\tBeta\nBlend Ai\tBeta\nBraze Cloud Mode (Actions)\tGA\nBraze Cohorts\tGA\nBraze Web Device Mode (Actions)\tGA\nCanny (Actions)\tGA\nChartMogul\tBeta\nCleverTap (Actions)\tGA\nClose\tGA\nCommandBar\tGA\nContentstack Cloud\tBeta\nContentstack Web\tBeta\nCordial (Actions)\tGA\nCriteo Audiences\tGA\nCustomer.io (Actions)\tGA\nDisplay and Video 360 (Actions)\tGA\nDrip (Actions)\tBeta\nDynamic Yield by Mastercard Audiences\tBeta\nEmarsys (Actions)\tBeta\nEncharge (Actions)\tGA\nEquals\tBeta\nFacebook Conversions API (Actions)\tGA\nFacebook Custom Audiences (Actions)\tBeta\nFriendbuy (Cloud Destination)\tGA\nFriendbuy (Web Destination)\tGA\nFullstory (Actions)\tGA\nFullstory Cloud Mode (Actions)\tGA\nGainsight Px Cloud (Actions)\tGA\nGameball (Actions)\tBeta\nGleap (Action)\tBeta\nGoogle Ads Conversions\tGA\nGoogle Analytics 4 Cloud\tGA\nGoogle Analytics 4 Web\tGA\nGoogle Sheets\tGA\nGWEN (Actions)\tBeta\nHubble (Actions)\tBeta\nHubSpot Cloud Mode (Actions)\tGA\nHubSpot Web (Actions)\tGA\nInleads AI\tGA\nInsider Audiences\tGA\nInsider Cloud Mode (Actions)\tGA\nIntercom Cloud Mode (Actions)\tGA\nIntercom Web (Actions)\tGA\nIterable (Actions)\tGA\nIterate Web (Actions)\tBeta\nJimo (Actions)\tBeta\nJune (Actions)\tGA\nKafka\tBeta\nKameleoon (Actions)\tBeta\nKlaviyo (Actions)\tGA\nKoala\tGA\nKoala (Cloud)\tGA\nLaunchDarkly (Actions)\tGA\nLaunchDarkly Audiences\tGA\nLinkedIn Audiences\tGA\nLinkedIn Conversions API\tGA\nListrak (Actions)\tGA\nLiveLike\tBeta\nLiveRamp Audiences\tBeta\nLogRocket\tGA\nLoops (Actions)\tGA\nMarketo Static Lists (Actions)\tGA\nMetronome (Actions)\tGA\nMixpanel (Actions)\tGA\nMoengage (Actions)\tGA\nMoloco MCM\tBeta\nMovable Ink (Actions)\tGA\nOptimizely Advanced Audience Targeting\tBeta\nOptimizely Data Platform\tBeta\nOptimizely Feature Experimentation (Actions)\tGA\nPardot (Actions)\tGA\nPendo Web (Actions)\tBeta\nPinterest Conversions API\tGA\nPlayerZero Web\tGA\nPodscribe (Actions)\tGA\nPostscript\tBeta\nPushwoosh\tBeta\nQualtrics\tGA\nReddit Conversions API\tGA\nRehook\tBeta\nRevX Cloud (Actions)\tBeta\nRipe Cloud Mode (Actions)\tGA\nRipe Device Mode (Actions)\tGA\nRokt Audiences (Actions)\tBeta\nRupt\tBeta\nSalesforce (Actions)\tGA\nSalesforce Marketing Cloud (Actions)\tGA\nSaleswings (Actions)\tGA\nSchematic\tBeta\nScreeb Web (Actions)\tBeta\nSegment Connections\tGA\nSegment Profiles\tGA\nSendGrid\tGA\nSingleStore\tGA\nSlack (Actions)\tGA\nSnapchat Conversions API\tGA\nSprig (Actions)\tGA\nStackAdapt\tBeta\nSurvicate (Actions)\tBeta\nTaboola (Actions)\tBeta\nTalon.One (Actions)\tGA\nThe Trade Desk Crm\tBeta\nTikTok Audiences\tGA\nTikTok Conversions\tGA\nTiktok Offline Conversions\tBeta\nTikTok Pixel\tBeta\nToplyne Cloud Mode (Actions)\tBeta\nTopsort\tBeta\nUpollo Web (Actions)\tBeta\nUsermaven (Actions)\tBeta\nUserMotion (Actions)\tBeta\nUserpilot Cloud (Actions)\tBeta\nUserpilot Web (Actions)\tBeta\nVoucherify (Actions)\tBeta\nVWO Cloud Mode (Actions)\tGA\nVWO Web Mode (Actions)\tBeta\nWebhooks (Actions)\tGA\nWisepops\tGA\nXtremepush (Actions)\tBeta\nYahoo Audiences\tBeta\nDestination Actions compatibility\nDestination Actions are available to all customers on all Segment plans.\nDestination Actions do not require that you disable or change existing destinations. However, to prevent data duplication in the destination tool, you should make sure you aren\u2019t sending the data through both a standard destination and the Actions destination at the same time.\nYou can still use the Event Tester with Destination Actions, and event delivery metrics are still collected and available in the destination information pages.\nIf you are using Protocols, Destination Actions actions are applied after schema filters and transformations. If you are using destination filters, Actions are applied after the filters - meaning that they are not applied to data that is filtered out.\nDestination Actions can not yet be accessed or modified using the Segment APIs.\nComponents of a Destination Action\n\nA Destination Action contains a hierarchy of components, that work together to ensure the right data is sent to the destination.\n\nCOMPONENT\tDESCRIPTION\nGlobal Settings\tDefine authentication and connection-related information like API and Secret keys.\nMappings\tHandle the individual calls to the destination. In them, you define what type of call you want to make to the destination, and what Triggers that call. Individual Destination Actions may come enabled with some predefined mappings to handle common events like Screen calls, Identify calls, and Track calls. Mappings have two components that make this possible: Triggers and an Action.\nTriggers\tEnable you to define when the corresponding Action fires. As part of a Trigger, you can use condition-based filters to narrow the scope of the Trigger. Triggers don\u2019t support matching on event fields containing .$ or .$., which reference an array type.\n\nSelf-service users can add a maximum of two conditions per Trigger.\nActions\tDetermine the information sent to the destination. In the Configure action section, you map the fields that come from your source, to fields that the destination expects to find. Fields on the destination side depend on the type of action selected.\n\nFor example, in the Amplitude (Actions) destination, you define your API and Secret keys in the destination\u2019s global settings. Then, the provided Page Calls mapping:\n\nTriggers the action on all incoming Page events.\nRuns the Log Event action, to map your incoming data to Amplitudes properties.\nSet up a destination action\n\nTo set up a new Actions-framework destination for the first time:\n\nLog in to the Workspace where you want to add the new destination, go to the Catalog page, and click the Destinations tab. (You can also get to this screen by clicking Add Destination either from an existing Source, or from your list of existing destinations.)\nClick the Destination Actions category in the left navigation, then click the destination you want to add.\nFrom the preview screen that appears, click Configure.\nIf prompted, select the source you want to connect to the new destination.\nEnter your credentials. This could be an API Key and secret key, or similar information that allows the destination to connect to your account.\nNext, choose how you want to set up the destination, and click Configure Actions.\nYou can choose Quick Setup to use the default mappings, or choose Customized Setup (if available) to create new mappings and conditions from a blank state. You can always edit these mappings later.\n(Optional) Click Suggest Mappings to get suggested mappings. Learn more about suggested mappings.\nOnce you\u2019re satisfied with your mappings, click Create Destination.\n\nYou must configure and enable at least one mapping to handle a connected source\u2019s event(s) in an Actions-framework destination in order for data to send downstream. Events send downstream in the order in which they appear in the mappings UI. There is no mechanism through which you can control the order of events that send to the downstream destinations outside of that.\n\nMigrate a classic destination to an actions-based destination\n\nMoving from a classic destination to an actions-based destination is a manual process. Segment recommends that you follow the procedure below:\n\nCreate the actions-based destination with your development or test source.\nCopy API keys, connection details, and other settings from the classic destination to the actions-based destination.\nRefer to the actions-based destination\u2019s documentation for information about migrating specific settings.\nDisable the classic version of the destination, and enable the actions-based version.\nVerify that data is flowing from the development or test source to the partner tool.\nRepeat the steps above with your production source.\nMigrate your destination filters from the classic destination to the actions destination\n\nYou can only migrate your destination filters using the Public API if you\u2019re on the business tier plan. This functionality isn\u2019t available in the Segment app.\n\nTo migrate your destination filters to your actions destination from the classic destination:\n\nSend a request to the Public API endpoint.\nUse List Filters from Destination . The destinationId can be found in the URL while viewing the destination in your Segment workspace.\nGrab the response and parse through the data.filters object. Each object returned inside the data.filters object is an individual filter associated with the specified destination.\nSend individual POST requests to the Public API endpoint.\nUse Create Filter for Destination , for each of the filters from step 2.\nSpecify the Actions destinationId, found in the URL when viewing that destination. The body of the request is the individual filters from step 2.\nIf the bodies of those requests don\u2019t already include the field \"enabled\": true, make sure to enable each of those filters after you create them.\nMigrate to an actions-based destination using Destination Filters\n\nFor a more comprehensive migration from a classic destination to an actions-based destination, follow the steps outlined below. This implementation strategy is only available for customers on a Segment Business Tier plan with access to Destination Filters. By adding additional line of defense with Destination Filters, you remove the possibility of duplicate events or dropped events and ensure that events sent before/after a specified received_at timestamp are sent to each destination.\n\nThis migration strategy involves configuring a destination filter on both the Classic destination and the Actions destination. Configure the classic destination filter to block events by the received_at field with a certain value, and the Actions destination to drop events until the received_at timestamp field reaches that same value. Destination Filters within the UI have a limitation where they cannot access any top-level fields, but this is not a limitation for Destination Filters created by the Public API using FQL. Because the received_at is a top-level field in the payload, you\u2019ll need to create a destination filter with the Public API and submit the request with that FQL information described below.\n\nBy combining these Filters, Segment sends events through the Classic integration up until a specified time and then blocks events after that time. Then the Actions integration blocks events until that specified time, and only allows events beginning at that specified time.\n\nThe following code samples show you how you can create filters for your destinations using the Create Filter for Destination Public API operation.\n\nClassic destination\n\nEndpoint: POST https://api.segmentapis.com/destination/classic_destination_id_from_url/filters\n\n// JSON BODY : \n{\n  \"sourceId\": \"add_source_id_here\",\n  \"destinationId\": \"classic_destination_id_from_url\",\n  \"title\": \"drop event after (timestamp) received_at > value April 4, 2023 19:55pm\",\n  \"description\": \"drop event after (timestamp) received_at > value April 4, 2023 19:55pm\",\n  \"if\": \"(received_at >= '2023-04-21T19:55:00.933Z')\",\n  \"actions\": [\n    {\n      \"type\":\"DROP\"\n    }\n  ],\n  \"enabled\": true\n}\n\nActions destination\n\nEndpoint: POST https://api.segmentapis.com/destination/actions_destination_id_from_url/filters\n\n// JSON BODY :\n{\n  \"sourceId\": \"add_source_id_here\",\n  \"destinationId\": \"actions_destination_id_from_url\",\n  \"title\": \"drop event before (timestamp) received_at < value April 4, 2023 19:55pm\",\n  \"description\": \"drop event before (timestamp) received_at < value April 4, 2023 19:55pm\",\n  \"if\": \"(received_at < '2023-04-21T19:55:00.933Z')\",\n  \"actions\": [\n    {\n      \"type\":\"DROP\"\n    }\n  ],\n  \"enabled\": true\n}\n\n\nAfter configuring the Destination Filter on both the Classic and Actions destination, see each destination\u2019s Filters tab and enable the filters. After completing the migration, you can disable the Classic destination on the Settings page, and remove each of the filters from both destinations.\n\nEdit a destination action\n\nYou can add or remove, disable and re-enable, and rename individual actions from the Actions tab on the destination\u2019s information page in the Segment app. Click an individual action to edit it.\n\nFrom the edit screen you can change the action\u2019s name and mapping, and toggle it on or off. See Customizing mappings for more information.\n\nWhen an Action is created, it\u2019s disabled by default, to ensure that it\u2019s only used after being fully configured. To begin sending data through an Action, enable it on the Actions page by selecting the toggle so that it appears blue.\n\nDisable a destination action\n\nIf you find that you need to stop an action from running, but don\u2019t want to delete it completely, you can click the action to select it, then click the toggle next to the action\u2019s name to disable it. This takes effect within minutes, and disables the action until you reenable it.\n\nDelete a destination action\n\nTo delete a destination action: click the action to select it, and click Delete (the trash can icon).\n\nThis takes effect within minutes, and removes the action completely. Any data that would have gone to the destination is not delivered. Once deleted, the saved action cannot be restored.\n\nTest a destination action\n\nTo test a destination action, follow the instructions in Testing Connections. You must enable a mapping in order to test the destination. Otherwise, this error occurs: You may not have any subscriptions that match this event.\n\nYou can also test within the mapping itself. To test the mapping:\n\nNavigate to the Mappings tab of your destination.\nSelect a mapping and click the \u2026 and select Edit Mapping.\nIn step 2 of the mappings edit page, click Load Test Event from Source to add a test event from the source, or you can add your own sample event.\nScroll to step 4 on the page, and click Test Mapping to test the mapping and view the response from the destination.\n\nTest Mapping might not return the events you're looking for\n\nSegment only surfaces a small subset of events for the Test Mapping feature and might not always return the event you\u2019re looking for. If you\u2019d like to test with a specific event, copy a specific event from your Source Debugger and paste it into the Add test event interface.\n\nCustomize mappings\n\nIf you use the default mappings for a destination action, you don\u2019t need to customize the mapping template for the action. You can edit the fields later if you find that the defaults no longer meet your needs.\n\nActions-based destinations have a limit of 50 individual mappings.\n\nTo create a custom destination action, start from the Actions tab. If necessary, click New Mapping to create a new, blank action.\n\nIn the edit panel, define the conditions under which the action should run.\nTest those conditions to make sure that they correctly match an expected event. This step looks for events that match the criteria in the debugger queue, so you might need to Trigger some events with the expected criteria to test your conditions. You can skip the test step if needed, and re-try it at any time.\nSelect data models to enrich your events with.\nSet up the data mapping from the Segment format to the destination tool format.\nYou can click the Source field, then select the Enrichments tab to view and select Enrichments to use.\nTest the mapping with data from a sample event. The edit panel shows you the mapping output in the format for the destination tool. The Select Object option sends the entire object from the event, while the Edit Object option lets you map each individual property. You can change your mapping as needed and re-test.\nWhen you\u2019re satisfied with the mapping, click Save. Segment returns you to the Mappings table.\nIn the Mappings table Status column, verify that the Enabled toggle is on for the mapping you just customized.\n\nThe required fields for a destination mapping appear automatically. Click the + sign to see optional fields.\n\nSuggested mappings\n\nSuggested mappings is fully available for RETL mappings, and is in public beta for event streams and connections.\n\nSegment offers suggested mappings that automatically propose relevant destination fields for both model columns and payload elements. For example, if your model includes a column or payload field named transaction_amount, the feature might suggest mapping it to a destination field like Amount or TransactionValue. This automation, powered by intelligent autocompletion, matches and identifies near-matching field names to streamline the setup. For more information, see Segment\u2019s suggested mappings blogpost and the Suggested Mappings Nutrition Label.\n\nReview the suggested mappings for accuracy before finalizing them as the suggestions aren\u2019t guaranteed to be 100% accurate.\n\nCoalesce function\n\nThe coalesce function takes a primary value and uses it if it is available. If the value isn\u2019t available, the function uses the fallback value instead.\n\nReplace function\n\nThe replace function allows you to replace a string, integer, or boolean with a new value. You have the option to replace up to two values within a single field.\n\nFlatten function\n\nThe flatten function allows you to flatten a nested object to an object with a depth of 1. Keys are delimited by the configured separator. For example, an object like {a: { b: { c: 1 }, d: 2 } } will be converted to { \u2018a.b.c\u2019: 1, \u2018a.d\u2019: 2 }.\n\nConditions\n\nSelf-service users can add a maximum of two conditions per Trigger.\n\nMapping fields are case-sensitive. The following type filters and operators are available to help you build conditions:\n\nEvent type (is/is not). This allows you to filter by the event types in the Segment Spec.\nEvent name (is, is not, contains, does not contain, starts with, ends with). Use these filters to find events that match a specific name, regardless of the event type.\n\nEvent property (is, is not, less than, less than or equal to, greater than, greater than or equal to, contains, does not contain, starts with, ends with, exists, does not exist). Use these filters to trigger the action only when an event with a specific property occurs.\n\nYou can specify nested properties using dot notation, for example context.app.name. If the property might appear in more than one format or location, you can use an ANY statement and add conditions for each of those formats. For example, you might filter for both context.device.type = ios as well as context.os.name = \"iPhone OS\" The does not exist operator matches both a null value or a missing property.\n\nThe available operators depend on the property\u2019s data type:\n\nDATA TYPE\tSUPPORTED OPERATORS\nstring\tis, is not, contains, does not contain, starts with, ends with\nstring or numeric\tis less than, is less than or equal to, is greater than, is greater than or equal to\nboolean\tis true, is false\n\nYou can combine criteria in a single group using ALL or ANY. Use an ANY to \u201csubscribe\u201d to multiple conditions. Use ALL when you need to filter for very specific conditions. You can only create one group condition per destination action. You cannot created nested conditions.\n\nUnsupported Special Characters\n\nMappings do not support the use of double quotes \u201c or a tilde ~ in the trigger fields. In mapping fields, the . character is not supported unless it\u2019s being used to access an object key. If a string has a . in it, that is not supported.\n\nLimitations\n\nMapping fields don\u2019t support dot notation. For example, properties.amount.cost or properties_amount.cost aren\u2019t supported.\n\nDestination Filters\n\nDestination filters are compatible with Destination Actions. Consider a Destination Filter when:\n\nYou need to remove properties from the data sent to the destination\nYou need to filter data from multiple types of call (for example, Track, Page, and Identify calls)\n\nIf your use case does not match these criteria, you might benefit from using Mapping-level Triggers to match only certain events.\n\nDuplicate Mappings\n\nYou can use the Duplicate Mappings feature to create an exact copy of a mapping. The duplicated mapping has the same configurations and enrichments as your original mapping.\n\nDuplicate Mappings supports Actions destinations, Reverse ETL destinations, and destinations connected to Engage Audiences and Journeys.\n\nTo duplicate your mappings:\n\nNavigate to Connections > Destinations and select the destination with the mappings you\u2019d like to copy.\nOn the destination\u2019s Mappings tab, select the menu button (\u2026) and click Duplicate Mapping.\nReview the popup and click Duplicate Mapping.\n\nSegment creates a disabled mapping with the name \u201cOriginal Mapping Name (Copy)\u201d. You must enable the mapping for data to flow.\n\nFAQ and troubleshooting\nValidation error when using the Event Tester\n\nWhen you send an event with an actions destination Event Tester that doesn\u2019t match the trigger of any configured and enabled mappings, you\u2019ll see an error message that states, You may not have any subscriptions that match this event. To resolve the error, create a mapping with a trigger to handle the event being tested, or update the test event\u2019s payload to match the trigger of any existing mappings.\n\nData not sending downstream\n\nIf no mappings are enabled to trigger on an event that has been received from the connected source, the destination will not send any events. Ensure that at least one mapping has been configured and enabled in the destination mappings for an event that you would like to reach downstream.\n\nEvents without mappings enabled to handle them display as being discarded due to \u201cNo matching mapping\u201d in a destination\u2019s Delivery Overview.\n\nMultiple mappings triggered by the same event\n\nWhen the same event triggers multiple mappings, a request will be generated for each mapping that\u2019s configured to trigger on an event. For example, for the Subscription Updated event, if two mappings are enabled and both have conditions defined to trigger on the Subscription Updated event, the two requests will be generated and sent to the destination for each Subscription Updated event.\n\nOauth \u201caccess token expired\u201d message shown in Segment UI\n\nAccess Tokens that were generated from initial authorization, for example, when you connect a destination via Oauth, are always short-lived. Commonly, the token remains valid for 30 minutes to 1 hour. When Segment receives 401 error responses from the destination after a token has expired, it will automatically make another request to the destination for a new token and will then retry the event. Therefore, 401 responses are sometimes expected and do not indicate an event failure. There are three event flows when events are received and sent to a destination:\n\nthrough source\nthrough event tester\nthrough actions tester in mapping screen\n\nThe underlying systems for these flows have their own copy of the token, which can expire at different points in time. Threfore, if you see a 401 error in a sample response, it is likely that you\u2019ll also see another request was made after it, to ask the downstream destination for a new token. Then one more request was made to actually send the data in your payload to the downstream destination.\n\nIs it possible to map a field from one event to another?\n\nSegment integrations process events through mappings individially. This means that no context is held that would allow you to map a value from one event to the field of a subsequent event. Each event itself must contain all of the data you\u2019d like to send downstream in regards to it. For example, you cannot send email in on an Identify call and then access that same email field on a Track call that comes in later if that Track call doesn\u2019t also have email set on it.\n\nI\u2019m getting a \u2018Couldn\u2019t load page\u2019 error when viewing or editing a mapping\n\nThis issue can occur due to a browser cache conflict or if an event property name includes a /. To resolve it, try clearing your browser cache or accessing the mapping page in an incognito window. Additionally, check if the mapped property name contains a /. If it does, rename the property to remove the / and update the mapping.\n\nThis page was last modified: 09 Jan 2025\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nBenefits of Destination Actions\nAvailable Actions-based Destinations\nDestination Actions compatibility\nComponents of a Destination Action\nSet up a destination action\nMigrate a classic destination to an actions-based destination\nEdit a destination action\nDisable a destination action\nDelete a destination action\nTest a destination action\nCustomize mappings\nSuggested mappings\nDuplicate Mappings\nFAQ and troubleshooting\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nRegional Segment\nRegional Segment\nFREE X\nTEAM X\nBUSINESS \u2713\nADD-ON X\n?\n\nOn July 10, 2023, the European Commission adopted the Adequacy Decision for the EU-US Data Privacy Framework (DPF). This concludes that EU personal data transferred to the United States under the DPF is adequately protected when compared to the protection in the EU. With this adequacy decision in place, personal data can safely flow from the EU to US companies participating in the DPF without additional safeguards in place.\n\nTwilio is certified under the DPF and relies on the DPF as its primary personal data transfer mechanism for EU-US personal data transfer. Twilio will rely on the DPF for any Swiss-US personal data transfers as soon as a corresponding Swiss adequacy decision is made. Twilio understands that interpretations of data residency are multi-faceted and some customers might still want their data to reside in the EU. Twilio Segment therefore offers a data residency solution outside of the DPF.\n\nSegment offers customers the option to lead on data residency by providing regional infrastructure in both Europe and the United States. The default region for all users is in Oregon, United States. You can configure workspaces to use the EU West Data Processing Region to ingest (for supported sources), process, filter, deduplicate, and archive data through Segment-managed archives hosted in AWS S3 buckets located in Dublin, Ireland. The regional infrastructure has the same rate limits and SLA as the default region.\n\nExisting Workspaces\n\nTo ensure a smooth transition from a US-based Segment workspace to an EU workspace, Segment will provide additional support and tooling to help with the transition later this year. Use the form link below to provide more information about your current setup and goals for transitioning.\n\nThe Segment UI doesn\u2019t support moving workspaces between regions. To request help with this move, complete the Data Residency Workspace Provisioning Flow form.\n\nClick to access the form\n\nRegional Data Ingestion\n\nRegional Data Ingestion enables you to send data to Segment from both Device-mode and Cloud-mode sources through regionally hosted API ingest points. The regional infrastructure can fail-over across locations within a region, but never across regions.\n\nCloud-event sources\n\nThe following cloud sources are supported in EU workspaces:\n\nAmazon S3\nFactual Engine\nIterable\nClient-side sources\n\nYou can configure Segment\u2019s client-side SDKs for JavaScript, iOS, Android, and React Native sources to send data to a regional host after you\u2019ve updated the Data Ingestion Region in that source\u2019s settings. Segment\u2019s EU instance only supports data ingestion from Dublin, Ireland with the events.eu1.segmentapis.com/ endpoint. If you are using the Segment EU endpoint with an Analytics-C# source, you must manually append v1 to the URL. For instance, events.eu1.segmentapis.com/v1.\n\nFor workspaces that use the EU West Data Processing region, the Dublin Ingestion region is preselected for all sources.\n\nTo set your Data Ingestion Region:\n\nGo to your source.\nSelect the Settings tab.\nClick Regional Settings.\nChoose your Data Ingestion Region.\nIf you\u2019re in the US West data processing region, you can select from: Dublin, Singapore, Oregon, and Sydney.\nIf you\u2019re in the EU West data processing region, Segment\u2019s EU instance only supports data ingestion from Dublin with the events.eu1.segmentapis.com/ endpoint.\n\nAll regions are configured on a per-source basis. You\u2019ll need to configure the region for each source separately if you don\u2019t want to use the default region.\n\nAll Segment client-side SDKs read this setting and update themselves automatically to send data to new endpoints when the app reloads. You don\u2019t need to change code when you switch regions.\n\nServer-side and project sources\n\nWhen you send data from a server-side or project source, you can use the host configuration parameter to send data to the desired region:\n\nOregon (Default) \u2014 https://events.segmentapis.com/v1\nDublin \u2014 https://events.eu1.segmentapis.com/\n\nIf you are using the Segment EU endpoint with an Analytics-C# source, you must manually append v1 to the URL. For instance, events.eu1.segmentapis.com/v1.\n\nHere is an example of how to set the host:\n\nAnalytics.Initialize(\"<YOUR WRITEKEY HERE>\", new Config().SetHost(\"https://events.eu1.segmentapis.com (https://events.eu1.segmentapis.com/)\"));\n\nCreate a new workspace with a different region\n\nUse this form if you need to transition from your existing US-based workspace to an EU workspace.\n\nTo create a workspace with a different data processing region, reach out your Segment account executive, and they will assist you with enabling the feature. Once the feature has been enabled, you\u2019ll be able to self-serve and create a new workspace in a different data processing region by following these steps:\n\nLog in to your Segment account.\nClick New Workspace.\nSelect your Data processing region. This determines the location in which Segment collects, processes, and stores data that\u2019s sent to and from your workspace. You can choose from US West or EU West.\nClick Create workspace.\n\nOnce you create a workspace with a specified data processing region, you can\u2019t change the region. You must create a new workspace to change the region.\n\nEU Storage Updates\nSegment Data Lakes (AWS)\n\nRegional Segment in the EU changes the way you configure the Segment Data Lakes (AWS) environment\n\nWarehouse Public IP Range\n\nUse Segment\u2019s custom CIDR 3.251.148.96/29 while authorizing Segment to write in to your Redshift or Postgres port. BigQuery doesn\u2019t require you to allow a custom IP address.\n\nKnown Limitations\n\nRegional Segment is currently limited to the EU. Future expansion of Regional Segment beyond the EU is under evaluation by Segment Product and R&D.\n\nEdge proxies are deprecated. Customers using Regional Endpoints may see US-based IP addresses in event payloads, Segment recommends using the US-based endpoint (api.segment.io) to preserve client IP addresses. For EU customers, Segment recommends using a Regionalized EU workspace.\n\nDestination support and Regional endpoint availability\n\nDon't see a regional endpoint for a tool you're using?\n\nAs more of the partner tools you use (Sources, Destinations, and Warehouses) start to support a regional endpoint, Segment will update this list. Your contact for that tool should have a timeline for when they\u2019re hoping to support regional data ingestion. You can also visit Segment\u2019s support page for any Segment-related questions.\n\nThe following integrations marked with a  (checkmark) support EU Regional endpoints.\n\nIntegrations available in EU workspaces do not guarantee data residency\n\nBefore you configure an integration, you should check directly with the integration partner to determine if they offer EU endpoints.\n\nAll\nDestinations\nWarehouses\nINTEGRATION\tUS WORKSPACE\tEU WORKSPACE W/ US ENDPOINT\tEU WORKSPACE W/ EU ENDPOINT\nDESTINATIONS\n1Flow\t\t\t\n1Flow Mobile Plugin\t\t\t\n1Flow Web (Actions)\t\t\t\n2mee\t\t\t\nAampe\t\t\t\nAB Smartly\t\t\t\nAB Tasty client side\t\t\t\nABsmartly (Actions)\t\t\t\nAccoil Analytics\t\t\t\nAcoustic (Actions)\t\t\t\nActable Predictive\t\t\t\nActions Pipedrive\t\t\t\nActiveCampaign\t\t\t\nAdikteev\t\t\t\nAdjust\t\t\t\nAdLearn Open Platform\t\t\t\nAdobe Analytics\t\t\t\nAdobe Target Cloud Mode\t\t\t\nAdobe Target Web\t\t\t\nAdQuick\t\t\t\nAdRoll\t\t\t\nAdtriba\t\t\t\nAggregations.io (Actions)\t\t\t\nAirship\t\t\t\nAirship (Actions)\t\t\t\nAkita Customer Success\t\t\t\nAlexa\t\t\t\nAlgolia Insights (Actions)\t\t\t\nAmazon Ads DSP and AMC\t\t\t\nAmazon EventBridge\t\t\t\nAmazon Kinesis\t\t\t\nAmazon Kinesis Firehose\t\t\t\nAmazon Lambda\t\t\t\nAmazon Personalize\t\t\t\nAmbassador\t\t\t\nAmberflo\t\t\t\nAmplitude\t\t\t\nAmplitude (Actions)\t\t\t\nAngler AI\t\t\t\nAnodot\t\t\t\nAppcues\t\t\t\nAppcues Mobile\t\t\t\nAppFit\t\t\t\nAppNexus\t\t\t\nAppsFlyer\t\t\t\nApptimize\t\t\t\nAsayer\t\t\t\nAstrolabe\t\t\t\nAtatus\t\t\t\nAttentive Mobile\t\t\t\nAttio (Actions)\t\t\t\nAttribution\t\t\t\nAuryc\t\t\t\nAutopilotHQ\t\t\t\nAvo\t\t\t\nAWS S3\t\t\t\nAzure Function\t\t\t\nBatch\t\t\t\nBeamer\t\t\t\nBing Ads\t\t\t\nBlackbaud Raiser's Edge NXT\t\t\t\nBlend Ai\t\t\t\nBlendo\t\t\t\nBlitzllama\t\t\t\nBloomreach Engagement\t\t\t\nBlueshift\t\t\t\nBranch Metrics\t\t\t\nBraze\t\t\t\nBraze Cloud Mode (Actions)\t\t\t\nBraze Cohorts\t\t\t\nBraze Web Device Mode (Actions)\t\t\t\nBreyta CRM\t\t\t\nBronto\t\t\t\nBucket\t\t\t\nBugHerd\t\t\t\nBugsnag\t\t\t\nButton\t\t\t\nBuzzBoard\t\t\t\nByteGain\t\t\t\nBytePlus\t\t\t\nCalixa\t\t\t\nCallingly\t\t\t\nCandu\t\t\t\nCanny (Actions)\t\t\t\nCastle\t\t\t\nChameleon\t\t\t\nChartbeat\t\t\t\nChartMogul\t\t\t\nChurned\t\t\t\nChurnZero\t\t\t\nClearbit Enrichment\t\t\t\nClearbit Reveal\t\t\t\nClearBrain\t\t\t\nCleverTap\t\t\t\nCleverTap (Actions)\t\t\t\nClicky\t\t\t\nClientSuccess\t\t\t\nCliff\t\t\t\nClose\t\t\t\nCommandBar\t\t\t\ncomScore\t\t\t\nContentstack Cloud\t\t\t\nContentstack Web\t\t\t\nConvertFlow\t\t\t\nConvertly\t\t\t\nConvertro\t\t\t\nCordial (Actions)\t\t\t\nCorrelated\t\t\t\nCountly\t\t\t\nCourier\t\t\t\nCrazy Egg\t\t\t\nCrisp\t\t\t\nCriteo App & Web Events\t\t\t\nCriteo Audiences\t\t\t\nCrittercism\t\t\t\nCrossing Minds\t\t\t\nCrowdPower\t\t\t\nCruncher\t\t\t\nCustify\t\t\t\nCustomer.io\t\t\t\nCustomer.io (Actions)\t\t\t\nData Lakes\t\t\t\nDelighted\t\t\t\nDelivr.ai Resolve\t\t\t\nDisplay and Video 360 (Actions)\t\t\t\nDoubleClick Floodlight\t\t\t\nDreamdata\t\t\t\nDrip\t\t\t\nDrip (Actions)\t\t\t\nDynamic Yield by Mastercard Audiences\t\t\t\nElevio\t\t\t\nEloqua\t\t\t\nEmail Aptitude\t\t\t\nEmarsys\t\t\t\nEmarsys (Actions)\t\t\t\nEMMA\t\t\t\nEncharge (Actions)\t\t\t\nEngage Messaging\t\t\t\nEnjoyHQ\t\t\t\nEPICA\t\t\t\nEquals\t\t\t\nErrorception\t\t\t\nevents.win\t\t\t\nEverflow\t\t\t\nExperiments by GrowthHackers\t\t\t\nExtole Platform\t\t\t\nFacebook App Events\t\t\t\nFacebook Conversions API (Actions)\t\t\t\nFacebook Custom Audiences (Actions)\t\t\t\nFacebook Offline Conversions\t\t\t\nFacebook Pixel\t\t\t\nFactorsAI\t\t\t\nFirebase\t\t\t\nFL0\t\t\t\nFlagship.io\t\t\t\nFlurry\t\t\t\nFoxMetrics\t\t\t\nFreshmarketer\t\t\t\nFreshsales\t\t\t\nFreshsales Suite - CRM\t\t\t\nFriendbuy (Cloud Destination)\t\t\t\nFriendbuy (Legacy)\t\t\t\nFriendbuy (Web Destination)\t\t\t\nFullStory\t\t\t\nFullstory (Actions)\t\t\t\nFullstory Cloud Mode (Actions)\t\t\t\nFunnelEnvy\t\t\t\nFunnelFox\t\t\t\nGainsight\t\t\t\nGainsight PX\t\t\t\nGainsight Px Cloud (Actions)\t\t\t\nGameball (Actions)\t\t\t\nGauges\t\t\t\nGist\t\t\t\nGleap (Action)\t\t\t\nGoogle Ads (Classic)\t\t\t\nGoogle Ads (Gtag)\t\t\t\nGoogle Ads Conversions\t\t\t\nGoogle Ads Remarketing Lists\t\t\t\nGoogle Analytics 4 Cloud\t\t\t\nGoogle Analytics 4 Web\t\t\t\nGoogle Cloud Function\t\t\t\nGoogle Cloud PubSub\t\t\t\nGoogle Cloud Storage\t\t\t\nGoogle Sheets\t\t\t\nGoogle Tag Manager\t\t\t\nGoSquared\t\t\t\nGraphJSON\t\t\t\nGroundswell\t\t\t\nGWEN (Actions)\t\t\t\nHasOffers\t\t\t\nHawkei\t\t\t\nHeap\t\t\t\nHello Bar\t\t\t\nHelp Scout\t\t\t\nHitTail\t\t\t\nHotjar\t\t\t\nHouseware\t\t\t\nHubble (Actions)\t\t\t\nHubSpot\t\t\t\nHubSpot Cloud Mode (Actions)\t\t\t\nHubSpot Web (Actions)\t\t\t\nHumanic AI\t\t\t\nhydra\t\t\t\nIBM UBX\t\t\t\nImpact Partnership Cloud\t\t\t\nImprovely\t\t\t\nIndicative\t\t\t\nInflection\t\t\t\nInkit\t\t\t\nInleads AI\t\t\t\nInMoment (formerly Wootric)\t\t\t\nInnovid\t\t\t\nInsider Audiences\t\t\t\nInsider Cloud Mode (Actions)\t\t\t\nInspectlet\t\t\t\nIntercom\t\t\t\nIntercom Cloud Mode (Actions)\t\t\t\nIntercom Web (Actions)\t\t\t\nIron.io\t\t\t\nIterable\t\t\t\nIterable (Actions)\t\t\t\nIterate Web (Actions)\t\t\t\nJimo\t\t\t\nJimo (Actions)\t\t\t\nJivox\t\t\t\njourny io\t\t\t\nJune\t\t\t\nJune (Actions)\t\t\t\nKable\t\t\t\nKafka\t\t\t\nKahuna\t\t\t\nKameleoon (Actions)\t\t\t\nKana\t\t\t\nKeen\t\t\t\nKevel\t\t\t\nKissmetrics\t\t\t\nKitemetrics\t\t\t\nKlaviyo\t\t\t\nKlaviyo (Actions)\t\t\t\nKoala\t\t\t\nKoala (Cloud)\t\t\t\nKochava\t\t\t\nKubit\t\t\t\nKustomer\t\t\t\nLaunchDarkly (Actions)\t\t\t\nLaunchDarkly Audiences\t\t\t\nLeanplum\t\t\t\nLearndot\t\t\t\nLibrato\t\t\t\nLinkedIn Audiences\t\t\t\nLinkedIn Conversions API\t\t\t\nLinkedIn Insight Tag\t\t\t\nListrak (Actions)\t\t\t\nLiveChat\t\t\t\nLiveIntent Audiences\t\t\t\nLiveLike\t\t\t\nLiveRamp Audiences\t\t\t\nLocalytics\t\t\t\nLogRocket\t\t\t\nLoops (Actions)\t\t\t\nLou\t\t\t\nLucky Orange\t\t\t\nLumen\t\t\t\nLytics\t\t\t\nmabl\t\t\t\nMadkudu\t\t\t\nMailChimp\t\t\t\nMailjet\t\t\t\nMailmodo\t\t\t\nMammoth\t\t\t\nMarketo Static Lists (Actions)\t\t\t\nMarketo V2\t\t\t\nMarkettailor\t\t\t\nMatcha\t\t\t\nMatomo\t\t\t\nMediaMath\t\t\t\nMetronome (Actions)\t\t\t\nMillennial Media\t\t\t\nMixpanel (Actions)\t\t\t\nMixpanel (Legacy)\t\t\t\nMoEngage\t\t\t\nMoengage (Actions)\t\t\t\nMoesif API Analytics\t\t\t\nMoloco MCM\t\t\t\nMonetate\t\t\t\nMoosend\t\t\t\nMouseflow\t\t\t\nMouseStats\t\t\t\nMovable Ink (Actions)\t\t\t\nMutiny\t\t\t\nNanigans\t\t\t\nNat\t\t\t\nNatero\t\t\t\nNavilytics\t\t\t\nNew Relic\t\t\t\nNielsen DCR\t\t\t\nNinetailed by Contentful\t\t\t\nNoora\t\t\t\nNudgespot\t\t\t\nOlark\t\t\t\nOneSignal (New)\t\t\t\nOptimizely Advanced Audience Targeting\t\t\t\nOptimizely Data Platform\t\t\t\nOptimizely Feature Experimentation (Actions)\t\t\t\nOptimizely Full Stack\t\t\t\nOptimizely Web\t\t\t\nOrb\t\t\t\nOrtto\t\t\t\nPardot (Actions)\t\t\t\nParsely\t\t\t\nPeaka\t\t\t\nPendo\t\t\t\nPendo Web (Actions)\t\t\t\nPerfect Audience\t\t\t\nPerkville\t\t\t\nPersistIQ\t\t\t\nPersonas Facebook Custom Audiences\t\t\t\nPersonyze\t\t\t\nPingdom\t\t\t\nPinterest Audiences\t\t\t\nPinterest Conversions API\t\t\t\nPinterest Tag\t\t\t\nPlanhat\t\t\t\nPlayerZero Web\t\t\t\nPlotline\t\t\t\nPodscribe (Actions)\t\t\t\nPodsights\t\t\t\nPointillist\t\t\t\nPostHog\t\t\t\nPostscript\t\t\t\nProductBird\t\t\t\nProfitWell\t\t\t\nProof Experiences\t\t\t\nProsperStack\t\t\t\nPushwoosh\t\t\t\nQualaroo\t\t\t\nQualtrics\t\t\t\nQuantcast\t\t\t\nQuanticMind\t\t\t\nQuora Conversion Pixel\t\t\t\nRabble AI\t\t\t\nRadiumOne Connect\t\t\t\nRamen\t\t\t\nRecombee AI\t\t\t\nReddit Conversions API\t\t\t\nRefersion\t\t\t\nRefiner\t\t\t\nRegal.io\t\t\t\nRehook\t\t\t\nRepeater\t\t\t\nResponsys\t\t\t\nRetina\t\t\t\nRevX Cloud (Actions)\t\t\t\nRichpanel\t\t\t\nRipe Cloud Mode (Actions)\t\t\t\nRipe Device Mode (Actions)\t\t\t\nRockerbox\t\t\t\nRokt\t\t\t\nRokt Audiences (Actions)\t\t\t\nRollbar\t\t\t\nRupt\t\t\t\nSaaSquatch v2\t\t\t\nSailthru v2\t\t\t\nSalescamp CRM\t\t\t\nSalesforce (Actions)\t\t\t\nSalesforce Marketing Cloud (Actions)\t\t\t\nSalesmachine\t\t\t\nSaleswings (Actions)\t\t\t\nSatisMeter\t\t\t\nSavio\t\t\t\nSchematic\t\t\t\nScopeAI\t\t\t\nScreeb\t\t\t\nScreeb Web (Actions)\t\t\t\nScuba Analytics\t\t\t\nSegment Connections\t\t\t\nSegment Data Lakes (Azure)\t\t\t\nSegment Profiles\t\t\t\nSegMetrics\t\t\t\nSelligent Marketing Cloud\t\t\t\nSendGrid\t\t\t\nSentry\t\t\t\nSerenytics\t\t\t\nShareASale\t\t\t\nSherlock\t\t\t\nSIGNL4 Alerting\t\t\t\nSimpleReach\t\t\t\nSingleStore\t\t\t\nSingular\t\t\t\nSkalin\t\t\t\nSlack\t\t\t\nSlack (Actions)\t\t\t\nSlicingDice\t\t\t\nSmartlook\t\t\t\nSnapboard\t\t\t\nSnapchat Audiences\t\t\t\nSnapchat Conversions API\t\t\t\nSnapEngage\t\t\t\nSpideo\t\t\t\nSpinnakr\t\t\t\nSplit\t\t\t\nSprig (Actions)\t\t\t\nSprig Cloud\t\t\t\nStackAdapt\t\t\t\nStartdeliver\t\t\t\nStartdeliver-v2\t\t\t\nStatsig\t\t\t\nStitch Data\t\t\t\nStonly\t\t\t\nStories\t\t\t\nStormly\t\t\t\nStrikedeck\t\t\t\nSurvicate\t\t\t\nSurvicate (Actions)\t\t\t\nSwrve\t\t\t\nTaboola (Actions)\t\t\t\nTalkable\t\t\t\nTalon.One\t\t\t\nTalon.One (Actions)\t\t\t\nTamber\t\t\t\nTaplytics\t\t\t\nTapstream\t\t\t\nThe Trade Desk Crm\t\t\t\nTikTok Audiences\t\t\t\nTikTok Conversions\t\t\t\nTiktok Offline Conversions\t\t\t\nTikTok Pixel\t\t\t\nToplyne Cloud Mode (Actions)\t\t\t\nTopsort\t\t\t\nTotango\t\t\t\nTrack JS\t\t\t\nTrackier\t\t\t\nTractionboard\t\t\t\nTrafficGuard\t\t\t\ntray.io\t\t\t\nTreasure Data\t\t\t\nTrustpilot\t\t\t\nTUNE\t\t\t\nTwitter Ads\t\t\t\nUnwaffle\t\t\t\nUpcall\t\t\t\nUpollo\t\t\t\nUpollo Web (Actions)\t\t\t\nUser.com\t\t\t\nUserGuiding\t\t\t\nUserIQ\t\t\t\nUserlist\t\t\t\nUsermaven (Actions)\t\t\t\nUserMotion (Actions)\t\t\t\nUserpilot Cloud (Actions)\t\t\t\nUserpilot Web (Actions)\t\t\t\nUserpilot Web Plugin\t\t\t\nUserVoice\t\t\t\nVariance\t\t\t\nVero\t\t\t\nVespucci\t\t\t\nVidora\t\t\t\nVisual Website Optimizer\t\t\t\nVitally\t\t\t\nVoucherify\t\t\t\nVoucherify (Actions)\t\t\t\nVWO Cloud Mode (Actions)\t\t\t\nVWO Web Mode (Actions)\t\t\t\nWalkMe\t\t\t\nWebEngage\t\t\t\nWebhooks (Actions)\t\t\t\nWhale Alerts\t\t\t\nWhale Watch\t\t\t\nWigzo\t\t\t\nWindsor\t\t\t\nWisepops\t\t\t\nWishpond\t\t\t\nWoopra\t\t\t\nXplenty\t\t\t\nXtremepush\t\t\t\nXtremepush (Actions)\t\t\t\nYahoo Audiences\t\t\t\nYandex Metrica\t\t\t\nYellowhammer\t\t\t\nYoubora\t\t\t\nZaius\t\t\t\nZapier\t\t\t\nZendesk\t\t\t\nZopim\t\t\t\nWAREHOUSES\nAmazon S3\t\t\t\nAWS S3\t\t\t\nAzure Synapse Analytics Warehouse\t\t\t\nBigQuery\t\t\t\nDatabricks\t\t\t\nIBM Db2 Warehouse\t\t\t\nPostgres\t\t\t\nRedshift\t\t\t\nSegment Data Lakes\t\t\t\nSnowflake\t\t\t\nSource Regional support\n\nDon't see regional support for a source you're using?\n\nAs more of the partner Sources start to support posting data to our regional endpoint, Segment will update this list. Your contact for that tool should have a timeline for when they\u2019re hoping to support regional data ingestion. You can also visit Segment\u2019s support page for any Segment-related questions.\n\nThe following Sources marked with a  (checkmark) are supported in EU workspaces.\n\nINTEGRATION\tUS WORKSPACE\tEU WORKSPACE\nSOURCES\n.NET\t\t\nActiveCampaign\t\t\nAircall\t\t\nAirship\t\t\nAlloy Flow\t\t\nAmazon S3\t\t\nAmplitude Cohorts\t\t\nAntavo\t\t\nApple\t\t\nAuthvia\t\t\nAutopilotHQ\t\t\nBeamer\t\t\nBlip\t\t\nBluedot\t\t\nBlueshift\t\t\nBraze\t\t\nCandu\t\t\nChatlio\t\t\nCleverTap\t\t\nClojure\t\t\nCommandBar\t\t\nConfigCat\t\t\nCustomer.io\t\t\nDelighted\t\t\nDrip\t\t\nElastic Path\t\t\nElastic Path CX Studio\t\t\nFacebook Ads\t\t\nFacebook Lead Ads\t\t\nFactual Engine\t\t\nFlutter\t\t\nFoursquare Movement\t\t\nFreshchat\t\t\nFriendbuy\t\t\nGladly\t\t\nGo\t\t\nGoogle Ads\t\t\nGWEN Webhooks\t\t\nHerow\t\t\nHTTP API\t\t\nHubSpot\t\t\nIBM Watson Assistant\t\t\nInflection\t\t\nInsider\t\t\nIntercom\t\t\nIterable\t\t\nJava\t\t\nJavascript\t\t\nJebbit\t\t\nKlaviyo\t\t\nKlenty\t\t\nKotlin\t\t\nKotlin (Android)\t\t\nLaunchDarkly\t\t\nLeanplum\t\t\nListrak\t\t\nLiveLike (Source)\t\t\nLooker\t\t\nMailchimp\t\t\nMailjet\t\t\nMailmodo\t\t\nMandrill\t\t\nMarketo\t\t\nMixpanel Cohorts\t\t\nMoEngage (Source)\t\t\nMoesif API Analytics\t\t\nNavattic\t\t\nNudgespot\t\t\nOne Creation\t\t\nOneSignal\t\t\nOneTrust\t\t\nPaytronix\t\t\nPendo\t\t\nPHP\t\t\nPixel Tracking API\t\t\nProveSource\t\t\nPushwoosh Source\t\t\nPython\t\t\nQualtrics\t\t\nQuin AI\t\t\nRadar\t\t\nReact Native\t\t\nRefiner\t\t\nRoku (alpha)\t\t\nRuby\t\t\nSalesforce\t\t\nSalesforce Marketing Cloud\t\t\nSelligent Marketing Cloud\t\t\nSendGrid\t\t\nSendGrid Marketing Campaigns\t\t\nShopify (by Littledata)\t\t\nShopify - Powered by Fueled\t\t\nStatsig\t\t\nStripe\t\t\nSynap\t\t\nTwilio\t\t\nUnity\t\t\nUpollo\t\t\nUserGuiding\t\t\nVero\t\t\nVoucherify\t\t\nWhite Label Loyalty\t\t\nWorkRamp\t\t\nXamarin\t\t\nYotpo\t\t\nZendesk\t\t\n\nThis page was last modified: 29 May 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nExisting Workspaces\nRegional Data Ingestion\nCreate a new workspace with a different region\nEU Storage Updates\nKnown Limitations\nDestination support and Regional endpoint availability\nSource Regional support\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nReplay\nReplay\nFREE X\nTEAM X\nBUSINESS \u2713\nADD-ON X\n?\n\nReplay takes an archived copy of your Segment data, and re-sends it to new or existing tools providing huge benefits to mature data systems. By archiving and replaying data, you can avoid vendor lock-in, and protect your system against data loss.\n\nReplays for tooling changes\n\nWith Replays, you can send your existing data to new tools. This means you can send a limited sample of your data to a new tool to test it out, and run similar tools in parallel to verify the data format or accuracy of the output. Finally, when you\u2019re ready to switch to a new tool, you can replay a full set of your data to the new tool to backfill it with data that extends before you set up the tool - no warm-up time or operational gap to disrupt your work.\n\nNote\n\nAny destinations which accept cloud-mode data (meaning data from Segment, and not directly from users\u2019 devices) can use replay, however they must also process timestamps on the data for replay to be useful.\n\nReplays for resilience\n\nWith Replays, you\u2019re protected from outages and errors. If a destination which you rely on experiences an outage, or is temporarily unable to accept incoming data, you can use Replays to re-send data to that tool once the service recovers. You can also use Replays to recover from errors caused by misconfigurations in your Segment systems. For example, if you send data in the wrong format, or want to apply destination filters. In this case, you can change your mapping using a destination filter, clear out the bad data, and replay it to that destination. You can also use this to update the schema in your data warehouse when it changes.\n\nFor more information, Contact us and our Success Engineers will walk you through the process.\n\nReplays considerations\n\nReplays are currently only available for Business Tier customers, and due to their complex nature are not self-serve. Contact us to learn more, or to request a replay for your workspace. When requesting a replay, include the workspace, the source to replay from, the destination tool or tools, and the time period.\n\nReplays can process unlimited data, but they\u2019re rate limited to respect limitations in downstream partner tools. If you\u2019re also sending data to the destination being replayed to in real time, then, when determining your replay\u2019s limit, you\u2019ll want to take into account the rate limit being used by real-time events. You should also account for a small margin of your rate limit to allow events to be retried.\n\nReplay time depends both on the tool Segment replays to and the amount of data included in the replay.\n\nReplays do not affect your MTU count, unless you are using a Repeater destination. Notify your team before initiating a Replay if you\u2019re using a Repeater destination.\n\nOnce a replay starts, you will not see replayed events in the Event Delivery tab.\n\nYou can initiate replays for some or all events, but you can\u2019t apply conditional filters that exclude certain rows of data from being replayed. You can set up destination filters to conditionally filter replayed events.\n\nThe destination is not required to be enabled in order for a replay to be successful, including Destination Functions.\n\nThe destination must be connected to the source, but can remain disabled while the replay is running.\nDestination filters are still considered when you run replays on disabled destinations.\nThere are a few exceptions for destinations that must be enabled for the replay to be successful : Amazon S3 and Google Cloud Source (GCS).\nReplay-eligible destinations\n\nReplays are available for any destinations which support cloud-mode data (meaning data routed through Segment) and which also process timestamps. Destinations that are only available in device-mode (meaning where data is sent directly from the users\u2019 devices to the destination tool) cannot receive Replays.\n\nNot all destinations support data deduplication, so you may need to delete, archive, or remove any older versions of the data before initiating a replay. contact Segment support if you have questions or want help.\n\nReplays & Destination Filters\n\nReplays are subject to the Destination Filters you\u2019ve configured on that destination. For example, if you request that Identify calls be included in the replay, but your destination has a Destination Filter that blocks Identify events, the filter then blocks all Identify events from making it to the destination. In this case, Segment recommends that you avoid including Identify events in the replay if you know they\u2019ll be blocked by the destination filter.\n\nWhen you request a replay, Segment asks you to provide a list of the events (type and/or name) that you want included in the replay. If you specify a list of events, then Segment only includes those specified events in the replay. If you need to exclude events in your replay, contact Segment support. The Segment team can help you handle filtering you\u2019re unable to do in the replay.\n\nReplays & Engage\n\nThere are two types of replays with Engage.\n\nReplay a Profile Source\u2019s data into Engage Space, (sending a standard source\u2019s data into an Engage Space), which can be configured to send over a specified timeframe as well as the ability to specify all or only a specific subset of events by type or name.\n\nReplay from an Engage Space to its connected destination, (sending data from an Engage Output Source to its connected destination), which includes all the computational data (Audiences, Computed Traits, Journeys) that destination is currently configured to receive, which can be configured to send over a specified timeframe as well as the ability to specify all or only a specific subset of events by type or name.\n\nNuances to Consider for Engage Replays\n\n1. Replay a Profile Source\u2019s data into Engage Space\n\nWhen a new Profile Source is connected to an Engage Space, the default option to replay the source\u2019s data seen over the past 30 days can be selected. To request a source\u2019s additional historical data be replayed to the Engage Space, contact Segment Support at friends@segment.com or create a ticket. Please see this documentation on further details of this process and what to include in your support request.\n\n2. Replay from an Engage Space to its connected destination\n\nSince each instance of a destination is connected to its own Engage \u201cOutput\u201d source, that source contains events for all of the computations that destination is connected to received data from, the list of output sources can be found under Unify > Unify Settings > Debugger. Because of this, it\u2019s not possible to replay only a specific computation\u2019s data to the destination, you should instead consider reaching out to Segment support to request a resync of that computation to its destination instead. However, if you would like to replay all failed events seen by that destination, which will encompass all connected computations, that can be achieved with a replay.\nNote: The replay will be sending historical data to the destination, potentially overwriting the destination with outdated data if more recent data has been sent from the computation to the destination. In this case, a resync of the computation might also be more advantageous to get the most up-to-date data resent to the destination.\nRate limits for replays are configurable and can be increased or decreased upon request. However, there are some destinations which have strict rate limits and cannot be configured to send data at a higher rate than what\u2019s stated within the table on Rate limits on Engage Event Destinations.\nEngage : Replay versus Resync\nReplay : A replay resends all events, specific events by type or name, or failed events over a specified period of time to the destination.\nResync : A resync sends events for a computation\u2019s (Audience, Computed Trait, Journey) entire current user base to its connected destination.\n\nThis page was last modified: 05 Jun 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nReplays for tooling changes\nReplays for resilience\nReplays considerations\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nUnify Onboarding Guide\nUnify Onboarding Guide\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\n\nThis guide walks you through the set up process for a simple Unify space, which you can use if your Segment implementation is simple. If your implementation is complex, you can use this to demonstrate and test Unify before working on a more complex configuration.\n\nIf you\u2019re using Engage, visit the Engage Foundations Onboarding Guide for additional steps to create audiences, connect to destinations, and more.\n\nUnify configuration requirements\n\nTo configure and use Unify, you need the following:\n\nA Segment account and Workspace.\nEvents flowing into Connections from your digital properties where most of your valuable user behavior occurs.\nUnify or Engage identity admin access. You must have edit access to identity resolution rules. You can check your permissions by navigating to Access Management in your workspace settings. See the Segment Access Management documentation for more details.\nStep 1: Create a new Developer space\n\nWhen you first start working with Unify, you should start by creating a \u201cDeveloper\u201d space. This is your experimental and test environment while you learn more about how Unify works. You can validate that identity resolution is working correctly in the Developer space, and then apply those changes to your Production space once you\u2019re sure everything is working as expected.\n\nThis two-space method prevents you from making untested configuration changes that immediately affect production data.\n\nStep 2: Invite teammates to your Segment space\n\nYou probably have teammates who help set up your Segment Workspace with the data you need. Invite them to your Unify dev space and grant them access to the space. Navigate to Access Management in your workspace settings to add them.\n\nStep 3: Connect production sources\nFrom your Segment space, navigate to Unify settings and click Profile sources.\nOn the screen that appears, choose one or two production sources from your Connections workspace. Segment recommends connecting your production website or App source as a great starting point.\n\nIf the source you want to add doesn\u2019t appear on the list, then check if the source is enabled. If the source is enabled, verify that you have set up a connection policy which enforces that you can only add sources with specific labels to this space. Read more about Segment\u2019s connection policy in the Space Setup docs.\n\nTip: It sounds a little counter- intuitive to connect a production source to a developer space, but your production sources have rich user data in them, which is what you need to build and validate user profiles.\n\nOnce you select sources, Segment starts a replay of one month of historical data from these sources into your Unify space. Segment does this step first so you have some user data to build your first profiles.\n\nThe replay usually takes several hours, but the duration will vary depending on how much data you have sent through these sources in the past one month. When the replay finishes, you are notified in the Sources tab under Settings, shown below.\n\nNote: Data replays start with the earliest (oldest) chronological events in the one month window, and finish with the most recent. Don\u2019t continue to the next step until all replays are marked complete. If you do, the data in your Unify data will be stale.\n\nOnce the Source(s) finish replaying, data from your connected Sources flows into Unify in near real time, just like it does for sources in your Segment workspace.\n\nStep 4: Check your profile data\n\nOnce the replay finishes, you can see the data replayed into Unify using the Profile explorer. You should have a lot! The data should include information from multiple sources and multiple sessions, all resolved into a single profile per user.\n\nBefore you continue, check a few user profiles to make sure they show an accurate and recent snapshot of your users.\n\nA good test is to look at your own user profile, and maybe some colleagues\u2019 profiles. Look in the Profile explorer for your Profile, and look at your event history, custom traits and identifiers. If these identifiers look correct across a few different profiles (and you can verify that they are all correct), then you\u2019re ready to create an audience.\n\nIf your user profiles look wrong, or you aren\u2019t confident users are being accurately defined and merged, stop here and troubleshoot. It\u2019s important to have accurate identity resolution before you continue. See the detailed Identity Resolution documentation to better understand how it works, and why you may be running into problems. (Still need help? Contact Segment for assistance.)\n\nStep 5: Create your production space\n\nOnce you validate that your data is flowing through Unify, you\u2019re ready to create a Production space. Segment recommends that you repeat the same steps outlined above, focusing on your production use cases and data sources.\n\nIf you\u2019re using Engage, view additional steps to complete your space set up in the Engage Foundations Onboarding Guide.\n\nYou can rename the Segment space UI name, but can\u2019t modify the space slug. As a result, you can\u2019t change the URL of a space.\n\nThis page was last modified: 12 Aug 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nUnify configuration requirements\nStep 1: Create a new Developer space\nStep 2: Invite teammates to your Segment space\nStep 3: Connect production sources\nStep 4: Check your profile data\nStep 5: Create your production space\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSpec\n/\nSpec: Semantic Events\nSpec: Semantic Events\n\nOne of the core components of the Segment Spec is the Track call. It describes any arbitrary event that the user has triggered. For some industry verticals and applications, Segment has standardized event names. For Ecommerce tracking, for example, there are specific event names and properties that we recognize semantically. This semantic meaning allows Segment to specially recognize and transform key events before sending them off to each different tool.\n\nThere are a few places where Segment has standardized event names and properties already:\n\nMobile\nA/B Testing\nEcommerce\nEmail\nLive Chat\nVideo\n\nIn the future Segment plans to standardize event names from other data sources as well.\n\nThis page was last modified: 21 Nov 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nHow To Guides\n/\nJoining User Profiles\nJoining User Profiles\n\nOne of the first questions we get when our customers start querying all of their data is, how do I join all this data together? For example, let\u2019s say you\u2019d like to know if support interactions in Zendesk increase revenue in Stripe, or which percentage of users opened your email campaign and visited your website or mobile app? The key to answering these advanced questions is tying your data together across these sources. To do that, you need a common user identifier.\n\nWhat is the user ID problem?\n\nEach SaaS tool you use has its own way of identifying users with a unique primary key. And, you will find each of these different IDs across different collections of tables in your database. So, when you want to start matching Joe Smith who entered a ticket in Zendesk and also clicked through a campaign in Mailchimp, it starts to get tricky.\n\nFor example, Stripe keeps track of users with a customer_id, Segment requires auser_id, and Marketo uses email to uniquely identify each person.\n\nTo effectively join across these sources, you need to understand how each id maps to each other. The best way to do this is to create a common identifier across tools.\n\nUse a common identifier when possible\n\nWhen you install a new tool (or use Segment to install all of them at once), you need to choose what you will put in the ID field. There are lots of different options for this: emails, twitter handles, usernames, and more.\n\nHowever, we suggest using the same ID you generate from your production database when you create a new user. Database IDs never change, so they are more reliable than emails and usernames that users can switch at their leisure. If you use this same database ID across as many tools as possible, it will be easier to join identities down the road. (In MongoDB, it would look something like this 507f191e810c19729de860ea.)\n\nanalytics.identify('1e810c197e', { // that's the user ID from the database\n\u00a0 name: 'Jane Kim',\n\u00a0 email: 'jane.kim@example.com'// also includes email\n\u00a0 });\n\n\nThough we wish you could use a database ID for everything, some tools force you to identify users with an email. Therefore, you should make sure to send email along to all of your other tools, so you can join on that trait as a fallback.\n\nFor Segment Destination Users\n\nIntegrating as many tools as possible through Segment will make your joins down the road a little easier. When you use Segment to identify users, we\u2019ll send the same ID and traits out to all the destinations you turn on in our interface. (More about Segment destinations.\n\nA few of our destination partners accept an external ID, where they will insert the same Segment user ID. Then you can join tables in one swoop. For example, Zendesk saves the Segment User ID as external_id, making a Segment-Zendesk join look like this:\n\nSELECT zendesk.external_id, users.user_id\nFROM zendesk.tickets zendesk\nJOIN segment.usersusers\nON zendesk.tickets.external_id = segment.user_id\n\n\nHere\u2019s a look at the Segment destinations that store the Segment User ID:\n\nTOOL\tCORRESPONDING TRAIT\tCORRESPONDING SOURCES TABLE\nZendesk\texternal_id\tzendesk.tickets.external_id\nMailchimp\tunique_email_id\tmailchimp.lists.unique_email_id\nIntercom\tuser_id\tintercom.users.user_id\nHow to merge identities\n\nWhether you\u2019re using Segment or not, we suggest creating a master user identities table that maps IDs for each of your sources.\n\nThis table will cut down on the number of joins you have to do because some IDs may only exist in one out of many tables related to a source.\n\nHere\u2019s sample query to create a master user identities table:\n\nCREATE TABLE user_identities AS (\nselect\nsegment.id as segment_id,\nsegment.email as email,\nzendesk.id as zendesk_id,\nstripe.id as stripe_id,\nsalesforce.id as salesforce_id,\nintercom.id as intercom_id\n\nfrom segment.users segment\n\n\u2013 Zendesk\nleftjoin zendesk.users zendesk on\n ( zendesk.external_id = segment.id\u2013 if enabled through Segment\nor zendesk.email = segment.email ) \u2013 fallback if not enabled through Segment\n\n\u2013 Stripe\nleft join stripe.customers stripe on\n stripe.email = segment.email\n\n\u2013 Salesforce\nleft join salesforce.leads salesforce on\n salesforce.email = segment.email\n\n\u2013 Intercom\nleft join intercom.users intercom on\n ( intercom.user_id = segment.id\u2013 if enabled through Segment\nor intercom.email = segment.email ) \u2013 fallback if not enabled through Segment\n\ngroup by 1,2,3,4,5,6\n\n)\n\n\nYou\u2019ll spit out a user table that looks something like this:\n\nSEGMENT_ID\tEMAIL\tZENDESK_ID\tSTRIPE_ID\tSALESFORCE_ID\tINTERCOM_ID\nmYhgYcRBC7\tziggy@stardust.com\t1303028105\tcus_6ll4iGAO7X8u7L\t00Q31000014XGRcEAO\t55c8923f67b8d6524600037f\nmYhgYcRBC7\tjustin@biebs.com\t1303028105\tcus_6ll3xVVSLIZomI\t00Q31000014XGRcEAO\t55c8923f67b8d6524600037f\n7adt7XG27c\tqueen@beyonce.com\t1472230319\tcus_6u2ZcW3uC8VwZa\t00Q31000014sKCqEAM\t5626dfed2e028608710000ce\nQZnP7cViH1\tkanye@kimye.com\t1486907299\tcus_6yrv9bwLgXN78s\t00Q31000015G7kIEAS\t55f6a142bd531ec6930005fa\n\nWhile creating this table in SQL is a good strategy, we\u2019d be remiss not to point out a few drawbacks to this approach. First, you need to run this nightly or at some regular interval. And, if you have a large user base, it might take a while to run. That said, it\u2019s probably still worth it.\n\nHow to run a query with your joined data\n\nSo what can you do once you have all of your ID\u2019s mapped? Answer some pretty nifty questions that is. Here are just a few SQL examples addressing questions that incorporate more than one source of customer data.\n\nSegment + Zendesk\n\n-- Which referral source is sending us the most tickets?\nSELECTsegment.referral_source,\nCOUNT(zendesk.ticket_id) AS count_of_tickets\nFROM zendesk.tickets zendesk\nLEFT JOIN segment.userssegment\nONusers.segment_id = segment.user_id\nGROUP BY 1\nORDER BY 2 desc\n\n\nStripe + Zendesk\n\n-- How many tickets do we receive across each pricing tier?\n\nSELECT stripe.plan_name AS plan_name,\nCOUNT(zendesk.ticket_id) AS count_of_tickets\n\n-- Start with Zendesk\nFROM zendesk.tickets zendesk\n\n-- Merge Users\nLEFT JOIN user_identities users\nON zendesk.id = users.zendesk_id\n\n-- Add Stripe\nLEFT JOIN stripe.charges stripe\nON users.stripe_id = stripe.customer_id\n\n-- Group by plan name, from most tickets to least\nGROUPBY1\nORDERBY2desc\n\nAdvanced Tips\n\nAn alternative to the lookup user table in SQL would be writing a script to grab user IDs across your third-party tools and dump them into your database.\n\nYou\u2019d have to ping the APIs of each tool with something like an email, and ask them to return the key or id for the corresponding user in their tool.\n\nA sample script, to run on a nightly cron job, would look something like this:\n\nvar request = require('superagent'); // https://www.npmjs.com/package/superagent\n\nvar username = '<your-username>';\nvar password = '<your-password>';\nvar host = 'https://segment.zendesk.com/api/v2/';\n\n/**\n\u00a0* Gets the user object in Zendesk by email address.\n\u00a0*\n\u00a0* @param {String} email\n\u00a0* @param {Function} fn\n\u00a0*/\n\nfunctiongetUserIds(email, fn) {\n\u00a0request\n\u00a0.get(host + 'users/search.json?query=' + email)\n\u00a0.auth(username, password)\n\u00a0.end(fn);\n}\n\n/**\n\u00a0* Get the first Zendesk user that matches 'kanye@kimye.com'\n\u00a0*/\n\ngetUserIds('kanye@kimye.com', function(err, res) {\nif (err) return err;\n// res.body.users will be an Array\n// res.body.users[0].id will return the `id` of the first user\n});\n\n\nHere is the documentation for Zendesk\u2019s API for more information.\n\nThis page was last modified: 21 Apr 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nWhat is the user ID problem?\nFor Segment Destination Users\nHow to merge identities\nAdvanced Tips\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGetting Started\n/\nGetting Started Guide\nGetting Started Guide\n\nWelcome to Segment! This doc mirrors Segment\u2019s in-product guide, and walks you through each of the tasks to level up your workspace strength and become familiar with Segment.\n\nThe guide is broken into three categories of tasks:\n\nBasics: These tasks allow you to send and debug your first data through Segment.\nInstrumentation: These tasks allow you to send additional types of data (track & identify) and give you an introduction to creating a data strategy.\nOptimization: These tasks guide you to expand your data coverage and optimize your workspace.\nBasics\n\nThe tasks included in Basics help you send and debug your very first data from a Source (a library that sends data to Segment), and into a Destination (tools you use to analyze or act on your data).\n\nThe Basic tasks include:\n\nInvite teammates\nAdd a Source\nAdd page or screen tracking\nAdd a Destination\nTesting and Debugging\nInvite Teammates\n\nSegment allows you to invite team members to your workspace. To decide who on your team should be added to Segment, think about who might be responsible for implementing, owning, or using your data in downstream tools.\n\nFor example, as a developer, you might invite:\n\nMarketing colleagues to inform what data might be needed to power campaigns or better understand conversion metrics,\nA data scientist or analyst to help inform data strategy and property structuring,\nProduct managers to help debug data flow, and to connect product analytics tools\n\nTo invite team members to your workspace:\n\nGo to Settings > Workspace Settings and click the Access Management tab.\nClick + Invite Team Member.\nEnter the email addresses of the team members you want to invite separated by a comma.\n(Optional) You can choose to Add Members to User Groups so that members inherit roles from user groups, or Add Individual Roles to bulk assign individuals roles to all invites.\nClick Invite.\nAdd a Source\n\nA Source is a website, server library, mobile SDK, or cloud application which can send data into Segment. It\u2019s where your data originates. Add a Source to collect data to understand who your customers are and how they\u2019re using your product. Create a source for each website or app you want to track.\n\nTo add a Source:\n\nGo to Connections.\nClick Add Source.\nClick the Source you\u2019d like to add. Note: More than 80% of workspaces start by adding their JavaScript website.\nClick Add Source.\nEnter a name for your source as well as any information on the setup page.\nClick Add Source.\n\nLearn More\n\nWhat is a Source?\nCreate a Source\nSources Catalog\nAdd page or screen tracking\n\nOnce you\u2019ve added your Segment Source, you\u2019re ready to send data into Segment. The simplest data to send into Segment is a Page call (for website Sources) or Screen call (for mobile Sources). Page and screen calls send automatically once you install the Segment snippet or SDK on your website or mobile app. Page and screen calls allow you to record whenever a user sees a page of your website or screen of your app, along with any optional properties about the page or screen.\n\nLearn how to install the Segment snippet or SDK on your website or mobile app to start sending data.\n\nLearn More\n\nInstall Segment\nSpec: Page\nSpec: Screen\nAdd a Destination\n\nDestinations are the business tools or apps that Segment forwards your data to. Adding Destinations allow you to act on your data and learn more about your customers in real time.\n\nTo add a Destination:\n\nNavigate to Connections.\nClick Add Destination.\nChoose the Destination you want to add and click Configure. Most users eventually add destinations for: Analytics, Advertising, Email Marketing, and/or Live Chat.\nSelect the Source you want to connect to your Destination.\nClick Next.\nGive your Destination a name.\nClick Save.\nConfigure the settings and enable your destination on the destination settings page.\n\nLearn More\n\nSending data to destinations\nDestination compatibility\nDestination connection modes\nTesting and Debugging\n\nThe Source Debugger is a real-time tool that helps you validate that API calls made from your website, mobile app, or servers arrive at your source. You can use the Source Debugger to make sure that your source functions properly and your events actively send.\n\nThe Debugger shows a live stream of events that flow through your Segment Source, so that you can check that your events send in the correct format. When you click on a specific event, you\u2019ll be able to see these two views of an event:\n\nThe Pretty view is a recreation of the API call you made that was sent to Segment.\nThe Raw view is the complete JSON object Segment receives from the calls you send. These calls include all the details about what is tracked: timestamps, properties, traits, ids, and contextual information Segment automatically collects the moment the data is sent.\n\nTo access your Source Debugger:\n\nNavigate to Connections > Sources and choose your source.\nClick on the Debugger tab.\n\nLearn More\n\nTesting and Debugging\nUsing the Source Debugger\nSegment University: Testing and Debugging\nInstrumentation\n\nThe tasks in this phase help you create a data strategy and send additional types of data (identify and track calls) to get a clearer picture of who your users are and what actions they\u2019re taking.\n\nThe Instrumentation tasks include:\n\nBasics\nInvite Teammates\nAdd a Source\nAdd page or screen tracking\nAdd a Destination\nTesting and Debugging\nInstrumentation\nSend an Identify call\nSend a Track call\nChoose what to track\nEvent anatomy and naming standards\nAdd a data warehouse\nAdd more destinations\nOptimization\nAdd more sources\nAdd a cloud source\nExplore Protocols\nExplore Engage\nSend an Identify call\n\nThe Identify call allows you to tie a user to their actions and record traits about them. It includes a unique User ID and any optional traits you know about the user, like their email, name, and address. Sending an Identify call is your first step towards understanding who your users are.\n\nAn example of the types of details you might want to learn and track about your users in an Identify call are:\n\nName\nEmail\nAddress\nCompany\nLifetime Value\n\nLearn More\n\nSpec: Identify\nPlan your identify and group calls\nSegment University: Identify\nSend a Track call\n\nThe Segment Track call allows you to record any actions your users perform, along with any properties that describe the action. Sending a track call is your first step towards understanding what your users are doing.\n\nEach action that a user takes is known as an event. Each event has a name and properties. For example, the User Registered event might have properties like plan or accountType.\n\nTo save time on instrumentation, be sure to check if one of Segment\u2019s Business Specs meets your needs.\n\nLearn More\n\nSpec: Track\nBest practices for event calls\nAnalytics Academy: The anatomy of a track call\nSegment University: The Track Method\nChoose what to track\n\nSegment recommends you to create and maintain a Tracking Plan to have data clarity and team alignment about what customer data you need to collect and why. It\u2019s best to think about the measurable business outcomes you\u2019re trying to track or improve, and then drill down to track the events needed for each business outcome.\n\nFor example, if you\u2019re looking to reduce cart abandonment, you may want to engage cart abandoners by sending emails and in-app messaging to them using Customer.io and Intercom. You also might want to track events like Product Added or Cart Viewed along this customer journey.\n\nSegment maintains a number of industry or product-specific specs to help you get started:\n\nB2B\nEcommerce\nVideo\nMobile\n\nLearn More\n\nData Collection Best Practices\nAnalytics Academy: How to create a successful data tracking plan\nSegment University: Planning your implementation\nEvent anatomy and naming standards\n\nWhen it comes to data collection, the best way to set your company up for success is to establish consistent naming conventions. This makes your code easier to read, and it helps everyone at your company understand what your events mean.\n\nSegment recommends the best practice of using an \u201cObject Action\u201d (Noun Verb) naming convention for all Track events (for example, Menu Clicked) and using noun_noun snake case for property names (for example, property_name). You can view all the event names you\u2019re currently tracking in the Schema view to ensure you\u2019re using consistent conventions and casing.\n\nTo view your event names in the Source Schema:\n\nNavigate to Connections > Sources.\nClick on the Source you want to view.\nClick on the Schema tab. Your event names are listed in the table.\n\nLearn More\n\nEvent naming best practices\nAnalytics Academy: Naming conventions for clean data\nAdd a data warehouse\n\nA data warehouse is a central location where you can store your raw customer data from multiple sources. A data warehouse gives you flexibility to query your data, which allows you to answer analytical questions that may not be possible with a standard analytics tool.\n\nA data warehouse also allows you to collect and compile data from third party tools as Cloud Sources in Segment, to help you gain a 360 view of your customer touchpoints.\n\nLearn More\n\nWhat\u2019s a warehouse?\nWarehouse FAQs\nAnalytics Academy: Why you should own your data\nAdd more destinations\n\nAdding more destinations allows you to connect all your business tools to run through Segment. This gives you the confidence that they are all acting on the same data. Most users connect a variety of marketing, advertising, product, and analytics tools.\n\nWith all your tools acting on the same set of customer data, you can personalize your customer engagement and deliver a consistent message across multiple channels\n\nTo add more destinations:\n\nNavigate to Connections.\nClick Add Destination.\nChoose the Destination you want to add and click Configure. Most users eventually add destinations for: Analytics, Advertising, Email Marketing, and/or Live Chat.\nSelect the Source you want to connect to your Destination.\nClick Next.\nGive you Destination a name.\nClick Save.\nConfigure the settings and enable your destination on the destination settings page.\nRepeat steps 1-7 for each destination you want to add.\n\nLearn More\n\nSegment Blog: Recipes\nAutomating Multi-Channel Re-Engagement Campaigns\nOptimization\n\nThe tasks in this phase help you to optimize your Segment implementation and take it to the next level.\n\nThe optimization tasks include:\n\nAdd more sources\nAdd a cloud source\nExplore Protocols\nExplore Engage\nAdd more sources\n\nAdding any additional data sources that you might have, like a mobile app, marketing website, server, or cloud tool will give you a more complete view of your customer. Each touchpoint you have with your customers is a potential area to gain a better understanding of them.\n\nTo add more sources:\n\nGo to Connections.\nClick Add Source.\nClick the Source you\u2019d like to add.\nClick Add Source.\nEnter a name for your source as well as any information on the setup page.\nClick Add Source.\nRepeat steps 1-6 for all the other sources you want to add.\n\nLearn More\n\nTracking users across channels and devices\nSources catalog\nAdd a cloud source\n\nCloud sources allow you to pull in customer data from third-party tools (like Twilio or Stripe) into a data warehouse for complex querying. Consolidating your customer data enables you to eliminate data silos to get a single view of your customer.\n\nBefore adding a cloud source, you need to make sure you:\n\nGet cloud source credentials.\nGet warehouse credentials.\nChoose your preferred sync time.\n\nOnce you have the necessary credentials, to add a cloud source:\n\nNavigate to Connections and click Add Source.\nClick on the cloud source you want to add and click Add Source.\nGive your cloud source a name and click Authenticate.\nEnter your credentials or log in using OAuth.\nEnable the source.\nNavigate to Connections > Destinations and select your warehouse.\nOn the Settings tab of your warehouse, enter the credentials for your warehouse if you don\u2019t already have one connected to Segment.\n\nLearn More\n\nCloud sources\nComparing Cloud Sources\nExplore Protocols\n\nProtocols automate and scale the data quality best practices developed over years of helping users implement Segment. Investing in data quality improves trust in your data, reduces time spent by your engineering and business teams navigating and validating data, and allows your business to grow faster.\n\nThere are steps to take when you use Protocols:\n\nAlign teams with a Tracking Plan\nValidate data quality with violations\nEnforce data standards with controls\nResolve data issues with transformations\n\nLearn More\n\nProtocols Overview\nProtocols FAQs\nIntro to Protocols\nExplore Engage\n\nEngage is a powerful personalization platform that enables you to create unified customer profiles in Segment, to build and enrich audiences, and to activate audiences across marketing tools.\n\nEngage allows you to enrich user profiles with custom traits, allowing you to create granular audiences for campaigns, advertising, and analysis.\n\nThis page was last modified: 24 Jan 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nBasics\nInstrumentation\nOptimization\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nTraits\n/\nPredictions\n/\nSuggested Predictive Audiences\nSuggested Predictive Audiences\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY PLUS \u2713\n?\n\nSuggested Predictive Audiences can help you improve customer engagement, drive higher conversion rates, and reduce ad spend.\n\nThis page explains what a Suggested Predictive Audience is, how to build a Suggested Predictive Audience, and what each available Audience targets.\n\nSuggested Predictive Audience basics\n\nA Suggested Predictive Audience is an out-of-the-box Audience template driven by machine learning.\n\nSegment offers five templates that are prebuilt with Predictions like likelihood to purchase and lifetime predicted value. Selecting a template generates a Predictive Audience whose members you can engage in a number of ways:\n\nSend an email or SMS campaign with a discount code\nPromote a new product line with a drip campaign\nTarget the Audience members with online ads\nSend personalized product recommendations\nBuild a Suggested Predictive Audience\n\nFollow these steps to build a Suggested Predictive Audience:\n\nIn your Segment workspace, navigate to Engage > Audiences.\nFrom the Audiences tab, select Go to Predictive Audiences.\nOn the Audience you want to build, click Build Audience > + Add Audience.\nSelect the Audience type you want to build, then click Next.\nOn the Set up requirements tab, confirm that you have the right events and traits required for the Suggested Predictive Audience, then click Next.\nIf you\u2019re missing a required event or trait, Segment prompts you to select it from the dropdown and match it to the required field(s).\nPreview your Audience, then click Next.\n(Optional:) Connect the new Audience to a Destination.\nGive your Suggested Predictive Audience a name, then click Create Audience.\n\nYour Suggested Predictive Audience is now live.\n\nSuggested Predictive Audience types\n\nEngage offers five Suggested Predictive Audiences. The following table summarizes the customers each Audience targets and the events and traits Engage uses to build the Audience:\n\nAUDIENCE\tTARGET\tBUILT WITH\nReady to buy\tCustomers who are likely to make a purchase\tLikelihood to buy\nOrder completed\nLong shots\tCustomers who have previously interacted with your brand but aren\u2019t currently engaged\tOrder Completed\nLikelihood to purchase\nHigh LTV\tCustomers with a high predicted lifetime value\tPredicted LTV\nPotential VIPs\tRecently active customers with high predicted lifetime value and high propensity to purchase\tPage Viewed\nLikelihood to Purchase\nPredicted LTV\nDormant\tInactive customers who are unlikely to purchase\tPage Viewed\nLikelihood to Purchase\nPredicted LTV\nAudience descriptions\nReady to buy\n\nChoose a Ready to buy Predictive Audience to target customers who show a high propensity to make a purchase.\n\nSegment builds this Audience with the Likelihood to Purchase prediction. Audience members show encouraging engagement and have a likelihood to buy in the top 20th percentile.\n\nLong shots\n\nChoose a Long shot Predictive Audience to target customers who have made a purchase but have a middling likelihood to buy.\n\nSegment builds this Audience with the Order Completed event and Likelihood to Purchase trait. Audience members have completed a purchase but currently have a likelihood to buy somewhere between the 25th and 65th percentile.\n\nHigh lifetime value\n\nChoose a High lifetime value Predictive Audience to target customers that show a high predicted lifetime value.\n\nSegment builds this Audience with the Predicted LTV prediction. Audience members are in the top 10th percentile of predicted lifetime value and Segment expects that they\u2019ll spend the most over the next 90 days.\n\nPotential VIPs\n\nChoose a Potential VIPs Predictive Audience to target customers exhibiting several promising marketing behaviors.\n\nSegment builds this Audience with the Page Viewed event and Likelihood to Purchase and Predicted LTV prediction. Audience members have been active on your site within the last two weeks, have a high predicted lifetime value, and a high propensity to purchase.\n\nDormant\n\nChoose a Dormant Predictive Audience to target inactive customers.\n\nSegment builds this Audience with the Page Viewed event and the Likelihood to Purchase Predictive Trait. Audience members have a low likelihood to purchase and haven\u2019t been active on your site in the last 60 days.\n\nThis page was last modified: 23 Aug 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSuggested Predictive Audience basics\nBuild a Suggested Predictive Audience\nSuggested Predictive Audience types\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nReverse Etl\n/\nReverse Etl Source Setup Guides\n/\nDatabricks Reverse ETL Setup\nDatabricks Reverse ETL Setup\n\nSet up Databricks as your Reverse ETL source.\n\nAt a high level, when you set up Databricks for Reverse ETL, the configured service-principal needs read permissions for any resources (databases, schemas, tables) the query needs to access. Segment keeps track of changes to your query results with a managed schema (__SEGMENT_REVERSE_ETL), which requires the configured service-principal to allow write permissions for that schema. Segment supports only OAuth (M2M) authentication. To generate a client ID and Secret, follow the steps listed in Databricks\u2019 OAuth machine-to-machine (M2M) authentication documentation.\n\nDatabricks Reverse ETL sources support Segment's dbt extension\n\nIf you have an existing dbt account with a Git repository, you can use Segment\u2019s dbt extension to centralize model management and versioning, reduce redundancies, and run CI checks to prevent breaking changes.\n\nRequired permissions\n\nMake sure the service principal you use to connect to Segment has permissions to use that warehouse. In the Databricks console go to SQL warehouses and select the warehouse you\u2019re using. Navigate to Overview > Permissions and make sure the service principal you use to connect to Segment has can use permissions.\n\nTo grant access to read data from the tables used in the model query, run:\n\n  GRANT USAGE ON SCHEMA <schema_name> TO `<service principal you are using to connect to Segment>`; \n  GRANT SELECT, READ_METADATA ON SCHEMA <schema_name> TO `<service principal you are using to connect to Segment>`; \n\n\nTo grant Segment access to create a schema to keep track of the running syncs, run:\n\n  GRANT CREATE on catalog <name of the catalog, usually hive_metastore or main if using unity-catalog> TO `<service principal you are using to connect to Segment>`;\n\n\nIf you want to create the schema yourself instead and then give Segment access to it, run:\n\n  CREATE SCHEMA IF NOT EXISTS __segment_reverse_etl; \n  GRANT ALL PRIVILEGES ON SCHEMA __segment_reverse_etl TO `<service principal you are using to connect to Segment>`;\n\nSet up guide\n\nTo set up Databricks as your Reverse ETL source:\n\nLog in to your Databricks account.\nNavigate to Workspaces and select the workspace you want to use.\nSelect SQL in the main navigation.\nSelect SQL Warehouses and select the warehouse you want to use. Note that Segment doesn\u2019t support the Compute connection parameters.\nGo to the Connection details tab and keep this page open.\nOpen your Segment workspace.\nNavigate to Connections > Sources and select the Reverse ETL tab.\nClick + Add Reverse ETL source.\nSelect Databricks and click Add Source.\nEnter the configuration setting for your Databricks source based on information from step 5\nHostname: adb-xxxxxxx.azuredatabricks.net\nHttp Path: /sql/1.0/warehouses/xxxxxxxxx\nPort: 443 (default)\nService principal client ID: <your client ID>\nOAuth secret: <OAuth secret used during connection>\nCatalog [optional]: If not specified, Segment will use the default catalog\nClick Test Connection to see if the connection works. If the connection fails, make sure you have the right permissions and credentials, then try again.\nClick Add source if the test connection is successful.\n\nSegment previously supported token-based authentication, but now uses OAuth (M2M) authentication at the recommendation of Databricks. If you previously set up your source using token-based authentication, Segment will continue to support it. If you want to create a new source or update the connection settings of an existing source, Segment only supports OAuth machine-to-machine (M2M) authentication.\n\nAfter you\u2019ve successfully added your Databricks source, add a model and follow the rest of the steps in the Reverse ETL setup guide.\n\nThis page was last modified: 22 May 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nRequired permissions\nSet up guide\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nHow To Guides\n/\nCollecting Pageviews on the Server Side\nCollecting Pageviews on the Server Side\n\nSegment believes that client-side collection is appropriate for collection of basic pageviews.\n\nIf you\u2019d like to track\u00a0page\u00a0calls from your server to Segment, Segment recommends doing it in addition to any client side tracking you\u2019re doing with analytics.js, and doing it in a separate \u201csource\u201d so that you can configure where to send the (probably redundant, albeit higher-fidelity) data.\n\nWith this approach, you might use a request \u201cmiddleware\u201d to log a\u00a0pageview\u00a0with every page load from your server.\n\nThere are a few things to be mindful of if you want to make sure you can attribute these (anonymous) page views to the appropriate user in your client-side source (eg, for effectively joining these tables together to do down-funnel behavioral attribution). You\u2019ll want to ensure they share an anonymousId by respecting one if it\u2019s already there, and setting it yourself if not. To do that, you can read and modify the\u00a0ajs_anonymous_id\u00a0cookie value in the request.\n\nBe sure to pass through as many fields as you can in Segment\u2019s\u00a0Page\u00a0and\u00a0Common\u00a0spec, so that you get full functionality in any downstream tools you choose to enable. Segment recommends specifically ensuring you pass the\u00a0url, path, host, title, search, and referrer\u00a0in the message\u00a0properties\u00a0and\u00a0ip and user-agent\u00a0in the message\u00a0context\u00a0.\n\nHere\u2019s an example of an express middleware function that covers all those edge cases:\n\nIf you have any questions or would like help generally adopting this method for other languages and frameworks, be sure to get in touch.\n\nimport express from 'express'\nimport Analytics from 'analytics-node'\nimport { stringify } from 'qs'\n\nconst app = express()\nconst analytics = new Analytics('write-key')\n\napp.use((req, res, next) => {\n  const { search, cookies, url, path, ip, host } = req\n\n  // populate campaign object with any utm params\n  const campaign = {}\n  if (search.utm_content) campaign.content = search.utm_content\n  if (search.utm_campaign) campaign.name = search.utm_campaign\n  if (search.utm_medium) campaign.medium = search.utm_medium\n  if (search.utm_source) campaign.source = search.utm_source\n  if (search.utm_term) campaign.keyword = search.utm_term\n\n  // grab userId if present\n  let userId = null\n  if (cookies.ajs_user_id) userId = cookies.ajs_user_id\n\n  // if no anonymousId, send a randomly generated one\n  // otherwise grab existing to include in call to segment\n  let anonymousId\n  if (cookies.ajs_anonymous_id) {\n    anonymousId = cookies.ajs_anonymous_id\n  } else {\n    anonymousId = = uuid.v4()\n    res.cookie('ajs_anonymous_id', anonymousId )\n  }\n\n  const referrer = req.get('Referrer')\n  const userAgent = req.get('User-Agent')\n\n  const properties = {\n    search: stringify(query)\n    referrer,\n    path,\n    host,\n    url\n    /* ++ any custom props (eg. title) */\n  }\n\n  const context = {\n    campaign,\n    userAgent,\n    ip\n  }\n\n  // send a call to segment\n  analytics.page(\n    anonymousId, // either random (matching cookie) or from client\n    userId, // might be null\n    properties,\n    context\n  )\n\n  // proceed!\n  next()\n})\n\n\nThis page was last modified: 10 Oct 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGetting Started\n/\nWhat's Next\nWhat's Next\n\nYou\u2019re just getting started with Segment, but there\u2019s so much more to explore!\n\nPrivacy tools and filtering\n\nSegment includes a free suite of Privacy tools to help your organization comply with regulations like the GDPR and the CCPA.\n\nThe Privacy Portal allows you to easily audit, monitor, and enforce privacy rules against your Segment data, to proactively protect your customers.\n\nImprove data quality with Protocols\n\nYou had a taste of the planning needed to set up clear, consistent, reliable and extensible data schemas in Planning a Full Install.\n\nBusiness tier customers can use Segment\u2019s Protocols package to help with this process, to keep track of what data is being collected where, and to normalize their data as it flows through Segment. Clean, consistent data helps you move faster to build marketing campaigns and act on analytics insights.\n\nWith Protocols, you can use Tracking Plans to build consensus in your organization about which events and property you intend to collect across your web, mobile or server-side data sources. Once defined, you can connect the Tracking Plan to your Sources to automatically validate the data is flowing correctly. You can also turn on enforcement to block bad data, and even fix incorrect data with Transformations.\n\nSingle view of the customer with Engage\n\nEngage is a powerful personalization platform that enables you to create unified customer profiles in Segment, to build and enrich audiences, and to activate audiences across marketing tools.\n\nWith Engage, you can create unified customer profiles, enrich those profiles with new traits, build Audiences using those profiles, and sync audiences to marketing tools to power personalized experiences, and better understand and market to your customers.\n\nMore learning resources\nSegment University\n\nSegment University is Segment\u2019s free, online classroom for learning the basics of Segment.\n\nAnalytics Academy\n\nAnalytics Academy is a series of lessons designed to help you understand the value of analytics as a discipline, and to help you think through your analytics needs, and get started creating robust and flexible analytics systems to help you grow.\n\nRecipes\n\nNeed ideas or prior art? Segment Recipes are some cool things you can do by hooking your Segment workspace up to different Destination tools. Everything from sending tailored onboarding emails, to joining and cleaning your data with third party tools\n\nOther Resources\n\nStill hungry for more? Check out our list of other Segment Resources!\n\nTechnical Support\n\nIf you\u2019re experiencing problems, have questions about implementing Segment, or want to report a bug, you can fill out our support contact form here and our Product Support Engineers will get back to you.\n\nYou need a Segment.com account in order to file a support request. Don\u2019t worry! You can always sign up for a free workspace if you don\u2019t already have one.\n\nBack to the index\n\nBack to the Getting Started index\n\nThis page was last modified: 27 Sep 2022\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nPrivacy tools and filtering\nImprove data quality with Protocols\nSingle view of the customer with Engage\nMore learning resources\nTechnical Support\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nWhat is Segment?\nWhat is Segment?\n\nWith Segment, you can collect, transform, send, and archive your first-party customer data. Segment simplifies the process of collecting data and connecting new tools, allowing you to spend more time using your data, and less time trying to collect it. You can use Segment to track events that happen when a user interacts with the interfaces. \u201cInterfaces\u201d is Segment\u2019s generic word for any digital properties you own: your website, mobile apps, and processes that run on a server or OTT device.\n\nWhen you capture interaction data in Segment, you can send it (often in real-time) to your marketing, product, and analytics tools, as well as to data warehouses. In most cases, you won\u2019t even need to touch your tracking code to connect to new tools.\n\nNEXT\nReady to get started?\n\nLet's walk through the steps to get up and running on Segment. Let's go!\n\nThis page was last modified: 14 Dec 2021\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nIntegration Error Codes\nIntegration Error Codes\nERROR CODES\tREASON\nerrors.discarded.400\tBad request - Refer to the error message to see which fields in your events might be invalid\nerrors.discarded.401\tUnauthorized - Your credentials are either invalid or have expired. Re-authenticate your account with the partner and update your authentication settings in Segment.\nerrors.discarded.403\tForbidden - You no longer have sufficient privilege to access the partner\u2019s API. Re-authenticate your account with the partner and update your auth settings in Segment.\nerrors.discarded.INVALID_SETTINGS\tYour configurations in Segment are invalid. Refer to the error message for more details.\nerrors.discarded.MESSAGE_REJECTED\tYour events are missing one or more required fields. Refer to the error message for more details.\nerrors.awaiting-retry.429\tRate limit exceeded. contact the partner to raise your limit or avoid sending events in burst.\nerrors.awaiting-retry.5xx\tThe partner\u2019s API report an internal error. Consider disabling this integration or contact the integration partner to address this issue.\nerrors.awaiting-retry.ENOTFOUND\nerrors.awaiting-retry.ECONNREFUSED\nerrors.awaiting-retry.ECONNRESET\nerrors.awaiting-retry.ECONNABORTED\nerrors.awaiting-retry.EHOSTUNREACH\nerrors.awaiting-retry.EAI_AGAIN\tSegment is unable to establish a connection with the partner\u2019s server. This could mean your Segment configurations contain some invalid settings or that the integration is no longer operational. If your configurations are valid, consider disabling this integration or contact the integration partner to address this issue.\nOthers\tRefer to the error message you receive for more information. See the article, Testing Connections for more debugging help.\n\nThis page was last modified: 06 Jul 2022\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nStorage\n/\nData Storage catalog\nData Storage catalog\nAWS S3\nAmazon S3\nAzure Synapse Analytics Warehouse\nBigQuery\nDatabricks\nGoogle Cloud Storage\n\nBETA\n\nIBM Db2 Warehouse\nPostgres\nRedshift\nSegment Data Lakes\nSnowflake\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nProfiles Insights\nProfiles Insights\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\n\nWith Profiles Insights, you can troubleshoot event data with transparent insight into your Unify profiles. View errors and violations, success logs, and an audit trail for events that flow into your profiles. You can also learn why certain issues occur, and take preventative action against future errors.\n\nGetting started\n\nTo get started with Profiles Insights, navigate to Unify > Profiles insights.\n\nFrom the Profiles Insights page, you can navigate to these tabs:\n\nErrors and violations\nSuccess logs\nAudit trail\nErrors and violations\n\nUse the errors and violations tab to view and help you troubleshoot any errors or violations that have occurred in your space.\n\nYou can filter results by ID type, time range, specific violations, and more.\n\nTo filter by external_id, you must enter an external_id value in the search bar.\n\nErrors\n\nErrors occur when a message didn\u2019t resolve to a profile because Segment didn\u2019t find a matching identifier or the system behaved unexpectedly. Click on an error log to view the error and next steps that Segment recommends.\n\nProfiles Insights flags the following error:\n\nERROR\tDESCRIPTION\nNo matching identifiers on event\tThe event didn\u2019t have any identifier types that matched your ID Resolution Settings. As a result, the event didn\u2019t resolve to a profile.\n\nProfile Insights won\u2019t surface errors if the event was dropped before entering Unify\u2019s Identity Resolution. Learn more about Unify ingestion limits.\n\nViolations\n\nViolations occur when incoming events don\u2019t comply with your Identity Resolution Settings. For example, when Segment drops an anonymous ID (lower priority) to resolve an event based on a matching user ID (higher priority), it results in a violation.\n\nLearn about identifier priorities in your Identity Resolution.\n\nFor any violations, Segment may drop lower priority identifiers or the identifiers that violate your Identity Resolution Settings. From the grid, you can click a log name to view the violation details and recommended next steps.\n\nYou can use the Message Payload tab to view raw messages for Track events and see exactly where the violation occurred.\n\nProfiles Insights flags the following violations:\n\nVIOLATION\tDESCRIPTION\nIdentifier value limit exceeded\tA lower priority identifier wasn\u2019t added to the associated profile(s) because the maximum number of values for the identifier type exceeds the limit.\nIdentifier value time limit exceeded\tA lower priority identifier wasn\u2019t added to the associated profile(s) because there\u2019s a limit to how many identifier type values can be added in a period of time.\nID value blocked\tThe identifier wasn\u2019t added to a profile because it\u2019s a blocked value.\nProfile merge limit exceeded\tThe profile exceeds the system imposed merge limit. Segment\u2019s default limit is 100.\nIdentifier mapping limit exceeded\tThe profile exceeds the system imposed mapping limit. Segment\u2019s default limit is 1,000.\nIdentifier value set to unique (legacy)\tThe profile exceeds the cardinality limit of an identifier type set to be unique. This violation only appears for existing spaces that have the enforce_unique field configured.\nIdentifier value limit exceeded (legacy)\tThe profile exceeds the cardinality limit of an identifier type. The cardinality limit used for this violation is the limit field in the identifier space.\nSuccess logs\n\nSuccess logs provide visibility into a profile\u2019s journey from creation to the point of merging into other profiles.\n\nUse the success logs to view:\n\nProfiles that Segment has merged\nIdentifiers that Segment has mapped to a profile\n\nYou can filter results by ID type, time range, incoming event type, and more.\n\nWhen you click a specific log, Segment displays merge or mapping details along with the message payload for Track events.\n\nAudit trail\n\nThe Audit trail displays all audit actions that occur in your Unify space. This can include, for example, a user creating an access token or modifying Unify settings.\n\nClick an audit log link to view the user who initiated the action, timestamp, and log details.\n\nFAQ\nWhat are the record display and search time limits for Profiles Insights?\n\nProfiles Insights can display up to 500 records and supports searches within a 30-day time frame.\n\nThis page was last modified: 23 Apr 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nGetting started\nErrors and violations\nSuccess logs\nAudit trail\nFAQ\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nProtocols\n/\nTracking Plan\n/\nTracking Plan Libraries\nTracking Plan Libraries\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nPROTOCOLS \u2713\n?\n\nTracking Plan Libraries make it easy to scale Tracking Plan creation within your workspace. You can create libraries for track events or track event properties. Editing Tracking Plan Libraries is identical to editing Tracking Plans.\n\nNote: Segment does support advanced JSON schema implementations and identify/group trait libraries.\n\nOnce created, you can import event or property Libraries into a Tracking Plan using a simple wizard flow.\n\nCreate a Tracking Plan Library\n\nTo create a new Library:\n\nIn the left navigation, click Protocols\nClick Libraries in the top navigation bar\nClick New Library and follow the steps to create an event or property library\n\nTracking Plan Event Libraries\n\nTracking Plan Event Libraries support Track events and associated properties. Event Libraries are helpful when you want to track a group of events consistently across tracking plans. For example, if you are an eCommerce company with more than one application, you may need to track eCommerce Spec events consistently across those sources. Instead of needing to re-create the eCommerce spec across all tracking plans, create a library and import the events to each Tracking Plan.\n\nTracking Plan Property Libraries\n\nTracking Plan property Libraries support Track event property groups. Property Libraries are helpful when you have more than one event in a Tracking Plan that share a common set of properties. For example, if you want to consistently include order_id, currency, cart_id and a products array of objects in your checkout flow events, you can create a Library with these properties including descriptions, data types and conditional filters.\n\nImport Libraries into a Tracking Plan\n\nYou can import event and property libraries into a Tracking Plan. Enter the Draft mode for a Tracking Plan and click the Import from Event or Property Library dropdown. A wizard will appear allowing you to either add the events to a tracking plan, or add properties to selected events already in the tracking plan. After adding your events or properties, remember to merge your changes!\n\nLibrary syncing\n\nWhen you import events or properties from a Library, you can enable syncing to ensure that changes made to the Library pass down to all synced Tracking plans. Syncing is important when you want to make sure all Tracking Plans define events and properties consistently. For example, it\u2019s best practice to create separate tracking plans for mobile and web sources as these two sources share some but not all events or properties. Library syncing is the best way to ensure that the shared events are consistently tracked across Tracking Plans, even as you make changes to the Library.\n\nTo enable syncing, select the desired Library from the Tracking Plan import flow, and toggle the syncing option. This selects all events or properties in the Library for import. Partial syncs are not supported.\n\nSyncing a Library makes events and properties un-editable, and bypasses the Tracking Plan merge step. You can add properties to synced events, but cannot remove those synced events unless you also remove the Library sync. To unsync a library, click View Synced Libraries from the Tracking Plan and click the overflow menu to unsync the Library.\n\nAll changes made to a synced library pass through to the Tracking Plans and may impact data deliverability\n\nThis page was last modified: 03 Aug 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nCreate a Tracking Plan Library\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGetting Started\n/\nUse Cases\n/\nUse Cases Setup\nUse Cases Setup\n\nUse Cases help you onboard quickly and efficiently to Segment by guiding you through specific steps tailored to your business needs.\n\nThis page walks you through the steps to set up a use case in your Segment instance.\n\nPermissions\n\nTo implement a use case, you\u2019ll need to be a Workspace Owner for your Segment account. See the Roles documentation for more information.\n\nYou can onboard to Segment with a Use Case if you\u2019re a new Business Tier customer or haven\u2019t yet connected a source and destination.\n\nUse case setup overview\n\nFrom a high level, setting Segment up with a use case takes place in four stages:\n\nPick your business goal. What do you want to achieve? Choose from 4 common business goals like optimizing advertising, personalizing first conversions, boosting retention, and increasing customer retention.\nSelect a use case. After you pick your business goal, Segment shows you several potential use cases from which to choose.\nFollow the in-app guide. With your use case chosen, Segment shows you an interactive checklist of events to track, as well as sources and destinations that Segment recommends you connect. You\u2019ll carry these steps out in a sandboxed development environment.\nTest and launch your setup. Push your connections to a production environment and verify that events flow as expected through the debugger. After you\u2019re done, your Segment instance is up and running.\nExample setup: Personalize winback\n\nThis section provides a detailed, step-by-step guide to setting up the Personalize Winback use case from the Personalize communications and product experiences business goal in your Segment account. All use cases follow this same setup flow.\n\nStep 1: Navigate to Use Cases\nLog in to your Segment account.\nIf you see the Welcome to Segment screen, click Get Started. If logging in takes you to your Segment workspace, click Guided Setup.\nStep 2: Pick your business goal and select a use case\n\nChoosing a use case\n\nSegment lets you implement one use case. If you\u2019re not sure which use case to choose, view Choosing a Use Case.\n\nIn the What is your business goal? screen, select Personalize communications and product experiences, then click Next.\nSegment moves you to the Which use case would you like to set up? screen. Choose Personalize winback, then click Next.\nSegment shows you information about dev and prod labels. After you\u2019ve read it, click Next.\nSegment takes you to the Setup checklist page.\nWorking with dev and prod environments\n\nFor most cases, you\u2019ll want to start with development or staging sources to test and debug your Segment implementation. This approach lets you verify that everything is working correctly before sending live data downstream. To facilitate this, Segment automatically creates development (dev) and production (prod) spaces for you and labels your sources accordingly to simplify tracking.\n\nSegment strongly recommends beginning your setup in the dev environment. This allows for thorough testing and debugging of your configuration. Once you\u2019re confident in your dev setup, Segment will guide you on how to apply these configurations to your live production sources.\n\nStep 3: Review suggested events\n\nChanging your use case\n\nOnce you\u2019ve reviewed the suggested events for a use case, you won\u2019t be able to change the use case. If you want to see a full breakdown of each use case before commiting to one, click Change use case to begin the use case flow again. You can also view the Use Cases Reference guide to see what Segment recommends for each use case.\n\nOn the Setup checklist page, you\u2019ll see the full checklist for the use case you\u2019ve chosen. This checklist applies to all use cases, though the suggested events, sources, and destinations differ between use cases.\n\nIn the Review suggested events list item, click Review.\nSegment shows you the recommended events and properties typically tracked for your use case.\nSet up event tracking based on the events and properties Segment shows.\n\nThis table shows Segment\u2019s recommended events and properties for the Personalize winback use case:\n\nEVENTS\tPROPERTIES\nPage Viewed\tpage_category, page_name\nPage Scrolled\tpct_scrolled, page_category\nOrder Completed\tnum_items, order_id, checkout_id, total, revenue, shipping, tax, affiliation, products\n\nMake sure that you\u2019re tracking these events to get the most of the Personalize winback campaign. For more information on event tracking, see Data Collection Best Practices.\n\nStep 4: Connect dev sources\n\nYou\u2019re now ready to connect sources to your dev environment.\n\nIn the Connect dev sources step, Segment shows you the recommended sources you should connect. For Personalized winback, these include Website, Mobile, and Reverse ETL.\nReview the recommended sources, then click Connect.\nSegment takes you to the Add a source setup. Choose the source(s) you want to add, then click Next.\nName your source, then click Create source.\nCarry out the source-specific steps, then click Next.\nTest your connection and troubleshoot it, if necessary. Click Done.\n(Optional:) Click Connect More and repeat steps 2 through 6 to add more sources.\n\nAdding a warehouse as a souce\n\nIf you connect a warehouse as a source, Segment automatically creates a Profiles destination that shows up in the Connect your data tab. Do not delete this destination, as Segment requires this destination to create profiles from your warehouse.\n\nCloud object sources\n\nIf you connect a cloud object source, you\u2019ll need to create a warehouse to sync profiles into Segment. For more information, see Cloud Sources.\n\nStep 5: Connect dev destinations\n\nWith sources connected, you can now connect destinations to your dev environment.\n\nUnder the Connect dev destinations step, Segment shows you the recommended sources you should connect. For Personalize winback, these include Reverse ETL, Personalization, and Analytics.\nReview the recommended destinations, then click Connect.\nSegment takes you to the Choose a Destination setup. Choose the destination(s) you want to add, then click Next.\nName your destination, then click Create Destination.\nChoose a source to connect to the destination, then click Next.\nCarry out the destination-specific steps, then click Done.\n(Optional:) Click Connect More and repeat steps 2 through 6 to add more destinations.\nStep 6: Publish your setup to a prod environment\n\nUntil this point, you\u2019ve set up event tracking and connected sources and destinations to a development environment.\n\nAfter you\u2019ve confirmed that data is flowing from your sources into your destinations as expected, you\u2019re ready to publish your setup to a production environment.\n\nOn the Setup checklist page, click the Prod environment tab.\nOn the Connect 1 prod source radio button, click Connect.\nSegment shows you the sources you previously connected in your dev environment. Click the source you want to connect to prod, then click Continue.\nCarry out any additional steps in the Add a Source page, click Create Source, then click Next. Segment returns you to the Prod environment tab.\nPublish the events set up in your dev environment sources to production. Check the debugger to verify that data is flowing into Segment correctly, then click Mark as complete.\nOn the Connect 1 prod destination bullet, click Connect.\nSegment shows you the destinations you previously connected in your dev environment. Click the source you want to connect to prod, then click Continue.\nChoose a source to connect to the destination, then click Next.\nName your destination, then click Create Destination.\n\nYour data is now in production, and you\u2019ve successfully configured Segment.\n\nActivate your data with Unify and Engage\n\nNow that you\u2019ve successfully set up Connections and Destinations, you can build upon your Segment implementation with Unify and Engage.\n\nAccessing Unify and Engage\n\nUnify and Engage may not yet be enabled for your account. To add Engage to your Segment workspace, click Request a demo in the Unify and Engage tabs on the Guided Setup page.\n\nStep 1: Set up identifiers with Unify\nIn the Guided Setup page, click Build profiles from your data.\nClick Add default identifiers. Segment displays the Select Identifiers popup.\nSelect as many of the recommended identifiers that best fit your use case; Segment recommends selecting all identifiers. Click Save.\nOn the Guided Setup page, click Mark complete.\n\nYour identifiers are now set up in your dev space, though it could take a few minutes for Segment to create profiles from your selected identifiers.\n\nFor more information, see the Unify documentation.\n\nStep 2: Create audiences with Engage\nClick the Engage customers with your data tab, then click Create audience. Segment takes you to the New Audience Builder.\nOn the Select Audience Type page, select either Users or Accounts, then click Next.\nConfigure, preview, and create your audience.\n\nSegment then begins sending your new audience(s) to the destinations in your dev environment. Verify in those destinations that the audiences are coming through as intended, then click Mark complete.\n\nFor more information on Audiences, see the Engage documentation.\n\nStep 3: Republish to a prod environment\n\nAt this point, you\u2019ll have already published your initial setup to a prod environment. Next, you\u2019ll publish your Unify and Engage setup to the same prod environment.\n\nReturn to the Prod environment tab.\nIn the Build profiles from your data tab, click Import rules.\nReview the rules that Segment will import, then click Import.\nIn the Engage customers with your data tab, click Create audience\nConfigure, preview, and create your audience. Segment returns you to the Guided Setup page.\n\nSegment then begins sending your new audience(s) to the destinations in your dev environment. Verify in those destinations that your audiences are coming through as intended, then click Mark complete.\n\nNext steps\n\nUse Cases pulls together a number of core Segment features, like Sources, Destinations, data collection, and Reverse ETL. View the documentation for each to learn how you can continue to expand and build on what you\u2019ve alreay achieved.\n\nThis page was last modified: 08 Oct 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nUse case setup overview\nExample setup: Personalize winback\nActivate your data with Unify and Engage\nNext steps\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nUse Cases for Twilio Engage Premier\nUse Cases for Twilio Engage Premier\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE PREMIER \u2713\n?\n\nUse Twilio Engage to build personalized marketing campaigns and improve:\n\nAcquisition through lookalike audience targeting or promo campaigns.\nConversion through seasonal or onboarding campaigns and lead nurture.\nRetention through customer loyalty and reactivation campaigns.\n\nTo help you get started with Engage, here are a few example campaigns.\n\nCustomer loyalty email campaign\n\nThis journey sends an exclusive promo code to repeat customers to promote ongoing loyalty.\n\nCreate the entry condition with the step name Loyalty Program.\nAll users who performed the Order Completed event at least 3 times and where the price is greater than 100, any time within 30 days.\nAdd a delay of 7 days.\nAdd a True/False split. Split the users around a computed trait of Order Refunded at least 1 time, any time within 30 days.\nFor the True branch, send the list of users to an email step to receive a refund survey.\nFor the False branch, send the list of users to an email step for a personalized message with a \u201c35OFF\u201d promo code.\nAdd a Conversion Goal of 35OFF Promo Used to track users who performed the Order Completed event at least 1 time where the promo_discount used equals 35OFF.\n\nBuild similar campaigns with SMS or use both email and SMS to contact subscribed users on their preferred channels.\n\nCart abandonment\n\nThis journey sends purchase reminders to cart abandonment users based on the channels they\u2019ve subscribed to.\n\nCreate the entry condition with the step name Product Added to Cart.\nAll users who performed the Product Added event at least 1 time within the last 7 days and who haven\u2019t performed Order Completed at least 1 time in the last 7 days.\nAdd a delay of 4 hours.\nAdd a True/False split. Split the users around a computed trait of Order Completed at least 1 time within 7 days.\nFor the False branch, add a multi-branch split.\nFor users who have the custom trait email_opt_in equals true:\nSend to an email step to receive a purchase reminder.\nFor users who have the custom trait SMS_opt_in equals true:\nSend to an SMS step to receive a purchase reminder text.\nFor users who have the custom trait email_opt_in equals false and who have the custom trait SMS_opt_in equals false:\nSend to an ads destination.\nOnboarding\n\nThis journey sends exclusive offers and onboarding emails based on user action.\n\nCreate the entry condition with the step name Visited Resort Site.\nAll users who performed the Page Viewed event at least 1 time within the last 7 days.\nSend to a destination to receive resort advertisements.\nAdd a delay of 2 days.\nAdd a Send Email step called Exclusive Offer. Send an exclusive offer code \u201cNORESORTFEE\u201d to waive the resort fee.\nAdd a conversion goal of Exclusive Offer Accepted to track users who performed the Order Completed event at least 1 time and where the promo_discount used equals NORESORTFEE.\nAdd a delay of 7 days.\nAdd a True/False split. Split the user around a computed trait of Order Completed at least 1 time in the last 7 days.\nFor the True branch, send the list of users to an email step to receive resort details and a list of amenities.\nFor the False Branch, send the list of users to an SMS step to receive a tour invite.\nLow recency campaign\n\nThis campaign sends personalized re-engagement email and SMS promo offers to low recency customers.\n\nCreate the entry condition with the step name Low Recency Customers.\nAll users who performed the Item Purchased event zero times within the last 180 days.\nAdd a Send Email step called Send Re-engagement Email.\nPersonalize the message with merge tags to invite users to view new products related to their most common purchase category.\nAdd a conversion goal to track users who performed the Product Viewed event at least 1 time within 7 days after message delivery.\nAdd a delay of 7 days.\nAdd a True/False split. Split the audience around a computed trait of Item Purchased at least 1 time within 7 days.\nFor the True branch, send the list of users to an SMS step to receive a purchase survey.\nFor the False branch, send the list of users to an SMS step to receive a discount code.\nAdd merge tags and test the SMS before you send it.\nDefine a conversion goal of Item Purchased at least 1 time within 7 days after message delivery to track the campaign performance.\nCompany updates and newsletters\n\nUse Engage to send one-time updates as part of a campaign.\n\nShare company updates or essential widespread communication.\nUpdate customers on changes to your terms of service or privacy policies.\nKeep customers informed through newsletters.\n\nThis campaign sends a semi-annual newsletter to subscribed users.\n\nCreate the entry condition with the step name Newsletter Opt-in.\nInclude all users who are in the audience Newsletter Opt-in.\nSend users to an email step to receive a newsletter.\n\nThis page was last modified: 21 Sep 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nCustomer loyalty email campaign\nCart abandonment\nOnboarding\nLow recency campaign\nCompany updates and newsletters\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nStorage\n/\nWarehouses\n/\nSpeeding Up Redshift Queries\nSpeeding Up Redshift Queries\n\nWaiting minutes and minutes, maybe even an hour, for your queries to compute is an unfortunate reality for growing companies. Whether your data has grown faster than your cluster, or you\u2019re running too many jobs in parallel, there are lots of reasons your queries might be slowing down.\n\nTo help you improve your query performance, this guide takes you through common issues and how to mitigate them.\n\nCommon Causes for Slow Queries\n1. Not enough space\n\nAs your data volume grows and your team writes more queries, you might be running out of space in your cluster.\n\nTo check if you\u2019re getting close to your max, run this query. It will tell you the percentage of storage used in your cluster. Segment recommends that you don\u2019t exceed 75-80% of your storage capacity. If you approach that limit, consider adding more nodes to your cluster.\n\nSELECT sum(pct_used);\nFROM svv_table_info;\n\n\nLearn how to resize your cluster.\n\n2. Inefficient queries\n\nAnother thing you\u2019ll want to check is if your queries are efficient. For example, if you\u2019re scanning an entire dataset with a query, you\u2019re probably not making the best use of your compute resources.\n\nSome tips for writing performant queries:\n\nConsider using\u00a0INNER joins\u00a0as they are more efficient than\u00a0LEFT joins.\n\nStay away from\u00a0UNION\u00a0whenever possible.\n\nSpecify multiple levels of conditionals when you can.\n\nUse\u00a0EXPLAIN\u00a0to show the query execution plan and cost.\n\nTo learn more about writing beautiful SQL, check out these resources:\n\nPeriscope on Query Performance\n\nMode on Performance Tuning SQL Queries\n\nChartio on Improving Query Performance\n\n3. Running multiple ETL processes and queries\n\nSome databases like Redshift have limited computing resources. Running multiple queries or ETL processes that insert data into your warehouse at the same time will compete for compute power.\n\nIf you have multiple ETL processes loading into your warehouse at the same time, especially when analysts are also trying to run queries, everything will slow down. Try to schedule them at different times and when your cluster is least active.\n\nIf you\u2019re a Segment Business Tier customer, you can schedule your sync times under Warehouses Settings.\n\nYou also might want to take advantage of Redshift\u2019s\u00a0Workload Management\u00a0that helps ensure fast-running queries won\u2019t get stuck behind long ones.\n\n4. Default WLM Queue Configuration\n\nAs mentioned before, Redshift schedules and prioritizes queries using\u00a0Workload Management. Each queue is configured to distribute resources in ways that can optimize for your use-case.\n\nThe default configuration is a single queue with only 5 queries running concurrently, but Segment discovered that the default only works well for low-volume warehouses. More often than not, adjusting this configuration can improve your sync times.\n\nBefore Segment\u2019s SQL statements, Segment uses\u00a0set query_group to \"segment\";\u00a0to group all the queries together. This allows you to create a queue that isolates Segment\u2019s queries from your own. The maximum concurrency that Redshift supports is 50 across\u00a0all\u00a0query groups, and resources like memory distribute evenly across all those queries.\n\nSegment\u2019s initial recommendation is for 2 WLM queues:\n\na queue for the\u00a0segment\u00a0query group with a concurrency of\u00a010\n\nleave the default queue with a concurrency of\u00a05\n\nGenerally, Segment is responsible for most writes in the databases Segment connects to, so having a higher concurrency allows Segment to write as fast as possible. If you\u2019re also using the same database for your own ETL process, you may want to use the same concurrency for both groups. You may even require additional queues if you have other applications writing to the database.\n\nEach cluster may have different needs, so feel free to stray from this recommendation if another configuration works better for your use-case. AWS provides some\u00a0guidelines, and you can always\u00a0contact us\u00a0as Segment is more than happy to share the learnings while working with Redshift.\n\nPro-tips for Segment Warehouses\n\nIn addition to following performance best practices, here are a some more optimizations to consider if you\u2019re using Segment Warehouses.\n\nFactors that affect load times\n\nWhen Segment is actively loading data into your data warehouse, Segment is competing for cluster space and storage with any other jobs you might be running. Here are the parameters that influence your load time for Segment Warehouses.\n\nVolume of data. Segment\u2019s pipeline needs to load and deduplicate data for each sync, so having more volume means these operations will take longer.\nNumber of sources.\u00a0When Segment starts a sync of your data into your warehouse, Segment kicks off a new job for every source you have in Segment. The more sources you have, the longer your load time could take. This is where the WLM queue and the concurrency setting can make a big difference.\nNumber and size of columns.\u00a0Column sizes and the number of columns also affect load time. If you have long property values or lots of properties per event, the load may take longer as well.\nPerformance optimizations\n\nTo make sure you have enough headroom for quick queries while using Segment Warehouses, here are some tips!\n\nSize up your cluster.\u00a0If you find your queries are getting slow at key times during the day, add more nodes to give enough room for Segment to load data and for your team to run their queries.\nDisable unused sources.\u00a0If you\u2019re not actively analyzing data from a source, consider disabling the source for your Warehouse (available for business tier). If you don\u2019t use a source anymore\u2014perhaps you were just playing around with it for testing, you might even want to remove it. This will kick off fewer jobs in Segment\u2019s ETL process.\nSchedule syncs during off times.\u00a0If you\u2019re concerned about query times and you don\u2019t mind data that\u2019s a little stale, you can schedule your syncs to run when most of your team isn\u2019t actively using the database. (Available for business tier customers.)\n\nHopefully these steps will help to speed up your workflow! If you need any other help, feel free to\u00a0contact us.\n\nThis page was last modified: 21 Apr 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nCommon Causes for Slow Queries\nPro-tips for Segment Warehouses\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nFunctions\n/\nSource Functions\nSource Functions\n\nSource functions allow you to gather data from any third-party applications without worrying about setting up or maintaining any infrastructure.\n\nAll functions are scoped to your workspace, so members of other workspaces cannot view or use them.\n\nFunctions is available to all customer plan types with a free allotment of usage hours. Read more about Functions usage limits, or see your workspace\u2019s Functions usage stats.\n\nCreate a source function\nFrom your workspace, go to Connections > Catalog and click the Functions tab.\nClick New Function.\nSelect Source as the function type and click Build.\n\nAfter you click Build, a code editor appears. Use the editor to write the code for your function, configure settings, and test the function\u2019s behavior.\n\nTip: Want to see some example functions? Check out the templates available in the Functions UI, or in the open-source Segment Functions Library. (Contributions welcome!)\n\nCode the source function\n\nSource functions must have an onRequest() function defined. This function is executed by Segment for each HTTPS request sent to this function\u2019s webhook.\n\nasync function onRequest(request, settings) {\n  // Process incoming data\n}\n\n\nThe onRequest() function receives two arguments:\n\nrequest - an object describing the incoming HTTPS request.\nsettings - set of settings for this function.\nRequest processing\n\nTo parse the JSON body of the request, use the request.json() method, as in the following example:\n\nasync function onRequest(request) {\n  const body = request.json()\n  console.log('Hello', body.name)\n}\n\n\nUse the request.headers object to get values of request headers. Since it\u2019s an instance of Headers, the API is the same in both the browser and in Node.js.\n\nasync function onRequest(request) {\n  const contentType = request.headers.get('Content-Type')\n  const authorization = request.headers.get('Authorization')\n}\n\n\nTo access the URL details, refer to request.url object, which is an instance of URL.\n\nasync function onRequest(request) {\n  // Access a query parameter (e.g. `?name=Jane`)\n  const name = request.url.searchParams.get('name')\n}\n\nSending messages\n\nYou can send messages to the Segment API using the Segment object:\n\nasync function onRequest(request) {\n  Segment.identify({\n    userId: 'user_id',\n    traits: {\n      name: 'Jane Hopper'\n    }\n  })\n\n  Segment.track({\n    event: 'Page Viewed',\n    userId: 'user_id',\n    properties: {\n      page_name: 'Summer Collection 2020'\n    }\n  })\n\n  Segment.group({\n    groupId: 'group_id',\n    traits: {\n      name: 'Clearbit'\n    }\n  })\n\n  Segment.set({\n    collection: 'products',\n    id: 'product_id',\n    properties: {\n      name: 'Nike Air Max'\n    }\n  })\n}\n\nIdentify\n\nUse Identify calls to connect users with their actions, and to record traits about them.\n\nSegment.identify({\n  userId: 'user_id',\n  traits: {\n    name: 'Jane Hopper'\n  }\n})\n\n\nThe Segment.identify() method accepts an object with the following fields:\n\nuserId - Unique identifier for the user in your database.\nanonymousId - A pseudo-unique substitute for a User ID, for cases when you don\u2019t have an absolutely unique identifier.\ntraits - Object with data about or related to the user, like name or email.\ncontext - Object with extra information that provides useful context, like locale or country.\nTrack\n\nTrack calls record actions that users perform, along with any properties that describe the action.\n\nSegment.track({\n  event: 'Page Viewed',\n  userId: 'user_id',\n  properties: {\n    page_name: 'Summer Collection 2020'\n  }\n})\n\n\nThe Segment.track() method accepts an object with the following fields:\n\nuserId - Unique identifier for the user in your database.\nanonymousId - A pseudo-unique substitute for a User ID, for cases when you don\u2019t have an absolutely unique identifier.\nproperties - Object with data that is relevant to the action, like product_name or price.\ncontext - Object with extra information that provides useful context, like locale or country.\nGroup\n\nGroup calls associate users with a group, like a company, organization, account, project, or team.\n\nSegment.group({\n  groupId: 'group_id',\n  traits: {\n    name: 'Clearbit'\n  }\n})\n\n\nThe Segment.group() method accepts an object with the following fields:\n\ngroupId - Unique identifier for the group in your database.\ntraits - Object with data that is relevant to the group, like group_name or team_name.\ncontext - Object with extra information that provides useful context, like locale or country.\nPage\n\nPage calls record whenever a user sees a page of your website, along with any other properties about the page.\n\nSegment.page({\n  name: 'Shoe Catalog',\n  properties: {\n    url: 'https://myshoeshop.com/catalog'\n  }\n})\n\n\nThe Segment.page() method accepts an object with the following fields:\n\nuserId - Unique identifier for the user in your database.\nanonymousId - A pseudo-unique substitute for a User ID, for cases when you don\u2019t have an absolutely unique identifier.\nname - Name of the page.\nproperties - Object with information about the page, like page_name or page_url.\ncontext - Object with extra information that provides useful context, like locale or country.\nScreen\n\nScreen calls record when a user sees a screen, the mobile equivalent of Page, in your mobile app.\n\nSegment.screen({\n  name: 'Shoe Feed',\n  properties: {\n    feed_items: 5\n  }\n})\n\n\nThe Segment.screen() method accepts an object with the following fields:\n\nuserId - Unique identifier for the user in your database.\nanonymousId - A pseudo-unique substitute for a User ID, for cases when you don\u2019t have an absolutely unique identifier.\nname - Name of the screen.\nproperties - Object with data about the screen, like screen_name.\ncontext - Object with extra information that provides useful context, like locale or country.\nAlias\n\nThe Alias call merges two user identities, effectively connecting two sets of user data as one.\n\nSegment.alias({\n  previousId: 'old-email@example.com',\n  userId: 'new-email@example.com'\n})\n\n\nThe Segment.alias() method accepts an object with the following fields:\n\npreviousId - Previous unique identifier for the user.\nuserId - Unique identifier for the user in your database.\nanonymousId - A pseudo-unique substitute for a User ID, for cases when you don\u2019t have an absolutely unique identifier.\nSet\n\nThe Set call uses the object API to save object data to your Redshift, BigQuery, Snowflake, or other data warehouses supported by Segment.\n\nSegment.set({\n  collection: 'products',\n  id: 'product_id',\n  properties: {\n    name: 'Nike Air Max 90',\n    size: 11\n  }\n})\n\n\nThe Segment.set() method accepts an object with the following fields:\n\ncollection - A collection name, which must be lowercase.\nid - An object\u2019s unique identifier.\nproperties - An object with free-form data.\n\nWhen you use the set() method, you won\u2019t see events in the Source Debugger. Segment only sends events to connected warehouses.\n\nRuntime and dependencies\n\nOn March 26, 2024, Segment is upgrading the Functions runtime environment to Node.js v18, which is the current long-term support (LTS) release.\n\nThis upgrade keeps your runtime current with industry standards. Based on the AWS Lambda and Node.js support schedule, Node.js v16 is no longer in Maintenance LTS. Production applications should only use releases of Node.js that are in Active LTS or Maintenance LTS.\n\nAll new functions will use Node.js v18 starting March 26, 2024.\n\nFor existing functions, this change automatically occurs as you update and deploy an existing function. Segment recommends that you check your function post-deployment to ensure everything\u2019s working. Your function may face issues due to the change in sytax between different Node.js versions and dependency compatibility.\n\nLimited time opt-out option\n\nIf you need more time to prepare, you can opt out of the update before March 19, 2024.\n\nNote that if you opt out:\n- The existing functions will continue working on Node.js v16.\n- You won\u2019t be able to create new functions after July 15, 2024.\n- You won\u2019t be able to update existing functions after August 15, 2024.\n- You won\u2019t receive future bug fixes, enhancements, and dependency updates to the functions runtime.\n\nContact Segment to opt-out or with any questions.\n\nNode.js 18\n\nSegment strongly recommends updating to Node.js v18 to benefit from future runtime updates, the latest security, and performance improvements.\n\nFunctions do not currently support importing dependencies, but you can contact Segment Support to request that one be added.\n\nThe following dependencies are installed in the function environment by default.\n\natob v2.1.2 exposed as atob\naws-sdk v2.488.0 exposed as AWS\nbtoa v1.2.1 exposed as btoa\nfetch-retry exposed as fetchretrylib.fetchretry\nform-data v2.4.0 exposed as FormData\n@google-cloud/automl v2.2.0 exposed as google.cloud.automl\n@google-cloud/bigquery v5.3.0 exposed as google.cloud.bigquery\n@google-cloud/datastore v6.2.0 exposed as google.cloud.datastore\n@google-cloud/firestore v4.4.0 exposed as google.cloud.firestore\n@google-cloud/functions v1.1.0 exposed as google.cloud.functions\n@google-cloud/pubsub v2.6.0 exposed as google.cloud.pubsub\n@google-cloud/storage v5.3.0 exposed as google.cloud.storage\n@google-cloud/tasks v2.6.0 exposed as google.cloud.tasks\nhubspot-api-nodejs exposed as hubspotlib.hubspot\njsforce v1.11.0 exposed as jsforce\njsonwebtoken v8.5.1 exposed as jsonwebtoken\nlibphonenumber-js exposed as libphonenumberjslib.libphonenumberjs\nlodash v4.17.19 exposed as _\nmailchimp marketing exposed as mailchimplib.mailchimp\nmailjet exposed as const mailJet = nodemailjet.nodemailjet;\nmoment-timezone v0.5.31 exposed as moment\nnode-fetch v2.6.0 exposed as fetch\noauth v0.9.15 exposed as OAuth\n@sendgrid/client v7.4.7 exposed as sendgrid.client\n@sendgrid/mail v7.4.7 exposed as sendgrid.mail\nskyflow exposed as skyflowlib.skyflow\nstripe v8.115.0 exposed as stripe\ntwilio v3.68.0 exposed as twilio\nuuidv5 v1.0.0 exposed as uuidv5.uuidv5\nwinston v2.4.6 exposed as const winston = winstonlib.winston\nxml v1.0.1 exposed as xml\nxml2js v0.4.23 exposed as xml2js\n\nzlib v1.0.5 exposed as zlib.zlib\n\n\nuuidv5 is exposed as an object. Use uuidv5.uuidv5 to access its functions. For example:\n\n  async function onRequest(request, settings) {\n       uuidv5 = uuidv5.uuidv5;\n       console.log(typeof uuidv5);\n\n        //Generate a UUID in the default URL namespace\n        var urlUUID = uuidv5('url', 'http://google/com/page');\n        console.log(urlUUID);\n\n        //Default DNS namespace\n        var dnsUUID = uuidv5('dns', 'google.com');\n        console.log(dnsUUID);\n    }\n\n\nzlib\u2019s asynchronous methods inflate and deflate must be used with async or await. For example:\n\nzlib = zlib.zlib;  // Required to access zlib objects and associated functions\nasync function onRequest(request, settings) {\n  const body = request.json();\n\n  const input = 'something';\n\n  // Calling inflateSync method\n  var deflated = zlib.deflateSync(input);\n\n  console.log(deflated.toString('base64'));\n\n  // Calling inflateSync method\n  var inflated = zlib.inflateSync(new Buffer.from(deflated)).toString();\n\n  console.log(inflated);\n\n  console.log('Done');\n  }\n\n\nThe following Node.js modules are available:\n\ncrypto Node.js module exposed as crypto.\nhttps Node.js module exposed as https.\n\nOther built-in Node.js modules aren\u2019t available.\n\nFor more information on using the aws-sdk module, see how to set up functions for calling AWS APIs.\n\nCaching\n\nBasic cache storage is available through the cache object, which has the following methods defined:\n\ncache.load(key: string, ttl: number, fn: async () => any): Promise<any>\nObtains a cached value for the provided key, invoking the callback if the value is missing or has expired. The ttl is the maximum duration in milliseconds the value can be cached. If omitted or set to -1, the value will have no expiry.\ncache.delete(key: string): void\nImmediately remove the value associated with the key.\n\nSome important notes about the cache:\n\nWhen testing functions in the code editor, the cache will be empty because each test temporarily deploys a new instance of the function.\nValues in the cache are not shared between concurrently-running function instances; they are process-local which means that high-volume functions will have many separate caches.\nValues may be expunged at any time, even before the configured TTL is reached. This can happen due to memory pressure or normal scaling activity. Minimizing the size of cached values can improve your hit/miss ratio.\nFunctions that receive a low volume of traffic may be temporarily suspended, during which their caches will be emptied. In general, caches are best used for high-volume functions and with long TTLs. The following example gets a JSON value through the cache, only invoking the callback as needed:\nconst ttl = 5 * 60 * 1000 // 5 minutes\nconst val = await cache.load(\"mycachekey\", ttl, async () => {\n    const res = await fetch(\"http://echo.jsontest.com/key/value/one/two\")\n    const data = await res.json()\n    return data\n})\n\nCreate settings and secrets\n\nSettings allow you to pass configurable variables to your function, which is the best way to pass sensitive information such as security tokens. For example, you might use settings as placeholders to use information such as an API endpoint and API key. This way, you can use the same code with different settings for different purposes. When you deploy a function in your workspace, you are prompted to fill out these settings to configure the function.\n\nFirst, add a setting in Settings tab in the code editor:\n\nClick Add Setting to add your new setting.\n\nYou can configure the details about this setting, which change how it\u2019s displayed to anyone using your function:\n\nLabel - Name of the setting, which users see when configuring the function.\nName - Auto-generated name of the setting to use in function\u2019s source code.\nType - Type of the setting\u2019s value.\nDescription - Optional description, which appears below the setting name.\nRequired - Enable this to ensure that the setting cannot be saved without a value.\nEncrypted - Enable to encrypt the value of this setting. Use this setting for sensitive data, like API keys.\n\nAs you change the values, a preview to the right updates to show how your setting will look and work.\n\nClick Add Setting to save the new setting.\n\nOnce you save a setting, it appears in the Settings tab for the function. You can edit or delete settings from this tab.\n\nNext, fill out this setting\u2019s value in Test tab, so that you can run the function and check the setting values being passed.\n\nNote, this value is only for testing your function.\n\nNow that you\u2019ve configured a setting and filled in a test value, you can add code to read its value and run the function:\n\nasync function onRequest(request, settings) {\n  const apiKey = settings.apiKey\n  //=> \"super_secret_string\"\n}\n\n\nWhen you deploy a source function in your workspace, you are prompted to fill out settings to configure the source. You can access these settings later by navigating to the Source Settings page for the source function.\n\nTest the source function\n\nYou can test your code directly from the editor in two ways: either by receiving real HTTPS requests through a webhook, or by manually constructing an HTTPS request from within the editor.\n\nThe advantage of testing your source function with webhooks is that all incoming data is real, so you can test behavior while closely mimicking the production conditions.\n\nNote: Segment has updated the webhook URL to api.segmentapis.com/functions. To use webhooks with your function, you must:\n\nGenerate a public API token.\nCreate a Public API Token, or follow these steps: In your Segment Workspace, navigate to Settings \u2192 Workspace settings \u2192 Access Management \u2192 Token. Click + Create Token. Create a description for the token and assign access. Click Create and save the access token before clicking Done.\nFor POST calls, use this Public API token in the Authorization Header, as Bearer Token : public_api_token\nTesting source functions with a webhook\n\nYou can use webhooks to test the source function either by sending requests manually (using any HTTP client such as cURL, Postman, or Insomnia), or by pasting the webhook into an external server that supports webhooks (such as Slack). A common Segment use case is to connect a Segment webhooks destination or webhook actions destination to a test source, where the Webhook URL/endpoint that is used corresponds to the provided source function\u2019s endpoint, then you can trigger test events to send directly to that source, which are routed through your Webhook destination and continue on to the source function: Source \u2192 Webhook destination \u2192 Source Function.\n\nFrom the source function editor, copy the provided webhook URL (endpoint) from the \u201cAuto-fill via Webhook\u201d dialog. Note : When a new source is created that utilizes a source function, the new source\u2019s endpoint (webhook URL) will differ from the URL that is provided in the source function\u2019s test environment.\n\nTo test the source function:\n\nSend a POST request to the source function\u2019s provided endpoint (webhook URL)\nInclude an event body\nThe request must include these Headers:\nContent-Type : application/json or Content-Type : application/x-www-form-urlencoded\nAuthorization : Bearer _your_public_api_token_\nTesting source functions manually\n\nYou can also manually construct the headers and body of an HTTPS request inside the editor and test with this data without using webhooks. The Content-Type Header is required when testing the function:\n\nContent-Type : application/json or Content-Type : application/x-www-form-urlencoded\n\nSave and deploy the function\n\nAfter you finish building your source function, click Configure to name it, then click Create Function to save it. The source function appears on the Functions page in your workspace\u2019s catalog.\n\nIf you\u2019re editing an existing function, you can Save changes without updating instances of the function that are already deployed and running.\n\nYou can also choose to Save & Deploy to save the changes, and then choose which already-deployed functions to update with your changes. You might need additional permissions to update existing functions.\n\nSource functions logs and errors\n\nYour function may encounter errors that you missed during testing, or you might intentionally throw errors in your code (for example, if the incoming request is missing required fields).\n\nIf your function throws an error, execution halts immediately. Segment captures the incoming request, any console logs the function printed, and the error, and displays this information in the function\u2019s Errors tab. You can use this tab to find and fix unexpected errors.\n\nFunctions can throw an Error or custom Error, and you can also add additional helpful context in logs using the console API. For example:\n\nasync function onRequest(request, settings) {\n  const body = request.json()\n  const userId = body.userId\n\n  console.log('User ID is', userId)\n\n  if (typeof userId !== 'string' || userId.length < 8) {\n    throw new Error('User ID is invalid')\n  }\n\n  console.log('User ID is valid')\n}\n\n\nWarning: Do not log sensitive data, such as personally-identifying information (PII), authentication tokens, or other secrets. You should especially avoid logging entire request/response payloads. Segment only retains the 100 most recent errors and logs for up to 30 days but the Errors tab may be visible to other workspace members if they have the necessary permissions.\n\nError types\nBad Request: is any error thrown by your code not covered by the other errors.\nInvalid Settings: A configuration error prevented Segment from executing your code. If this error persists for more than an hour, contact Segment Support.\nMessage Rejected: Your code threw InvalidEventPayload or ValidationError due to invalid input.\nUnsupported Event Type: Your code doesn\u2019t implement a specific event type (for example, onTrack()) or threw an EventNotSupported error.\nStatusCode: 429, TooManyRequestsException: Rate Exceeded: Rate limit exceeded. These events will be retried when the rate becomes available.\nfailed calling Tracking API: the message is too large and over the maximum 32KB limit: Segment\u2019s Tracking API can only handle API requests that are 32KB or smaller. Reduce the size of the request for Segment to accept the event.\nRetry: Your code threw RetryError indicating that the function should be retried.\n\nSegment only attempts to run your source function again if a Retry error occurs.\n\nManaging source functions\nSource functions permissions\n\nFunctions have specific roles which can be used for access management in your Segment workspace.\n\nAccess to functions is controlled by two permissions roles:\n\nFunctions Admin: Create, edit, and delete all functions, or a subset of specified functions.\nFunctions Read-only: View all functions, or a subset of specified functions.\n\nYou also need additional Source Admin permissions to enable source functions, connect destination functions to a source, or to deploy changes to existing functions.\n\nEditing and deleting source functions\n\nIf you are a Workspace Owner or Functions Admin, you can manage your source function from the Functions tab in the catalog.\n\nConnecting source functions\n\nYou must be a Workspace Owner or Source Admin to connect an instance of your function in your workspace.\n\nFrom the Functions tab, click Connect Source and follow the prompts to set it up in your workspace.\n\nAfter configuring, find the webhook URL - either on the Overview or Settings \u2192 Endpoint page.\n\nCopy and paste this URL into the upstream tool or service to send data to this source.\n\nSource function FAQs\nWhat is the retry policy for a webhook payload?\n\nSegment retries invocations that throw RetryError or Timeout errors up to six times. After six attempts, the request is dropped. The initial wait time for the retried event is a random value between one and three minutes. Wait time increases exponentially after every retry attempt. The maximum wait time between attempts can reach 20 minutes.\n\nI configured RetryError in a function, but it doesn\u2019t appear in my source function error log.\n\nRetry errors only appear in the source function error logs if the event has exhausted all six retry attempts and, as a result, has been dropped.\n\nWhat is the maximum payload size for the incoming webhook?\n\nThe maximum payload size for an incoming webhook payload is 512 KiB.\n\nWhat is the timeout for a function to execute?\n\nThe execution time limit is five seconds, however Segment strongly recommends that you keep execution time as low as possible. If you are making multiple external requests you can use async / await to make them concurrently, which will help keep your execution time low.\n\nDoes Segment alter incoming payloads?\n\nSegment alphabetizes payload fields that come in to deployed source functions. Segment doesn\u2019t alphabetize payloads in the Functions tester. If you need to verify the exact payload that hits a source function, alphabetize it first. You can then make sure it matches what the source function ingests.\n\nDoes the source function allow GET requests?\n\nGET requests are not supported with a source function. Source functions can only receive data through POST requests.\n\nCan I use a Source Function in place of adding a Tracking Pixel to my code?\n\nNo. Tracking Pixels operate client-side only and need to be loaded onto your website directly. Source Functions operate server-side only, and aren\u2019t able to capture or implement client-side tracking code. If the tool you\u2019re hoping to integrate is server-side, then you can use a Source Function to connect it to Segment.\n\nWhat is the maximum data size that can be displayed in console.logs() when testing a Function?\n\nThe test function interface has a 4KB console logging limit. Outputs surpassing this limit will not be visible in the user interface.\n\nCan I send a custom response from my Source Function to an external tool?\n\nNo, Source Functions can\u2019t send custom responses to the tool that triggered the Function\u2019s webhook. Source Functions can only send a success or failure response, not a custom one.\n\nWhy am I seeing the error \u201cFunctions are unable to send data or events back to their originating source\u201d when trying to save my Source Function?\n\nThis error occurs because Segment prevents Source Functions from sending data back to their own webhook endpoint (https://fn.segmentapis.com). Allowing this could create an infinite loop where the function continuously triggers itself.\n\nTo resolve this error, check your Function code and ensure the URL https://fn.segmentapis.com is not included. This URL is used to send data to a Source Function and shouldn\u2019t appear in your outgoing requests. Once you remove this URL from your code, you\u2019ll be able to save the Function successfully.\n\nThis page was last modified: 26 Nov 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nCreate a source function\nCode the source function\nCreate settings and secrets\nTest the source function\nSave and deploy the function\nSource functions logs and errors\nManaging source functions\nSource function FAQs\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSources\n/\nSchema\n/\nUsing Schema Controls\nUsing Schema Controls\n\nOnce you have enabled destinations for a given source, all of the data you track will be routed to your connected tools and warehouses. If you no longer wish to send all data to a particular destination, you can disable the destination from the Source overview page.\u00a0\n\nSegment gives you the power to control exactly what data is allowed into your destinations, so you can protect the integrity of your data, and the decisions you make with it. You can send all of your data to a warehouse and only two specific events to an analytics tool. You can also block rogue events from all of your warehouses and end tools.\n\nFilter specific events from being sent to specific destinations\n\nAn integrations object may be passed in the options of \u00a0group, identify, page and track methods, allowing selective destination filtering. By default all destinations are enabled.\n\nAll customers can filter specific events from being sent to specific destinations (except for warehouses) by updating their tracking code. Here is an example showing how to send a single message only to Intercom and Google Analytics:\n\nanalytics.identify('user_123', {\n  email: 'jane.kim@example.com',\n  name: 'Jane Kim'\n}, {\n  integrations: {\n    'All': true,\n    'Intercom': true,\n    'Google Analytics': true,\n    'Mixpanel': false\n  }\n});\n\n\nDestination flags are case sensitive and match the Destination\u2019s name in the docs (for example, \u201cAdLearn Open Platform\u201d, \u201cawe.sm\u201d, \u201cMailChimp\u201d, etc.).\n\nIf you\u2019re on Segment\u2019s Business plan, you can filter track calls right from the Segment UI on your Source Schema page by clicking on the field in the Integrations column and then adjusting the toggle for each tool. Segment recommends using the UI if possible since it\u2019s a much simpler way of managing your filters and can be updated with no code changes on your side.\n\nBlock or disable specific events and properties from being sent to all destinations\n\nIf you no longer want to track an event, you can either remove it from your code or, if you\u2019re on the Business plan, you can block track calls right from the Segment UI on your Source Schema page by adjusting the toggle for each event.\n\nOnce you block an event in Segment, Segment stops forwarding it to all of your destinations, including your warehouses. You can remove it from your code at your leisure. In addition to blocking track calls, Business plan customers can block all Page and Screen calls, as well as Identify traits and Group properties.\u00a0\n\nAdd a new event using the New Event button\n\nThe New Event button in your source schema adds the event to the source schema only. It does not add any events to your tracking code. If you want to track an event, you still need to manually add it to your source code.\n\nA use case for this feature might be to enable schema filtering for a new event before it arrives in the source to prevent it from reaching specific downstream destinations.\n\nExport your Source Schema\n\nSegment allows users with Source Read-only permissions to download Source Schemas as a CSV file, maximizing portability and access to event data. You can download a copy of your schema by visiting the Source Schema page.\n\nYou can export Track, Identify, and Group Source Schemas.\n\nDownload a CSV\n\nYou can only download one Source Schema CSV schema type (Track, Identify, or Group) per source at the same time.\n\nTo download a Source Schema CSV file:\n\nSign in to Segment and select a source.\nClick the Schema tab in the source header.\nOn the Source Schema page, select a schema type (Track, Identify, or Group) and a timeframe (7 days or 30 days).\nClick the Download CSV button.\nA toast pops up on the top of the page, with the message \u201cYour file is processing. When your file is ready it will be available to download from the Download History page.\u201d\nOpen the Download History page by clicking the link in the toast or following the instructions in the view download history section.\nOnce the file status column indicates that the download was successful, click the Download CSV link to download your CSV to your computer. If the file status column shows that the download has failed, return to the Source Schema page and try the download again.\nThe Source Schema CSV name has the following format:\nworkspaceSlug-sourceSlug-schemaType--yyyy-mm-dd--hh-mm-utc\n\nAll events and properties are now included in the CSV file\n\nWhen you export a Source Schema, all events and properties are included in the CSV file regardless of the filters or search parameters currently applied to the Source Schema view.\n\nView download history\n\nYou can view the Source Schema exports from the last 14 days on the Download History page.\n\nTo access the Download History page:\n\nSign in to Segment and select a source.\nClick the Schema tab in the source header.\nClick the View Download History link.\nTrack event CSV format\n\nThe Track event CSV file contains the following columns:\n\nEvent Name\nLast Seen At (UTC)\nIf greater than your selected timeframe (7 days or 30 days) the value is \u201cmore than 7 days ago\u201d or \u201cmore than 30 days ago\u201d\nProperty Name\nAllowed\nBlocked\nTotal\nPlanned (available for Protocols customers with a connected Tracking Plan)\nValues are \u201cplanned\u201d or \u201cunplanned\u201d\n\nLabels in your exported CSV\n\nIf you use labels, they appear as columns in your CSV. The column headers are keys, and the column data contains values.\n\nIdentity and Group event CSV format\n\nThe Identify and Group CSV files contain the following columns:\n\nTrait Name\nLast Seen At (UTC)\nIf greater than your selected timeframe (7 days or 30 days) the value is \u201cmore than 7 days ago\u201d or \u201cmore than 30 days ago\u201d\nAllowed\nBlocked\nTotal\nPlanned (available for Protocols customers with a connected Tracking Plan)\nValues are \u201cplanned\u201d or \u201cunplanned\u201d\n\nThe exported schema doesn\u2019t include actual values (for example, personal data) for the events, properties, and traits you are tracking for a specific source.\n\nSee the Segment Schema Limits for more information on how to manage the Source Schema.\n\nThis page was last modified: 22 Jun 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nFilter specific events from being sent to specific destinations\nBlock or disable specific events and properties from being sent to all destinations\nAdd a new event using the New Event button\nExport your Source Schema\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGetting Started\n/\nA Full Segment Implementation\nA Full Segment Implementation\n\nBefore you start implementing from your tracking plan, let\u2019s review the Segment methods, what they do, and when you should use each.\n\nSegment methods in detail\n\nSegment\u2019s libraries generate and send messages to our tracking API in JSON format, and provide a standard structure for the basic API calls. We also provide recommended JSON structure (also known as a schema, or \u2018Spec\u2019) that helps keep the most important parts of your data consistent, while allowing great flexibility in what other information you collect and where.\n\nThere are six calls in the basic tracking API, which answer specific questions:\n\nIdentify: Who is the user?\nTrack: What are they doing?\nPage: What web page are they on?\nScreen: What app screen are they on?\nGroup: What account or organization are they part of?\nAlias: What was their past identity?\n\nAmong these calls, you can think of Identify, Group, and Alias as similar types of calls, all to do with updating our understanding of the user who is triggering Segment messages. You can think of these calls as adding information to, or updating an object record in a database. Objects are described using \u201ctraits\u201d, which you can collect as part of your calls.\n\nThe other three, Track, Page, and Screen, can be considered as increasingly specific types of events. Events can occur multiple times, but generate separate records which append to a list, instead of being updated over time.\n\nA Track call is the most basic type of call, and can represent any type of event. Page and Screen are similar and are triggered by a user viewing a page or screen, however Page calls can come from both web and mobile-web views, while Screen calls only occur on mobile devices. Because of the difference in platform, the context information collected is very different between the two types of calls.\n\nSegment recommends that you always use the Page and Screen calls when recording a page-view, rather than creating a \u201cPage Viewed\u201d Track event, because the Page/Screen calls automatically collect more contextual information.\n\nAnatomy of a Segment message\n\nThe most basic Segment message requires only a userID or anonymousID; all other fields are optional to allow for maximum flexibility. However, a normal Segment message has three main parts: the common fields, the \u201ccontext\u201d object, and the properties (if it\u2019s an event) or traits (if it\u2019s an object).\n\nThe common fields include information specific to how the call was generated, like the timestamp and library name and version. The fields in the context object are usually generated by the library, and include information about the environment in which the call was generated: page path, user agent, OS, locale settings, etc. The properties and traits are optional and are where you customize the information you want to collect for your implementation.\n\nAnother common part of a Segment message is the integrations object, which you can use to explicitly filter which destinations the call is forwarded to. However this object is optional, and is often omitted in favor of non-code based filtering options.\n\nIdentify calls\nanalytics.identify (user_id: \"12345abcde\",\n  traits: {\n    email: 'michael.phillips@segment.com',\n    name: 'Michael Phillips',\n    city: 'New York',\n    state: 'NY',\n    internal: True })\n\n\nThe Identify call allows Segment to know who is triggering an event.\n\nWhen to call Identify\n\nCall Identify when the user first provides identifying information about themselves (usually during log in), or when they update their profile information.\n\nWhen called as part of the login experience, you should call Identify as soon as possible after the user logs in. When possible, follow the Identify call with a Track event that records what caused the user to be identified.\n\nWhen you make an Identify call as part of a profile update, you only need to send the changed information to Segment. You can send all profile info on every Identify call if that makes implementation easier, but this is optional.\n\nLearn More\n\nBest Practices for Identifying Users\n\nTraits in Identify calls\n\nThese are called traits for Identify calls, and properties for all other methods.\n\nThe most important trait to pass as part of the Identify call is userId, which uniquely identifies a user across all applications.\n\nYou should use a hash value to ensure uniqueness, although other values are acceptable; for example, email address isn\u2019t the best thing to use as a userid, but is usually acceptable since it will be unique, and doesn\u2019t change often.\n\nBeyond that, the Identify call is your opportunity to provide information about the user that can be used for future reporting, so you should try to send any fields that you might want to report on later.\n\nConsider using Identify and traits when:\n\nGathering user profile data (for example, company, city/state, job title, or other user-level data)\nGathering company-level data (for example, company size, number of seats, etc)\nHow to Call Identify\n\nYou can call Identify from any of Segment\u2019s device-based or server-based libraries, including Javascript, iOS, Android, Ruby, and Python.\n\nHere are two examples of calling Identify from two different libraries:\n\nJavaScript Identify call\nRuby Identify call\nanalytics.identify(\"12345abcde\", {\n  \"email\": \"michael.phillips@segment.com\",\n  \"name\": \"Michael Phillips\",\n  \"city\": \"New York\",\n  \"state\": \"NY\",\n  \"internal\": True\n});\n\nUsing analytics.reset()\n\nWhen a user explicitly signs out of one of your applications, you can call analytics.reset() to stop logging further event activity to that user, and create a new anonymousId for subsequent activity (until the user logins in again and is subsequently identify-ed). This call is most relevant for client-side Segment libraries, as it clears cookies in the user\u2019s browser.\n\nMake a reset() call as soon as possible after sign-out occurs, and only after it succeeds (not immediately when the user clicks sign out). For more info on this call, see the JavaScript source documentation.\n\nPage and Screen\n\nThe Page and Screen calls tell Segment what web page or mobile screen the user is on. This call automatically captures important context traits, so you don\u2019t have to manually implement and send this data.\n\nPAGE CONTEXT AUTO-CAPTURED\tSCREEN CONTEXT AUTO-CAPTURED\t\u00a0\t\u00a0\ntitle\twindow.location.title\tapp\tbuild, name, namespace, version\nurl\twindow.location.url\tdevice\tadTrackingEnabled, advertisingId (IDFA/AAID), device ID, manufacturer, model, type (android/ios)\npath\twindow.location.path\tlibrary\tname, version\nreferrer\twindow.document.referrer\tlocale\twindow.document.referrer\nsearch\twindow.location.search\tnetwork\tcellular, wifi\nip\taddress\tip\taddress\nuserAgent\tstring\tos\tname, version\ncampaign\tutm_source, utm_medium, utm_campaign, utm_content\tscreen\theight, width\nPage and Screen call properties\n\nYou can always override the auto-collected Page/Screen properties with your own, and set additional custom page or screen properties.\n\nSome downstream tools (like Marketo) require that you attach specific properties (like email address) to every Page call.\n\nThis is considered a destination-specific implementation nuance, and you should check the documentation for each destination you plan to use and make a list of these nuances before you start implementation.\n\nNamed Page & Screen Calls\n\nYou can specify a page \u201cName\u201d at the start of the page or Screen call, which is especially useful to make list of page names into something more succinct for analytics. For example, on an ecommerce site you might want to call analytics.page( \"Product\" ) and then provide properties for that product:\n\nNamed Page Call for Javascript\nNamed Screen Call for iOS\nanalytics.page(\"Product\", {\n  \"category\": \"Smartwatches\",\n  \"sku\": \"13d31\"\n});\n\nWhen to Call Page\n\nSegment automatically calls a Page event whenever a web page loads. This might be enough for most of your needs, but if you change the URL path without reloading the page, for example in single page web apps, you must call Page manually .\n\nIf the presentation of user interface components don\u2019t substantially change the user\u2019s context (for example, if a menu is displayed, search results are sorted/filtered, or an information panel is displayed on the exiting UI) measure the event with a Track call, not a Page call.\n\nNote: When you trigger a Page call manually, make sure the call happens after the UI element is successfully displayed, not when it is called. It shouldn\u2019t be called as part of the click event that initiates it.\n\nFor more info on Page calls, review Page spec and Analytics.js docs.\n\nWhen to call Screen\n\nSegment Screen calls are essentially the Page method, except for mobile apps. Mobile Screen calls are treated similarly to standard Page tracking, only they contain more context traits about the device. The goal is to have as much consistency between web and mobile as is feasible.\n\nTrack calls\n\nThe Track call allows Segment to know what the user is doing.\n\nWhen to call Track\n\nThe Track call is used to track user and system events, such as:\n\nThe user interacting with a UI component (for example, \u201cButton Clicked\u201d)\nA significant UI component appearing, other than a page (for example, search results or a payment dialog)\nEvents and Properties\n\nYour Track calls should include both events and properties. Events are the actions you want to track, and properties are the data about the event that are sent with each event.\n\nProperties are powerful. They enable you to capture as much context about the event as you\u2019d like, and then cross-tabulate or filter your downstream tools. For example, let\u2019s say an eLearning website is tracking whenever a user bookmarks an educational article on a page. Here\u2019s what a robust analytics.js Track call could look like:\n\nanalytics.track('Article Bookmarked', {\n  \"title\": 'How to Create a Tracking Plan',\n  \"course\": 'Intro to Data Strategy',\n  \"author\": 'Dr. Anna Lytics',\n  \"publish_year\": '2019',\n  \"publish_month\": '03',\n  \"length\": 'Medium - 1000-2000 words',\n  \"assets\": {'Infographics','Interactive Charts'},\n  \"topics\": {'Data Planning','Segment','Data Flow'},\n  \"button_location\": 'Subheader - 3rd Column'\n});\n\n\nWith this Track call, we can analyze which authors had the most popular articles, which months and years led to the greatest volume of bookmarking overall, which button locations drive the most bookmark clicks, or which users gravitate towards infographics related to Data Planning.\n\nEvent Naming Best Practices\n\nEach event you track must have a name that describes the event, like \u2018Article Bookmarked\u2019 above. That name is passed in at the beginning of the Track call, and should be standardized across all your properties so you can compare the same actions on different properties.\n\nSegment\u2019s best practice is to use an \u201cObject Action\u201d (Noun<>Verb) naming convention for all Track events, for example, \u2018Article Bookmarked\u2019.\n\nSegment maintains a set of Business Specs which follow this naming convention around different use cases such as eCommerce, B2B SaaS, and Mobile.\n\nLet\u2019s dive deeper into the Object Action syntax that all Segment Track events should use.\n\nObjects are Nouns\n\nNouns are the entities or objects that the user or the system acts upon.\n\nIt\u2019s important to be thoughtful when naming objects so that they are referred to consistently within an application, and so that you refer to the same objects that might exist in multiple applications or sites by the same name.\n\nUse the following list of objects to see if there is a logical match with your application. If you have objects that aren\u2019t in this list, name it in a way that makes sense if it were to appear in other applications, and/or run it by Product Analytics.\n\nSome suggested Nouns\nMenu\nNavigation Drawer (the \u201cHamburger\u201d menu in the upper left corner of a UI)\nProfile\nAccount\nVideo\nActions are Verbs\n\nVerbs indicate the action taken by either a user on your site. When you name a new Track event, consider if you can describe the current interaction using a verb from the list below.\n\nIf you can\u2019t, choose a verb that describes what the user is trying to do in your specific case, but try to be flexible enough so that you could use it in other scenarios.\n\nSome suggested Verbs\nApplied - Applying a new format to the UI results.\nClicked - Catch-all for events where a user activated some part of the UI but no other verb captures the intent.\nCreated/Deleted - The user- or system-initiated action of creating or deleting an object (e.g., new search, favorite, post)\nDisplayed/Hidden - The user- or system-initiated action of hiding or displaying an element\nEnabled/Disabled - Enabling or disabling some feature (e.g., audible alarms, emails, etc).\nRefreshed - When a set of search results is refreshed.\nSearched - When an app is searched\nSelected - User clicked on an individual search result.\nSorted - The user or UI action that causes data in a table, for example, to be sorted\nUnposted - Making a previously publicly-viewable posting private.\nUpdated - The user action that initiates an update to an object (profile, password, search, etc.; typically be making a call to the backend), or the they system having actually completed the update (often this tracking call will be made in response to a server-side response indicating that the object was updated, which may or may not have an impact on the UI).\nViewed - (exactly what it says on the tin)\nProperty naming best practices\n\nSegment recommends that you record property names using snake case (for example property_name), and that you format property values to match how they are captured. For example, a username value would be captured in whatever case it the user typed it in as.\n\nUltimately, you can decide to use a casing different from our recommendations; however, the single most important aspect is that you\u2019re consistent across your entire tracking with one casing method.\n\nYou can read more about best practices for Track calls, .\n\nAll of the basic Segment methods have a common structure and common fields which are automatically collected on every call. You can see these in the common fields documentation.\n\nCommon properties to send with a Track call\n\nThe following properties should be sent with every Track call:\n\nEVENT CONTEXT\tPROPERTY NAME\tDESCRIPTION\nAny Track call\tinitiator\tStates whether the event was initiated by the user or the system.\nAny Track call\tdisplay_format\tResponsive or not (or some other indicator of the current page layout template)\nSearch Initiated or Search Results Displayed\t[Search Parameters]\tAll search parameters, with the names being the snake case version of the internal names.\nSearch Results Displayed\ttotal_result_count\tThe total number of results returned that match the search parameters. This number represents the number of results that could be returned to the user even if only a subset of those were actually returned (for example, if the results are paginated).\nPaginated List Displayed\ttotal_items_pages\tThe total number of pages of items available to be viewed by the user.\nPaginated List Displayed\titems_per_page\tThe number of possible items in each page of items (for example, if the UI is showing 50 search results per page). The actual number of items in the current page may be less than this number if, for example, the system is displaying the last page of results and there aren\u2019t enough results to fill to the page\u2019s maximum (for example, if there are 27 results when the page could display as many as 50).\nPaginated List Displayed\tcurrent_item_page\tThe current page number displayed to the user.\nExternal Link Clicked\tdestination_url\tThe URL that the user is taken to when clicked. Ideally, this will be the final destination (for example, after any redirects), but only the immediate destination is likely in most cases.\nItem List Sorted\tsort_column\tThe internal name of the column that was sorted.\nItem List Sorted\tsort_direction\tWhether the items sort in ascending or descending order.\nHow to call Track\n\nYou can make a Track call from any of Segment\u2019s client-side or server-side libraries, including JavaScript, iOS, Android, Ruby, and Python. Here are two examples of calling Track from two different libraries:\n\nJavaScript Track call\nRuby Track call\nanalytics.track('Article Bookmarked', {\n  \"title\": 'How to Create a Tracking Plan',\n  \"course\": 'Intro to Data Strategy',\n  \"author\": 'Dr. Anna Lytics',\n});\n\nBACK\nPlanning a Full Installation\n\nThink through your goals, plan your calls, and set yourself up for success.\n\nNEXT\nSending data to Destinations\n\nUnlock the power of Segment with Destinations.\n\nThis page was last modified: 07 Nov 2023\n\nFurther reading\nBest Practices for Event Calls\nBest Practices for Identifying Users\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSegment methods in detail\nAnatomy of a Segment message\nIdentify calls\nTraits in Identify calls\nHow to Call Identify\nUsing analytics.reset()\nPage and Screen\nTrack calls\nEvent Naming Best Practices\n\nRelated content\n\nBest Practices for Event Calls\nBest Practices for Identifying Users\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nIdentity Resolution\n/\nIdentity Resolution Onboarding\nIdentity Resolution Onboarding\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\n\nThe steps in this guide pertain to spaces created after October 5th, 2020. For spaces created before October 5th, 2020, please refer to Identity Resolution Settings.\n\nWorkspace owners, administrators, and users with the Identity Admin role can edit Identity Resolution Settings.\n\nSegment creates and merges user profiles based on a space\u2019s Identity Resolution configuration. Segment searches for identifiers such as userId, anonymousId, and email on incoming events and matches them to existing profiles or creates new profiles. These identifiers display in the Identities tab of a User Profile in the Profile explorer.\n\nNavigate to Unify > Profile explorer to view identities attached to a profile, along with custom traits, event history, and more.\n\nFlat matching logic\n\nAfter receiving a new event, Segment looks for profiles that match any of the identifiers on the event.\n\nBased on the existence of a match, one of three actions can occur:\n\n1: Create a new profile When there are no pre-existing profiles that have matching identifiers to the event, Segment creates a new user profile.\n\n2: Add to existing profile When there is one profile that matches all identifiers in an event, Segment attempts to map the traits, identifiers, and events on the call to that existing profile. If there is an excess of any identifier on the final profile, Segment defers to the Identity Resolution rules outlined below.\n\n3: Merge existing profiles When there are multiple profiles that match the identifiers in an event, Segment checks the Identity Resolution rules outlined below, and attempts to merge profiles.\n\nIdentity Resolution settings\n\nIdentity Admins should first configure Identity Resolution Settings to protect the identity graph from inaccurate merges and user profiles.\n\nDuring the space creation process, the first step is to choose an Identity Resolution configuration. If this is your first space, you have the option to choose a Segment-suggested Out-of-the-Box configuration or a custom Identity Resolution setup. All other spaces have a third option of importing settings from a different space.\n\nOut-of-the-box\n\nFor most first-time users, Segment recommends that you use the out-of-the-box configuration and answer a short series of questions for a best-fit setup for your use-case.\n\nIf you have custom unique identifiers or don\u2019t have a canonical user_id, you\u2019re automatically redirected to the Identity Resolution Settings page to complete your setup.\n\nCustom rules\n\nIf you\u2019re familiar with identity or have custom identifiers, Segment recommends that you select Custom Rules.\n\nSegment redirects you to the Identity Resolution Settings page where you can add Default Identifiers or Custom Identifiers.\n\nSegment\u2019s 11 default are:\n\nEXTERNAL ID TYPE\tMESSAGE LOCATION IN TRACK OR IDENTIFY CALL\nuser_id\tuserId\nemail\ttraits.email or context.traits.email\nandroid.id \tcontext.device.id when context.device.type = \u2018android\u2019\nandroid.idfa\tcontext.device.advertisingId when context.device.type = \u2018android\u2019 AND context.device.adTrackingEnabled = true\nandroid.push_token\tcontext.device.token when context.device.type = \u2018android\u2019\nanonymous_id\tanonymousId\nga_client_id\tcontext.integrations[\u2018Google Analytics\u2019].clientId when explicitly captured by users\ngroup_id \tgroupId\nios.id \tcontext.device.id when context.device.type = \u2018ios\u2019\nios.idfa\tcontext.device.advertisingId when context.device.type = \u2018ios\u2019 AND context.device.adTrackingEnabled = true\nios.push_token\tcontext.device.token when context.device.type = \u2018ios\u2019\n\nYou can also provide a trait or property key to match on to add custom identifiers. You can preview the locations where Segment looks for the identifier. Segment accepts both camelCase and snake_case for context.traits, traits, and properties, but accepts lowercase types for identifiers only in the context.externalIds object.\n\nBlocked values\n\nSegment recommends that you proactively prevent using certain values as identifiers. While these values remain in the payload on the event itself, it is not promoted to an identifier Segment uses to determine user profiles.\n\nThis is important when developers have a hard-coded value for fields like user_id during QA or development that then erroneously make it to production. This may cause hundreds of profiles to merge incorrectly and can have costly consequences if these spaces already feed data into a production email marketing tool or push notification tool downstream.\n\nIn the past, Segment has seen certain default values that cause large amounts of profiles to merge incorrectly. Segment suggests that for every identifier, customers opt into automatically blocking the following suggested values:\n\nVALUE\tTYPE\nZeroes and Dashes (^[0-]*$)\tPattern (REGEX)\n-1\tExact Match\nnull\tExact Match\nanonymous\tExact Match\n\nBefore sending data through, Segment also recommends that you add any default hard-coded values that your team uses during the development process, such as void or abc123.\n\nLimit\n\nIdentity Admins can specify the total number of values allowed per identifier type on a profile during a certain period. For example, in the image below, the anonymous_id field has a limit of 5 Weekly.\n\nThis will vary depending on how companies define a user today. In most cases, companies rely on user_id to distinguish user profiles and Segment defaults to the following configurations:\n\nIDENTIFIER\tLIMIT\nuser_id\t1\nall other identifiers\t5\n\nSpecific cases may deviate from this default. For example, a case where a user can have more than one user_id but one email, like when shopify_id and an internal UUID define a user. In this case, an example configuration may be:\n\nIDENTIFIER\tLIMIT\nemail\t1\nuser_id\t2\nall other identifiers\t5\n\nWhen you choose the limit on an identifier, ask the following questions about each of the identifiers you send to Segment:\n\nIs it an immutable ID? An immutable ID, such as user_id, should have 1 ever per user profile.\nIs it a constantly changing ID? A constantly changing ID, such as anonymous_id or ga_client_id, should have a short sliding window, such as 5 weekly or 5 monthly, depending on how often your application automatically logs out the user.\nIs it an ID that updates on a yearly basis? Most customers will have around five emails or devices at any one time, but can update these over time. For identifiers like email, android.id, or ios.id, Segment recommends a longer limit like 5 annually.\nPriority\n\nSegment considers the priority of an identifier once that identifier exceeds the limit on the final profile.\n\nFor example, consider a Segment space with the following Identity Resolution configurations:\n\nIDENTIFIER\tLIMIT\tPRIORITY\nuser_id\t1\t1\nemail\t5\t2\nanonymous_id\t5\t3\n\nA profile already exists with user_id abc123 and email jane@example1.com. A new event comes in with new user_id abc456 but the same email jane@example1.com.\n\nIf this event maps to this profile, the resulting profile would then contain two user_id values and one email. Given that user_id has a limit of 1, this exceeds the limit of that identifier. As a result, Segment checks the priority of the user_id identifier. Because email and user_id are the two identifiers on the event and email ranks lower than user_id, Segment demotes email as an identifier on the incoming event and tries again.\n\nAt this point, the event searches for any profiles that match just the identifier user_id abc456. Now there are no existing profiles with this identifier, so Segment creates a new profile with user_id abc456.\n\nBy default, Segment explicitly orders user_id and email as rank 1 and 2, respectively. All other identifiers are in alphabetical order beginning from rank 3. This means that if the identifiers sent with events flowing into Segment are user_id, email, anonymous_id, and ga_client_id, the rank would be as follows:\n\nIDENTIFIER\tPRIORITY\nuser_id\t1\nemail\t2\nanonymous_id\t3\nga_client_id\t4\n\nIf a new android.id identifier appeared without first giving it explicit order, the order would automatically reshuffle to:\n\nIDENTIFIER\tPRIORITY\nuser_id\t1\nemail\t2\nandroid.id\t3\nanonymous_id\t4\nga_client_id\t5\n\nIf you require an explicit order for all identifiers, configure this in the Identity Resolution Settings page before sending in events.\n\nWhen choosing the priority of your identifier, ask the following questions about each of the identifiers you send to Segment:\n\nIs it an immutable ID? Give immutable IDs, such as user_id, highest priority.\nAre they unique IDs? Give Unique IDs such as email higher priority than possibly shared identifiers like android.id or ios.id.\nDoes it temporarily identify a user? Identifiers such as anonymous_id, ios.idfa, and ga_client_id are constantly updated or expired for a user. Generally speaking, rank these lower than identifiers that permanently identify a user.\nImporting from an existing space\n\nThis option is available to new spaces after you create an initial Dev space. Segment recommends this option when identity settings are validated as correct in the initial Dev space and should be copied into the Production space.\n\nYou can review the identifiers, priorities, limits, and blocked values before you complete the import.\n\nConnect a source\n\nAfter you configure Identity Resolution settings, the next step is to connect a source to the Segment space.\n\nCreate an audience\n\nAfter you connect a source, Segment creates user profiles based off of replayed and newly incoming data.\n\nThe next step, which is important in the Dev space, is to create an audience to ensure that user profiles have populated correctly and that the Identity Resolution settings follow expected business logic.\n\nFor example, if there should be 100,000 distinct users who have a user_id, this would be a great way to validate that the Identity Resolution settings have calculated profiles correctly.\n\nFor more information about how to create audiences and traits, see Segment\u2019s Audiences docs.\n\nThis page was last modified: 15 Oct 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nIdentity Resolution settings\nConnect a source\nCreate an audience\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nSegment App\n/\nSupport Access\nSupport Access\n\nTo best assist you after you submit a support ticket, Segment\u2019s Customer Success Engineers may request temporary access to your workspace. Once you grant access to your workspace, CSEs can access your workspace for up to 7 days, or until you revoke access.\n\nSupport Access is not available for workspaces using forced-SSO at this time.\n\nGranting a Segment Support Engineer access to your account\n\nSupport Access is available for all Segment Workspace Owners and can be found on the Support Access tab of the Workspace Settings page.\n\nClick Grant Access to allow a Segment Support Engineer to access your account.\n\nSupport privileges\n\nWhen you grant Support Access, the Segment Support Engineers can do everything that you can do in your workspace.\n\nHow do I know it\u2019s working?\n\nWorkspace Owners of Business Tier workspaces can view any Support Access actions in the Audit Trail. You can see and monitor:\n\nSupport Access Granted - you enabled Support Access in your Workspace Settings\nSupport Login - a Segment Support Engineer accessed your workspace with your account\nSupport Access Revoked - you revoked Support Access from your account\n\nThis page was last modified: 10 Nov 2022\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nGranting a Segment Support Engineer access to your account\nSupport privileges\nHow do I know it\u2019s working?\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nApi\n/\nPublic Api\n/\nSegment Query Language Reference\nSegment Query Language Reference\nFREE X\nTEAM \u2713\nBUSINESS \u2713\nADD-ON X\n?\n\nSegment\u2019s query language lets you define audience segments and computed traits. With clear syntax and practical functionality, the language simplifies the process of defining conditions and computations, helping you extract valuable insights from customer data.\n\nThis reference provides a comprehensive overview of the Segment query language.\n\nSegment's query language in private beta\n\nSegment\u2019s query language is in private beta, and Segment is actively working on this feature. Some functionality may change before it becomes generally available.\n\nOverview\n\nAudience definitions specify the criteria for identifying users or accounts as members of a particular audience, while computed trait definitions outline the logic for aggregating or calculating values stored as traits on user or account level profiles.\n\nWith Segment\u2019s query language, you can create these definitions and use them with Segment APIs to generate audiences and computed traits.\n\nAvailable functions and operators\n\nThis section outlines the functions and operators you can use with the query language.\n\nSyntax\n\nFollow these syntax rules when you create definitions:\n\nAll definitions consist of expressions connected by optional junctions.\nExpressions are composed of chained functions, starting with an extractor and ending with a result.\n. serves as the delimiter when chaining functions.\nAudience definitions must return a boolean result (for example, a comparator), while computed trait definitions must return a scalar.\nFunctions have well-defined return types that determine the permissible functions in the call chain.\nWhen you use junctions, AND holds precedence over OR, but parentheses offer control over expression combination.\nEach definition allows a maximum of 50 primary expressions.\nSyntactic sugar\n\nThe language supports the following syntactic sugar adjustments:\n\nThe language automatically wraps a \u2018literal\u2019 extractor function around string or number inputs wherever a scalar expression expects them.\nYou can invoke the boolean comparator functions equals, differs, greater_than, at_least, less_than, and at_most by omitting the period and parenthesis and replacing the function name with the equivalent symbols =, !=, >, >=, <, and <=. Regardless of the syntactic sugar, the comparison still dictates the operations allowed in the call-chain.\nDefinition type\n\nThe definition type (USERS or ACCOUNTS) determines whether the computation operates at the user or account level. For account-level audiences, you can apply additional functions ANY (to verify that all underlying users meet the defined conditions) and ALL (to check if any of the underlying users meet the defined conditions).\n\nThese functions use the association between accounts and users to determine audience membership.\n\nFunctions\n\nThe following tables list the query languages\u2019s available functions.\n\nExtractors\nEVENT\t\u00a0\nSyntax\tevent({s: String})\ns - the name of the event to build an extractor for\nReturn Type\tVectorExtractor\nExample\tevent('Shoes Bought')\nTRAIT\t\u00a0\nSyntax\ttrait({s: String})\ns - the name of the the trait to reference\nReturn Type\tScalarExtractor\nDescription\tSimilar to the event operator, the trait operator is used to specify profile trait filter criteria.\nNotes\tYou can reference other audiences by using the audience key as the trait name.\nExample\ttrait('total_spend')\nPROPERTY\t\u00a0\nSyntax\tproperty({s: String})\ns - the name of the property to build an extractor for\nIn the context of funnel audiences, you can add a parent prefix to reference the parent event.\nproperty(parent: {s: String})\nReturn Type\tScalarExtractor\nNotes\tOnly valid within a where function or a Reducer.\nExample\tproperty('total')\nCONTEXT\t\u00a0\nSyntax\tcontext({s: String})\ns - the name of the context to build an extractor for\nReturn Type\tScalarExtractor\nNotes\tOnly valid within a where function or a Reducer.\nExample\tcontext('page.url')\nLITERAL\t\u00a0\nSyntax\tliteral({a: Any})\na - the value to treat as a literal expression\nOperations allowed in call-chain\tNone allowed; typically used within another function, like a comparison (with syntactic sugar, this would appear on the right side of the comparison). The outer function or comparison dictates the operations allowed in the call-chain.\nExample\tliteral(100)\n\nFilters\nWHERE\t\u00a0\nSyntax\twhere({e: Comparator})\ne - a subexpression terminating in a boolean Comparator\nReturn Type\tStreamFilter\nDescription\tFilters the stream to only items where a property satisfies a particular condition.\nNotes\tThe parameter is a sub-expression, something that terminates in a boolean Comparator.\nExample\twhere({property('price_usd') > 100})\nSOURCES\t\u00a0\nSyntax\tsources({exclude: {a: Array}})\na - an array of source ids to exclude\nReturn Type\tStreamFilter\nDescription\tFilters the stream to only items whose source id does not match the exclusion list.\nExample\tsources({exclude: 'QgRHeujRJBM9j18yChyC', '/;hSBZDqGDPvXCKHbikPm'})\nWITHIN\t\u00a0\nSyntax\twithin({d: Integer} {u: TimeUnit})\nd - duration value\nu - hour (s) day (s)\nIn the context of funnel audiences, you can add a parent prefix to reference the parent event.\nwithin(parent: {d: Integer} {u: TimeUnit})\nReturn Type\tWindowedFilter\nDescription\tProvides time windowing so that events are only looked at over a specified number of hours or days into the past. You can add a prefix to direct the evaluation to be relative to the timestamp of a different event.\nExample\twithin(7 days)\nBETWEEN\t\u00a0\nSyntax\tbetween({s: Integer} {su: TimeUnit}, {e: Integer} {eu: TimeUnit})\ns - start value\nsu - hour (s) day (s)\ne - end value\neu - hour (s) day (s)\nReturn Type\tWindowedFilter\nDescription\tYou can add a prefix to direct the evaluation to be relative to the timestamp of a different event.\nExample\tbetween(7 days, 10 days)\nReducers\nCOUNT\t\u00a0\nSyntax\tcount()\nReturn Type\tScalar\nDescription\tCounts the number of entries in a stream and returns the result.\nExample\tcount()\nSUM\t\u00a0\nSyntax\tsum({s: EventPropertyExtractor})\ns - property to sum\nReturn Type\tScalar\nExample\tsum(property('spend'))\nAVG\t\u00a0\nSyntax\tavg({s: EventPropertyExtractor})\ns - property to average\nReturn Type\tScalar\nExample\tavg(property('spend'))\nMAX\t\u00a0\nSyntax\tmax({s: EventPropertyExtractor}) or max({s: EventPropertyExtractor} as type)\ns - property to get the maximum value of\ntype - number, string\nReturn Type\tScalar\nNotes\tIf no type is passed, Segment assumes number as the type and selects the greatest value. You can override the behavior to select the max based on lexicographical ordering by specifying as string.\nExample\tmax(property('spend'))\nmax(property('spend') as string)\nMIN\t\u00a0\nSyntax\tmin({s: EventPropertyExtractor}) or min({s: EventPropertyExtractor} as type)\ns - property to get the minimum value of\ntype - number, string\nReturn Type\tScalar\n\u00a0\t\u00a0\nNotes\tIf no type is passed, Segment assumes number as the type and selects the smallest value. You can override the behavior to select the max based on lexicographical ordering by specifying as string.\nExample\tmin(property('spend'))\nmin(property('spend') as string)\nMODE\t\u00a0\nSyntax\tmode({s: EventPropertyExtractor}, {d: Integer}) or mode({s: EventPropertyExtractor} as type, {d: Integer})\ns - the property to find the most frequent value of\nd - minimum frequency expected\ntype - number, string, array\nReturn Type\tScalar\nDescription\tFind the most frequent value for a given property name.\nNotes\tIf no type is passed, Segment assumes string as the type and selects the most frequent value assuming all data is a string. number will behave the same as string. array will also behave the same way, except when used in combination with the $ operator where instead of treating each individual value within the array separately Segment will instead treat the whole array as a string.\nExample\tmode(property('spend'), 2)\nmode(property('spend') as array, 2)\nFIRST\t\u00a0\nSyntax\tfirst({s: EventPropertyExtractor})\ns - the property to find the first value of\nReturn Type\tScalar\nDescription\tFind the first value for the given property name within the stream of filterable data extracted.\nExample\tfirst(property('spend'))\nLAST\t\u00a0\nSyntax\tlast({s: EventPropertyExtractor})\ns - the property to find the last value of\nReturn Type\tScalar\nDescription\tFind the last value for the given property name within the stream of filterable data extracted.\nExample\tlast(property('spend'))\nUNIQUE\t\u00a0\nSyntax\tunique({s: EventPropertyExtractor})\ns - property to get the unique values of\nReturn Type\tListScalar\nDescription\tGenerate a unique list of values for the given property name.\nExample\tunique(property('spend'))\nComparisons\nEQUALS\t\u00a0\nSyntax\tequals({v: Scalar})\nv - value to compare for equality\nReturn Type\tComparator\nExample\tequals(500)\nSyntactic Sugar: == 500\nDIFFERS\t\u00a0\nSyntax\tdiffers({v: Scalar})\nv - value to compare for inequality\nReturn Type\tComparator\nNotes\t\u2018differs\u2019 only returns true if the value exists and is not equal. If null values need to be considered then use \u2018NOT (expression) = (value)\u2019 or add a condition to check for nulls \u2018(expression) != (value) OR (expression).absent()\u2019.\nExample\tdiffers(500)\nSyntactic Sugar: != 500\nABSENT\t\u00a0\nSyntax\tabsent()\nReturn Type\tComparator\nDescription\tReturns true when a value is null. Equivalent to NOT (expression).exists().\nExample\tabsent()\nEXISTS\t\u00a0\nSyntax\texists()\nReturn Type\tComparator\nDescription\tReturns true when a value is set, meaning not null. Equivalent to NOT (expression).absent().\nExample\texists()\nGREATER_THAN\t\u00a0\nSyntax\tgreater_than({n: Scalar})\nn - value to compare\nReturn Type\tComparator\nExample\tgreater_than(500)\nSyntactic Sugar: > 500\nAT_LEAST\t\u00a0\nSyntax\tat_least({n: Scalar})\nn - value to compare\nReturn Type\tComparator\nExample\tat_least(500)\nSyntactic Sugar: >= 500\nLESS_THAN\t\u00a0\nSyntax\tless_than({n: Scalar})\nn - value to compare\nReturn Type\tComparator\nExample\tless_than(500)\nSyntactic Sugar: < 500\nAT_MOST\t\u00a0\nSyntax\tat_most({n: Scalar})\nn - value to compare\nReturn Type\tComparator\nExample\tat_most(500)\nSyntactic Sugar: <= 500\nCONTAINS\t\u00a0\nSyntax\tcontains({a: Array})\na - array of possible values\nReturn Type\tComparator\nDescription\tMatches when the value contains one of the elements of the parameter array as a substring.\nExample\tcontains('shoes','shirts')\nOMITS\t\u00a0\nSyntax\tomits({s: String})\ns - string to search for if missing in a containing string\nReturn Type\tComparator\nDescription\tEvaluates to true when a substring isn\u2019t present in a containing string, equivalent to NOT (expression).contains(<argument>).\nExample\tomits('shoes')\nSTARTS_WITH\t\u00a0\nSyntax\tstarts_with({s: String})\ns - string to search for at start of containing string\nReturn Type\tComparator\nExample\tstarts_with('total')\nENDS_WITH\t\u00a0\nSyntax\tends_with({s: String})\ns - string to search for at end of containing string\nReturn Type\tComparator\nExample\tends_with('total')\nONE_OF\t\u00a0\nSyntax\tone_of({a: Array})\na - array of possible values\nReturn Type\tComparator\nDescription\tMatches when the value exactly matches one of the values from the parameter array.\nExample\tone_of('shoes','shirts')\nBEFORE_DATE\t\u00a0\nSyntax\tbefore_date({t: Timestamp})\nt - ISO 8601 timestamp\nReturn Type\tComparator\nExample\tbefore_date('2023-12-07T18:50:00Z')\nAFTER_DATE\t\u00a0\nSyntax\tafter_date({t: Timestamp})\nt - ISO 8601 timestamp\nReturn Type\tComparator\nExample\tafter_date('2023-12-07T18:50:00Z')\nWITHIN_LAST\t\u00a0\nSyntax\twithin_last({d: Integer} {u: TimeUnit})\nd - duration value\nu - hour(s), day(s)\nReturn Type\tComparator\nDescription\tRepresents the date range between today and the past d days - inclusive where today represents the current date at the time Segment determines audience membership or calculates the trait.\nExample\twithin_last(7 days)\nWITHIN_NEXT\t\u00a0\nSyntax\twithin_next({d: Integer} {u: TimeUnit})\nd - duration value\nu - hour(s), day(s)\nReturn Type\tComparator\nDescription\tRepresents the date range between today and the next d days - inclusive where today represents the current date at the time Segment determines audience membership or calculates the trait.\nExample\twithin_next(7 days)\nBEFORE_LAST\t\u00a0\nSyntax\tbefore_last({d: Integer} {u: TimeUnit})\nd - duration value\nu - hour(s), day(s)\nReturn Type\tComparator\nDescription\tRepresents the date range between today - d days and any past date prior to that - inclusive where today represents the current date at the time Segment determines audience membership or calculates the trait.\nExample\tbefore_last(7 days)\nAFTER_NEXT\t\u00a0\nSyntax\tafter_next({d: Integer} {u: TimeUnit})\nd - duration value\nu - hour(s), day(s)\nReturn Type\tComparator\nDescription\tRepresents the date range between today + d days and any future date - inclusive where today represents the current date at the time Segment determines audience membership or calculates the trait.\nExample\tafter_next(7 days)\nJunctions\nAND\t\u00a0\nSyntax\t{Comparator} AND {Comparator}\nBase Type\tJunction\nReturn Type\tComparator\nDescription\tTrue only if both subexpressions evaluate to true.\nOR\t\u00a0\nSyntax\t{Comparator} OR {Comparator}\nBase Type\tJunction\nReturn Type\tComparator\nDescription\tTrue if either subexpression evaluates to true.\nNOT\t\u00a0\nSyntax\tNOT ({Comparator})\nBase Type\tJunction\nReturn Type\tComparator\nDescription\tTrue only if the subexpression evaluates to false.\nANY\t\u00a0\nSyntax\tANY ({Comparator})\nBase Type\tJunction\nReturn Type\tComparator\nDescription\tUsed to evaluate an aggregatable boolean expression to determine if any expression is true. Used to specify account-level audience queries that aggregate across user-level queries.\nALL\t\u00a0\nSyntax\tALL ({Comparator})\nBase Type\tJunction\nReturn Type\tComparator\nNotes\tUsed to evaluate an aggregatable boolean expression to determine if every expression is true. Used to specify account-level audience queries that aggregate across user-level queries.\nReturn Type\nEXTRACTOR\t\u00a0\nOperations\tNone included\nVECTOREXTRACTOR (EXTENDS EXTRACTOR, STREAMFILTER)\t\u00a0\nBase Type\tExtractor, StreamFilter\nOperations allowed in call-chain\twhere, sources, within, between, count, sum, avg, max, min, mode, first, last, unique (inherited from StreamFilter)\nNotes\tA VectorExtractor represents extractions of data sets that need to be filtered and reduced to a scalar. Adds isVector property to entire expression.\nSCALAREXTRACTOR (EXTENDS EXTRACTOR, SCALAR)\t\u00a0\nBase Type\tExtractor, Scalar\nOperations allowed in call-chain\tequals, differs, absent, exists, greater_than, at_least, less_than, at_most, contains, omits, starts_with, ends_with, one_of, before_date, after_date, within_last, before_last, after_next (inherited from Scalar)\nNotes\tA ScalarExtractor represents extractions of a single data element, like a field value or a trait value.\nEVENTPROPERTYEXTRACTOR (EXTENDS EXTRACTOR)\t\u00a0\nBase Type\tExtractor, Scalar\nOperations allowed in call-chain\tNone\nNotes\tUsed to refer to properties for comparison purposes.\nFILTER\t\u00a0\nOperations allowed in call-chain\tNone included\nSTREAMFILTER (EXTENDS FILTER)\t\u00a0\nBase Type\tFilter\nOperations allowed in call-chain\twhere, sources, within, between, count, sum, avg, max, min, mode, first, last, unique\nWINDOWEDFILTER (EXTENDS STREAMFILTER)\t\u00a0\nBase Type\tStreamFilter\nOperations allowed in call-chain\twhere, sources, within, between, count, sum, avg, max, min, mode, first, last, unique\nSCALAR\t\u00a0\nOperations allowed in call-chain\tequals, differs, absent, exists, greater_than, at_least, less_than, at_most, contains, omits, starts_with, ends_with, one_of, before_date, after_date, within_last, before_last, after_next, within_next\nLISTSCALAR\t\u00a0\nOperations allowed in call-chain\tcount\nCOMPARATOR\t\u00a0\nBase Type\tComparator\nOperations allowed in call-chain\tNone allowed; once an expression is terminated with a Comparator, it is completed.\nJUNCTION\t\u00a0\nBase Type\tJunction\nNotes\tPreserves any set properties set by subexpressions.\nExamples\nAudiences\n\nSuppose you wanted to collect all users who performed the Shoes Bought event at least once within the last seven days, where the purchase price was greater than or equal to 100.\n\nAnother way to think of this scenario would be:\n\nCollect all users who performed the Shoes Bought event.\nFilter down to only consider events with a price greater than or equal to 100.\nFilter for events that occurred within the last seven days.\nOnly include users who have one or more of the previous events.\n\nHere\u2019s how you could do that in Segment\u2019s query language:\n\nevent('Shoes Bought').where( property('price') >= 100 ).within(7 days).count() >= 1\n\nBought and returned\n\nThis example collects:\n\nall users who performed the Shoes Bought event at least once within the last 30 days\nwhere the price was greater than or equal to the average spend\nand the user performed the Shoes Returned event at least once, five days after the Shoes Bought event\nevent('Shoes Bought').where( \nproperty('price') >= trait('avg_spend')\nAND \nevent('Shoes Returned').within(parent: 5 days).count() >= 1 \n).within(30 days).count() >= 1\n\nDid not perform Shoes Bought\n\nThis example collects all users who did not perform the Shoes Bought event at least once and don\u2019t have a total_spend trait with a value greater than 200:\n\nNOT ( event('Shoes Bought').count() >= 1 AND trait('total_spend') > 200 )\n\nBought with minimum total spend\n\nThis example collects all accounts where all associated users performed the Shoes Bought event at least once and have a total_spend trait greater than 200:\n\nALL ( event('Shoes Bought').count() >= 1 AND trait('total_spend') > 200 )\n\nNo users bought at least once\n\nThis example collects all accounts where no associated users performed the Shoes Bought event at least once:\n\nALL NOT event('Shoes Bought').count() >= 1\n\nAny users bought at least once\n\nThis example collects all accounts where any associated users performed the Shoes Bought event at least once:\n\nANY event('Shoes Bought').count() >= 1\n\nComputed Traits\n\nSuppose you wanted to calculate the average spend based on all Shoes Bought events performed within the last 30 days for each user.\n\nAnother way to think of this would be:\n\nFind all Shoes Bought events.\nFilter down to only consider events that occurred within the last 30 days.\nFor these events, calculate the average spend for each user.\n\nHere\u2019s how you could do that in Segment\u2019s query language:\n\nevent('Shoes Bought').within(30 days).avg(property('spend'))\n\nCalculate minimum spend\n\nThis example calculates the minimum spend for each user, based on all Shoes Bought events, where the price was greater than 100 and the brand was My_Brand:\n\nevent('Shoes Bought').where( property('price') > 100 AND property('brand') = 'My Brand' ).min(property('spend'))\n\nCalculate first seen spend\n\nThis example calculates the first-seen spend value for each user, based on all Shoes Bought events performed within the last 30 days:\n\nevent('Shoes Bought').within(30 days).first(property('spend'))\n\nMost frequent spend value\n\nThis example calculates the most frequent spend value for each user, based on all Shoes Bought events performed within the last 30 days. It only considers spend values that have a minimum frequency of 2:\n\nevent('Shoes Bought').within(30 days).mode(property('spend'), 2)\n\n\nThis page was last modified: 04 Jun 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nOverview\nAvailable functions and operators\nFunctions\nReturn Type\nExamples\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSpec\n/\nSpec: Track\nSpec: Track\n\nThe Track API call is how you record any actions your users perform, along with any properties that describe the action.\n\nSegment University: The Track Method\n\nCheck out our high-level overview of the Track method in Segment University. (Must be logged in to access.)\n\nEach action is known as an event. Each event has a name, like User Registered, and properties. For example, a User Registered event might have properties like plan or accountType. Calling Track in one of our sources is one of the first steps to getting started with Segment.\n\nHere\u2019s the payload of a typical Track call with most common fields removed:\n\n{\n  \"type\": \"track\",\n  \"event\": \"User Registered\",\n  \"properties\": {\n    \"plan\": \"Pro Annual\",\n    \"accountType\" : \"Facebook\"\n  }\n}\n\n\nAnd here\u2019s the corresponding JavaScript event that would generate the above payload:\n\nanalytics.track(\"User Registered\", {\n  plan: \"Pro Annual\",\n  accountType: \"Facebook\"\n});\n\n\nBased on the library you use, the syntax in the examples might be different. You can find library-specific documentation on the Sources Overview page.\n\nBeyond the common fields, the Track call has the following fields:\n\nFIELD\t\tTYPE\tDESCRIPTION\nevent\trequired\tString\tName of the action that a user has performed. See the Event field docs for more details.\nproperties\toptional\tObject\tFree-form dictionary of properties of the event, like revenue See the Properties docs for a list of reserved property names.\nExample\n\nHere\u2019s a complete example of a Track call:\n\n{\n  \"anonymousId\": \"23adfd82-aa0f-45a7-a756-24f2a7a4c895\",\n  \"context\": {\n    \"library\": {\n      \"name\": \"analytics.js\",\n      \"version\": \"2.11.1\"\n    },\n    \"page\": {\n      \"path\": \"/academy/\",\n      \"referrer\": \"\",\n      \"search\": \"\",\n      \"title\": \"Analytics Academy\",\n      \"url\": \"https://segment.com/academy/\"\n    },\n    \"userAgent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.86 Safari/537.36\",\n    \"ip\": \"108.0.78.21\"\n  },\n  \"event\": \"Course Clicked\",\n  \"integrations\": {},\n  \"messageId\": \"ajs-f8ca1e4de5024d9430b3928bd8ac6b96\",\n  \"properties\": {\n    \"title\": \"Intro to Analytics\"\n  },\n  \"receivedAt\": \"2015-12-12T19:11:01.266Z\",\n  \"sentAt\": \"2015-12-12T19:11:01.169Z\",\n  \"timestamp\": \"2015-12-12T19:11:01.249Z\",\n  \"type\": \"track\",\n  \"userId\": \"AiUGstSDIg\",\n  \"originalTimestamp\": \"2015-12-12T19:11:01.152Z\"\n}\n\nCreate your own Track call\n\nUse the following interactive code pen to see what your Track calls look like with user-provided information:\n\nSample Track call\nEvent:\nTitle:\nIntro to Analytics\nProtocols 101\nReverse ETL\nSample Track Call\nSample output goes here!\n\nIdentities\n\nThe User ID is a unique identifier for the user performing the actions. Check out the User ID docs for more detail.\n\nThe Anonymous ID can be any pseudo-unique identifier, for cases where you don\u2019t know who the user is, but you still want to tie them to an event. Check out the Anonymous ID docs for more detail.\n\nNote: In our browser and mobile libraries a User ID is automatically added from the state stored by a previous identify call, so you do not need to add it yourself. They will also automatically handle Anonymous IDs under the covers.\n\nEvent\n\nEvery Track call records a single user action. Segment calls these \u201cevents\u201d, and recommend that you make your event names human-readable, so that everyone on your team (even you, after all that caffeine) can know what they mean instantly.\n\nDon\u2019t use nondescript names like Event 12 or TMDropd. Instead, use unique but recognizable names like Video Recorded and Order Completed.\n\nSegment recommends event names built from a noun and past-tense verb. For more information about best practices in event naming, check out Segment\u2019s Analytics Academy lesson on best practices for naming conventions for clean data.\n\nSegment has standardized a series of reserved event names that have special semantic meaning. We map these events to tools that support them whenever possible. See the Semantic Events docs for more detail.\n\nProperties\n\nProperties are extra pieces of information you can tie to events you track. They can be anything that will be useful while analyzing the events later. Segment recommends sending properties whenever possible because they give you a more complete picture of what your users are doing.\n\nSegment has reserved some properties that have semantic meanings, and handle them in special ways. For example, we always expect revenue to be a dollar amount that we send to tools that handle revenue tracking.\n\nYou should only use reserved properties for their intended meaning.\n\nThe following are all of the reserved properties Segment has standardized that apply to all events. Check out the Semantic Events docs for properties specific to individual reserved events.\n\nPROPERTY\tTYPE\tDESCRIPTION\nrevenue\tNumber\tAmount of revenue an event resulted in. This should be a decimal value, so a shirt worth $19.99 would result in a revenue of 19.99.\ncurrency\tString\tCurrency of the revenue an event resulted in. This should be sent in the ISO 4127 format. If this isn\u2019t set, Segment assumes the revenue to be in US dollars.\nvalue\tNumber\tAn abstract \u201cvalue\u201d to associate with an event. This is typically used in situations where the event doesn\u2019t generate real-dollar revenue, but has an intrinsic value to a marketing team, like newsletter signups.\n\nNote: You might be used to some destinations recognizing special properties differently. For example, Mixpanel has a special track_charges method for accepting revenue. Luckily, you don\u2019t have to worry about those inconsistencies. Just pass along revenue. Segment will handle all of the destination-specific conversions for you automatically. Same goes for the rest of the reserved properties.\n\nSending Traits in a Track Call - Destination Actions\n\nAll events have the ability to include additional event data in the context object. There may be instances when your team may want to include user traits or group traits in a Track event, such as having a single event trigger multiple events in an Actions destination. Since user Traits are not standard fields for a Track event, in order to do this, you\u2019ll need to explicitely pass the user\u2019s traits into the event payload\u2019s context.traits object. For instructions on how to pass fields to the context object for a specific library, please see the related library\u2019s Source documentation.\n\nSegment\u2019s Actions destinations allows your team to build individual actions that are triggered based on a set of configured conditions. By adding the user\u2019s latest traits to the Track event\u2019s context.traits object, its possible to build two separate Actions to be triggered by this single event. For example, if your team would like to send an Identify event anytime the specific Track event \u201cButton Clicked\u201d is triggered, simply add the available traits into the Track event\u2019s payload, then build a destination Actions for the Track event : Event Name is Button Clicked, and a destination Action for the Identify event : All of the following conditions are true: Event Name is Button Clicked, Event Context traits exists, and then both Actions will have access to reference the context.traits fields within its mappings.\n\nFor more information on the context object, please see the Spec: Common Fields documentation.\n\nFIELD\t\tTYPE\tDESCRIPTION\ncontext\toptional\tObject\tDictionary of extra information that provides useful context about a message, but is not directly related to the API call like ip address or locale See the Context field docs for more details.\n\nIf the Example Payload shared above is modified as the event Button Clicked with \"username\": \"testing-123\" in the context.traits object, then the event\u2019s payload would be :\n\n{\n  \"anonymousId\": \"23adfd82-aa0f-45a7-a756-24f2a7a4c895\",\n  \"context\": {\n    \"library\": {\n      \"name\": \"analytics.js\",\n      \"version\": \"2.11.1\"\n    },\n    \"page\": {\n      \"path\": \"/academy/\",\n      \"referrer\": \"\",\n      \"search\": \"\",\n      \"title\": \"Analytics Academy\",\n      \"url\": \"https://segment.com/academy/\"\n    },\n    \"userAgent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.86 Safari/537.36\",\n    \"ip\": \"108.0.78.21\",\n    \"traits\": {\n      \"username\": \"testing-123\"\n    }\n  },\n  \"event\": \"Button Clicked\",\n  \"integrations\": {},\n  \"messageId\": \"ajs-f8ca1e4de5024d9430b3928bd8ac6b96\",\n  \"properties\": {\n    \"title\": \"Intro to Analytics\"\n  },\n  \"receivedAt\": \"2015-12-12T19:11:01.266Z\",\n  \"sentAt\": \"2015-12-12T19:11:01.169Z\",\n  \"timestamp\": \"2015-12-12T19:11:01.249Z\",\n  \"type\": \"track\",\n  \"userId\": \"AiUGstSDIg\",\n  \"originalTimestamp\": \"2015-12-12T19:11:01.152Z\"\n}\n\n\nHere\u2019s what that Identify Action would look like :\n\nContext\n\nContext is a dictionary of extra information that provides useful context about a datapoint, for example the user\u2019s ip address or locale. You should only use Context fields for their intended meaning.\n\nFIELD\tTYPE\tDESCRIPTION\nactive\tBoolean\tWhether a user is active.\n\nThis is usually used to flag an .identify() call to just update the traits but not \u201clast seen.\u201d\napp\tObject\tdictionary of information about the current application, containing name, version, and build.\n\nThis is collected automatically from the mobile libraries when possible.\ncampaign\tObject\tDictionary of information about the campaign that resulted in the API call, containing name, source, medium, term, content, and any other custom UTM parameter.\n\nThis maps directly to the common UTM campaign parameters.\ndevice\tObject\tDictionary of information about the device, containing id, advertisingId, manufacturer, model, name, type, and version.\nip\tString\tCurrent user\u2019s IP address.\nlibrary\tObject\tDictionary of information about the library making the requests to the API, containing name and version.\nlocale\tString\tLocale string for the current user, for example en-US.\nnetwork\tObject\tDictionary of information about the current network connection, containing bluetooth, carrier, cellular, and wifi. If the context.network.cellular and context.network.wifi fields are empty, then the user is offline.\nos\tObject\tDictionary of information about the operating system, containing name and version.\npage\tObject\tDictionary of information about the current page in the browser, containing path, referrer, search, title and url. This is automatically collected by Analytics.js.\nreferrer\tObject\tDictionary of information about the way the user was referred to the website or app, containing type, name, url, and link.\nscreen\tObject\tDictionary of information about the device\u2019s screen, containing density, height, and width.\ntimezone\tString\tTimezones are sent as tzdata strings to add user timezone information which might be stripped from the timestamp, for example America/New_York.\ngroupId\tString\tGroup / Account ID.\n\nThis is useful in B2B use cases where you need to attribute your non-group calls to a company or account. It is relied on by several Customer Success and CRM tools.\ntraits\tObject\tDictionary of traits of the current user.\n\nThis is useful in cases where you need to track an event, but also associate information from a previous Identify call. You should fill this object the same way you would fill traits in an identify call.\nuserAgent\tString\tUser agent of the device making the request.\nuserAgentData\tObject\tThe user agent data of the device making the request. This always contains brands, mobile, platform, and may contain bitness, model, platformVersion,uaFullVersion, fullVersionList, wow64, if requested and available.\n\nThis populates if the Client Hints API is available on the browser.\n\nThis may contain more information than is available in the userAgent in some cases.\nchannel\tString\twhere the request originated from: server, browser or mobile\n\nThis page was last modified: 12 Apr 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nExample\nIdentities\nEvent\nProperties\nSending Traits in a Track Call - Destination Actions\nContext\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nProfiles Sync\n/\nSet up Profiles Sync\nSet up Profiles Sync\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\n\nOn this page, you\u2019ll learn how to set up Profiles Sync, enable historical backfill, and adjust settings for warehouses that you\u2019ve connected to Profiles Sync.\n\nInitially Setting up Profiles Sync\n\nIdentity Resolution setup\n\nTo use Profiles Sync, you must first set up Identity Resolution.\n\nTo set up Profiles Sync, first create a warehouse, then connect the warehouse within the Segment app.\n\nBefore you begin, prepare for setup with these tips:\n\nTo connect your warehouse to Segment, you must have read and write permissions with the warehouse Destination you choose.\nDuring step 2, you\u2019ll copy credentials between Segment and your warehouse destination. To streamline setup, open your Segment workspace in one browser tab and open another with your warehouse account.\nMake sure to copy any IP addresses Segment asks you to allowlist in your warehouse destination.\nStep 1: Select a warehouse\n\nYou\u2019ll first choose the destination warehouse to which Segment will sync profiles. Profiles Sync supports the Snowflake, Redshift, BigQuery, Azure, Postgres, and Databricks warehouse Destinations. Your initial setup will depend on the warehouse you choose.\n\nThe following table shows the supported Profiles Sync warehouse destinations and the corresponding required steps for each. Select a warehouse, view its Segment documentation, then carry out the warehouse\u2019s required steps before moving to step 2 of Profiles Sync setup:\n\nWAREHOUSE DESTINATION\tREQUIRED STEPS\nSnowflake\tFollow the steps in Snowflake Getting Started.\nRedshift\tFollow the steps in Redshift Getting Started.\nBigQuery\tFollow the steps in BigQuery Getting Started.\nAzure\tFollow the steps in Azure Synapse Analytics Getting Started.\nPostgres\tFollow the steps in Postgres Getting Started.\nDatabricks\tFollow the steps in the Databricks Getting Started.\n\nAfter you\u2019ve finished the required steps for your chosen warehouse, you\u2019re ready to connect your warehouse to Segment. Because you\u2019ll next enter credentials from the warehouse you just created, leave the warehouse tab open to streamline setup.\n\nProfiles Sync permissions\n\nTo allow Segment to write to the warehouse you\u2019re using for Profiles Sync, you\u2019ll need to set up specific permissions.\n\nFor example, if you\u2019re using BigQuery, you must create a service account for Segment and assign the following roles:\n\nBigQuery Data Owner\nBigQuery Job User\n\nReview the required steps for each warehouse in the table above to see which permissions you\u2019ll need.\n\nProfiles Sync roles\n\nThe following Segment access roles apply to Profiles Sync:\n\nUnify and Engage read-only: Read-only access to Profiles Sync, including the sync history and configuration settings. With these roles assigned, you can\u2019t download PII or edit Profiles Sync settings.\n\nUnify read-only and Engage user: Read-only access to Profiles Sync, including the sync history and configuration settings. With these roles assigned, you can\u2019t download PII or edit Profiles Sync settings.\n\nUnify and Engage Admin access: Full edit access to Profiles Sync, including the sync history and configuration settings.\n\nStep 2: Connect the warehouse and enable Profiles Sync\n\nAfter selecting your warehouse, you can connect it to Segment.\n\nDuring this step, you\u2019ll copy credentials from the warehouse you just set up and enter them into the Segment app. The specific credentials you\u2019ll enter depend on the warehouse you chose during step 1.\n\nSegment may also display IP addresses you\u2019ll need to allowlist in your warehouse. Make sure to copy the IP addresses and enter them into your warehouse account.\n\nTo connect your warehouse:\n\nConfigure your database.\nBe sure to log in with a user who has read and write permissions so that Segment can write to your database.\nSegment shows an IP address to allowlist. Copy it to your warehouse destination.\nEnter a schema name to help you identify this space in the warehouse, or use the default name provided.\nThe schema name can\u2019t be changed after the warehouse is connected.\nEnter your warehouse credentials, then select Test Connection.\nIf the connection test succeeds, Segment enables the Next button. Select it.\nIf the connection test fails, verify that you\u2019ve correctly entered the warehouse credentials, then try again.\nStep 3: Set up Selective Sync\n\nSet up Selective Sync to control the exact tables and columns that Segment will sync to your connected data warehouse.\n\nData will be backfilled to your warehouse based on the last two months of history.\n\nYou can sync the following tables:\n\nTYPE\tTABLES\tBACKFILL\nProfile raw tables\t- external_id_mapping_updates\n- id_graph_updates\n- profile_traits_updates\tComplete\nProfile materialized tables\t- user_identifier\n- user_traits\n- profile_merges\tComplete\nEvent type tables\t- Identify\n- Page\n- Group\n- Screen\n- Alias\n- Track\t2 months\nTrack event tables\tTo view and select individual track tables, don\u2019t sync track tables during the initial setup. Edit your sync settings after enabling Profiles Sync and waiting for the first sync to complete.\t2 months\nUsing Selective Sync\n\nUse Selective Sync to manage the data you send to your warehouses by choosing which tables and columns (also known as properties) to sync. Syncing fewer tables and properties will lead to faster and more frequent syncs, faster queries, and using less disk space.\n\nYou can access Selective Sync in two ways:\n\nFrom the Set Selective Sync page as you connect your warehouse to Profiles Sync.\nFrom the Profiles Sync settings (Profiles Sync > Settings > Selective sync).\n\nYou\u2019ll see a list of event type tables, event tables, and tables Segment materializes available to sync. Select the tables and properties that you\u2019d like to sync, and be sure the ones you\u2019d like to prevent from syncing aren\u2019t selected.\n\nRegardless of schema size, only the first 5,000 collections and 5,000 properties per collection can be managed using your Segment space. To edit Selective Sync settings for any collection which exceeds this limit, contact Segment support.\n\nYou must be a workspace owner to change Selective Sync settings.\n\nWhen to use Selective Sync\n\nUse Selective Sync when you want to prevent specific tables and properties from syncing to your warehouse. Segment stops syncing from disabled tables or properties, but will not delete any historical data from your warehouse.\n\nIf you choose to re-enable a table or property to sync again, only new data generated will sync to your warehouse. Segment doesn\u2019t backfill data that was omitted with Selective Sync.\n\nUsing historical backfill\n\nProfiles Sync sends profiles to your warehouse hourly once setup completes. Setup is complete after an initial automated backfill syncs all profile data. To initiate the backfill, the Profiles Sync requires live data flowing into your workspace. If live data isn\u2019t available, you can send test data to trigger the backfill sooner. Backfill can also sync historical profiles to your warehouse.\n\nYou can only use historical backfill for tables that you enable with Selective Sync during setup. Segment does not backfill tables that you disable with Selective Sync.\n\nWhen Segment runs historical backfills:\n\nProfile raw and materialized tables sync your entire historical data to your warehouse.\nProfiles Sync gathers the last two months of all events for Event type and Track event tables and syncs them to your warehouse.\n\nSegment lands the data on an internal staging location, then removes the backfill banner. Segment then syncs the backfill data to your warehouse.\n\nReach out to Segment support if your use case exceeds the scope of the initial setup backfill.\n\nWhile historical backfill is running, you can start building materialized views and running sample queries.\n\nStep 4 (Optional): Materialize key views using a SQL automation tool\n\nDuring setup, you have the option of setting up materialized key views in one of two ways:\n\nYou can choose to materialize views on your own by using profiles raw tables. You may want to materialize your own tables if, for example, you want to transform additional data or join Segment profile data with external data before materialization.\n\nYou can choose to use Segment\u2019s open source dbt models by using profiles materialized tables.\n\nYou can alternatively use tables that Segment materializes and syncs to your data warehouse.\n\nTo start seeing unified profiles in your warehouse and build attribution models, you\u2019ll need to materialize the tables that Profiles Sync lands into three key views:\n\nid_graph: the current state of relationships between segment ids\nexternal_id_mapping: the current-state mapping between each external identifier you\u2019ve observed and its corresponding, fully-merged canonical_segment_id\nprofile_traits: the last seen value for all custom traits, computed traits, SQL traits, audiences, and journeys associated with a profile in a single row\n\nSee Tables you materialize for more on how to materialize these views either on your own, or with Segment\u2019s open source dbt models.\n\nNote that dbt models are in beta and need modifications to run efficiently on BigQuery, Synapse, and Postgres warehouses. Segment is actively working on this feature.\n\nProfiles Sync limits\n\nAs you use Profiles Sync, keep the following limits in mind:\n\nFor event tables, Segment can only backfill up to 2,000 tables for each workspace.\nSegment can only initiate backfills after a successful sync with > 0 rows.\nFor every sync, the total dataset Segment can sync is limited to 20TB.\nWorking with synced warehouses\nMonitor Profiles Sync\n\nYou can view warehouse sync information in the overview section of the Profiles Sync page. Segment displays the dates and times of the last and next syncs, as well as your sync frequency.\n\nIn the Syncs table, you\u2019ll find reports on individual syncs. Segment lists your most recent syncs first. The following table shows the information Segment tracks for each sync:\n\nDATA TYPE\tDEFINITION\nSync status\t- Success, which indicates that all rows synced correctly\n- Partial success, indicating that some rows synced correctly\n- Failed, indicating that no rows synced correctly\nDuration\tLength of sync time, in minutes\nStart time\tThe date and time when the sync began\nSynced rows\tThe number of rows synced to the warehouse\n\nSelecting a row from the Syncs table opens a pane that contains granular sync information. In this view, you\u2019ll see the sync\u2019s status, duration, and start time. Segment also displays a nuanced breakdown of the total rows synced, sorting them into identity graph tables, event type tables, and event tables.\n\nIf the sync failed, Segment shows any available error messages in the sync report.\n\nSettings and maintenance\n\nThe Settings tab of the Profiles Sync page contains tools that can help you monitor and maintain your synced warehouse.\n\nDisable or delete a warehouse\n\nIn the Basic settings tab, you can disable warehouse syncs or delete your connected warehouse altogether.\n\nTo disable syncs, toggle Sync status to off. Segment retains your warehouse credentials but stops further syncs. Toggle Sync status back on at any point to continue syncs.\n\nTo delete your warehouse, toggle Sync status to off, then select Delete warehouse. Segment doesn\u2019t retain credentials for deleted warehouses; to reconnect a deleted warehouse, you must set it up as a new warehouse.\n\nConnection settings\n\nIn the Connection settings tab, you can verify your synced warehouse\u2019s credentials and view IP addresses you\u2019ll need to allowlist so that Segment can successfully sync profiles.\n\nIf you have write access, you can verify that your warehouse is successfully connected to Segment by entering your password and then selecting Test Connection.\n\nChanging your synced warehouse\n\nIf you\u2019d like to change the warehouse connected to Profiles Sync, reach out to Segment support.\n\nSync schedule\n\nSegment supports hourly syncs.\n\nThis page was last modified: 07 Nov 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nInitially Setting up Profiles Sync\nProfiles Sync limits\nWorking with synced warehouses\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nTraits\n/\nCustom Traits\nCustom Traits\n\nCustom traits are user or account traits collected from the Identify calls you send to Segment. For example, these could be demographics like age or gender, account-specific like plan, or even things like whether a user has seen a particular A/B test variation. From your sources, send custom traits as pieces of information that you know about a user in an Identify call.\n\nAs opposed to computed traits which are computed from your source data, or SQL Traits which are computed from warehouse data, custom traits are created from source events you pass into Segment and have no trait limits.\n\nComparing trait types\n\nView the table below to better understand how Segment collects custom, computed, and SQL traits.\n\nYou can use the Profile explorer (Unify > Profile explorer) to view traits attached to a profile.\n\nTRAIT TYPE\tDESCRIPTION\nCustom traits\tTraits created from source events you pass into Segment. From your sources, send custom traits as pieces of information that you know about a user in an Identify call.\nComputed traits\tTraits collected from computations off of event and event property data from your sources. Create user or account-level calculations like most_viewed_page or total_num_orders for a customer. Learn more by viewing types of computed traits.\nSQL traits\tTraits created by running SQL queries on data in your warehouse. SQL traits are a type of computed trait. SQL traits help you import traits from your data warehouse back into Segment to build audiences or enhance data that you send to other destinations.\nUsing custom traits\n\nHere\u2019s the payload of a typical Identify call with custom traits (with most common fields removed):\n\n{\n  \"type\": \"identify\",\n  \"traits\": {\n    \"name\": \"John Smith\",\n    \"email\": \"john@example.com\",\n    \"plan\": \"premium\",\n    \"logins\": 5\n  },\n  \"userId\": \"97980cfea0067\"\n}\n\n\nAnd here\u2019s the corresponding JavaScript event that would generate the above payload:\n\nanalytics.identify(\"97980cfea0067\", {\n  name: \"John Smith\",\n  email: \"john@example.com\",\n  plan: \"premium\",\n  logins: 5\n});\n\n\nAny source event where there\u2019s a traits object and key value pairs generates custom traits.\n\nCustom traits are mutable and update to the latest value seen by the user\u2019s Identify events.\n\nWhen an audience that previously generated Identify events is deleted, the data for the audience key is still attached to profiles that entered the audience and becomes visible in Segment as a custom trait.\n\nReserved custom traits\n\nSegment has reserved some custom traits that have semantic meanings for users, and will handle them in special ways. For example, Segment always expects email to be a string of the user\u2019s email address. Segment sends this on to destinations like Mailchimp that require an email address for their tracking.\n\nOnly use reserved custom traits for their intended meaning.\n\nReserved custom traits Segment has standardized:\n\nTRAIT\tTYPE\tDESCRIPTION\naddress\tObject\tStreet address of a user optionally containing: city, country, postalCode, state, or street\nage\tNumber\tAge of a user\navatar\tString\tURL to an avatar image for the user\nbirthday\tDate\tUser\u2019s birthday\ncompany\tObject\tCompany the user represents, optionally containing: name (String), id (String or Number), industry (String), employee_count (Number) or plan (String)\ncreatedAt\tDate\tDate the user\u2019s account was first created. Segment recommends using ISO-8601 date strings.\ndescription\tString\tDescription of the user\nemail\tString\tEmail address of a user\nfirstName\tString\tFirst name of a user\ngender\tString\tGender of a user\nid\tString\tUnique ID in your database for a user\nlastName\tString\tLast name of a user\nname\tString\tFull name of a user. If you only pass a first and last name Segment automatically fills in the full name for you.\nphone\tString\tPhone number of a user\ntitle\tString\tTitle of a user, usually related to their position at a specific company. Example: \u201cVP of Engineering\u201d\nusername\tString\tUser\u2019s username. This should be unique to each user, like the usernames of Twitter or GitHub.\nwebsite\tString\tWebsite of a user\n\nTo learn more about using an Identify call to tie custom traits to profiles, visit Segment\u2019s Identify documentation.\n\nThis page was last modified: 12 Apr 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nComparing trait types\nUsing custom traits\nReserved custom traits\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nUsing the Profile Source Debugger\nUsing the Profile Source Debugger\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\n\nThe Profile Source Debugger enables you to inspect and monitor events that Segment sends downstream.\n\nBecause Segment generates a unique source for every destination connected to a Space, the Debugger gives you insight into how Segment sends events before they reach their destination. Even when a destination is removed, you can\u2019t delete and shouldn\u2019t disable this source for Segment to function as designed. The source will be reused by Segment as needed.\n\nThe Debugger provides you with the payload information you need to troubleshoot potential formatting issues and ensure Segment sends events as your destinations expect.\n\nModifying or disabling these system-generated sources could result in unforeseen issues and data loss.\n\nTurning off these sources might impede profile updates, while removing them could compromise the stability of the Engage space, potentially making it irrecoverable. To ensure the system\u2019s integrity, Segment highly recommends leaving these sources enabled.\n\nWorking with the Debugger\n\nNavigate to the Debugger tab on the Unify settings page of the space you want to debug. Select the source you want to inspect in the Debugger.\n\nThe Debugger presents a stream of incoming events. The event inspector displays three tabs for each event:\n\nPretty view shows the actual API call Segment sends to your destination.\nRaw view shows the full JSON object Segment sends to your destination from the calls you sent, including timestamps, properties, traits, and ids.\nViolations displays any violations triggered by the event.\n\nSimilar to the Connections Debugger, you can search through events using information contained within the event\u2019s payload.\n\nThis page was last modified: 22 Nov 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nWorking with the Debugger\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nUnify Limits\nUnify Limits\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY PLUS \u2713\n?\n\nBeginning November 6, 2024, new Unify Plus and Engage users can refer to this page for Segment\u2019s product limits. Existing users prior to this date can continue to refer to the Engage product limits in the Engage Default Limits documentation.\n\nTo provide consistent performance and reliability at scale, Segment enforces default use and rate limits within Unify. Most customers do not exceed these limits.\n\nTo learn more about custom limits and upgrades, contact your dedicated Customer Success Manager or friends@segment.com.\n\nUnify Plus limits\n\nUnify Plus customers receive the following based on their signup date:\n\nUnify Plus beginning November 6, 2024: 50 Computed Traits, 10 Predictions, 3 Recommendation Traits\nUnify Plus before November 6, 2024: 50 Computed Traits, 5 Predictions\n\nUnify Plus limits vary based on your Engage plan:\n\nEngage Plus: 100 Audiences, 75 Journey Steps, 10 Recommendation Audiences\nEngage Foundations (available for renewal only as of November 6, 2024): 100 Audiences, 75 Journey Steps\n\nVisit Segment\u2019s pricing page to learn more.\n\nDefault limits\nNAME\tLIMIT\tDETAILS\nInbound Data Throughput\t1000 events per second\tTotal event stream from sources connected to Engage, including historical data replays. Segment may slow request processing once this limit is reached.\nOutbound Downstream Destination Rate Limits\tReduced retries when failures exceed 1000 events per second\tOutbound Destination requests may fail for reasons outside of Segment\u2019s control. For example, most Destinations enforce their own rate limits. As a result, Segment may deliver data faster than the Destination can accept.\n\nWhen Destination requests fail, Segment tries to deliver the data again. However, if more than 1000 requests per second fail or if the failure rate exceeds 50% for over 72 hours, Segment may reduce additional delivery attempts until the failure condition resolves.\nAudiences and Computed Traits\nNAME\tLIMIT\tDETAILS\nCompute Concurrency\t5 new concurrent audiences or computed traits\tSegment computes five new audiences or computed traits at a time. Once the limit is reached, Segment queues additional computations until one of the five finishes computing.\nEdit Concurrency\t2 concurrent audiences or computed traits\tYou can edit two concurrent audiences or computed traits at a time. Once the limit is reached, Segment queues and locks additional computations until one of the two finishes computing.\nBatch Compute Concurrency Limit\t10 (default) per space\tThe number of batch computations that can run concurrently per space. When this limit is reached, Segment delays subsequent computations until current computations finish.\nCompute Throughput\t10000 computations per second\tComputations include any Track or Identify call that triggers an audience or computed trait re-computation. Once the limit is reached, Segment may slow audience processing.\nEvents Lookback History\t3 years\tThe period of time for which Segment stores audience and computed traits computation events.\nReal-time to batch destination sync frequency\t2-3 hours\tThe frequency with which Segment syncs real-time audiences to batch destinations.\nEvent History\t1970-01-01\tEvents with a timestamp less than 1970-01-01 aren\u2019t always ingested, which could impact audience backfills with event timestamps prior to this date.\nEngage Data Ingest\t1x the data ingested into Connections\tThe amount of data transferred into the Compute Engine.\nAudience Frequency Update\t1 per 8 hours\tAudiences that require time windows (batch audiences), funnels, dynamic properties, or account-level membership are processed on chronological schedules. The default schedule is once every eight hours; however, this can be delayed if the \u201cBatch Compute Concurrency Limit\u201d is reached. Unless otherwise agreed upon, the audiences will compute at the limit set forth.\nEvent Properties (Computed Traits)\t10,000\tFor Computed Traits that exceed this limit, Segment will not persist any new Event Properties and will drop new trait keys and corresponding values.\nSQL Traits\nNAME\tLIMIT\tDETAILS\nSQL Traits - Rows\t25 million\tThe number of rows each SQL trait can return.\nSQL Traits - Columns\t25\tThe number of columns each SQL trait can return.\nProfile API\n\nThese limits are set per each Unify/Engage Space.\n\nNAME\tLIMIT\tDETAILS\nProfile API Throughput\t100 requests per second\tIf requests exceed 100 per second, the Profile API returns HTTP Error 429 Too Many Requests.\nEvents Lookback History\t14 days\tThe Profile API retrieves up to 14 days of a profile\u2019s historical events within a collection. This applies to Track events, not traits sent through Identify calls.\nIdentity\nNAME\tLIMIT\tDETAILS\nIdentity Merges\t100 merges\tEngage supports up to 100 merges per profile in its identity graph. Merges occur when a common external_id joins two existing profiles. For example, if a user initiates a mobile session but then signs in through a web application, a common identifier like user_id will join the two user profiles. No additional merges will be added once the profile reaches this limit. Event resolution for the profile, however, will continue.\nIdentity Mappings\t1000 mappings\tEngage supports up to 1000 mappings per profile in its identity graph. Mappings are external identifier values like a user_id, email, mobile advertising id, or any custom identifier. No additional mappings will be added once the profile reaches this limit. Event resolution for the profile, however, will continue.\nIdentify calls\t300 traits\tEngage rejects Identify events with 300 or more traits. If your use case requires more than 300 traits, you can split the traits into multiple Identify calls.\nUnify ingestion limitations\n\nUnify will silently drop events if:\n\nThe groupId has more than 500 characters.\nEvents have more than 300 properties/traits.\nmessageId is longer than 100 characters.\nThe groupId is empty in a Group call or context.groupId is empty in a Track call.\n\nThis page was last modified: 05 Nov 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nUnify Plus limits\nDefault limits\nAudiences and Computed Traits\nSQL Traits\nProfile API\nIdentity\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nJourneys\n/\nJourneys Analytics\nJourneys Analytics\n\nSegment maintains analytics for each Journey and its individual steps. As a result, you can view both granular and high-level performance metrics that give you insight into your Journeys.\n\nJourney-Level Analytics\n\nJourneys Level Analytics is a collection of statistics that can help you assess how a Journey is performing.\n\nWhere individual messaging analytics give you focused insight into specific Journey events, Journey Level Analytics shows you a high-level overview of a Journey\u2019s effectiveness.\n\nAccess a Journey\u2019s Analytics\n\nFollow these steps to view the Analytics for a specific Journey:\n\nIn your Segment workspace, navigate to Engage > Journeys.\nSelect a Journey from the Journeys list.\nThe Analytics tiles display as part of the Journey\u2019s overview.\n\nJourneys in draft status don\u2019t display Analytics.\n\nJourney-Level Analytics statistics\n\nThe following table shows the statistics available for a Journey:\n\nSTATISTIC\tDESCRIPTION\nEntered\tThe total number of users who entered your Journey\nIn progress\tThe total number of users who have entered the Journey without yet exiting it\nCompleted\tThe total number of users who entered the Journey and reached any final step\nExits\tThe total number of users who have exited the Journey\n\nCompleted and exits are mutually exclusive. The \u201cSearch for a user\u201d search box excludes users who have exited the Journey.\n\nUse the date picker to view a Journey\u2019s analytics over a specific time frame in any 180 day period.\n\nThe following table shows descriptions of the time frames you can select:\n\nTIME FRAME\tDESCRIPTION (BASED ON UTC)\nToday\tToday, beginning at midnight\nYesterday\tThe day before today\nLast 7 days\tThe past seven days, including today\nLast 30 days\tThe past 30 days, including today\nLast 90 days\tThe past 90 days, including today\nLast 180 days\tThe past 180 days, including today\nCustom date range\tThe period between two dates, including the selected dates\nStep-Level Analytics\n\nDisplayed with each step of your Journey, Step-Level Analytics shows you how many users made it to the step you\u2019re viewing. You can use this data to gain context for how users flow through your Journey.\n\nChanging the calculation percentage\n\nWith Step-Level Analytics, you can configure two settings that give you granular insight into each step\u2019s performance:\n\nPrevious step or entry step, which calculates the displayed percentage based on either the number of users in the entry step or the number of users in the previous step\nTotal or unique users, which lets you change the displayed percentage to account for re-entry\nPrevious step or entry step\n\nBy default, Engage calculates an individual step\u2019s analytics as a percentage of the number of users in the previous step. However, you can also view step analytics as a percentage of the initial number of users in the Journey\u2019s entry step.\n\nFor example, suppose your Journey\u2019s entry step contained 100 users, and 50 proceeded to the next step. For both calculation options, Engage would display 50% and 50 for the next step. If 25 users from the second step reached step three, however, Engage would display 50% and 25 for previous-step based calculations but 25% and 25 for entry-step based calculations.\n\nTo change this base percentage, select Calculate % based on, then select Entry step or Previous step.\n\nTotal or unique users\n\nIf you\u2019ve enabled re-entry for your Journey, you can also configure Step-Level Analytics to calculate the step\u2019s percentage based on unique or total users. Selecting Unique generates a percentage based on unique users, while Total includes users who have re-entered the Journey.\n\nFor more information on re-entry settings in Journeys, view Journey re-entry.\n\nThis page was last modified: 30 Apr 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nJourney-Level Analytics\nJourney-Level Analytics statistics\nStep-Level Analytics\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nSegment vs. Tag Managers\nSegment vs. Tag Managers\n\nTag managers, also known as Tag Management Systems (TMS), were a popular solution before the mainstream adoption of mobile apps. They primarily helped Digital Analytics and Online Marketers manage web tags or \u201cbeacons\u201d on a website.\n\nBuilt on an older technology, tag managers inject either a piece of JavaScript or an ad pixel into a website. They carry out rules that marketers create for each tag, like firing an ad channel pixel when that network refers a website visitor. Every tag requires users to create rules. No data is stored, and no code is eliminated.\n\nIn addition to ad networks, today\u2019s data-driven businesses use a variety of tools to optimize their product and marketing spends. In order to a/b test copy, nurture sales leads, email customers, and provide fast support, businesses integrate variety of analytics and marketing tools. Segment makes it easy to install, try, and use them all. Tag managers primarily focus on ad networks, and can\u2019t support modern tools without extensive customization.\n\nRather than \u201cfiring and forgetting,\u201d Segment takes a data-centric, deliberate approach to destinations. You don\u2019t need to set up special parameters for each tool \u2013 Segment does that for you. Segment structures your data so we can understand what it is, and can translate it correctly for each destination we send it to. Segment works because\u00a0all of these tools\u00a0operate on the same customer data: who is on your app and what are they doing. Segment collects this data once, then translates and sends it to every tool you use. Because Segment also archives the data, Segment can replay your historical data into new tools, and send your raw data to a data storage solution for later analysis.\n\n\tSegment\tTag Managers\nCore Competency\tIntegrates complex tools with minimal effort, stores a complete copy of clickstream data, exports data to SQL databases\tLoads JavaScript into webpages, inserts advertising pixels based on rule settings\nData Storage\tStores clickstream data in one comprehensive set; replays historical data into new tools; exports data into SQL databases and internal systems\tDoes not store data; cannot load historical data into new tools; cannot translate and load historical data into SQL databases\nDevice Compatibility\tTracks user events in mobile, web, and server environments. Server libraries include Python, Node, Ruby, PHP, .NET, Java, Clojure, Go, Rust and Xamarin\tOperates on web; limited functionality on mobile; does not support server destinations\nUser Interface\tDelivers sleek user experience; automatically translate data for new tools when you enable a destination\tRequires that you configure settings and rules for each pixel to fire\nTool Integrations\tFully integrates analytics, advertising, email, customer support, marketing automation, usability tracking, error testing, and CRM tools with the flick of a switch\tManages ad pixels; requires custom engineering work to integrate any other complex tool\n\nEvery organization\u2019s data stack and business requirements are unique. Segment also works well in tandem with a tag manager. For example, Segment sends data directly to the Google Tag Manager (GTM) destination.\n\nWhile you can use Segment\u2019s Analytics.js library through a tag manager, Segment doesn\u2019t recommended this for a few reasons:\n\nA hybrid approach makes it difficult to determine the root cause of technical problems, and complicates troubleshooting. Segment cannot guarantee destination compatibility in a \u201chybrid\u201d Segment-tag-manager installation, and cannot guarantee support on these installations. All QA and regression testing assumes a native installation of Analytics.js on the page.\n\nOne of Segment\u2019s main charters is to not lose data. Our system and cloud infrastructure is designed to ensure that data loss does not happen. If you implement the entry point of data capture (Segment\u2019s libraries) using a Tag Manager, you introduce risk of data loss and make it difficult or impossible to troubleshoot.\n\nThis implementation behind a tag manager can introduce major delays and performance issues, which can cause delays with events that need to occur early in your funnel.\n\nThe biggest challenge is around triggering cascading events. Browsers are notorious for dropping calls. When you use a TMS to initiate Segment events you are introducing a second point of failure for those events.\n\nThis page was last modified: 16 Feb 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nContent\n/\nMobile Push Template\nMobile Push Template\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE PREMIER \u2713\n?\n\nEngage Premier entered an End of Sale (EOS) period effective June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. Segment recommends exploring the following pages in preparation of a migration or future MCM needs:\n\nTwilio Marketing Campaigns\n\nPreferred ISV Partners:\n\nAirship Blog\nBloomreach Blog\nBraze Blog\nInsider Blog\nKlaviyo Blog\nTwilio Engage Foundations Documentation\n\n\nUse Twilio Engage to build mobile push templates to include throughout your marketing campaigns.\n\nMobile push template types\n\nYou can choose between two mobile push template types:\n\nMedia, which contains media and text content\nText, which contains text content\nBuild a mobile push message template\n\nTo build mobile push templates in Engage, first configure Engage for mobile.\n\nFollow these steps to build a mobile push template:\n\nNavigate to Engage > Content and click Create template.\nSelect Push, then click Configure.\nEnter a template name and select your template\u2019s language.\nSelect your template\u2019s content type, then click Next.\nFor media templates, enter your message\u2019s title in the Title field, its body in the Body field, add the media URL, then add any desired merge tags.\nFor text templates, enter your message\u2019s title in the Title field, its body in the Body field, then add any desired merge tags.\nSelect a click behavior.\nClick Test or Save to save your template.\nClick behaviors\n\nWhen you build a mobile push template, you can choose between three click behaviors, which determine what happens when a user taps on the mobile push:\n\nBEHAVIOR\tDESCRIPTION\nOpen app\tOpens an app. You can specify a URL to take the user to a specific screen with your app. If you don\u2019t enter a URL, this behavior will take the user to the app\u2019s home screen.\nOpen URL\tOpens the specified URL.\nCustom action\tTakes any value as text input. Your app determines how to handle the value. For example, you could enter a custom action of open_settings, and then instruct your application to open the settings application when a user taps the push and the push arrives with click behavior = open_settings.\nTest your mobile push template\n\nPush tokens\n\nPush tokens are unique identifiers Segment associates with each profile. For mobile push, you\u2019ll need to configure identity resolution settings for the push tokens ios.push_token and android.push_token. Using the Profile explorer, you can find a profile\u2019s push tokens by opening a profile and then selecting the Identities tab. You can only send mobile pushes to profiles with push tokens enabled.\n\nFollow these steps to test your mobile push:\n\nChoose a template to test:\nFor new templates, select Test once you\u2019ve finished building a template.\nFor existing templates, navigate to Engage > Content > Push, select the template you want to test, then click Test.\nMobile push templates have a content size limit of 4KB.\nChoose a messaging service and add a recipient.\nYou can add recipients using an email address or user ID.\nClick Send test push.\n\nSegment verifies that the profile you\u2019re sending a test to has a push token, then sends the test. If the test mobile push doesn\u2019t work as expected, confirm that the profile you\u2019re sending to has a push token.\n\nAdvanced settings\nBadge count settings\n\nBadge counts appear in the corner of an app icon on your user\u2019s device. Badge counts show the number of unread notifications. During push notification setup, you can set badge count behavior from the badge count dropdown.\n\nChoose from these badge count settings:\n\nIncrease by: for each new notification, the badge count increases by the number you enter. Increase by is the standard behavior for badge counts.\nDecrease by: for each new notification, the previous badge count decreases by the number you enter. Use Decrease by to send notifications quietly.\nSet to: replaces all previous sent notifications with the number you enter.\nAction buttons\n\nAction buttons sit below a push notification and let your users take action on the push. You can use action buttons to encourage users to make a purchase, visit a website, or share content on social media, for example.\n\nFollow these steps to add an action button:\n\nUnder Advanced Settings, click + Add action button.\nEnter an action button identifier.\nEnter the action button text. This is the text the user will see on the action button.\nChoose an open action. You can choose from open app, open URL, or a custom action.\n\nYou can add up to three action buttons for each push notification.\n\nPersonalize with merge tags\n\nPersonalize mobile push content in Engage using profile traits as merge tags in your messages.\n\nTo personalize a mobile push, click Add merge tags in the template builder and select the profile traits to include in your message.\n\nEngage inserts the selected traits inside merge tags based on cursor placement in the message. This allows you to personalize each mobile push you send to recipients. You can also use liquid templating to create dynamic content in the template editor.\n\nTo learn more about profile traits, visit Segment\u2019s Computed Traits and SQL Traits documentation.\n\nNext steps\n\nNow that you\u2019ve built a mobile push template, you\u2019re ready to begin sending mobile push campaigns.\n\nThis page was last modified: 15 Jul 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nMobile push template types\nBuild a mobile push message template\nTest your mobile push template\nAdvanced settings\nPersonalize with merge tags\nNext steps\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nFiltering your Segment Data\nFiltering your Segment Data\n\nThere are many ways you can use Segment to filter event and object based data to control which destinations it reaches. This document lists the most commonly used ways you can filter data in Segment, and explains when you\u2019d use each.\n\nFiltering with the Integrations Object\n\nThe Integrations object is the only filtering method that cannot be edited using the Segment web app. As such, it is both the most reliable, and the most complicated filtering option to change. The integrations object is available to all customers regardless of Segment plan.\n\nUse this option when you absolutely, for sure, 100% know that you always, or never want this data in a specific destination or set of destinations. You can also build logic in your app or site to conditionally enable or disable destinations by rewriting this object, however this is not recommended as it is time consuming to change, especially for mobile apps.\n\nThe Integrations object filters track, page, group, identify, and screen events from both client and cloud based sources, and routes or prevents them from getting to the listed destinations.\n\nYou can use the integrations JSON object as part of your Segment payloads to control how Segment routes your data to specific destinations. An example payload is below:\n\n{\n  \"anonymousId\": \"507f191e810c19729de860ea\",\n  \"context\": {\n    \"locale\": \"en-US\",\n    \"page\": {\n      \"title\": \"Analytics Academy\",\n      \"url\": \"https://segment.com/academy/\"\n    }\n  },\n  \"integrations\": {\n    \"All\": true,\n    \"Mixpanel\": false,\n    \"Salesforce\": false,\n    \"My Destination Function (My Workspace)\": true\n  }\n}\n\n\nBy default, the integrations object is set to 'All': true. You do not need to include this flag in the object to use this behavior, but if you\u2019ll be using the integrations object frequently to control destination filtering, you might want to do this to make it explicit for later readers. Change this to 'All': false to prevent any downstream destinations from receiving data, not including data warehouses. If you set 'Segment.io': false in the integrations object, Analytics.js 2.0 drops the event before it reaches your Source Debugger. You can also add destinations to the object by key, and provide a true or false value to allow or disallow data to flow to them on an individual basis. The Destination Info box at the top of each destination page lets you know how to refer to each destination in the Integrations object.\n\nIf you are using multiple instances of a destination, any settings you set in the integrations object are applied to all instances of the destination. You cannot specify an instance of a destination to apply Integrations object settings to.\u00a0\n\nNote that destination flags are case sensitive and match the destination\u2019s name in the docs (for example, \u201cAdLearn Open Platform\u201d, \u201cawe.sm\u201d, or \u201cMailChimp\u201d).\n\nThe syntax to filter data to a data warehouse is different. Refer to the Warehouse FAQs for more details.\n\nDestination filters\n\nDestination filters allow you to control the data flowing into each specific destination, by examining event payloads, and conditionally preventing data from being sent to destinations. You can filter out entire events, or just specific fields in the properties, in the traits, or in the context of your events. Destination filters support cloud-based (server-side), actions-based, and mobile and web device-mode destinations. Destination filters aren\u2019t available for, and don\u2019t prevent data from reaching your warehouse(s) or S3 destinations.\n\nDestination filters are only available in workspaces that are on a Business Tier plan.\n\nKeep these limitations in mind when using destination filters.\n\nTo set up destination filters from the Segment web app for the destination from which you want to exclude data:\n\n(For web device-mode destinations only) Enable device mode destination filters for your Analytics.js source. To do this, go to your Javascript source and navigate to Settings > Analytics.js and turn the toggle on for Destination Filters.\nNOTE: Destination filters for web device-mode only supports the Analytics.js 2.0 source.\nNavigate to Connections > Destinations and select the destination you want to set up filters for.\nGo to the Filters tab and click + New Filter to create a destination filter. See the Destination Filters documentation for more details.\n\nYou can set up destination filters using the options presented in the Segment web app, or using Segment\u2019s Filter Query Logic (FQL). If you use FQL, your query syntax is limited to 5KB per query.\n\nPer-Source schema integrations filters\n\nIntegration filters allow you to quickly change which destinations receive specific Track, Identify, or Group events. Access this tool in any Source that is receiving data by navigating to the Schema tab. Schema integration filters are available to workspaces that are on a Business Tier plan only.\n\nYou can apply Integrations filters to specific events regardless of whether the source is connected to a Tracking Plan. To update which destination an event can be sent to, click the Integrations dropdown menu to see a list of the destinations each call is sent to. You can turn those destinations on or off from within the dropdown menu.\n\nThe events filtered out of individual destinations using this method still arrive in your data warehouse(s). Warehouses do not appear in the integration filters dropdown, and you cannot prevent data from flowing to Warehouses using this feature - to do that use Warehouse Selective Sync.\n\nIntegration filters are all-or-nothing for each event. If you require more detailed control over which events are sent to specific destinations, you can use Destination Filters to inspect the event payload, and conditionally drop the data or forward it to the destination.\n\nIntegration filters won\u2019t override an existing value in the integrations object. If the integration object already has a value for the integration, the per source schema integration filters will not override this. For example, if you\u2019re sending events to Appsflyer with the appsflyerId passed into the integration object:\n\nintegrations: {\n  Appsflyer: {\n    appsflyerId: 'xxxxxx'\n  }\n}\n\n\nFor the same event you have Appsflyer turned off using the per source schema integrations filter, this filter won\u2019t override the above object with a false value, and events still send downstream. In this scenario, you can use destination filters to drop the event before it sends downstream.\n\nSchema event filters\n\nYou can use Schema Event Filters to discard and permanently remove Page, Screen and Track events from event-based sources, preventing them from reaching any destinations or warehouses, as well as omit identify traits and group properties. Use this if you know that you\u2019ll never want to access this data again. This functionality is similar to filtering with the Integrations object, however it can be changed from within the Segment app without touching any code.\n\nWhen you enable these filters, Segment stops forwarding the data to all of your Cloud- and device-mode destinations, including warehouses, and your data is no longer stored in Segment\u2019s warehouses for later replay.\n\nUse this when you need to disable an event immediately, but may need more time to remove it from your code, or when you want to temporarily disable an event for testing. In addition to blocking track calls, you can block all page and screen calls, as well as omit identify traits and group properties.\n\nIf the Source is not connected to a tracking plan, you\u2019ll find event filter toggles next to the Integration filters in the source\u2019s schema tab. When an event is set to block, the entire event is blocked. This means no destinations receive it, including data warehouses.\n\nWhen you block an event using Schema filters, it won\u2019t be considered in the MTU count unless blocked event forwarding is enabled.\n\nWhen an event is blocked, the name of the event or property appears on your Schema page with a counter which shows how many times it has been blocked. By default, data from blocked events and properties is not recoverable. You can always re-enable the event to continue sending it to downstream destinations.\n\nIn most cases, blocking an event immediately stops that event from sending to destinations. In rare cases, it can take up to 6 hours for an event to completely stop arriving in all Destinations.\n\nThis feature is only available if the Source is not connected to a Tracking Plan, and is only available in workspaces that are on a Business Tier plan.\n\nProtocols Tracking Plan blocking and property omission\n\nIf you\u2019re using Protocols, and you\u2019re confident that your tracking plan includes exactly the events and properties you want to record, you can tell Segment to block unplanned events or malformed JSON. When you do this, Segment discards any data coming from the Source that doesn\u2019t conform to the tracking plan.\n\nBy default, the blocked events are permanently discarded: they do not flow to Destinations, and cannot be Replayed (similar to Schema Controls). However, you can opt to send data in violation of the tracking plan to a new Segment Source so you can monitor it. (This source can affect your MTU count.)\n\nIf you have Protocols in your workspace, and have a tracking plan associated with the Source, you\u2019ll see additional options in the Schema Configuration section of the Source\u2019s Settings page. From this page you can choose how to handle data violations across different types of calls and properties, whether that be blocking events entirely or omitting violating properties.\n\nDestination Insert Function\n\nA customizable way to filter or alter data going from a source to a cloud-mode destination is to use Insert Functions). This feature gives you the ability to receive data from your Segment source, write custom code to alter or block it, and then pass that altered payload to a downstream cloud-mode destination.\n\nWarehouse Selective Sync\n\nWarehouse Selective Sync allows you to stop sending specific data to specific warehouses. You can use this to stop syncing specific events or properties that aren\u2019t relevant, and could be slowing down your warehouse syncs. See the Warehouse Selective Sync documentation to learn more.\n\nThis feature is only available to Business Tier customers, and you must be a Workspace Owner to change Selective Sync settings.\n\nPrivacy Portal filtering\n\nThe Privacy Portal is available to all Segment customers, because Segment believes that data privacy is a right, and that anyone collecting data should have tools to help ensure their users\u2019 privacy. More enhancements are available to BT customers who may need tools for managing complex implementations.\n\nThe Privacy Portal tools allow you to inspect your incoming calls and their payloads, detect potential Personally Identifiable Information (PII) in properties using matchers, classify the information by different categories of risk, and use those categories to determine which Destinations may or may not receive the data. Learn more about these features in the Privacy Portal documentation.\n\nThis page was last modified: 02 Feb 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nFiltering with the Integrations Object\nDestination filters\nPer-Source schema integrations filters\nSchema event filters\nProtocols Tracking Plan blocking and property omission\nDestination Insert Function\nWarehouse Selective Sync\nPrivacy Portal filtering\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nProtocols\n/\nUse Transformations to fix bad data\nUse Transformations to fix bad data\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nPROTOCOLS \u2713\n?\nWhat is a Transformation?\n\nWith Transformations, you can change data as it flows through Segment to either correct bad data or customize data for a specific destination. Change event and property names to align events with your Tracking Plan, or to conform to a destination-specific requirement. For example, you can create a Transformation to change an event name from completed_order to Order Completed to conform to Segment\u2019s ecommerce spec.\n\nYou can also use Segment\u2019s Public API to transform events, properties, and property values for many use cases.\n\nTransformations are very powerful and should be applied with care.\n\nTransformations irrevocably change the event payloads flowing through Segment and immediately affect either all destinations, or a single downstream destination, depending on your settings.\n\nAs soon as you apply a Transformation, the original tracking payloads are not easily recoverable.\n\nSegment\u2019s goal is to make Transformations a powerful tool that complements a well structured Tracking Plan. Together, these features help your organization scale and achieve high data quality. For that reason Segment recommends that you start your data quality strategy with a clearly defined Tracking Plan. Without this critical component, the risk of creating conflicting or detrimental transformations increases.\n\nOther important notes\nTransformations cannot be applied retroactively: They only apply to data moving forward. However, you can manually extract and re-send (or even Replay) events through a source with an active destination Transformation, which will send the transformed events to your destinations.\nTransformations are available to Protocols customers: If you\u2019re interested in this feature, contact your Account Executive or CSM to learn more about the Protocols package.\nSource-level transformations are irrevocable: When applied at the source, a transformation permanently changes the structure of the event. The original events are not easily recoverable or Replayable. Assume that transformed data cannot be recovered.\nDevice-mode destinations are NOT supported: Source scoped transformations will only apply to cloud-mode destinations, warehouses, and S3 destinations. Destination scoped transformations will only apply to cloud-mode destinations.\nTransformation order: Segment processes source-scoped transformations before the events reach destination filters. Segment processes destination-scoped transformations after the events pass through destination filters.\nMultiple instances of the same destination: If multiple instances of a destination are connected to a single source, then the same transformation applies across every instance.\nTransformation scope: Transformations can be applied to event names, properties, and traits. However, it\u2019s important to note that transformations cannot be applied to top-level attributes like userId.\nView all Transformations\n\nAll Protocols Transformations are listed in the Transformations tab in the Protocols section. The list view supports filtering and sorting to organize transformations by transformation type, source, and destination.\n\nTransformations can be enabled and disabled directly from the list view using the toggle.\n\nTransformations can be deleted and edited by clicking on the overflow menu. When editing a Transformation, only the resulting event or property names, and Transformation name can be edited. If you want to select a different event or source, create a separate Transformation rule.\n\nTransformations created using the Public API\n\nOn the Transformations page in the Segment app, you can view and rename transformations that you created with the Public API. In some cases, you can edit these transformations in the UI.\n\nCreate a Transformation\n\nTo create a Transformation, navigate to the Transformations tab in Protocols and click New Transformation in the top right. A three-step wizard guides you through creating a transformation.\n\nWorkspace Owner or Source Admin permissions are required to create and edit transformations. Source Read-only permissions are required to view transformations.\n\nStep 1: Select the transformation type\n\nTo create a Transformation, you first need to select which type of transformation you want to create. For each transformation type, Segment displays a description, use cases, and example payload. Current transformation types available in your Segment workspace include:\n\nRename track event: Rename track event name at the source or per destination. The events listed in the event names dropdown menu correspond to the events listed on the source schema view.\n\nEdit track event properties: Rename multiple properties and/or change property data structure at the source or per destination\n\nEdit identify or group event traits: Rename multiple traits and/or change trait data structure at the source or per destination\n\nView more use cases of Transformations available in both your workspace and Segment\u2019s Public API.\n\nStep 2: Set up the transformation\n\nDepending on the transformation type you selected, relevant drop-down selectors and fields are presented to define how you want to transform the data.\n\nMultiple transformations cannot be created for the same source + type + event + destination combo. This restriction blocks circular transformations (for example, order_updated to orderUpdated to order_updated), minimizes unexpected transformations, and enables easy filtering across each dimension.\n\nRegardless of the type of transformation selected, first select a source. Each Transformation can only apply to a single source. While this makes it more difficult to apply transformations broadly, it ensures you are only transforming data relevant to the selected source.\n\nAfter selecting the source, you will need to select a scope. Scope determines where Segment applies the transformation.\n\nSource-scoped Transformations only apply to cloud-mode, S3, and data warehouse destinations.\n\nSource scope: Events are transformed in all active Segment cloud-mode destinations, warehouses, and S3 destinations. This scope is best when you want to fix malformed events before sending them to all destinations. These transformations should be treated as a temporary solution to hold you over while your engineering team fixes the root event.\n\nDestination scope: Events are transformed in ONLY the selected cloud-mode destination. Device-mode destinations, S3, and data warehouses are not currently supported. Use the Destination scope when you want to customize an event to the unique requirements of a destination. These transformations can exist permanently.\n\nDepending on the type of transformation you selected, you will need to enter the relevant event, property, or trait mappings to create the transformation.\n\nSelect an Event: After you select the scope, use the search box to choose the event to transform. You can only select a single track event, identify or group call. If you are renaming the event, simply enter the new name in the provided text field.\n\nRename properties or traits: To rename properties or traits within a selected event, click + Add Property. The dropdown that appears contains the properties or traits sent with the selected event. Segment supports JSON Path notation to select nested objects up to four levels deep. For example, order.id selects the id property in the order object. Segment does not support .$. notation to select a property from an array of objects. For example, the following event, which generates products.$.product_id, is unsupported.\n\nanalytics.track('Example', {\n  products: [{\n      product_id: \"123\"\n  }],\n})\n\n\nIn this scenario, we do not support the transformation of product_id.\n\nAfter selecting a property/trait, select JSON Path or Simple String to change the property/trait. Simple string will change the name in-line, while JSON path allows you to move the property/trait in or out of an object.\n\nWhen you see properties that have the escape character \\ in them - this escape character \\ is added to differentiate between a property name that has a . in it, and a nested field, like so:\n\ningredients.salad \u2192 \"ingredients\": { \"salad\": \"yum\" }\ningredients\\.salad \u2192 \"ingredients.salad\": \"yum\"\n\nStep 3: Name the transformation and enable it\n\nEnter a descriptive name to act as a label for the transformation. This label helps you organize your Transformations, and Segment recommends that you make this descriptive and focused on the problem you\u2019re solving. For example Fix misnamed order_completed event for ecommerce spec is much better than Map order_completed.\n\nIn this step, you can also choose to keep the Transformation disabled, so you can come back and edit it later. To update, enable, or disable a Transformation, click on the overflow menu and select Edit Transformation.\n\nUse Cases\n\nHere\u2019s a list of Segment Transformations with some use case examples.\n\nRename an event: Change an event name from viewed_product to Product Viewed.\n\nRename a property or trait: Change the property name revenue to total for a specific destination.\n\nUpdate a property value: Use Segment\u2019s Public API to transform the property currency to have the value USD.\n\nProperty Transformations\n\nAssigning static values: If you want to create a new property and set a static value, use Segment\u2019s Public API to create new_property: static_value. Segment currently supports setting static values for top-level fields, as well as fields within the context or properties object with propertyValueTransformations. However, Segment doesn\u2019t support changing fields outside the properties or traits object with propertyRenames. You can use propertyValueTransformations on a single object to assign the same value to different fields or on multiple objects to assign a static value to the same field across objects.\nCasing functions: Use Segment\u2019s Public API to transform property value casing to lowercase, uppercase, snakecase, kebabcase, or titlecase. When transforming data with casing functions, use the fqlDefinedProperties array to define the FQL you want to use and the new or existing propertyName you\u2019d like to transform.\nStatic and dynamic value casing: Use casing functions to transform property values to create uniform tracking data. For example, you can convert usa to USA to keep your downstream data consistent.\nYou can transform these properties using static casing functions:\n\nfqlDefinedProperties\": [{\"fql\": \"uppercase(\"United States)\", \"propertyName\": \"properties.propertyValue1\"}]\n\n\nor dynamic casing functions:\n\nfqlDefinedProperties\": [{\"fql\": \"lowercase(properties.propertyValue1)\", \"propertyName\": \"properties.propertyValue1\"}]\n\nCreate a new property with applied casing: Use Segment\u2019s Public API to create a new property and set the value of the new property to the transformed value of an existing property. You can dynamically assign the value of one existing property to another, or assign the value of an existing property to a new property without applying casing functions.\nFor example, create a new property (prop2) with a value of lowercase(properties.prop1) by including the following snippet in your payload:\n\nfqlDefinedProperties\": [{\"fql\": \"lowercase(properties.prop1)\", \"propertyName\": \"properties.prop2\"}]\n\nNote that you can only assign one property to fqlDefinedProperties array.\nNote that you cannot use fqlDefinedProperties along with event or property rename or property value transformations.\n\nSegment displays an error if the following property conflicts occur:\n\nYou create a property value transformation when one already exists for the same property value.\nTwo property paths in propertyValueTransformations are the same.\nA property path in propertyValueTransformations is the same as a property name in propertyRenames.\n\nThis page was last modified: 24 Jul 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nWhat is a Transformation?\nView all Transformations\nCreate a Transformation\nUse Cases\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nUser Subscriptions Overview\nUser Subscriptions Overview\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE PREMIER \u2713\n?\n\nEngage Premier entered an End of Sale (EOS) period effective June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:\n\nTwilio Marketing Campaigns\n\nPreferred ISV Partners:\n\nAirship Blog\nBloomreach Blog\nBraze Blog\nInsider Blog\nKlaviyo Blog\nTwilio Engage Foundations Documentation\n\n\nSegment associates subscription states with each email address and phone number external id in your audiences. Subscription states indicate the level of consent end users have given to receive your marketing campaigns.\n\nKnowledge of subscription states will help you understand which users can and cannot receive your campaigns. This page provides an overview of user subscriptions.\n\nThe Four Subscription states\n\nEmail addresses and phone numbers in your audience have one of the four following user subscription states:\n\nsubscribed; users who opted in to your marketing campaigns\nunsubscribed; users who unsubscribed from your marketing campaigns\ndid-not-subscribe; users who are neither subscribed nor unsubscribed\nNo subscription status; users who never gave Segment the email or phone number in your audience\n\nIt\u2019s best practice to only send Engage campaigns to users with a subscribed status. However, if you need to send an email to someone who hasn\u2019t subscribed, you can create an email campaign that you send to all users.\n\nTo learn how Segment determines user subscription states, read the User Subscription State documentation.\n\nSetting User Subscriptions\n\nYou can set user subscriptions manually when you upload contacts using Engage\u2019s CSV uploader. You can also use a CSV upload to correct contacts with outdated subscription states.\n\nMost user subscriptions update programmatically, using Segment APIs. The Public API and the Identify call handle subscription state changes made when users sign up to or change their subscription status to your marketing materials with online forms or within notification centers.\n\nView the Setting User Subscriptions page to learn more about user subscription changes.\n\nThis page was last modified: 15 Jul 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nThe Four Subscription states\nSetting User Subscriptions\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nReverse Etl\n/\nSet up Reverse ETL\nSet up Reverse ETL\n\nThere are four components to Reverse ETL: Sources, Models, Destinations, and Mappings.\n\nFollow these 4 steps to set up Reverse ETL:\n\nAdd a source\nAdd a model\nAdd a destination\nCreate mappings\nStep 1: Add a source\n\nIn Reverse ETL, a source is your data warehouse.\n\nYou need to be a user that has both read and write access to the warehouse.\n\nTo add your warehouse as a source:\n\nNavigate to Connections > Sources and select the Reverse ETL tab in the Segment app.\nClick + Add Reverse ETL source.\nSelect the source you want to add.\nFollow the corresponding guide to set up the required permissions for your Reverse ETL source:\nAzure Reverse ETL setup guide\nBigQuery Reverse ETL setup guide\nDatabricks Reverse ETL setup guide\nPostgres Reverse ETL setup guide\nRedshift Reverse ETL setup guide\nSnowflake Reverse ETL setup guide\nStep 2: Add a model\n\nModels define sets of data you want to sync to your Reverse ETL destinations. A source can have multiple models. Segment supports SQL models and dbt models.\n\nSQL editor\nNavigate to Connections > Sources and select the Reverse ETL tab. Select your source and click Add Model.\nClick SQL Editor as your modeling method. (Segment will add more modeling methods in the future.)\nEnter the SQL query that\u2019ll define your model. Your model is used to map data to your Reverse ETL destination(s).\nChoose a column to use as the unique identifier for each record in the Unique Identifier column field.\nThe Unique Identifier should be a column with unique values per record to ensure checkpointing works as expected. It can potentially be a primary key. This column is used to detect new, updated, and deleted records.\nClick Preview to see a preview of the results of your SQL query. The data from the preview is extracted from the first 10 records of your warehouse.\nSegment caches preview queries and result sets in the UI, and stores the preview cache at the source level.\nClick Next.\nEnter your Model Name.\nClick Create Model.\ndbt model\n\nUse Segment\u2019s dbt extension to centralize model management and versioning. Users who set up a BigQuery, Databricks, Postgres, Redshift, or Snowflake source can use Segment\u2019s dbt extension to centralize model management and versioning, reduce redundancies, and run CI checks to prevent breaking changes.\n\nIf you use dbt Cloud with Reverse ETL, you can create up to 5 mappings that use the sync strategy dbt Cloud, which extracts data from your warehouse and syncs it with your destination after a job in dbt Cloud is complete.\n\nStep 3: Add a destination\n\nIn Reverse ETL, destinations are the business tools or apps you use that Segment syncs the data from your warehouse to. A model can have multiple destinations.\n\nReverse ETL supports the destinations in the Reverse ETL catalog. If the destination you want to send data to is not listed in the Reverse ETL catalog, use the Segment Connections Destination to send data from your Reverse ETL warehouse to your destination.\n\nEngage users can use the Segment Profiles Destination to create and update Profiles that can then be accessed through Profile API and activated within Twilio Engage.\n\nSeparate endpoints and credentials required to set up third party destinations\n\nBefore you begin setting up your destinations, note that each destination has different authentication requirements. See the documentation for your intended destination for more details.\n\nTo add your first destination:\n\nNavigate to Connections > Destinations and select the Reverse ETL tab.\nClick Add Reverse ETL destination.\nSelect the destination you want to connect to and click Configure.\nSelect the Reverse ETL source you want to connect the destination to.\nEnter the Destination name and click Create Destination.\nEnter the required information on the Settings tab of the destination.\nNavigate to the destination settings tab and enable the destination. If the destination is disabled, then Segment won\u2019t be able to start a sync.\nStep 4: Create mappings\n\nMappings enable you to map the data you extract from your warehouse to the fields in your destination. A destination can have multiple mappings.\n\nTo create a mapping:\n\nNavigate to Connections > Destinations and select the Reverse ETL tab.\nSelect the destination that you want to create a mapping for.\nClick Add Mapping.\nSelect the model to sync from.\nIn the Define sync behavior section, select the Action you want to sync.\nActions determine the information sent to the destination. The list of Actions are unique to each destination.\nSelect which records to send to your destination after Segment completes extracting data based on your model.\nSome destinations have sync modes, which let you specify how Segment should send data to the destination. Sync modes are unique to each destination.\nDestinations without sync modes let you select from the following options:\nAdded records\nUpdated records\nAdded or updated records\nDeleted records\nIn the Map fields section, define how to map the record columns from your model to your destination. Map the fields that come from your source to fields that the destination expects to find. Fields on the destination side depend on the type of Action selected.\nIf you\u2019re setting up a Destination Action, some mapping fields might require data to be in the form of an object or array. See the supported objects and arrays for mapping for more information.\n(Optional) Use the Suggested Mappings feature to identify and match near-matching field names to streamline the field mapping process.\nIn the Send test record section, select a test record to preview the fields that you mapped to your destination. When you\u2019ve verified that the records appear as expected, click Next.\nEnter a name for your mapping. The name initially defaults to the Action\u2019s name, for example, Track Event, but you can make changes to this default name.\nSelect how often you want Segment to sync your data under Schedule configuration.\nInterval: Extractions perform based on a selected time cycle. Select one of the following options: 15 minutes, 30 minutes, 1 hour, 2 hours, 4 hours, 6 hours, 8 hours, 12 hours, 1 day.\nDay and time: Extractions perform at specific times on selected days of the week.\nSelect the destination you\u2019d like to enable on the My Destinations page under Reverse ETL > Destinations.\nTurn the toggle on for the Mapping Status. Events that match the trigger condition in the mapping will be sent to the destination.\nIf you disable the mapping state to the destination, events that match the trigger condition in the mapping won\u2019t be sent to the destination.\n\nUse Segment\u2019s Duplicate mappings feature to create an exact copy of an existing mapping. The copied mapping has the same configurations and enrichments as your original mapping.\n\nSupported object and arrays\n\nWhen you set up destination actions in Reverse ETL, depending on the destination, some mapping fields may require data as an object or array.\n\nObject mapping\n\nYou can send data to a mapping field that requires object data. An example of object mapping is an Order completed model with a Products column that\u2019s in object format.\n\nExample:\n\n    {\n        \"product\": {\n            \"id\": 0001,\n            \"color\": \"pink\",\n            \"name\": \"tshirt\",\n            \"revenue\": 20,\n            \"inventory\": 500\n        }\n    }\n\n\nTo send data to a mapping field that requires object data, you can choose between these two options:\n\nOPTION\tDETAILS\nCustomize object\tThis enables you to manually set up the mapping fields with any data from the model. If the model contains some object data, you can select properties within the object to set up the mappings as well.\nSelect object\tThis enables you to send all nested properties within an object. The model needs to provide data in the format of the object.\n\nCertain object mapping fields have a fixed list of properties they can accept. If the names of the nested properties in your object don\u2019t match with the destination properties, the data won\u2019t send. Segment recommends you to use Customize Object to ensure your mapping is successful.\n\nArray mapping\n\nTo send data to a mapping field that requires array data, the model must provide data in the format of an array of objects. An example is an Order completed model with a Product purchased column that\u2019s in an array format.\n\nExample:\n\n    [\n    {\n        \"currency\": \"USD\",\n        \"price\": 40,\n        \"productName\": \"jacket\",\n        \"purchaseTime\": \"2021-12-17 23:43:47.102\",\n        \"quantity\": 1\n    },\n    {\n        \"currency\": \"USD\",\n        \"price\": 5,\n        \"productName\": \"socks\",\n        \"quantity\": 2\n    }\n    ]\n\n\nTo send data to a mapping field that requires array data, you can choose between these two options:\n\nOPTION\tDETAILS\nCustomize array\tThis enables you to select the specific nested properties to send to the destination.\nSelect array\tThis enables you to send all nested properties within the array.\n\nCertain array mapping fields have a fixed list of properties they can accept. If the names of the nested properties in your array don\u2019t match the destination properties, the data won\u2019t send. Segment recommends you to use the Customize array option to ensure your mapping is successful.\n\nObjects in an array don\u2019t need to have the same properties. If a user selects a missing property in the input object for a mapping field, the output object will miss the property.\n\nNull value management\n\nYou can choose to exclude null values from optional mapping fields in your syncs to some destinations. Excluding null values helps you maintain data integrity in your downstream destinations, as syncing a null value for an optional field may overwrite an existing value in your downstream tool.\n\nFor example, if you opt to sync null values with your destination and an end user fills out a form but chooses to leave an optional telephone number field blank, the existing telephone number you have on file in your destination could be overwritten with the null value. By opting out of null values for your downstream destination, you would preserve the existing telephone number in your destination.\n\nBy default, Segment syncs null values from mapped fields to your downstream destinations. Some destinations do not allow the syncing of null values, and will reject requests that contain them. Segment disables the option to opt out of syncing null values for these destinations.\n\nTo opt out of including null values in your downstream syncs:\n\nNavigate to Connections > Destinations and select the Reverse ETL tab.\nSelect the destination and the mapping you want to edit.\nClick Edit mapping.\nUnder Optional fields, select the field you want to edit.\nIn the field dropdown selection, disable the Sync null values toggle.\nInitial sync for a given mapping\n\nAfter you\u2019ve set up your source, model, destination, and mappings for Reverse ETL, your data will extract and sync to your destination(s) right away if you chose an interval schedule. If you set your data to extract at a specific day and time, the extraction will take place then.\n\nEdit Reverse ETL syncs\nEdit your model\n\nTo edit your model:\n\nNavigate to Connections > Destinations and select the Reverse ETL tab.\nSelect the source and the model you want to edit.\nOn the overview tab, click Edit to edit your query.\nClick the Settings tab to edit the model name or change the schedule settings.\nSuggested mappings\n\nSuggested mappings is fully available for RETL mappings.\n\nSegment offers suggested mappings that automatically propose relevant destination fields for model columns and payload elements. For example, if your model includes a column or payload field named transaction_amount, the feature might suggest mapping it to a destination field like Amount or TransactionValue. This automation, powered by intelligent autocompletion, matches and identifies near-matching field names to streamline the mappings setup process. For more information, see Segment\u2019s suggested mappings blog post and the Suggested Mappings Nutrition Facts Label.\n\nReview the suggested mappings for accuracy before finalizing them, as Segment can\u2019t guarantee all of the suggested mappings are accurate.\n\nEdit your mapping\n\nTo edit your mapping:\n\nNavigate to Connections > Destinations and select the Reverse ETL tab.\nSelect the destination and the mapping you want to edit.\nSelect the \u2026 three dots and click Edit mapping. If you want to delete your mapping, select Delete.\n\nThis page was last modified: 18 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nStep 1: Add a source\nStep 2: Add a model\nStep 3: Add a destination\nStep 4: Create mappings\nInitial sync for a given mapping\nEdit Reverse ETL syncs\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nApi\n/\nPublic Api\n/\nDestination Filter Query Language\nDestination Filter Query Language\n\nThe Segment Public API is available\n\nSegment\u2019s Public API is available for Team and Business tier customers to use. You can use the Public API and Config APIs in parallel, but moving forward any API updates will come to the Public API exclusively.\n\nPlease contact your account team or friends@segment.com with any questions.\n\nThis reference provides a comprehensive overview of the Segment Destination Filter query language. For information on the Destination Filters API (including information on migrating from the Config API), visit the Destination Filters API reference.\n\nThe Transformations API uses Filter Query Language (FQL) to filter JSON objects and conditionally apply transformations. You can use FQL statements to:\n\nApply filters that evaluate to true or false based on the contents of each Segment event. If the statement evaluates to true, the transformation is applied, and if it is false the transformation is not applied.\nDefine new properties based on the result of an FQL statement.\n\nIn addition to boolean and equality operators like and and >=, FQL has built-in functions that make it more powerful such as contains( str, substr ) and match( str, pattern ).\n\nExamples\n\nGiven the following JSON object:\n\n{\n  \"event\": \"Button Clicked\",\n  \"type\": \"track\",\n  \"context\": {\n    \"library\": {\n      \"name\": \"analytics.js\",\n      \"version\": \"1.0\"\n    }\n  },\n  \"properties\": {\n    \"features\": [\"discounts\", \"dark-mode\"]\n  }\n}\n\n\nThe following FQL statements will evaluate as follows:\n\nFQL\tRESULT\nevent = 'Button Clicked'\ttrue\nevent = 'Screen Tapped'\tfalse\ncontext.path.path = '/login'\tfalse\ntype = 'identify' or type = 'track'\ttrue\nevent = 'Button Clicked' and type = 'track'\ttrue\nmatch( context.library.version, '1.*' )\ttrue\nmatch( context.library.version, '2.*' )\tfalse\ntype = 'track' and ( event = 'Click' or match( event, 'Button *' ) )\ttrue\n!contains( context.library.name, 'js' )\tfalse\n'dark-mode' in properties.features\ttrue\n'blink' in properties.features\tfalse\nField Paths\n\nFQL statements may refer to any field in the JSON object including top-level properties like userId or event as well as nested properties like context.library.version or properties.title using dot-separated paths. For example, the following fields can be pointed to by the associated field paths:\n\n{\n  \"type\": \"...\",       // type\n  \"event\": \"...\",      // event\n  \"context\": {         // context\n    \"library\": {       // context.library\n      \"name\": \"...\"    // context.library.name\n    },\n    \"page\": {          // context. page\n      \"path\": \"...\",   // context.page.path\n    }\n  }\n}\n\nEscaping Field Paths\n\nIf your field name has a character not in the set of {a-z A-Z 0-9 _ -}, you must escape it using a \\ character. For example, the nested field below can be referred to by properties.product\\ 1.price:\n\n{\n  \"properties\": {\n    \"product 1\": {\n      \"price\": \"19.99\"\n    }\n  }\n}\n\nOperators\nBoolean\nOPERATOR\tLEFT SIDE\tRIGHT SIDE\tRESULT\nand\tbool or null\tbool or null\ttrue if the left and right side are both true, false otherwise.\nor\tbool or null\tbool or null\ttrue if at least one side is true, false if either side is false or null.\nUnary\nOPERATOR\tRIGHT SIDE\tRESULT\n!\tbool\tNegates the right-hand side.\nComparison\nOPERATOR\tLEFT SIDE\tRIGHT SIDE\tRESULT\n=\tstring, number, list, bool, or null\tstring, number, list, bool, or null\ttrue if the left and right side are the same type and are strictly equal, false otherwise.\n!=\tstring, number, list, bool, or null\tstring, number, list, bool, or null\ttrue if the left and right side are different types or if they are not strictly equal, false otherwise.\n>\tnumber\tnumber\ttrue if the left side is greater than the right side.\n>=\tnumber\tnumber\ttrue if the left side is greater than or equal to the right side.\n<\tnumber\tnumber\ttrue if the left side is less than the right side.\n<=\tnumber\tnumber\ttrue if the left side is less than or equal to the right side.\nin\tstring, number, bool, or null\tlist\ttrue if the left side is contained in the list of values.\nSubexpressions\n\nYou can use parentheses to group subexpressions for more complex \u201cand / or\u201d logic as long as the subexpression evaluates to true or false:\n\nFQL\ntype = 'track' and ( event = 'Click' or match( 'Button *', event ) )\n( type = 'track' or type = 'identify' ) and ( properties.enabled or match( traits.email, '*@company.com' ) )\n!( type in ['track', 'identify'] )\nFunctions\nFUNCTION\tRETURN TYPE\tRESULT\ncontains( s string, sub string )\tbool\tReturns true if string s contains string sub.\nlength( list or string )\tnumber\tReturns the number of elements in a list or number of bytes (not necessarily characters) in a string. For example, a is 1 byte and\u30a2 is 3 bytes long. Please note that you can\u2019t use this function with JSON as the argument. Using JSON may result in the function not working.\nlowercase( s string )\tstring\tReturns s with all uppercase characters replaced with their lowercase equivalent.\nuppercase( s string )\tstring\tReturns s with all lowercase characters replaced with their uppercase equivalent.\nsnakecase( s string )\tstring\tReturns s with all space characters replaced by underscores. For example, kebabcase(\"test string\") returns test_string.\nkebabcase( s string )\tstring\tReturns s with all space characters replaced by dashes. For example, kebabcase(\"test string\") returns test-string.\ntitlecase( s string )\tstring\tReturns s with all space characters replaced by dashes. For example, titlecase(\"test string\") returns Test String.\ntypeof( value )\tstring\tReturns the type of the given value: \"string\", \"number\", \"list\", \"bool\", or \"null\".\nmatch( s string, pattern string )\tbool\tReturns true if the glob pattern pattern matches s. See below for more details about glob matching.\nbool( list or string or number or nil )\tbool\tConverts the value to a boolean value.\nstring( list or string or number or nil )\tstring\tConverts the value to a string value.\nnumber( number or string )\tnumber\tConverts the value to a number value.\n\nFunctions handle null with sensible defaults to make writing FQL more concise. For example, you can write length( userId ) > 0 instead of typeof( userId ) =\n'string' and length( userId ) > 0.\n\nFUNCTION\tRESULT\ncontains( null, string )\tfalse\nlength( null )\t0\nlowercase( null )\tnull\ntypeof( null )\t\"null\"\nmatch( null, string )\tfalse\nmatch( string, pattern )\n\nThe match( string, pattern ) function uses \u201cglob\u201d matching to return true if the given string fully matches a given pattern. Glob patterns are case sensitive. If you only need to determine if a string contains another string, you should use contains().\n\nPATTERN\tSUMMARY\n*\tMatches zero or more characters.\n?\tMatches one character.\n[abc]\tMatches one character in the given list. In this case, a, b, or c will be matched.\n[a-z]\tMatches a range of characters. In this case, any lowercase letter will be matched.\n\\x\tMatches the character x literally. This is useful if you need to match *, ? or ] literally. For example, \\*.\nPATTERN\tRESULT\tREASON\nmatch( 'abcd', 'a*d' )\ttrue\t* matches zero or more characters.\nmatch( '', '*' )\ttrue\t* matches zero or more characters.\nmatch( 'abc', 'ab' )\tfalse\tThe pattern must match the full string.\nmatch( 'abcd', 'a??d' )\ttrue\t? matches one character only.\nmatch( 'abcd', '*d' )\ttrue\t* matches one or more characters even at the beginning or end of the string.\nmatch( 'ab*d', 'ab\\*d' )\ttrue\t\\* matches the literal character *.\nmatch( 'abCd', 'ab[cC]d' )\ttrue\t[cC] matches either c or C.\nmatch( 'abcd', 'ab[a-z]d' )\ttrue\t[a-z] matches any character between a and z.\nmatch( 'abcd', 'ab[A-Z]d' )\tfalse\t[A-Z] matches any character between A and Z but c is not in that range because it is lowercase.\nError Handling\n\nIf your FQL statement is invalid (for example userId = oops\"), your Segment event will not be sent on to downstream Destinations. Segment defaults to not sending the event to ensure that invalid FQL doesn\u2019t cause sensitive information like PII to be incorrectly sent to Destinations.\n\nFor this reason, Segment recommends that you use the Destination Filters \u201cPreview\u201d API to test your filters without impacting your production data.\n\nThis page was last modified: 04 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nExamples\nField Paths\nOperators\nSubexpressions\nFunctions\nError Handling\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nReverse Etl\n/\nReverse ETL Catalog\nReverse ETL Catalog\n\nReverse ETL supports the Actions destinations listed in this catalog. Most destinations not listed here are supported through the Segment Connections destination.\n\nTwilio Engage Premier Subscriptions users can use the Segment Profiles destination to enrich their warehouse data.\n\nThe following destinations natively support Reverse ETL. If you don\u2019t see your destination listed in the Reverse ETL catalog, use the Segment Connections destination to send data from your Reverse ETL warehouse to other destinations listed in the catalog.\n\nActable Predictive\n\nBETA\n\nActions Pipedrive\n\nBETA\n\nAdobe Target Cloud Mode\nAggregations.io (Actions)\n\nBETA\n\nAirship (Actions)\nAlgolia Insights (Actions)\nAmplitude (Actions)\nAppFit\n\nBETA\n\nAttio (Actions)\n\nBETA\n\nAvo\nBlackbaud Raiser's Edge NXT\n\nBETA\n\nBlend Ai\n\nBETA\n\nBraze Cloud Mode (Actions)\nBraze Cohorts\nCanny (Actions)\nCleverTap (Actions)\nClose\nCordial (Actions)\nCriteo Audiences\nCustomer.io (Actions)\nEmarsys (Actions)\n\nBETA\n\nEncharge (Actions)\nEquals\n\nBETA\n\nFacebook Conversions API (Actions)\nFacebook Custom Audiences (Actions)\n\nBETA\n\nFriendbuy (Cloud Destination)\nFullstory Cloud Mode (Actions)\nGainsight Px Cloud (Actions)\nGleap (Action)\n\nBETA\n\nGoogle Ads Conversions\nGoogle Analytics 4 Cloud\nGoogle Sheets\nGWEN (Actions)\n\nBETA\n\nHubSpot Cloud Mode (Actions)\nInsider Audiences\nInsider Cloud Mode (Actions)\nIntercom Cloud Mode (Actions)\nIterable (Actions)\nJune (Actions)\nKafka\n\nBETA\n\nKlaviyo (Actions)\nKoala (Cloud)\nLaunchDarkly (Actions)\nLinkedIn Audiences\nListrak (Actions)\nLiveLike\n\nBETA\n\nLoops (Actions)\nMarketo Static Lists (Actions)\nMetronome (Actions)\nMixpanel (Actions)\nMoengage (Actions)\nMovable Ink (Actions)\nOptimizely Feature Experimentation (Actions)\nPardot (Actions)\nPinterest Conversions API\nPodscribe (Actions)\nPushwoosh\n\nBETA\n\nQualtrics\nRehook\n\nBETA\n\nRevX Cloud (Actions)\n\nBETA\n\nRipe Cloud Mode (Actions)\nSalesforce (Actions)\nSalesforce Marketing Cloud (Actions)\nSaleswings (Actions)\nSegment Connections\nSegment Profiles\nSendGrid\nSlack (Actions)\nSnapchat Conversions API\nStackAdapt\n\nBETA\n\nTalon.One (Actions)\nTikTok Conversions\nTiktok Offline Conversions\n\nBETA\n\nToplyne Cloud Mode (Actions)\n\nBETA\n\nUserpilot Cloud (Actions)\n\nBETA\n\nVoucherify (Actions)\n\nBETA\n\nVWO Cloud Mode (Actions)\nWebhooks (Actions)\nSegment Connections destination\n\nIf you don\u2019t see your destination listed in the Reverse ETL catalog, use the Segment Connections destination to send data from your Reverse ETL warehouse to other destinations listed in the catalog.\n\nThe Segment Connections destination enables you to mold data extracted from your warehouse in Segment Spec API calls that are then processed by Segment\u2019s HTTP Tracking API. The requests hit Segment\u2019s servers, and then Segment routes your data to any destination you want. Get started with the Segment Connections destination.\n\nThe Segment Connections destination sends data to Segment\u2019s Tracking API, which has cost implications. New users count as new MTUs and each call counts as an API call. For information on how Segment calculates MTUs and API calls, please see MTUs, Throughput and Billing.\n\nSend data to Engage with Segment Profiles\n\nEngage Premier Subscriptions users can use Reverse ETL to sync subscription data from warehouses to destinations.\n\nTo get started with using Reverse ETL for subscriptions:\n\nNavigate to Engage > Audiences and select the Profile explorer tab.\nClick Manage subscription statuses and select Update subscription statuses.\nSelect Sync with RETL as the method to update your subscription statuses.\nClick Configure.\nIn the Reverse ETL catalog, select the Reverse ETL source you want to use.\nSet up the source. Refer to the add a source section for more details on how to set up the source.\nAdd the Segment Profiles destination as your Reverse ETL destination. Refer to add a destination for more details on how to set up the destination.\nOnce your destination is set, go to the Mappings tab of your destination and click Add Mapping.\nSelect the model you want to use and then select Send Subscriptions.\nClick Create Mapping.\nFollow the steps in the Create Mappings section to set your mappings.\n\nThis page was last modified: 10 Sep 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSegment Connections destination\nSend data to Engage with Segment Profiles\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nIdentity Resolution\n/\nIdentity Resolution Settings\nIdentity Resolution Settings\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\n\nThe steps in this guide pertain to spaces created before September 27th, 2020. For spaces created after September 27th, 2020, please refer to the Identity Resolution Onboarding docs.\n\nConfigure Identity Graph rules\n\nBefore you connect a source to Unify, Segment recommends that you first review the default Identity settings and configure custom rules as needed. Segment applies configuration updates to all new data flowing through the space after you save your changes. As a result, if this is your first time setting up your Identity Graph, Segment recommends that you get started with a Dev space in the Space Setup docs.\n\nWorkspace owners and users with the Identity Admin role can edit the Identity Resolution table.\n\nChanging Identity Resolution rules\n\nMaking a space\u2019s Identity Resolution rules less restrictive by changing the limit shouldn\u2019t cause any issues to existing or future profiles.\n\nHowever, making a space\u2019s rules more restrictive might have an impact existing profiles that don\u2019t adhere to the new rules (for example, decreasing an identifier\u2019s limit or changing the priority of identifiers).\n\nSegment recommends to get started with a Dev space in the Space Setup docs, test the rules with the expected data, and then create an identical Production space with those rules. Document any changes to a space\u2019s Identity Resolution rules, and don\u2019t update rules to be more restrictive after profiles already exist outside the bounds of those new rules.\n\nExternalIDs\n\nSegment creates and merges user profiles based on externalIDs used as identifiers. You can view these externalIDs in the Identities tab of a User Profile in the Profile explorer.\n\nBy default, Segment promotes the following traits and IDs in track and identify calls to externalIDs:\n\nEXTERNAL ID TYPE\tMESSAGE LOCATION IN TRACK OR IDENTIFY CALL\nuser_id\tuserId\nemail\ttraits.email or context.traits.email\nandroid.id\tcontext.device.id when context.device.type = \u2018android\u2019\nandroid.idfa\tcontext.device.advertisingId when context.device.type = \u2018android\u2019 AND context.device.adTrackingEnabled = true\nandroid.push_token\tcontext.device.token when context.device.type = \u2018android\u2019\nanonymous_id\tanonymousId\nbraze_id\tcontext.Braze.braze_id or context.Braze.braze_id when Braze is connected as a destination\ncross_domain_id\tcross_domain_id when XID is enabled for the workspace\nga_client_id\tcontext.integrations[\u2018Google Analytics\u2019].clientId when explicitly captured by users\nios.id\tcontext.device.id when context.device.type = \u2018ios\u2019\nios.idfa\tcontext.device.advertisingId when context.device.type = \u2018ios\u2019 AND context.device.adTrackingEnabled = true\nios.push_token\tcontext.device.token when context.device.type = \u2018ios\u2019\n\nYou\u2019ll notice that these identifiers have the Default label next to it under Identifier Type.\n\nTo create your own custom externalID, click Add Identifier, and add the following:\n\nIdentifier Name\nValue Limit\nBlocked Values\n\nThese custom identifiers must be sent in the custom externalIds field in the context object of any call to the Segment API. The four fields below are all required:\n\nKEY\tVALUE\nid\tvalue of the externalID\ntype\tname of externalID type (app_id, ecommerce_id, shopify_id, and more)\ncollection\tusers if a user-level identifier or accounts if a group-level identifier\nencoding\tnone\n\nThe following example payload adds a custom phone externalID type:\n\nanalytics.track('Subscription Upgraded', {\n   plan: 'Pro',\n   mrr: 99.99\n}, {\n  externalIds: [\n    {\n      id: '123-456-7890',\n      type: 'phone',\n      collection: 'users',\n      encoding: 'none'\n    }\n  ]\n})\n\n\nSegment recommends that you add custom externalIDs to the Identity Resolution table before events containing this identifier flow through the space. Once an event with a new type of externalID flows into the space, the externalID is automatically added to the table if it wasn\u2019t manually added. When the externalID is automatically added, it defaults to the preset priority and limit, as explained below.\n\nFlat matching logic\n\nWhen a new event flows into Unify, Segment looks for profiles that match any of the identifiers on the event.\n\nBased on the existence of a match, one of three actions can occur:\n\n1: Create a new profile When there are no pre-existing profiles that have matching identifiers to the event, Segment creates a new user profile.\n\n2: Add to existing profile When there is one profile that matches all identifiers in an event, Segment attempts to map the traits, identifiers, and events on the call to that existing profile. If there\u2019s an excess of any identifier on the final profile, Segment defers to the Identity Resolution rules outlined below.\n\n3: Merge existing profiles When there are multiple profiles that match the identifiers in an event, Segment checks the Identity Resolution rules outlined below, and attempts to merge profiles.\n\nOne common example of a use-case that can cause inaccurate merges is the Shared iPad setup. For example, many companies now have iPads available in-store for customers to register for an account or submit order information. If different users submit information on the same device, there will now be multiple events sent with the same deviceID. Without Identity Resolution rules in place, Segment might see all these different users merged into the same user profile based on this common identifier.\n\nSegment\u2019s three Identity Resolution rules allow Identity Admins to block incorrect values from causing incorrect merges, to set the maximum number of values allowed per externalID, and to customize the priority of these externalIDs.\n\nIdentity Resolution rules\n\nThe following rules exist to increase the likelihood that identities are resolved correctly.\n\nBlocked values\n\nSegment recommends that you proactively block certain values from being used as identifiers. While these values will remain in the payload on the event itself, they are not promoted to the externalID object Segment uses to determine user profiles.\n\nThis is important when developers have a hard-coded value for fields like user_id during QA or development that then erroneously makes it production. This can cause hundreds of profiles to merge incorrectly and can have costly consequences when these spaces are already feeding data into a production email marketing tool or push notification tool downstream.\n\nIn the past, certain default values cause large amounts of profiles to merge incorrectly. Segment suggests that for every externalID, customers opt into automatically blocking the following suggested values:\n\nVALUE\tTYPE\nZeroes and Dashes (^[0-]*$)\tPattern (REGEX)\n-1\tExact Match\nnull\tExact Match\nanonymous\tExact Match\n\nBefore sending data through, Segment also recommends adding any default hard-coded values that your team uses during the development process, such as void or abc123.\n\nLimit\n\nIdentity Admins can specify the total number of values allowed per identifier type on a profile during a certain period. For example, in the image below, the anonymous_id field has a limit of 5 Weekly.\n\nThis will vary depending on how companies define a user today. In most cases, companies rely on user_id to distinguish user profiles and Segment defaults to the following configurations:\n\nIDENTIFIER\tLIMIT\nuser_id\t1\nall other identifiers\t5\n\nSpecific cases may deviate from this default. For example, a case where a user can have more than one user_id but one email, like when shopify_id and an internal UUID define a user. In this case, an example configuration may be:\n\nIDENTIFIER\tLIMIT\nemail\t1\nuser_id\t2\nall other identifiers\t5\n\nWhen you choose the limit on an identifier, ask the following questions about each of the identifiers you send to Segment:\n\nIs it an immutable ID? An immutable ID, such as user_id, should have 1 ever per user profile.\nIs it a constantly changing ID? A constantly changing ID, such as anonymous_id or ga_client_id, should have a short sliding window, such as 5 weekly or 5 monthly, depending on how often your application automatically logs out the user.\nIs it an ID that updates on a yearly basis? Most customers will have around five emails or devices at any one time, but can update these over time. For identifiers like email, android.id, or ios.id, Segment recommends a longer limit like 5 annually.\nPriority\n\nSegment considers the priority of an identifier once that identifier exceeds the limit on the final profile.\n\nFor example, consider a Unify space with the following Identity Resolution configurations:\n\nIDENTIFIER\tLIMIT\tPRIORITY\nuser_id\t1\t1\nemail\t5\t2\nanonymous_id\t5\t3\n\nA profile already exists with user_id abc123 and email jane@example1.com. A new event comes in with new user_id abc456 but the same email jane@example1.com.\n\nIf this event maps to this profile, the resulting profile would then contain two user_id values and one email. Given that user_id has a limit of 1, this exceeds the limit of that identifier. As a result, Segment checks the priority of the user_id identifier. Because email and user_id are the two identifiers on the event and email ranks lower than user_id, Segment demotes email as an identifier on the incoming event and tries again.\n\nAt this point, the event searches for any profiles that match just the identifier user_id abc456. Now there are no existing profiles with this identifier, so Segment creates a new profile with user_id abc456.\n\nBy default, Segment explicitly orders user_id and email as rank 1 and 2, respectively. All other identifiers are in alphabetical order beginning from rank 3. This means that if the identifiers sent with events flowing into profiles are user_id, email, anonymous_id, and ga_client_id, the rank would be as follows:\n\nIDENTIFIER\tPRIORITY\nuser_id\t1\nemail\t2\nanonymous_id\t3\nga_client_id\t4\n\nIf a new android.id identifier appeared without first giving it explicit order, the order would automatically reshuffle to:\n\nIDENTIFIER\tPRIORITY\nuser_id\t1\nemail\t2\nandroid.id\t3\nanonymous_id\t4\nga_client_id\t5\n\nIf you require an explicit order for all identifiers, configure this in the Identity Resolution settings page before sending in events.\n\nWhen choosing the priority of your identifier, ask the following questions about each of the identifiers you send to Segment:\n\nIs it an immutable ID? Give immutable IDs, such as user_id, highest priority.\nAre they unique IDs? Give Unique IDs such as email higher priority than possibly shared identifiers like android.id or ios.id.\nDoes it temporarily identify a user? Identifiers such as anonymous_id, ios.idfa, and ga_client_id are constantly updated or expired for a user. Generally speaking, rank these lower than identifiers that permanently identify a user.\n\nThis page was last modified: 07 Nov 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nConfigure Identity Graph rules\nExternalIDs\nFlat matching logic\nIdentity Resolution rules\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nUsage And Billing\n/\nAccount Management\nAccount Management\nWhat is the difference between an account and a workspace?\n\nAn account is associated to a single user and is attached to the email address you sign up with. A workspace houses all of your sources, and can have one or several user accounts as owners and/or read-only members.\n\nWhat if I change my domain name?\n\nYou don\u2019t need to do anything if you change your domain name. If the new domain name will serve as the same Segment source, make sure you use the same Segment write key that you used with the old domain.\n\nYou may claim ownership of your domain for the purposes of single sign-on login association, but it currently has no bearing on data collection.\n\nI\u2019m on a legacy API plan. Why can\u2019t I add the integration I want?\n\nSome of Segment\u2019s previous plans, including the legacy API plan, limited integration usage. If you want to add an integration that\u2019s not available on your current plan, move to a new Team plan. Team plans include all integrations, along with other additional features.\n\nWill deleting my account cancel my subscription?\n\nNo. Deleting your account only stops you from accessing workspaces through your login. The workspace is where the subscription is managed, and it will not be deleted. Data will still flow into Segment and your Destinations, and you will still be charged if you delete your account but don\u2019t delete your workspace.\n\nHow do I delete my account?\n\nTo delete your account, go to the User Settings menu, and click Delete Account at the bottom of the page.\n\nOnce the account is deleted you will not have access to workspaces associated with your account that are attached to the email address you signed up with.\n\nHow do I delete my workspace entirely?\n\nTo delete your workspace, go to your Workspace Settings, click the General tab, then click Delete Workspace.\n\nYou should also change your write keys for each source and remove all Segment snippets from your codebase.\n\nHow do I change my account email address?\n\nTo update the email address associated with your Segment account:\n\nGo to User Settings.\nUpdate your email address in the Email field.\nClick Update Profile.\n\nYou\u2019ll need to authenticate and verify your new email address for the change to take effect. Note that workspace owners can\u2019t make this change for other users.\n\nWhat happens if I change my workspace name or slug?\n\nChanging your workspace name or slug won\u2019t impact any sources or destinations you\u2019ve already configured. If you\u2019re using Segment\u2019s Public API, you\u2019ll need to change the slug in your request URLs.\n\nWe were unable to save your changes, please try again\n\nIf you see this error message when trying to change a workspace slug, it often means the slug is already taken.\n\nCan I recover a source or workspace after I delete it?\n\nNo. Deleted sources and workspaces cannot be recovered.\n\nCan I move a source from one workspace to another?\n\nThough workspaces can\u2019t be merged, you can move an existing source to a single workspace to the same effect. For example, you might move existing sources to one workspace so that you can unify all of your data across teams and gain a broader view of your customer data tracking.\n\nTo move a source between workspaces, navigate to the source\u2019s Settings tab, then click Transfer to Workspace. Choose the workspace you\u2019re moving the source to, then click Transfer Source.\n\nWhen you transfer a source from one workspace to another, Segment migrates all connected non-storage destinations.\n\nThe person who transfers the source must be a workspace owner for both the origin and recipient workspaces, otherwise the recipient workspace won\u2019t appear in the dropdown list.\n\nTracking Plans do not transfer\n\nSegment recommends that you disconnect Tracking Plans from Sources before you initiate a workspace transfer. Once the transfer is complete, add and reconnect your Tracking Plans in the new workspace.\n\nSources can't be transferred to EU workspaces\n\nThough transferring sources to the EU workspace is not blocked in the UI, the transfer will not work as expected. This feature is not supported for cross region migration. Segment recommends that you re-create the source in the new workspace.\n\nThis page was last modified: 05 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nWhat is the difference between an account and a workspace?\nWhat if I change my domain name?\nI\u2019m on a legacy API plan. Why can\u2019t I add the integration I want?\nWill deleting my account cancel my subscription?\nHow do I delete my account?\nHow do I delete my workspace entirely?\nHow do I change my account email address?\nWhat happens if I change my workspace name or slug?\nCan I recover a source or workspace after I delete it?\nCan I move a source from one workspace to another?\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nHow to Guides Index\nHow to Guides Index\n\nSegment\u2019s How-to Guides provide an in-depth walk through and examples of the many things you can do to implement, automate, engage with, and begin analyzing your data. We\u2019ve also got a series of Quickstart Guides for each of our Source libraries.\n\nImplementation\nWhat are best practices for identifying users?\nShould I collect data on the client or server?\nHow do I collect page views on the server side?\nHow do I import historical data?\nHow do I join user profiles?\nHow do I migrate code from other analytics tools?\nEngagement and Automation\nWhat role does Segment play in Attribution?\nHow do I automate multi-channel re-engagement campaigns?\nHow do I create a push notification?\nHow do we track customers across channels and devices?\nHow do I set up a dynamic coupon program to reward loyal customers?\nHow do we set up event-triggered notifications or alerts?\nAnalytics\nHow do I forecast Long Term Value with SQL and Excel for e-commerce?\nHow do I measure my advertising funnel?\nHow do I measure the ROI of my Marketing Campaigns?\nQuickstart Guides\nAnalytics.js (Javascript) Quickstart Guide\n.NET Quickstart Guide\nGo Library Quickstart Guide\nPython Library Quickstart Guide\nJava Library Quickstart Guide\nPHP Library Quickstart Guide\nNode.js Library Quickstart Guide\nRuby Library Quickstart Guide\niOS Quickstart Guide\nAndroid Quickstart Guide\n\nThis page was last modified: 25 Apr 2022\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nImplementation\nEngagement and Automation\nAnalytics\nQuickstart Guides\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSpec\n/\nSpec: AI Copilot\nSpec: AI Copilot\n\nThis page is a guide for developers who want to track interactions with AI copilots using Segment. It explains what data to send to Segment, letting you understand customer interactions with AI copilots.\n\nOverview\n\nAI copilots are like virtual assistants that help customers in conversations.\n\nEach conversation starts when a customer sends their first message or question. Throughout the conversation, Segment can track various events that capture key moments, like messages sent and received, tools invoked, and media generated.\n\nWhile some copilot conversations have clear ending points, which occur when the customer explicitly indicates that the conversation is over, the tracked events provide valuable insights into the entire conversation flow.\n\nTracked events\n\nIn this section, you\u2019ll find the tracked semantic events that serve as a starting point for AI copilot events. You can extend them based on your own requirements.\n\nThis table lists the events that you can track from any conversation:\n\nEVENT\tDEFINITION\tFIELDS\nConversation Started\tWhen a new conversation begins\tconversationId\nMessage Sent\tWhen the first message is added to a thread by user\tconversationId, messageId, message_body, role (default is \"customer\")\nMessage Received\tNon-custom response (text/voice) to user prompt by copilot\tconversationId, messageId, message_body, role (default is \"agent\")\nConversation Ended\tWhen a conversation is completed\tconversationId, message_count\nAction Invoked\tWhen the model or user invokes a capability or tool\tconversationId, messageId, type, action\nMedia Generated\tWhen the model generates an image/video/audio\tconversationId, messageId, type, sub_type\nComponent Loaded\tWhen a new custom (non-text/voice) component is shown to a user\tconversationId, messageId, type\nFeedback Submitted\tWhen a user rates a conversation or message\tconversationId, messageId, rating\nIdentify\tWhen a new user is identified anonymously or known\tuserId and/or anonymousId\nStandard Track Calls\tFor all events sent to Segment based on user actions taken, like items purchased, support requested\tconversationId, messageId, ...\nLive chat events\n\nSegment can also track the following live chat events:\n\nConversation Started\nMessage Sent\nMessage Received\nCustom Component Loaded\nAction Invoked\nMedia Generated\nConversation Ended\nEvent details\n\nThis section contains the structure and properties of each AI copilot tracked event.\n\nConversation Started\n\nThe Conversation Started event should be sent when a customer sends their first message.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\nconversationId\tstring\tThe conversation\u2019s unique identifier.\n\nHere\u2019s an example of a Conversation Started call:\n\n{\n  \"userId\": \"123\",\n  \"action\": \"track\",\n  \"event\": \"Conversation Started\",\n  \"properties\": {\n    \"conversationId\": \"1238041hdou\"\n  }\n}\n\nMessage Sent\n\nThe Message Sent event should be sent when a user adds a new message to a thread. The default for role is \"customer\".\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\nconversationId\tstring\tThe conversation\u2019s unique identifier.\nmessageId\tstring\tThe message\u2019s unique identifier.\nmessage_body\tstring\tThe message\u2019s content.\nrole\tstring\tThe message\u2019s sender; default is \"customer\".\n\nHere\u2019s an example of a Message Sent call:\n\n{\n  \"userId\": \"123\",\n  \"action\": \"track\",\n  \"event\": \"Message Sent\",\n  \"properties\": {\n    \"conversationId\": \"1238041hdou\",\n    \"messageId\": \"msg123\",\n    \"message_body\": \"What's the best stock in the Nasdaq right now?\",\n  \"role\": \"customer\"\n  }\n}\n\nMessage Received\n\nThe Message Received event should be sent when the copilot gives a non-custom response (either text or voice) to something the user asked.\n\nThe default for role is \"agent\". You can extend role to different agent type, like ai_agent, human_agent, task_automation_agent, and so on.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\nconversationId\tstring\tThe conversation\u2019s unique identifier.\nmessageId\tstring\tThe message\u2019s unique identifier.\nmessage_body\tstring\tThe received message\u2019s content.\nrole\tstring\tThe message\u2019s sender; default is \"agent\".\n{\n  \"userId\": \"123\",\n  \"action\": \"track\",\n  \"event\": \"Message Received\",\n  \"properties\": {\n    \"conversationId\": \"1238041hdou\",\n    \"messageId\": \"msg124\",\n    \"message_body\": \"Thank you for reaching out. How can I assist you today?\"\n  },\n  \"role\": \"agent\"\n}\n\nConversation Ended\n\nThe Conversation Ended event should be sent when a customer or agent explicitly indicates that the conversation has ended or deletes the chat.\n\nThis event supports the following semantic property:\n\nPROPERTY\tTYPE\tDESCRIPTION\nconversationId\tstring\tThe conversation\u2019s unique identifier.\n\nHere\u2019s an example of a Conversation Ended call:\n\n{\n  \"userId\": \"123\",\n  \"action\": \"track\",\n  \"event\": \"Conversation Ended\",\n  \"properties\": {\n    \"conversationId\": \"1238041hdou\"\n  }\n}\n\nAction Invoked\n\nThe Action nvoked event should be sent when the copilot or user uses a custom capability or tool, like making a call to an external API.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\nconversationId\tstring\tThe conversation\u2019s unique identifier.\nmessageId\tstring\tThe message\u2019s unique identifier.\ntype\tstring\tThe type of action invoked.\naction\tString\tThe specific action taken with the tool.\nrole\tstring\tThe message\u2019s sender; default is \"customer\".\n\nHere\u2019s an example of an Action Invoked call:\n\n{\n  \"userId\": \"123\",\n  \"action\": \"track\",\n  \"event\": \"Action Invoked\",\n  \"properties\": {\n    \"conversationId\": \"1238041hdou\",\n    \"messageId\": \"msg125\",\n    \"type\": \"Inventory Request\",\n    \"action\": \"check stock level\",\n  \"role\": \"customer\"\n  }\n}\n\nMedia Generated\n\nThis event should be sent when an image, video, or custom audio is generated by the model.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\nconversationId\tstring\tThe conversation\u2019s unique identifier.\nmessageId\tstring\tThe message\u2019s unique identifier.\ntype\tstring\tThe type of media generated (like \"image\", \"video\")\nsub_type\tString\tMedia data type (like \"gif\", \"mp4\", \"wav\")\nrole\tstring\tThe message\u2019s sender; default is \"agent\".\n\nHere\u2019s an example of a Media Generated call:\n\n{\n  \"userId\": \"123\",\n  \"action\": \"track\",\n  \"event\": \"Media Generated\",\n  \"properties\": {\n    \"conversationId\": \"1238041hdou\",\n    \"messageId\": \"msg126\",\n    \"role\": \"agent\",\n    \"type\": \"image\",\n    \"sub_type\": \"gif\"\n  }\n}\n\nComponent Loaded\n\nThis event should be sent when a new, custom component is shown to the user that isn\u2019t text or voice.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\nconversationId\tstring\tThe conversation\u2019s unique identifier.\nmessageId\tstring\tThe message\u2019s unique identifier.\ntype\tstring\tThe type of custom component loaded.\n\nHere\u2019s an example of a Component Loaded call:\n\n{\n  \"userId\": \"123\",\n  \"action\": \"track\",\n  \"event\": \"Component Loaded\",\n  \"properties\": {\n    \"conversationId\": \"1238041hdou\",\n    \"messageId\": \"msg127\",\n    \"type\": \"Stock Price Chart\"\n  }\n}\n\nFeedback Submitted\n\nThis event should be sent when a user rates a conversation or message.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\nconversationId\tstring\tThe conversation\u2019s unique identifier.\nmessageId\tstring\tThe message\u2019s unique identifier.\nrating\tnumber\tThe rating given by the user.\n\nHere\u2019s an example of a Feedback Submitted call:\n\n{\n  \"userId\": \"123\",\n  \"action\": \"track\",\n  \"event\": \"Feedback Submitted\",\n  \"properties\": {\n    \"conversationId\": \"1238041hdou\",\n    \"messageId\": \"msg128\",\n    \"rating\": 5\n  }\n}\n\nIdentify\n\nThis event should be sent when a new user is identified, either anonymously or as a known user.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\nuserId\tstring\tThe user\u2019s unique identifier.\nanonymousId\tstring\tThe user\u2019s anonymous identifier (if applicable).\n\nHere\u2019s an example of an Identify call:\n\n{\n  \"userId\": \"123\" || \"anonymousId\" : \"768923ihuy32\",\n  \"action\": \"identify\",\n  \"properties\":\n}\n\nStandard Track calls\n\nWhen sending events to Segment based on user actions, like items purchased or support requested, make sure to include relevant identifiers for accurate tracking and analysis.\n\nThese identifiers include conversationId and messageId, among others, depending on the specific tracked action:\n\nIDENTIFIER\tDESCRIPTION\nconversationId\tThe conversation\u2019s unique identifier. This identifier is crucial to tracking actions within a messaging or support context.\nmessageId\tThe message\u2019s unique identifier. This identifier is especially important for actions like messages read, media generated, or feedback submitted.\n\nFor example, to track an event where a user makes a purchase, the standard Track call could look like this:\n\n{\n  \"userId\": \"user123\",\n  \"action\": \"track\",\n  \"event\": \"Item Purchased\",\n  \"properties\": {\n    \"conversationId\": \"conv456\",\n    \"messageId\": \"msg789\",\n    \"itemId\": \"item101112\",\n    \"itemName\": \"Super Widget\",\n    \"itemPrice\": 19.99,\n    \"currency\": \"USD\"\n  }\n}\n\n\nThis page was last modified: 04 Apr 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nOverview\nTracked events\nEvent details\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nAudiences\n/\nGenerative Audiences\nGenerative Audiences\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nWith Generative Audiences, part of Segment\u2019s CustomerAI, use generative AI to create Engage Audiences with natural language prompts.\n\nDescribe your desired audience based on events performed, profile traits, or existing audiences in your workspace. Based on your prompt, Segment builds the audience with generative AI.\n\nFor more details on AI usage and data, see Generative Audiences Nutrition Facts Label.\n\nIn this article, you\u2019ll learn how to use Generative Audiences along with some best practices.\n\nCreate an audience with Generative Audiences\n\nTo create an audience with Generative Audiences:\n\nFrom the Segment app, navigate to Engage > Audiences.\nClick + New audience, then select Audience from the dropdown menu.\nSelect your audience type. Generative Audiences is available for all audience types except Linked Audiences.\nFrom the Build screen, click Build with AI.\nEnter your audience prompt in the description box.\nUse a minimum of 20 characters and up to 300 characters maximum.\nClick Build. Based on your prompt, CustomerAI generates audience conditions for your review.\nSegment displays a progress bar until the audience conditions are generated.\n\nTo help you write your prompt, view these example prompts and best practices.\n\nTo use Generative Audiences, a workspace owner must first accept the Customer AI Terms and Conditions.\n\nModify an audience description\n\nOnce Segment generates the audience conditions, the prompt box remains open for reference. You can close this box, or modify your audience description and click Build with AI again.\n\nModifying an audience description overwrites the existing conditions previously generated. You can also edit any conditions straight from the audience builder.\n\nExample prompts\n\nUse the following examples to help you get started with audience prompts.\n\nTo build an audience with customers who haven\u2019t made a purchase in the last 30 days, enter: Customers who haven't purchased in the last 30 days.\n\nTo find all profiles that have recently opened an email, enter: Profiles that recently opened an email.\n\nTo build an audience with customers who spend over $50 on an order, enter: Customers who have orders greater than $50.\n\nYou\u2019ll have more accurate results if you base your audience prompts on specific events and traits that are in your Segment space.\n\nUsing negative conditions\n\nBelow are a few examples of how CustomerAI configures audience conditions for negative prompts. Negative conditions might include, for example, building an audience of users without a certain profile trait, or who haven\u2019t performed certain events.\n\nPrompt: \u201cCustomers who have not purchased in the last 30 days.\u201d\nExpected output: Segment generates audience conditions where the event is performed at most 0 times.\nPrompt: \u201cCustomers who don\u2019t have a phone number.\u201d\nExpected output: Segment generates audience conditions where the trait doesn\u2019t exist.\nPrompt: \u201cCustomers who haven\u2019t received an email in the last 6 months.\u201d\nExpected output: Segment generates audience conditions where the event has been performed exactly 0 times.\nBest practices\n\nAs you use Generative Audiences, keep the following best practices in mind:\n\nAvoid using any customer Personal Identifiable Information (PII) or sensitive data. Personal, confidential, or sensitive information isn\u2019t required to use CustomerAI.\nWrite specific descriptions. CustomerAI generates more accurate conditions when you use the names of existing events and traits.\nEnsure that all events and traits you reference exist in your workspace.\nTry different prompts. If you don\u2019t receive what you want on the first try, rewrite your prompt. Submitting a new prompt replaces existing conditions.\nPreview your audience to ensure you\u2019re matching with the correct profiles prior to moving on to the next step.\nView events and traits in your workspace\n\nAs you\u2019re writing your prompt, you can view traits and events that are active in your workspace from the audience builder. After you add a condition in the builder, click the property field to view active and inactive traits or events in your workspace.\n\nYou can also use the Profile explorer (Unify > Profile explorer) to view specific events and traits associated with profiles in your Segment space.\n\nLearn more about using existing events and traits to build audiences.\n\nDue to a limited space schema, CustomerAI may not recognize some events or traits that are inactive in your workspace.\n\nError handling\n\nEngage uses the following error messages with Generative Audiences:\n\nERROR MESSAGE\tCAUSE\nSomething went wrong\tAn unknown exception occurred.\nSomething went wrong. Try again later.\tThe AI service is down, or the LLM returned an error.\nSegment had trouble creating an audience from this description. Try rewording it using these best practices.\tThe prompt referenced an invalid or non-existing trait, audience, or event within the workspace. You may also see this when an audience description is impossible to build or misunderstood.\nYour plan only supports a compute history of ## days.\tThe prompt is asking the audience to include a look back window greater than your workspace\u2019s event look back limit. Reword your prompt to include a look back window of less than the limit.\nKnown limitations\nLimited space schema\n\nSegment\u2019s generative AI service is handled by a third party that needs context about your Engage workspace and has limitations to how many contextual parameters Segment can send it. Segment solves this limitation by including only the most recently used properties and values for events and traits within your Engage space. As a result, some event and traits within your workspace may not be recognized.\n\nLanguage support\n\nAt this time, Segment only supports audience description prompts in the English language. Support in other languages is currently unavailable and might provide undesired results.\n\nThis page was last modified: 18 Jul 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nCreate an audience with Generative Audiences\nExample prompts\nBest practices\nError handling\nKnown limitations\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nEngage Settings\nEngage Settings\n\nThe Engage settings tab contains key information about the services you\u2019ve connected to your Engage space.\n\nChannels settings\n\nThe Engage settings Channels tab shows the credentials that you used to connect Engage with the SendGrid and Twilio services that power Engage campaigns.\n\nTo change the SendGrid or Twilio accounts associated with your Channels campaigns, select the pencil icon next to the service you need to modify. Engage then displays the email or SMS service setup screen with the fields that you\u2019ll need to edit.\n\nIf you see no credentials listed under the Channels tab, it means you\u2019ve not yet set up Channels; refer to the Twilio Engage onboarding page for instructions on how to connect Engage to both Twilio and SendGrid.\n\nDestinations settings\n\nThe Destinations tab lists the downstream tools receiving your Engage data. Selecting a destination from the list gives you a detailed view of the audiences, computed traits, and journeys that Segment sends to the destination.\n\nTo add a destination, select the + Add destination button, or navigate to Connections > Destinations within your Segment workspace. To learn more about sending Engage information to Segment destinations, view the Using Engage Data documentation.\n\nYou can delete a destination from the Destinations tab in the Engage settings (Engage > Engage settings > Destinations).\n\nWarehouse sources\n\nBy connecting your existing warehouses to Engage, you can import customer or account data and use it to build SQL traits. The Warehouse sources tab displays the warehouses sending data to Engage.\n\nTo add a new data warehouse, select the + Add warehouse source button. For more information on working with your imported warehouse data in Engage, read the Engage SQL traits guide.\n\nEngage Events source\n\nThe Engage Events source lets you sync subscription states, messaging events, and marketing analytics to downstream destinations. To find your Engage Events source in your Segment workspace, navigate to Connections > Sources and select Engage Events.\n\nFor more information, view the Engage Events Source documentation.\n\nThis page was last modified: 30 Jan 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nChannels settings\nDestinations settings\nWarehouse sources\nEngage Events source\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nProtocols\n/\nValidate\n/\nReview and Resolve Event Violations\nReview and Resolve Event Violations\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nPROTOCOLS \u2713\n?\n\nUpon connecting your Tracking Plan to a Source, you will be able to view violations grouped by event. To view violations, click on the Violations button located on the Schema tab in a Source. A filter can be applied to only show events with violations within the past hour, 24 hours, and 7 days.\n\nTo view detailed violations for an event, click on the specific event. Specific violations include:\n\nMissing required properties\nInvalid property value data types\nProperty values that do not pass applied conditional filtering\n\nIn the event detail violations view, a filter can be applied to only show violations in the past hour, 24 hours, and 7 days.\n\nTo view a specific violation, simply click on the violation to view recent sample payloads that generated the violation. These payloads can then be used to help engineering quickly pinpoint the root cause and release a fix.\n\nIf you want to analyze or build custom alerts based on inbound violations, you can enable violation forwarding here.\n\nTrack violations use the event field for aggregation, while Page and Screen violations use the name field. If these are not properly implemented in your events, violations cannot be aggregated correctly.\n\nThis page was last modified: 27 Feb 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nAnalytics Overview\nAnalytics Overview\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE PREMIER \u2713\n?\n\nTwilio Engage provides you with analytics that give you insight into the performance of your email, SMS, and WhatsApp campaigns.\n\nOn this page, you\u2019ll learn how Engage calculates campaign analytics and which messaging metrics you can view.\n\nAccess a campaign\u2019s analytics\n\nYou\u2019ll find a campaign\u2019s analytics within its parent Journey, using the following instructions:\n\nWithin your Space, select the Journeys tab.\nFrom the Journeys table, select the Journey you want to view.\nFrom the Journey overview, select the email, SMS, or WhatsApp campaign you want to view.\nA side panel appears that displays your campaign\u2019s analytics.\nHow Engage measures campaign analytics\n\nUnderstanding when and how Engage measures campaign analytics will help you as you review your analytics reports.\n\nEngage begins tracking campaign performance after you send a campaign. As a result, a campaign\u2019s analytics only reflect conversions that occurred after campaign publication. For example, suppose you send an email campaign promoting a sale in your online store. If a customer purchases a qualifying product before receiving your campaign, their purchase would not count as a conversion.\n\nChanging a campaign\u2019s base metrics\n\nClicking on a campaign in a Journey opens a side panel that shows your campaign\u2019s analytics. You can change both the date range and the base percentage type for any campaign.\n\nThe date range picker initially inherits the date range set in the campaign\u2019s parent Journey, but you can use the campaign date range picker to define a range for the specific campaign you\u2019re viewing. Changing a campaign\u2019s date range won\u2019t impact the parent Journey\u2019s date range.\n\nBy default, Engage bases a campaign\u2019s metrics on the number of sent messages. The Calculate metrics using dropdown lets you change this denominator value so that Engage bases analytics on the number of delivered messages. Changing the denominator won\u2019t impact the Converted metric, though, since conversions are already based on delivered messages.\n\nEmail metrics\n\nThe following table lists the email campaign metrics that Engage tracks:\n\nMETRIC\tDESCRIPTION\nSent\tThe number of emails campaigns that you sent.\nDelivered\tThe number of emails campaigns that were accepted by the receiving inbox server.\nTrue Opened\tThe number of times that your email campaigns were opened, not including machine opens.\nClicked\tThe number of times that recipients clicked within your email campaigns.\nConverted\tThe number of conversions that took place after campaign publication; based on delivered messages.\nClick-to-Open Rate\tThe number of clicks compared to the number of opens for a campaign.\nBounced\tThe number of email campaigns that bounced instead of being delivered.\nUnsubscribed\tThe number of campaign recipients who chose to unsubscribe from within the email campaigns.\nSpam Reported\tThe number of recipients who marked your email as spam.\n\nSendGrid powers Engage\u2019s email campaign event analytics. For more details on email metrics, view SendGrid\u2019s Marketing Campaigns Statistics Overview.\n\nUnderstanding machine opens\n\nMachine opens occur when an email client automatically opens an email, giving the impression that a user opened your email even if they haven\u2019t.\n\nSegment tracks machine opens and subtracts them from the total number of opened emails. Segment displays this number in the True Opened tile, which gives a more reliable count of how many real users opened your campaign.\n\nHover over the True Opened tile for any campaign to see a full comparison of machine opens and true opens.\n\nSMS metrics\n\nThe following table lists the SMS campaign metrics that Engage tracks:\n\nMETRIC\tDESCRIPTION\nQueued\tThe number of SMS campaigns queued for delivery, but not yet sent.\nSent\tThe number of SMS campaigns that you sent.\nDelivered\tThe number of SMS campaigns that were accepted by the user\u2019s carrier.\nUndelivered\tThe number of undelivered SMS campaigns.\nFailed\tThe number of SMS campaigns that didn\u2019t send.\nWhatsApp metrics\n\nThe following table lists the WhatsApp campaign metrics that Engage tracks:\n\nMETRIC\tDESCRIPTION\nQueued\tThe number of WhatsApp campaigns queued for delivery, but not yet sent.\nSent\tThe number of WhatsApp campaigns that you sent.\nDelivered\tThe number of WhatsApp campaigns delivered to the user\u2019s device.\nOpened\tThe number of opened WhatsApp campaigns, based on users who have turned on read receipts.\nUndelivered\tThe number of undelivered WhatsApp campaigns.\nFailed\tThe number of WhatsApp campaigns that didn\u2019t send.\n\nThis page was last modified: 21 Jun 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nAccess a campaign\u2019s analytics\nHow Engage measures campaign analytics\nEmail metrics\nSMS metrics\nWhatsApp metrics\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGetting Started\n/\nUse Cases Overview\nUse Cases Overview\n\nUse Cases are pre-built Segment setup guides tailored to common business goals.\n\nUse Cases eliminate guesswork with a structured approach to onboarding, helping you configure Segment correctly and align its features to your business objectives.\n\nYou can onboard to Segment with a Use Case if you\u2019re a new Business Tier customer or haven\u2019t yet connected a source and destination.\n\nOnboard to Segment with Use Cases\nChoosing a Use Case\n\nNot sure where to start? Read through Segment's Choosing a Use Case guide, which breaks down the available business goals and their associated use cases.\n\nStep-by-Step Use Cases Setup Guide\n\nFollow the steps in the Use Cases Setup guide to get up and running with Segment.\n\nUse Cases Reference\n\nLooking for something more technical? View the Use Cases Reference, which lists the tracking events, connections, and destinations Segment recommends for each use case.\n\nTake the next step\n\nExplore the following core Segment features, all of which power Use Cases.\n\nConnections\n\nCollect event data from your mobile apps, websites, and servers.\n\nDestinations\n\nForward your data to the business tools and apps your business uses.\n\nUnify\n\nTrack user interactions, resolve their identities, and explore Profiles.\n\nEngage\n\nBuild, enrich, and activate audiences with Segment's personalization platform.\n\nThis page was last modified: 08 Oct 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nOnboard to Segment with Use Cases\nTake the next step\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nHow To Guides\n/\nCreating a Push Notification\nCreating a Push Notification\n\nLike emails, push notifications are an extremely powerful way to re-engage customers on mobile apps. Push notifications are personal, so targeting them precisely using customer behavioral data (from Segment) is crucial.\n\nFor example,\u00a0Wanelo\u00a0accepts direct product feeds from retailers. For any of these retailers, when a product goes on sale, they can send a push notification to the people who have saved that product in their profile.\n\nPush messaging focuses around three key features:\n\nContent: Diversify your messaging just as you would with an investment portfolio. you want to target your consumers with right content and avoid opt out for push. For example, Netflix uses push notifications to let users know when their favorite shows are available. Rather than sending every user a notification every time any new show or season is released.\n\nFrequency:\u00a0Consider your App Store Category. News/Sports apps send push notifications daily or multiple times a day if it\u2019s \u201cgame day\u201d. So do Social Networking/Messaging apps. However, apps that are utilitarian, for example, food and drink, health and fitness, or productivity only message when necessary.\n\nTiming: Always send push notifications to users in their local timezone. In general, mobile usage peaks between 6pm - 10pm.\n\nChoose a destination\n\nSelf evaluate when trying to choose a destination that suits your needs.\n\nWhat\u2019s your user base size? Is it more than 10k? If not, you can try demo versions of mobile marketing automation libraries.\nAre you looking for a tool only to support push notification or provide an entire marketing suite?\nHow do push notifications create an impact in your app (engagement, retargeting, or social impact)?\nHow can deep links in push notifications fit into your app needs?\n\nYou will find many alternatives, but choosing the right one for your app is important!\n\nKey metrics for a successful push\nBuild trust with your user\n\nAsk users to opt in to push notifications upon app install or after the first time they use an app, so it\u2019s easier to be transparent about how users can opt out later.\n\nGive users control\n\nLet your customers decide what notifications they want to receive. It may help to break up your notifications into categories so you can empower your customers with this decision.\n\nCreate user segments\n\nCreating lists of your app users based on characteristics or events that align to specific campaigns will help you better target your mobile marketing efforts.\n\nPersonalize messages\n\nMake sure to use deep linking to guide users to the specific screen relevant to that offer.\n\nControl timing\n\nPay attention to user time zones and customize messages based on time of year (holidays) to make brand personable.\n\nRight frequency\n\nThe ideal frequency depends on the type of app you have.\n\nA/B test push messages\n\nTest different action words, phrases, message lengths, and more.\n\nMarketing automation\n\nTo \u201cauto-enroll\u201d new users into existing campaigns.\n\nMeasure the right metrics\n\nDon\u2019t silo the success of your campaign to just app opens.\n\nThis page was last modified: 10 Oct 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nPrivacy\n/\nConsent Management\n/\nFrequently Asked Questions\nFrequently Asked Questions\nFREE X\nTEAM X\nBUSINESS \u2713\nADDON X\n?\nIs Segment\u2019s Consent Manager part of Consent Management?\n\nNo. Segment\u2019s deprecated open-source Consent Manager, which captures end user cookie consent, is not part of Segment\u2019s Consent Management product, which focuses only on enforcing end user consent. Enforcing end user consent means sharing your end users\u2019 data with only the destinations they consented to share data with and blocking the flow of their data to all other destinations.\n\nSegment recommends moving from the deprecated, open-source Consent Manager to one that meets your legal compliance requirements.\n\nWhat destinations support consent enforcement?\n\nAll event streams destinations, with the exception of AWS S3 and Engage destinations, support consent enforcement.\n\nCan I share current end user consent preferences with my destinations?\n\nYou can use the Destination Actions framework to share the current status of your end-users\u2019 consent with your Actions destinations.\n\nFor more information, see the Sharing consent with Actions destinations documentation.\n\nCan I use a Consent Management Platform (CMP) other than OneTrust to collect consent from my end users?\n\nYes, you can use any commercially available CMP or custom solution to collect consent from your end users. If you use a CMP other than OneTrust, you must generate your own wrapper or other mechanism to add the following objects to the events collected from your sources:\n\nIncludes the consent object on every event\nGenerates the Segment Consent Preference Updated event every time a user provides or updates their consent preferences. This event must contain their anonymousId or userId.\n\nSegment provides guidance about creating your own wrapper in the @segment/analytics-consent-tools GitHub repository.\n\nThis page was last modified: 02 May 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nIs Segment\u2019s Consent Manager part of Consent Management?\nWhat destinations support consent enforcement?\nCan I share current end user consent preferences with my destinations?\nCan I use a Consent Management Platform (CMP) other than OneTrust to collect consent from my end users?\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nPrivacy\n/\nAccount & Data Deletion\nAccount & Data Deletion\n\nSegment allows you to delete specific data relating to an individual end user, all data from associated with a source, all data related to a Unify space, or all data in your entire workspace.\n\nDelete individual user data\n\nTo delete the data for an individual user from you workspace, follow the instructions on the User Deletion and Suppression page.\n\nDelete data from a source\n\nTo delete the data for an entire source, email the Customer Success team (friends@segment.com) to create a support ticket. In your email to Customer Success, include the following information:\n\nYour workplace slug\nThe source from which you\u2019d like to delete data\nThe time frame for the data you\u2019d like to delete*\n\n*Due to the way Segment stores data internally, source-level deletions can only be scoped to one day in granularity. Deletion requests for smaller time frames are not supported.\n\nDeleting source data\n\nWhen Segment deletes your data for a particular source, the deletion is not forwarded to sources or data storage providers associated with your account: your data is only removed from Segment\u2019s S3 archive buckets. To remove your data from external sources, reach out to the individual source about their deletion practices.\n\nDelete the data from a Unify space\n\nWorkspace Owners can delete a Unify space and all of its associated data by sending an email to the Customer Success team (friends@segment.com) to create a support ticket. In your email to Customer Success, include the following information:\n\nWorkspace slug\nUnify space name\n\nSegment waits for 5 calendar days after your request before starting a space deletion. If you want to cancel your Unify space deletion request, email the Customer Success team (friends@segment.com) during the first 5 calendar days after your initial request.\n\nData removed during a Unify space deletion\n\nWhen you delete a Unify space, Segment removes all profiles, computed traits, audiences, journeys, and other settings related to the Unify space from internal Segment servers. Unify space deletion doesn\u2019t delete data from connected Twilio Engage destinations. To remove your data from external destinations, reach out to the individual destination about their deletion practices.\n\nDelete your workspace data\n\nWorkspace admins can delete all of the data associated with a workspace, including customer data.\n\nTo delete all data from one workspace:\n\nSign in to the Segment app, select the workspace you\u2019d like to delete, and click Settings.\nOn the General Settings page, click the Delete Workspace button.\nFollow the prompts on the pop-up to delete your workspace.\n\nTo delete data from all workspaces in which you have workspace admin permissions:\n\nSign in to the Segment app.\nNavigate to the User Settings page.\nClick the Delete Account button, located at the bottom of the page.\nOn the popup, enter your password and select Yep, delete my account anyway! to delete your account.\n\nAfter you delete your workspace or account, Segment removes all data associated with each workspace within 30 days in a process called a complete data purge. For a data purge status update, email the Customer Success team (friends@segment.com).\n\nIf you don\u2019t delete your workspace after you stop using Segment, your data remains in Segment\u2019s internal servers until you submit a written deletion request.\n\nPurging data from workspaces deleted prior to March 31, 2022\n\nIf you deleted your workspace prior to March 31, 2022, and would like to have data associated with your workspace purged from Segment\u2019s S3 archive buckets, email the Customer Success team (friends@segment.com) to create a support ticket. In your email to Customer Success, include either the slug or the ID of the workspace you\u2019d like to have purged from internal Segment servers.\n\nWhat is a complete data purge?\n\nA complete data purge is the way Segment removes all workspace and customer data from internal servers across all product areas. To trigger a complete data purge, either delete your workspace or raise a support ticket with the Customer Success team by emailing (friends@segment.com). In your email to Customer Success, include either the slug or the ID of the workspace that you\u2019d like to delete. Deletions related to data purges will not be forwarded to your connected third-party destinations or raw data destinations.\n\nSegment waits for five calendar days before beginning a complete data purge to safeguard against malicious deletion requests. If you notice your workspace or account has been maliciously deleted, reach out to friends@segment.com to cancel the data purge. After the five-day grace period, the deletion will be irreversible.\n\nThis page was last modified: 03 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nDelete individual user data\nDelete data from a source\nDelete the data from a Unify space\nDelete your workspace data\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nHow To Guides\n/\nMeasuring the ROI of Your Marketing Campaigns\nMeasuring the ROI of Your Marketing Campaigns\n\nThe purpose of marketing campaigns is to drive traffic (and sales). But how do you know which campaigns yield the most conversions or what channel across the campaigns was most effective?\n\nThis guide provides you with the tools to answer these questions with SQL so that your marketing team can reproduce the hit campaigns and consistently generate loyal customers.\n\nTalk to a product specialist to learn how companies like Warby Parker and Crate & Barrel use a data warehouse to increase engagement and sales.\n\nAnalyze campaign performance\n\nThe goal of marketing campaigns is to drive engagement and conversions. Most commonly performed by attracting traffic to the site, these campaigns use UTM parameters for attribution. In our analysis, we\u2019ll be heavily relying on UTM parameters to analyze not only campaign, but also channel performance.\n\nLearn how to effectively use UTM parameters in your marketing campaign strategies.\n\nFor our analysis walkthrough, we\u2019ll use fictitious e-commerce and marketing data from on-demand artisanal toast company, Toastmates.\n\nToastmates is currently running these two campaigns:\n\n\u201cNational Toast Day\u201d, where $5 off was applied if you made a purchase on that day\n\u201cA Toast To Your Friend\u201d, where you can buy toast for a friend at $5 off\n\nEach of these campaigns used a combination of channels. Here is a table with the channels and corresponding UTM parameters so when we build the SQL query, we can make sure all of the traffic sources are accounted for.\n\nWe\u2019ll use SQL below to measure the performance of each campaign and what that means for future marketing activities.\n\nBuild the funnel\n\nThe following query creates a table where each row is a customer and the columns are the date time when a key funnel event happens that have the context_campaign_name to match that of the UTM_campaign . The key funnel events in this analysis are Store Visited(based on a page view to the store URL), Product Viewed , and Order Completed . Given that each channel may have some key top of the funnel action that is unique to itself, let\u2019s save that analysis for when we\u2019re analyzing across channels.\n\nFeel free to copy and paste the below query for your analysis so long as you replace national-toast-day with your own UTM campaign parameter.\n\n\u00a0 \u00a0 with\n\n\u00a0 \u00a0 users as (\n\u00a0 \u00a0 \u00a0 \u00a0 select \u00a0*\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 from \u00a0toastmates.users\n\u00a0 \u00a0 ),\n\n\u00a0 \u00a0 page_viewed as (\n\u00a0 \u00a0 \u00a0 \u00a0 select \u00a0p.received_at as page_viewed_at,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 p.context_campaign_name,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 p.user_id\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 from \u00a0toastmates.pages p\n\u00a0 \u00a0 \u00a0left join \u00a0users u\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on \u00a0u.id = p.user_id\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0where \u00a0p.context_campaign_name is not null\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0and \u00a0p.url ilike '%toastmates.com/store%'\n\u00a0 \u00a0 ),\n\n\u00a0 \u00a0 product_viewed as (\n\u00a0 \u00a0 \u00a0 \u00a0 select \u00a0v.received_at as product_viewed_at,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 v.context_campaign_name,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 v.user_id\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 from \u00a0toastmates.product_viewed v\n\u00a0 \u00a0 \u00a0left join \u00a0users u\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on \u00a0u.id = v.user_id\n\u00a0 \u00a0 ),\n\n\u00a0 \u00a0 order_completed as (\n\u00a0 \u00a0 \u00a0 \u00a0 select \u00a0c.received_at as order_completed_at,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 c.context_campaign_name,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 c.user_id\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 from \u00a0toastmates.order_completed c\n\u00a0 \u00a0 \u00a0left join \u00a0users u\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on \u00a0u.id = c.user_id\n\u00a0 \u00a0 )\n\n\u00a0 \u00a0 \u00a0 \u00a0 select \u00a0p.user_id as user_id,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 page_viewed_at,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 product_viewed_at,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 order_completed_at,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 p.context_campaign_name\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 from \u00a0page_viewed p\n\u00a0 \u00a0 \u00a0left join \u00a0product_viewed v\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on \u00a0p.user_id = v.user_id\n\u00a0 \u00a0 \u00a0left join \u00a0order_completed c\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on \u00a0p.user_id = l.user_id\n\u00a0 \u00a0 \u00a0 order by \u00a05 desc\n\n\nHere are the first four rows of the resulting table:\n\nThen, we can use tweak the query above into the one below to perform some simple COUNT and SUM on the previous table to get conversion metrics as well as total revenue derived from the campaign.\n\n\u00a0 \u00a0 with\n\n\u00a0 \u00a0 users as (\n\u00a0 \u00a0 \u00a0 \u00a0 select \u00a0*\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 from \u00a0toastmates.users\n\u00a0 \u00a0 ),\n\n\u00a0 \u00a0 page_viewed as (\n\u00a0 \u00a0 \u00a0 \u00a0 select \u00a0p.received_at as page_viewed_at,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 p.context_campaign_name,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 p.user_id\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 from \u00a0toastmates.pages p\n\u00a0 \u00a0 \u00a0left join \u00a0users u\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on \u00a0u.id = p.user_id\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0where \u00a0p.context_campaign_name is not null\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0and \u00a0p.url ilike '%toastmates.com/store%'\n\u00a0 \u00a0 ),\n\n\u00a0 \u00a0 product_viewed as (\n\u00a0 \u00a0 \u00a0 \u00a0 select \u00a0v.received_at as product_viewed_at,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 v.context_campaign_name,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 v.user_id\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 from \u00a0toastmates.product_viewed v\n\u00a0 \u00a0 \u00a0left join \u00a0users u\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on \u00a0u.id = v.user_id\n\u00a0 \u00a0 ),\n\n\u00a0 \u00a0 order_completed as (\n\u00a0 \u00a0 \u00a0 \u00a0 select \u00a0c.received_at as order_completed_at,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 c.context_campaign_name,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 c.total,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 c.user_id\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 from \u00a0toastmates.order_completed c\n\u00a0 \u00a0 \u00a0left join \u00a0users u\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on \u00a0u.id = c.user_id\n\u00a0 \u00a0 )\n\n\u00a0 \u00a0 \u00a0 \u00a0 select \u00a0p.context_campaign_name,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 count(page_viewed_at) as store_visits,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 count(product_viewed_at) as product_views,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 count(order_completed_at) as orders_completed,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sum(total) as total_revenue\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 from \u00a0page_viewed p\n\u00a0 \u00a0 \u00a0left join \u00a0product_viewed v\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on \u00a0p.user_id = v.user_id\n\u00a0 \u00a0 \u00a0left join \u00a0order_completed c\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on \u00a0p.user_id = l.user_id\n\u00a0 \u00a0 \u00a0 group by \u00a05\n\u00a0 \u00a0 \u00a0 order by \u00a05 desc\n\n\nHere is the resulting table:\n\nThis analysis not only gives us a great snapshot of the conversion points along each campaign\u2019s funnel, but also shows that we\u2019ve generated $3,100.37 from the National Toast Day campaign and $3,824.68 from the Toast Your Friend campaign. Also we can see that the quality of the traffic from the National Toast Day is higher, but we\u2019ve had more total traffic from Toast Your Friend, which makes sense since it\u2019s an ongoing campaign.\n\nBut this is not yet ROI, since we haven\u2019t incorporated the spend\u2014the labor of your marketing team and the paid acquisition channels to source part of this traffic\u2014that went into these channels.\n\nAdd campaign costs\n\nThe main costs that are incorporated in an ROI calculation are salaries (pro-rated by person-hour) and media spend. While we could conceivably create a custom, static table in SQL that contains the spend information over time, the faster and more practical way would be a back of the envelope calculation.\n\nThe costs associated with a given campaign consist of two major pieces: the person-hour cost and any associated media spend.\n\nCalculating the pro-rated person-hour is an estimate of the number of hours and people used to set up and manage the campaign, then multiplied by the hourly rates based off their annual salaries.\n\nThe media spend is the advertising cost for distributing creatives to generate traffic to your store\n\nWant to easily export advertising data from Google Adwords or Facebook Ads? Check out Segment Sources.\n\nWhen we have the aggregate cost numbers, the formula for ROI is:\n\nCampaign ROI = (Profit Attributed to Campaign \u2013 Campaign Cost) / Campaign Cost\n\n\nHere is a spreadsheet to illustrate the ROI calculation for both campaigns:\n\nThough ROI numbers are one success metric, it\u2019s an important benchmark for comparing performance when launching new campaigns or comparing against past campaigns.\n\nBut how can we go one step further and see what worked and what didn\u2019t? One approach is to see which channels convert better, so you know how to adjust your marketing spend or media buys in your current campaigns or future ones.\n\nAnalyze channel performance\n\nA single campaign can include a wide variety of channels: email, display ads, push notifications, forums, etc. all of which yields different engagement and conversion rates. Effective marketers will keep a pulse on each channel throughout the duration of the campaign to understand whether a target audience is being saturated, a creative refresh is needed (for advertising), or how to efficiently allocate future spend towards a source that converts.\n\nThe analysis is similar to measuring the performance across a single campaign, with the only change being finding events where we focus on context_campaign_medium or context_campaign_source instead of context_campaign_name . The SQL below measures the conversion rates at key funnel events for national-toast-day , but broken down by utm_medium .\n\nYou can copy the below into your favorite editor, as long as you change out the context_campaign_name and context_campaign_medium parameters to ones that applies to your business.\n\n\u00a0 \u00a0 with\n\n\u00a0 \u00a0 users as (\n\u00a0 \u00a0 \u00a0 \u00a0 select \u00a0*\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 from \u00a0toastmates.users\n\u00a0 \u00a0 ),\n\n\u00a0 \u00a0 page_viewed as (\n\u00a0 \u00a0 \u00a0 \u00a0 select \u00a0p.received_at as page_viewed_at,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 p.context_campaign_name,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 p.user_id\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 from \u00a0site.pages p\n\u00a0 \u00a0 \u00a0left join \u00a0users u\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on \u00a0u.id = p.user_id\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0where \u00a0p.context_campaign_name = 'national-toast-day'\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0and \u00a0p.context_campaign_medium is not null\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0and \u00a0p.url ilike '%toastmates.com/store%'\n\u00a0 \u00a0 ),\n\n\u00a0 \u00a0 product_viewed as (\n\u00a0 \u00a0 \u00a0 \u00a0 select \u00a0v.received_at as product_viewed_at,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 v.context_campaign_medium,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 v.user_id\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 from \u00a0toastmates.product_viewed v\n\u00a0 \u00a0 \u00a0left join \u00a0users u\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on \u00a0u.id = v.user_id\n\u00a0 \u00a0 ),\n\n\u00a0 \u00a0 order_completed as (\n\u00a0 \u00a0 \u00a0 \u00a0 select \u00a0c.received_at as order_completed_at,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 c.context_campaign_medium,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 c.user_id,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 c.total\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 from \u00a0toastmates.order_completed c\n\u00a0 \u00a0 \u00a0left join \u00a0users u\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on \u00a0u.id = c.user_id\n\u00a0 \u00a0 )\n\n\u00a0 \u00a0 \u00a0 \u00a0 select \u00a0p.context_campaign_medium as utm_medium,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 count(page_viewed_at) as store_visits,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 count(product_viewed_at) as product_views,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 count(order_completed_at) as orders_completed,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sum(c.total) as total_revenue\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 from \u00a0page_viewed p\n\u00a0 \u00a0 \u00a0left join \u00a0product_viewed_at v\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on \u00a0p.user_id = c.user_id\n\u00a0 \u00a0 \u00a0left join \u00a0order_completed c\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 on \u00a0p.user_id = c.user_id\n\u00a0 \u00a0 \u00a0 group by \u00a01\n\u00a0 \u00a0 \u00a0 order by \u00a01 desc\n\n\nThe resulting table:\n\nSince the National Toast Day campaign is relatively new, the majority of the traffic is from the email and an article (\u201cnews\u201d). But we can see that the social channels have a lower conversion from store visits to product views. Email has the best overall conversion to revenue, which may be attributed to the recipients already familiar with the Toastmates brand or having previously had a stellar end-to-end shopping experience.\n\nWe can further breakdown this analysis by seeing which email, display ads, and social channels performed the best, by adding utm_source and utm_content ,assuming that you\u2019ve properly added them in your earned and paid media links. Also note that this preliminary analysis in SQL doesn\u2019t account for double-counted users, who had impressions with our brand on multiple channels (e.g. someone seeing a display ad, yet converted on the email outreach). Fortunately, there are multi-touch attribution models that can be applied to better understand the weights of each activity towards conversion.\n\nLearn more about multi-touch attribution models.\n\nBuild repeatable hit marketing campaigns\n\nMeasuring the ROI and performance of marketing campaigns and marketing channels tells a compelling story about what types of campaigns resonate with your audience. How does your audience like to be engaged? Text, push notifications, email? What campaign messaging hooks work the best in getting them back at your store?\n\nYou can apply this analytical approach and performance measurement techniques to a wide variety of marketing activities, such as offline marketing, billboards, or sponsoring events. These insights can empower your team to focus on what works and eliminate what doesn\u2019t.\n\nTalk to a product specialist to learn how companies like Warby Parker and Crate & Barrel use a data warehouse to increase engagement and sales.\n\nThis page was last modified: 21 Apr 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nAnalyze campaign performance\nBuild the funnel\nAnalyze channel performance\nBuild repeatable hit marketing campaigns\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSpec\n/\nEcommerce\n/\nSpec: V2 Ecommerce Events\nSpec: V2 Ecommerce Events\n\nSegment\u2019s e-commerce spec helps define the journey for a customer as they browse your store, click on promotions, view products, add those products to a cart, and complete a purchase.\n\nNote\n\nNot all destinations support every event listed here and accept arrays as properties. Refer to individual destination documentation for more information on supported events and properties.\n\nEvent lifecycles\n\nHere is a list of supported events for our various categories within the customer journey.\n\nBrowsing overview\nACTION\tDESCRIPTION\nProducts Searched\tUser searched for products\nProduct List Viewed\tUser viewed a product list or category\nProduct List Filtered\tUser filtered a product list or category\nPromotions overview\nACTION\tDESCRIPTION\nPromotion Viewed\tUser viewed promotion\nPromotion Clicked\tUser clicked on promotion\nCore ordering overview\nACTION\tDESCRIPTION\nProduct Clicked\tUser clicked on a product\nProduct Viewed\tUser viewed a product details\nProduct Added\tUser added a product to their shopping cart\nProduct Removed\tUser removed a product from their shopping cart\nCart Viewed\tUser viewed their shopping cart\nCheckout Started\tUser initiated the order process (a transaction is created)\nCheckout Step Viewed\tUser viewed a checkout step\nCheckout Step Completed\tUser completed a checkout step\nPayment Info Entered\tUser added payment information\nOrder Completed\tUser completed the order\nOrder Updated\tUser updated the order\nOrder Refunded\tUser refunded the order\nOrder Cancelled\tUser cancelled the order\nCoupons overview\nACTION\tDESCRIPTION\nCoupon Entered\tUser entered a coupon on a shopping cart or order\nCoupon Applied\tCoupon was applied on a user\u2019s shopping cart or order\nCoupon Denied\tCoupon was denied from a user\u2019s shopping cart or order\nCoupon Removed\tUser removed a coupon from a cart or order\nWishlisting overview\nACTION\tDESCRIPTION\nProduct Added to Wishlist\tUser added a product to the wish list\nProduct Removed from Wishlist\tUser removed a product from the wish list\nWishlist Product Added to Cart\tUser added a wishlist product to the cart\nSharing overview\nACTION\tDESCRIPTION\nProduct Shared\tShared a product with one or more friends\nCart Shared\tShared the cart with one or more friends\nReviewing overview\nACTION\tDESCRIPTION\nProduct Reviewed\tUser reviewed a product\n\nThe following sections list more detail for each lifecycle event as well as an example API call.\n\nBrowsing\n\nBrowsing lifecycle events represent key events that a customer might have while browsing your apps.\n\nProducts Searched\n\nFire this event when a visitor searches for products.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\nquery\tString | Object\tQuery the user searched with\n\nExample:\n\nanalytics.track('Products Searched', {\n  query: 'blue roses'\n});\n\nProduct List Viewed\n\nFire this event when a visitor views a product list or category.\n\nNote\n\nNot all destinations accept arrays as properties. Refer to individual destination documentation for more information on supported events and properties.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\nlist_id\tString\tProduct list being viewed\ncategory\tString\tProduct category being viewed\nproducts\tArray<Product>\tProducts displayed in the product list\nproducts.$.product_id\tString\tProduct id displayed on the list\nproducts.$.sku\tString\tSku of the product being viewed\nproducts.$.category\tString\tProduct category being viewed\nproducts.$.name\tString\tName of the product being viewed\nproducts.$.brand\tString\tBrand associated with the product\nproducts.$.variant\tString\tVariant of the product\nproducts.$.price\tNumber\tPrice ($) of the product being viewed\nproducts.$.quantity\tNumber\tQuantity of a product\nproducts.$.coupon\tString\tCoupon code associated with a product (for example, MAY_DEALS_3)\nproducts.$.position\tNumber\tPosition in the product list (ex. 3)\nproducts.$.url\tString\tURL of the product page\nproducts.$.image_url\tString\tImage url of the product\n\nExample:\n\nanalytics.track('Product List Viewed', {\n  list_id: 'hot_deals_1',\n  category: 'Deals',\n  products: [\n    {\n      product_id: '507f1f77bcf86cd799439011',\n      sku: '45790-32',\n      name: 'Monopoly: 3rd Edition',\n      price: 19,\n      position: 1,\n      category: 'Games',\n      url: 'https://www.example.com/product/path',\n      image_url: 'https://www.example.com/product/path.jpg'\n    },\n    {\n      product_id: '505bd76785ebb509fc183733',\n      sku: '46493-32',\n      name: 'Uno Card Game',\n      price: 3,\n      position: 2,\n      category: 'Games'\n    }\n  ]\n});\n\n\nNote\n\nThe Product List Viewed event is aliased to the Viewed Product Category event (from e-commerce v1 spec).\n\nProduct List Filtered\n\nSend this event when a visitor filters a product list or category.\n\nNote\n\nNot all destinations accept arrays as properties. Refer to individual destination docs for more information on supported events and properties.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\nlist_id\tString\tProduct list being viewed\ncategory\tString\tProduct category being viewed\nfilters\tArray\tProduct filters that the customer is using\nfilters.$.type\tString\tId of the filter type that the customer is using\nfilters.$.value\tString\tId of the selection that the customer chose\nsorts\tArray<Sort>\tProduct sorting that the customer is using\nsorts.$.type\tString\tId of the sort type that the customer is using\nsorts.$.value\tString\tId of the selection type the the customer is using (ascending, descending)\nproducts\tArray\tProducts displayed in the product list\nproducts.$.product_id\tString\tProduct id displayed on the list\nproducts.$.sku\tString\tSku of the product being viewed\nproducts.$.category\tString\tProduct category being viewed\nproducts.$.name\tString\tName of the product being viewed\nproducts.$.brand\tString\tBrand associated with the product\nproducts.$.variant\tString\tVariant of the product\nproducts.$.price\tNumber\tPrice ($) of the product being viewed\nproducts.$.quantity\tNumber\tQuantity of a product\nproducts.$.coupon\tString\tCoupon code associated with a product (for example, MAY_DEALS_3)\nproducts.$.position\tNumber\tPosition in the product list (ex. 3)\nproducts.$.url\tString\tURL of the product page\nproducts.$.image_url\tString\tImage url of the product\n\nExample:\n\n\nanalytics.track('Product List Filtered', {\n  list_id: 'todays_deals_may_11_2019',\n  filters: [\n    {\n      type: 'department',\n      value: 'beauty'\n    },\n    {\n      type: 'price',\n      value: 'under-$25'\n    },\n  ],\n  sorts: [\n    {\n      type: 'price',\n      value: 'desc'\n    }\n  ],\n  products: [\n    {\n      product_id: '507f1f77bcf86cd798439011',\n      sku: '45360-32',\n      name: 'Special Facial Soap',\n      price: 12.60,\n      position: 1,\n      category: 'Beauty',\n      url: 'https://www.example.com/product/path',\n      image_url: 'https://www.example.com/product/path.jpg'\n    },\n    {\n      product_id: '505bd76785ebb509fc283733',\n      sku: '46573-32',\n      name: 'Fancy Hairbrush',\n      price: 7.60,\n      position: 2,\n      category: 'Beauty'\n    }\n  ]\n});\n\nPromotions\n\nPromotion view and click events help you gather analytics on internal offers within your web or mobile app. For example, when a banner advertisement is shown in your web or app\u2019s home page, you can fire a Viewed Promotion event. If the user proceeds to click the advertisement, fire the Clicked Promotion event.\n\nPromotion Viewed\n\nFire this event when a user views a promotion.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\tEXAMPLE\npromotion_id\tString\tPromotion\u2019s ID\tpromo_1\ncreative\tString\tPromotion\u2019s creative\ttop_banner_2\nname\tString\tPromotion\u2019s name\t75% store-wide shoe sale\nposition\tString\tPromotion\u2019s position\thome_banner_top\n\nExample:\n\nanalytics.track('Promotion Viewed', {\n  promotion_id: 'promo_1',\n  creative: 'top_banner_2',\n  name: '75% store-wide shoe sale',\n  position: 'home_banner_top'\n});\n\n\nNote\n\nThe Promotion Viewed event is aliased to the Viewed Promotion event.\n\nPromotion Clicked\n\nFire this event when a visitor clicks an internal offer promotion.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\tEXAMPLE\npromotion_id\tString\tPromotion\u2019s ID\tpromo_1\ncreative\tString\tPromotion\u2019s creative\ttop_banner_2\nname\tString\tPromotion\u2019s name\t75% store-wide shoe sale\nposition\tString\tPromotion\u2019s position\thome_banner_top\n\nExample:\n\nanalytics.track('Promotion Clicked', {\n  promotion_id: 'promo_1',\n  creative: 'top_banner_2',\n  name: '75% store-wide shoe sale',\n  position: 'home_banner_top'\n});\n\n\nNote\n\nThe Promotion Clicked event is aliased to the Clicked Promotion event.\n\nCore Ordering\n\nThese events represent the customer journey in regards to product ordering.\n\nProduct Clicked\n\nFire this event when a visitor clicks a product.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\nproduct_id\tString\tDatabase id of the product being viewed\nsku\tString\tSku of the product being viewed\ncategory\tString\tProduct category being viewed\nname\tString\tName of the product being viewed\nbrand\tString\tBrand associated with the product\nvariant\tString\tVariant of the product\nprice\tNumber\tPrice of the product being viewed\nquantity\tNumber\tQuantity of a product\ncoupon\tString\tCoupon code associated with a product (for example, MAY_DEALS_3)\nposition\tNumber\tPosition in the product list (ex. 3)\nurl\tString\tURL of the product page\nimage_url\tString\tImage url of the product\n\nExample:\n\nanalytics.track('Product Clicked', {\n  product_id: '507f1f77bcf86cd799439011',\n  sku: 'G-32',\n  category: 'Games',\n  name: 'Monopoly: 3rd Edition',\n  brand: 'Hasbro',\n  variant: '200 pieces',\n  price: 18.99,\n  quantity: 1,\n  coupon: 'MAYDEALS',\n  position: 3,\n  url: 'https://www.example.com/product/path',\n  image_url: 'https://www.example.com/product/path.jpg'\n});\n\n\nNote\n\nThe sku and product_id don\u2019t have to be different. If they are different, typically the product_id is a database identifier, like 9714107479 and the sku is a public-facing identifier like SEG-02.\nThe Product Clicked event is aliased to the Clicked Product event from e-commerce v1 spec.\nProduct Viewed\n\nFire this event when a visitor views a product. That view might happen on a page, screen, or preview modal.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\nproduct_id\tString\tDatabase id of the product being viewed\nsku\tString\tSku of the product being viewed\ncategory\tString\tProduct category being viewed\nname\tString\tName of the product being viewed\nbrand\tString\tBrand associated with the product\nvariant\tString\tVariant of the product\nprice\tNumber\tPrice ($) of the product being viewed\nquantity\tNumber\tQuantity of a product\ncoupon\tString\tCoupon code associated with a product (for example, MAY_DEALS_3)\ncurrency\tString\tCurrency of the transaction\nposition\tNumber\tPosition in the product list (ex. 3)\nvalue\tNumber\tTotal value of the product after quantity\nurl\tString\tURL of the product page\nimage_url\tString\tImage url of the product\n\nExample:\n\nanalytics.track('Product Viewed', {\n  product_id: '507f1f77bcf86cd799439011',\n  sku: 'G-32',\n  category: 'Games',\n  name: 'Monopoly: 3rd Edition',\n  brand: 'Hasbro',\n  variant: '200 pieces',\n  price: 18.99,\n  quantity: 1,\n  coupon: 'MAYDEALS',\n  currency: 'usd',\n  position: 3,\n  value: 18.99,\n  url: 'https://www.example.com/product/path',\n  image_url: 'https://www.example.com/product/path.jpg'\n});\n\n\nNote\n\nThe sku and product_id don\u2019t have to be different. If they are different, typically the product_id is a database identifier, like 9714107479 and the sku is a public-facing identifier like SEG-02.\nThe Product Viewed event is aliased to the Viewed Product event from e-commerce v1 spec.\nProduct Added\n\nFire this event when a visitor adds a product to their shopping cart.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\ncart_id\tString\tCart ID to which the product was added to\nproduct_id\tString\tDatabase id of the product being viewed\nsku\tString\tSku of the product being viewed\ncategory\tString\tProduct category being viewed\nname\tString\tName of the product being viewed\nbrand\tString\tBrand associated with the product\nvariant\tString\tVariant of the product\nprice\tNumber\tPrice ($) of the product being viewed\nquantity\tNumber\tQuantity of a product\ncoupon\tString\tCoupon code associated with a product (for example, MAY_DEALS_3)\nposition\tNumber\tPosition in the product list (ex. 3)\nurl\tString\tURL of the product page\nimage_url\tString\tImage url of the product\n\nExample:\n\nanalytics.track('Product Added', {\n  cart_id: 'skdjsidjsdkdj29j',\n  product_id: '507f1f77bcf86cd799439011',\n  sku: 'G-32',\n  category: 'Games',\n  name: 'Monopoly: 3rd Edition',\n  brand: 'Hasbro',\n  variant: '200 pieces',\n  price: 18.99,\n  quantity: 1,\n  coupon: 'MAYDEALS',\n  position: 3,\n  url: 'https://www.example.com/product/path',\n  image_url: 'https://www.example.com/product/path.jpg'\n});\n\n\nNote\n\nThe sku and product_id don\u2019t have to be different. If they are different, typically the product_id is a database identifier, like 9714107479 and the sku is a public-facing identifier like SEG-02.\nThe Product Added event is aliased to the Added Product event from e-commerce v1 spec.\nProduct Removed\n\nFire this event when a visitor removes a product from their shopping cart.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\ncart_id\tString\tCart ID to which the product was removed from\nproduct_id\tString\tDatabase id of the product being viewed\nsku\tString\tSku of the product being viewed\ncategory\tString\tProduct category being viewed\nname\tString\tName of the product being viewed\nbrand\tString\tBrand associated with the product\nvariant\tString\tVariant of the product\nprice\tNumber\tPrice ($) of the product being viewed\nquantity\tNumber\tQuantity of a product\ncoupon\tString\tCoupon code associated with a product (for example, MAY_DEALS_3)\nposition\tNumber\tPosition in the product list (ex. 3)\nurl\tString\tURL of the product page\nimage_url\tString\tImage url of the product\n\nExample:\n\nanalytics.track('Product Removed', {\n  cart_id: 'ksjdj92dj29dj92d2j',\n  product_id: '507f1f77bcf86cd799439011',\n  sku: 'G-32',\n  category: 'Games',\n  name: 'Monopoly: 3rd Edition',\n  brand: 'Hasbro',\n  variant: '200 pieces',\n  price: 18.99,\n  quantity: 1,\n  coupon: 'MAYDEALS',\n  position: 3,\n  url: 'https://www.example.com/product/path',\n  image_url: 'https://www.example.com/product/path.jpg'\n});\n\n\nNote\n\nThe sku and product_id don\u2019t have to be different. If they are different, typically the product_id is a database identifier, like 9714107479 and the sku is a public-facing identifier like SEG-02.\nThe Product Removed event is aliased to the Removed Product event from e-commerce v1 spec.\nCart Viewed\n\nFire this event when a visitor views a shopping cart.\n\nNote\n\nNot all destinations accept arrays as properties. Refer to individual destination documentation for more information on supported events and properties.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\ncart_id\tString\tShopping cart ID\nproducts\tArray\tProducts displayed in the product list\nproducts.$.product_id\tString\tProduct ID displayed on the list\nproducts.$.sku\tString\tSku of the product being viewed\nproducts.$.category\tString\tProduct category being viewed\nproducts.$.name\tString\tName of the product being viewed\nproducts.$.brand\tString\tBrand associated with the product\nproducts.$.variant\tString\tVariant of the product\nproducts.$.price\tNumber\tPrice ($) of the product being viewed\nproducts.$.quantity\tNumber\tQuantity of a product\nproducts.$.coupon\tString\tCoupon code associated with a product (for example, MAY_DEALS_3)\nproducts.$.position\tNumber\tPosition in the product list (ex. 3)\nproducts.$.url\tString\tURL of the product page\nproducts.$.image_url\tString\tImage url of the product\n\nExample:\n\nanalytics.track('Cart Viewed', {\n  cart_id: 'd92jd29jd92jd29j92d92jd',\n  products: [\n    {\n      product_id: '507f1f77bcf86cd799439011',\n      sku: '45790-32',\n      name: 'Monopoly: 3rd Edition',\n      price: 19,\n      position: 1,\n      category: 'Games',\n      url: 'https://www.example.com/product/path',\n      image_url: 'https://www.example.com/product/path.jpg'\n    },\n    {\n      product_id: '505bd76785ebb509fc183733',\n      sku: '46493-32',\n      name: 'Uno Card Game',\n      price: 3,\n      position: 2,\n      category: 'Games'\n    }\n  ]\n});\n\nCheckout Started\n\nFire this event whenever an order/transaction was started. Fire on the page that the customer lands on after they press the checkout button.\n\nNote\n\nNot all destinations accept arrays as properties. Refer to individual destination documentation for more information on supported events and properties.\n\nBe sure to include all items in the cart as event properties, with the same properties from the previous calls, like so:\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\norder_id\tString\tOrder/transaction ID\naffiliation\tString\tStore or affiliation from which this transaction occurred (for example, Google Store)\nvalue\tNumber\tRevenue ($) with discounts and coupons added in. For better flexibility and total control over tracking, we let you decide how to calculate how coupons and discounts are applied\nrevenue\tNumber\tRevenue ($) associated with the transaction (excluding shipping and tax)\nshipping\tNumber\tShipping cost associated with the transaction\ntax\tNumber\tTotal tax associated with the transaction\ndiscount\tNumber\tTotal discount associated with the transaction\ncoupon\tString\tTransaction coupon redeemed with the transaction\ncurrency\tString\tCurrency code associated with the transaction\nproducts\tArray\tProducts in the order\nproducts.$.product_id\tString\tDatabase id of the product being viewed\nproducts.$.sku\tString\tSku of the product being viewed\nproducts.$.category\tString\tProduct category being viewed\nproducts.$.name\tString\tName of the product being viewed\nproducts.$.brand\tString\tBrand associated with the product\nproducts.$.variant\tString\tVariant of the product\nproducts.$.price\tNumber\tPrice ($) of the product being viewed\nproducts.$.quantity\tNumber\tQuantity of a product\nproducts.$.coupon\tString\tCoupon code associated with a product (for example, MAY_DEALS_3)\nproducts.$.position\tNumber\tPosition in the product list (ex. 3)\nproducts.$.url\tString\tURL of the product page\nproducts.$.image_url\tString\tImage url of the product\n\nExample:\n\nanalytics.track('Checkout Started', {\n  order_id: '50314b8e9bcf000000000000',\n  affiliation: 'Google Store',\n  value: 30,\n  revenue: 25.00,\n  shipping: 3,\n  tax: 2,\n  discount: 2.5,\n  coupon: 'hasbros',\n  currency: 'USD',\n  products: [\n    {\n      product_id: '507f1f77bcf86cd799439011',\n      sku: '45790-32',\n      name: 'Monopoly: 3rd Edition',\n      price: 19,\n      quantity: 1,\n      category: 'Games',\n      url: 'https://www.example.com/product/path',\n      image_url: 'https://www.example.com/product/path.jpg'\n    },\n    {\n      product_id: '505bd76785ebb509fc183733',\n      sku: '46493-32',\n      name: 'Uno Card Game',\n      price: 3,\n      quantity: 2,\n      category: 'Games'\n    }\n  ]\n});\n\n\nNote\n\nThe sku and product_id don\u2019t have to be different. If they are different, typically the product_id is a database identifier, like 9714107479 and the sku is a public-facing identifier like SEG-02.\nThe Checkout Started event is aliased to the Started Order event from Segment\u2019s GA Enhanced E-Commerce destinations.\nCheckout Step Viewed\n\nFire this event whenever a checkout step is viewed.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\ncheckout_id\tString\tCheckout transaction ID\nstep\tNumber\tNumber representing a step in the checkout process\nshipping_method\tString\tString representing the shipping the method chosen\npayment_method\tString\tString representing the payment method chosen\n\nExample:\n\nanalytics.track('Checkout Step Viewed', {\n  checkout_id: '50314b8e9bcf000000000000',\n  step: 2,\n  shipping_method: 'Fedex',\n  payment_method: 'Visa'\n});\n\n\nNote\n\nshipping_method and payment_method are semantic properties. If you want to send that information, do so in this exact spelling.\n\nYou can have as many or as few steps in the checkout funnel as you\u2019d like. Note that you\u2019ll still need to track the Order Completed event per Segment\u2019s standard e-commerce tracking API after you\u2019ve tracked the checkout steps.\n\nNote\n\nThe Checkout Step Viewed event is aliased to the Viewed Checkout Step event from Segment\u2019s GA Enhanced E-Commerce destinations.\n\nCheckout Step Completed\n\nFire this event whenever a checkout step is completed.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\ncheckout_id\tString\tCheckout transaction ID\nstep\tNumber\tNumber representing a step in the checkout process\nshipping_method\tString\tString representing the shipping the method chosen\npayment_method\tString\tString representing the payment method chosen\n\nExample:\n\nanalytics.track('Checkout Step Completed', {\n  checkout_id: '50314b8e9bcf000000000000',\n  step: 2,\n  shipping_method: 'Fedex',\n  payment_method: 'Visa'\n});\n\n\nNote\n\nshipping_method and payment_method are semantic properties. If you want to send that information, do so in this exact spelling.\n\nYou can have as many or as few steps in the checkout funnel as you\u2019d like. Note that you\u2019ll still need to track the Order Completed event per Segment\u2019s standard e-commerce tracking API after you\u2019ve tracked the checkout steps.\n\nNote\n\nThe Checkout Step Completed event is aliased to the Completed Checkout Step event from Segment\u2019s GA Enhanced E-Commerce destinations.\n\nPayment Info Entered\n\nFire this event whenever payment information has been successfully entered.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\ncheckout_id\tString\tCheckout transaction ID\norder_id\tString\tOrder ID (optional)\nstep\tNumber\tNumber representing a step in the checkout process\nshipping_method\tString\tString representing the shipping the method chosen\npayment_method\tString\tString representing the payment method chosen\n\nExample:\n\nanalytics.track('Payment Info Entered', {\n  checkout_id: '39f39fj39f3jf93fj9fj39fj3f',\n  order_id: 'dkfsjidfjsdifsdfksdjfkdsfjsdfkdsf'\n});\n\n\nNote\n\nshipping_method and payment_method are semantic properties. If you want to send that information, do so in this exact spelling.\n\nYou can have as many or as few steps in the checkout funnel as you\u2019d like. Note that you\u2019ll still need to track the Order Completed event per Segment\u2019s standard e-commerce tracking API after you\u2019ve tracked the checkout steps.\n\nOrder Updated\n\nFire this event whenever an order/transaction was updated.\n\nNote\n\nNot all destinations accept arrays as properties. Refer to individual destination documentation for more information on supported events and properties.\n\nBe sure to include all items in the cart as event properties, with the same properties from the previous calls, like so:\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\norder_id\tString\tOrder/transaction ID\naffiliation\tString\tStore or affiliation from which this transaction occurred (for example, Google Store)\ntotal\tNumber\tRevenue ($) with discounts and coupons added in\n\nNote that our Google Analytics Ecommerce destination accepts total or revenue, but not both. For better flexibility and total control over tracking, we let you decide how to calculate how coupons and discounts are applied\nrevenue\tNumber\tRevenue ($) associated with the transaction (excluding shipping and tax)\nshipping\tNumber\tShipping cost associated with the transaction\ntax\tNumber\tTotal tax associated with the transaction\ndiscount\tNumber\tTotal discount associated with the transaction\ncoupon\tString\tTransaction coupon redeemed with the transaction\ncurrency\tString\tCurrency code associated with the transaction\nproducts\tArray\tProducts in the order\nproducts.$.product_id\tString\tDatabase id of the product being viewed\nproducts.$.sku\tString\tSku of the product being viewed\nproducts.$.category\tString\tProduct category being viewed\nproducts.$.name\tString\tName of the product being viewed\nproducts.$.brand\tString\tBrand associated with the product\nproducts.$.variant\tString\tVariant of the product\nproducts.$.price\tNumber\tPrice ($) of the product being viewed\nproducts.$.quantity\tNumber\tQuantity of a product\nproducts.$.coupon\tString\tCoupon code associated with a product (for example, MAY_DEALS_3)\nproducts.$.position\tNumber\tPosition in the product list (ex. 3)\nproducts.$.url\tString\tURL of the product page\nproducts.$.image_url\tString\tImage url of the product\n\nExample:\n\nanalytics.track('Order Updated', {\n      order_id: '50314b8e9bcf000000000000',\n      affiliation: 'Google Store',\n      total: 27.50,\n      revenue: 25.00,\n      shipping: 3,\n      tax: 2,\n      discount: 2.5,\n      coupon: 'hasbros',\n      currency: 'USD',\n      products: [\n        {\n          product_id: '507f1f77bcf86cd799439011',\n          sku: '45790-32',\n          name: 'Monopoly: 3rd Edition',\n          price: 19,\n          quantity: 1,\n          category: 'Games',\n          url: 'https://www.example.com/product/path',\n  image_url: 'https://www.example.com/product/path.jpg'\n        },\n        {\n          product_id: '505bd76785ebb509fc183733',\n          sku: '46493-32',\n          name: 'Uno Card Game',\n          price: 3,\n          quantity: 2,\n          category: 'Games'\n        }\n      ]\n    });\n\n\nNote\n\nThe sku and product_id don\u2019t have to be different. If they are different, typically the product_id is a database identifier, like 9714107479 and the sku is a public-facing identifier like SEG-02.\nThe Order Updated event is aliased to the Updated Order event from Segment\u2019s GA Enhanced E-Commerce destinations.\nOrder Completed\n\nFire this event whenever an order/transaction was successfully completed by the customer.\n\nNote\n\nNot all destinations accept arrays as properties. Refer to individual destination documentation for more information on supported events and properties.\n\nBe sure to include all items in the cart as event properties, with the same properties from the previous calls, like so:\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\ncheckout_id\tString\tCheckout ID\norder_id\tString\tOrder/transaction ID\naffiliation\tString\tStore or affiliation from which this transaction occurred (for example, Google Store)\nsubtotal\tNumber\tOrder total after discounts but before taxes and shipping\ntotal\tNumber\tSubtotal ($) with shipping and taxes added in. Note that our Google Analytics Ecommerce destination accepts total or revenue, but not both. For better flexibility and total control over tracking, we let you decide how to calculate how coupons and discounts are applied\nrevenue\tNumber\tRevenue ($) associated with the transaction (including discounts, but excluding shipping and taxes)\nshipping\tNumber\tShipping cost associated with the transaction\ntax\tNumber\tTotal tax associated with the transaction\ndiscount\tNumber\tTotal discount associated with the transaction\ncoupon\tString\tTransaction coupon redeemed with the transaction\ncurrency\tString\tCurrency code associated with the transaction\nproducts\tArray\tProducts in the order\nproducts.$.product_id\tString\tDatabase id of the product being viewed\nproducts.$.sku\tString\tSku of the product being viewed\nproducts.$.category\tString\tProduct category being viewed\nproducts.$.name\tString\tName of the product being viewed\nproducts.$.brand\tString\tBrand associated with the product\nproducts.$.variant\tString\tVariant of the product\nproducts.$.price\tNumber\tPrice ($) of the product being viewed\nproducts.$.quantity\tNumber\tQuantity of a product\nproducts.$.coupon\tString\tCoupon code associated with a product (for example, MAY_DEALS_3)\nproducts.$.position\tNumber\tPosition in the product list (ex. 3)\nproducts.$.url\tString\tURL of the product page\nproducts.$.image_url\tString\tImage url of the product\n\nExample:\n\nanalytics.track('Order Completed', {\n  checkout_id: 'fksdjfsdjfisjf9sdfjsd9f',\n  order_id: '50314b8e9bcf000000000000',\n  affiliation: 'Google Store',\n  total: 27.50,\n  subtotal: 22.50,\n  revenue: 22.50,\n  shipping: 3,\n  tax: 2,\n  discount: 2.5,\n  coupon: 'hasbros',\n  currency: 'USD',\n  products: [\n    {\n      product_id: '507f1f77bcf86cd799439011',\n      sku: '45790-32',\n      name: 'Monopoly: 3rd Edition',\n      price: 19,\n      quantity: 1,\n      category: 'Games',\n      url: 'https://www.example.com/product/path',\n      image_url: 'https:///www.example.com/product/path.jpg'\n    },\n    {\n      product_id: '505bd76785ebb509fc183733',\n      sku: '46493-32',\n      name: 'Uno Card Game',\n      price: 3,\n      quantity: 2,\n      category: 'Games'\n    }\n  ]\n});\n\n\nNote\n\nThe sku and product_id don\u2019t have to be different. If they are different, typically the product_id is a database identifier, like 9714107479 and the sku is a public-facing identifier like SEG-02.\nThe Order Completed event is aliased to the Completed Order event from E-Commerce spec v1 - 5/11/16.\nOrder Refunded\n\nFire this event whenever an order/transaction was refunded.\n\nBe sure to include all items in the cart as event properties, with the same properties from the previous \u201cOrder Completed\u201d call.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\norder_id\tString\tOrder/transaction ID\n\nExample:\n\nanalytics.track('Order Refunded', {\n  order_id: '50314b8e9bcf000000000000',\n  total: 30,\n  currency: 'USD',\n  products: [\n    {\n      product_id: '507f1f77bcf86cd799439011',\n      sku: '45790-32',\n      name: 'Monopoly: 3rd Edition',\n      price: 19,\n      quantity: 1,\n      category: 'Games',\n      url: 'https://www.example.com/product/path',\n      image_url: 'https://www.example.com/product/path.jpg'\n    },\n    {\n      product_id: '505bd76785ebb509fc183733',\n      sku: '46493-32',\n      name: 'Uno Card Game',\n      price: 3,\n      quantity: 2,\n      category: 'Games'\n    }\n  ]\n});\n\n\nNote\n\nThe sku and product_id don\u2019t have to be different. If they are different, typically the product_id is a database identifier, like 9714107479 and the sku is a public-facing identifier like SEG-02.\n\nOrder Cancelled\n\nFire this event whenever an order/transaction was cancelled.\n\nNote\n\nNot all destinations accept arrays as properties. Refer to individual destination documentation for more information on supported events and properties.\n\nBe sure to include all items in the cart as event properties, with the same properties from the previous calls.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\norder_id\tString\tOrder/transaction ID\naffiliation\tString\tStore or affiliation from which this transaction occurred (for example, Google Store)\ntotal\tNumber\tRevenue ($) with discounts and coupons added in.\nrevenue\tNumber\tRevenue ($) associated with the transaction (excluding shipping and tax)\nshipping\tNumber\tShipping cost associated with the transaction\ntax\tNumber\tTotal tax associated with the transaction\ndiscount\tNumber\tTotal discount associated with the transaction\ncoupon\tString\tTransaction coupon redeemed with the transaction\ncurrency\tString\tCurrency code associated with the transaction\nproducts\tArray\tProducts in the order\nproducts.$.product_id\tString\tDatabase id of the product being viewed\nproducts.$.sku\tString\tSku of the product being viewed\nproducts.$.category\tString\tProduct category being viewed\nproducts.$.name\tString\tName of the product being viewed\nproducts.$.brand\tString\tBrand associated with the product\nproducts.$.variant\tString\tVariant of the product\nproducts.$.price\tNumber\tPrice ($) of the product being viewed\nproducts.$.quantity\tNumber\tQuantity of a product\nproducts.$.coupon\tString\tCoupon code associated with a product (for example, MAY_DEALS_3)\nproducts.$.position\tNumber\tPosition in the product list (ex. 3)\nproducts.$.url\tString\tURL of the product page\nproducts.$.image_url\tString\tImage url of the product\n\nExample:\n\nanalytics.track('Order Cancelled', {\n  order_id: '50314b8e9bcf000000000000',\n  affiliation: 'Google Store',\n  total: 30,\n  revenue: 25.00,\n  shipping: 3,\n  tax: 2,\n  discount: 2.5,\n  coupon: 'hasbros',\n  currency: 'USD',\n  products: [\n    {\n      product_id: '507f1f77bcf86cd799439011',\n      sku: '45790-32',\n      name: 'Monopoly: 3rd Edition',\n      price: 19,\n      quantity: 1,\n      category: 'Games',\n      url: 'https://www.example.com/product/path',\n      image_url: 'https://www.example.com/product/path.jpg'\n    },\n    {\n      product_id: '505bd76785ebb509fc183733',\n      sku: '46493-32',\n      name: 'Uno Card Game',\n      price: 3,\n      quantity: 2,\n      category: 'Games'\n    }\n  ]\n});\n\n\nNote\n\nThe sku and product_id don\u2019t have to be different. If they are different, typically the product_id is a database identifier, like 9714107479 and the sku is a public-facing identifier like SEG-02.\n\nCoupons\n\nThese are events that might occur when dealing with coupons in your ecommerce.\n\nCoupon Entered\n\nFire this event whenever a coupon is entered either on a cart or on an order/transaction.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\norder_id\tString\tOrder/transaction ID, if applicable\ncart_id\tString\tCart ID, if applicable\ncoupon_id\tString\tCoupon ID\n\nExample:\n\nanalytics.track('Coupon Entered', {\n      order_id: '50314b8e9bcf000000000000',\n      cart_id: '923923929jd29jd92dj9j93fj3',\n      coupon_id: 'may_deals_2016'\n    });\n\n\nNote\n\nThis has no effect in GA enhanced e-commerce, as that destination pulls from the coupon field on the order events. Refer to Segment\u2019s Google Analytic documentation for more information.\n\nCoupon Applied\n\nFire this event whenever a coupon is successfully applied to either a cart or an order/transaction.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\norder_id\tString\tOrder/transaction ID, if applicable\ncart_id\tString\tCart ID, if applicable\ncoupon_id\tString\tCoupon ID\ncoupon_name\tString\tCoupon name\ndiscount\tNumber\tMonetary discount applied through the coupon\n\nExample:\n\nanalytics.track('Coupon Applied', {\n      order_id: '50314b8e9bcf000000000000',\n      cart_id: '923923929jd29jd92dj9j93fj3'\n      coupon_id: 'may_deals_2016',\n      coupon_name: 'May Deals 2016',\n      discount: 23.32\n    });\n\n\nNote\n\nThis has no effect in GA enhanced e-commerce, as that destination pulls from the coupon field on the order events. Refer to Segment\u2019s Google Analytic documentation for more information.\n\nCoupon Denied\n\nFire this event whenever a coupon is denied from either a cart or an order/transaction.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\norder_id\tString\tOrder/transaction ID, if applicable\ncart_id\tString\tCart ID, if applicable\ncoupon_id\tString\tCoupon ID\ncoupon_name\tString\tCoupon name\nreason\tString\tReason the coupon was denied\n\nExample:\n\nanalytics.track('Coupon Denied', {\n      order_id: '50314b8e9bcf000000000000',\n      cart_id: '923923929jd29jd92dj9j93fj3'\n      coupon: 'may_deals_2016',\n      reason: 'Coupon expired'\n    });\n\n\nNote\n\nThis has no effect in GA enhanced e-commerce, as that destination pulls from the coupon field on the order events. Refer to Segment\u2019s Google Analytic documentation for more information.\n\nCoupon Removed\n\nFire this event whenever a coupon is removed from either a cart or an order/transaction.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\norder_id\tString\tOrder/transaction ID, if applicable\ncart_id\tString\tCart ID, if applicable\ncoupon_id\tString\tCoupon ID\ncoupon_name\tString\tCoupon name\ndiscount\tNumber\tMonetary discount applied through the coupon\n\nExample:\n\nanalytics.track('Coupon Removed', {\n  order_id: '50314b8e9bcf000000000000',\n  cart_id: '923923929jd29jd92dj9j93fj3'\n  coupon_id: 'may_deals_2016',\n  coupon_name: 'May Deals 2016',\n  discount: 23.32\n});\n\n\nNote\n\nThis has no effect in GA enhanced e-commerce, as that destination pulls from the coupon field on the order events. Refer to Segment\u2019s Google Analytic documentation for more information.\n\nWishlisting\n\nThese events may occur if your ecommerce supports wishlist features.\n\nProduct Added to Wishlist\n\nFire this event when a customer adds a product to their wish list.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\nwishlist_id\tString\tWishlist ID to which the product was added to\nwishlist_name\tString\tWishlist name to which the product was added to\nproduct_id\tString\tDatabase id of the product being viewed\nsku\tString\tSku of the product being viewed\ncategory\tString\tProduct category being viewed\nname\tString\tName of the product being viewed\nbrand\tString\tBrand associated with the product\nvariant\tString\tVariant of the product\nprice\tNumber\tPrice ($) of the product being viewed\nquantity\tNumber\tQuantity of a product\ncoupon\tString\tCoupon code associated with a product (for example, MAY_DEALS_3)\nposition\tNumber\tPosition in the product list (ex. 3)\nurl\tString\tURL of the product page\nimage_url\tString\tImage url of the product\n\nExample:\n\nanalytics.track('Product Added to Wishlist', {\n  wishlist_id: 'skdjsidjsdkdj29j',\n  wishlist_name: 'Loved Games',\n  product_id: '507f1f77bcf86cd799439011',\n  sku: 'G-32',\n  category: 'Games',\n  name: 'Monopoly: 3rd Edition',\n  brand: 'Hasbro',\n  variant: '200 pieces',\n  price: 18.99,\n  quantity: 1,\n  coupon: 'MAYDEALS',\n  position: 3,\n  url: 'https://www.example.com/product/path',\n  image_url: 'https://www.example.com/product/path.jpg'\n});\n\n\nNote\n\nThe sku and product_id do not have to be different. If they are different, typically the product_id is a database identifier, like 9714107479 and the sku is a public-facing identifier like SEG-02.\n\nProduct Removed from Wishlist\n\nFire this event when a customer removes a product from their wish list.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\nwishlist_id\tString\tWishlist ID to which the product was added to\nwishlist_name\tString\tWishlist name to which the product was added to\nproduct_id\tString\tDatabase id of the product being viewed\nsku\tString\tSku of the product being viewed\ncategory\tString\tProduct category being viewed\nname\tString\tName of the product being viewed\nbrand\tString\tBrand associated with the product\nvariant\tString\tVariant of the product\nprice\tNumber\tPrice ($) of the product being viewed\nquantity\tNumber\tQuantity of a product\ncoupon\tString\tCoupon code associated with a product (for example, MAY_DEALS_3)\nposition\tNumber\tPosition in the product list (ex. 3)\nurl\tString\tURL of the product page\nimage_url\tString\tImage url of the product\n\nExample:\n\nanalytics.track('Product Removed from Wishlist', {\n  wishlist_id: 'skdjsidjsdkdj29j',\n  wishlist_name: 'Loved Games',\n  product_id: '507f1f77bcf86cd799439011',\n  sku: 'G-32',\n  category: 'Games',\n  name: 'Monopoly: 3rd Edition',\n  brand: 'Hasbro',\n  variant: '200 pieces',\n  price: 18.99,\n  quantity: 1,\n  coupon: 'MAYDEALS',\n  position: 3,\n  url: 'https://www.example.com/product/path',\n  image_url: 'https://www.example.com/product/path.jpg'\n});\n\n\nNote\n\nThe sku and product_id don\u2019t have to be different. If they are different, typically the product_id is a database identifier, like 9714107479 and the sku is a public-facing identifier like SEG-02.\n\nWishlist Product Added to Cart\n\nFire this event when a customer moves a product from their wish list to their cart.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\nwishlist_id\tString\tWishlist ID to which the product was added to\nwishlist_name\tString\tWishlist name to which the product was added to\ncart_id\tString\tCart ID to which this product was added to\nproduct_id\tString\tDatabase ID of the product being viewed\nsku\tString\tSku of the product being viewed\ncategory\tString\tProduct category being viewed\nname\tString\tName of the product being viewed\nbrand\tString\tBrand associated with the product\nvariant\tString\tVariant of the product\nprice\tNumber\tPrice ($) of the product being viewed\nquantity\tNumber\tQuantity of a product\ncoupon\tString\tCoupon code associated with a product (for example, MAY_DEALS_3)\nposition\tNumber\tPosition in the product list (ex. 3)\nurl\tString\tURL of the product page\nimage_url\tString\tImage url of the product\n\nExample:\n\nanalytics.track('Wishlist Product Added to Cart', {\n  wishlist_id: 'skdjsidjsdkdj29j',\n  wishlist_name: 'Loved Games',\n  cart_id: '99j2d92j9dj29dj29d2d',\n  product_id: '507f1f77bcf86cd799439011',\n  sku: 'G-32',\n  category: 'Games',\n  name: 'Monopoly: 3rd Edition',\n  brand: 'Hasbro',\n  variant: '200 pieces',\n  price: 18.99,\n  quantity: 1,\n  coupon: 'MAYDEALS',\n  position: 3,\n  url: 'https://www.example.com/product/path',\n  image_url: 'https://www.example.com/product/path.jpg'\n});\n\n\nNote\n\nThe sku and product_id don\u2019t have to be different. If they are different, typically the product_id is a database identifier, like 9714107479 and the sku is a public-facing identifier like SEG-02.\n\nSharing\n\nWith many ecommerce stores integrating with social apps or other sharing capabilities, these events might be useful if you are tracking customers sharing product information.\n\nProduct Shared\n\nFire this event when a customer shares a product.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\nshare_via\tString\tMethod of sharing\nshare_message\tString\tMessage that the sender sent\nrecipient\tString\tRecipient of the sharing\nproduct_id\tString\tDatabase ID of the product being viewed\nsku\tString\tSku of the product being viewed\ncategory\tString\tProduct category being viewed\nname\tString\tName of the product being viewed\nbrand\tString\tBrand associated with the product\nvariant\tString\tVariant of the product\nprice\tNumber\tPrice ($) of the product being viewed\nurl\tString\tURL of the product page\nimage_url\tString\tImage url of the product\n\nExample:\n\nanalytics.track('Product Shared', {\n  share_via: 'email',\n  share_message: 'Hey, check out this item',\n  recipient: 'friend@example.com',\n  product_id: '507f1f77bcf86cd799439011',\n  sku: 'G-32',\n  category: 'Games',\n  name: 'Monopoly: 3rd Edition',\n  brand: 'Hasbro',\n  variant: '200 pieces',\n  price: 18.99,\n  url: 'https://www.example.com/product/path',\n  image_url: 'https://www.example.com/product/path.jpg'\n});\n\n\nNote\n\nThe sku and product_id don\u2019t have to be different. If they are different, typically the product_id is a database identifier, like 9714107479 and the sku is a public-facing identifier like SEG-02.\n\nCart Shared\n\nFire this event when a customer shares a shopping cart.\n\nNote\n\nNot all destinations accept arrays as properties. Refer to individual destination docs for more information on supported events and properties.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\nshare_via\tString\tMethod of sharing\nshare_message\tString\tMessage that the sender sent\nrecipient\tString\tRecipient of the sharing\ncart_id\tString\tShopping cart ID\nproducts\tArray\tProducts displayed in the product list\nproducts.$.product_id\tString\tProduct id displayed on the list\n\nExample:\n\nanalytics.track('Cart Shared', {\n  share_via: 'email',\n  share_message: 'Hey, check out this item',\n  recipient: 'friend@example.com',\n  cart_id: 'd92jd29jd92jd29j92d92jd',\n  products: [\n    { product_id: '507f1f77bcf86cd799439011' },\n    { product_id: '505bd76785ebb509fc183733' }\n  ]\n});\n\nReviewing\n\nThese events can be useful for tracking product related reviews.\n\nProduct Reviewed\n\nFire this event when a customer reviews a product.\n\nThis event supports the following semantic properties:\n\nPROPERTY\tTYPE\tDESCRIPTION\nproduct_id\tString\tProduct\u2019s ID\nreview_id\tString\tReview ID\nreview_body\tString\tReview body\nrating\tString\tReview rating\n\nExample:\n\nanalytics.track('Product Reviewed', {\n  product_id: '507f1f77bcf86cd799439011',\n  review_id: 'kdfjrj39fj39jf3',\n  review_body: 'I love this product',\n  rating: '5'\n});\n\n\nThis page was last modified: 17 Apr 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nEvent lifecycles\nBrowsing\nPromotions\nCore Ordering\nCoupons\nWishlisting\nSharing\nReviewing\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nProtocols\n/\nValidate\n/\nConnect a Tracking Plan\nConnect a Tracking Plan\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nPROTOCOLS \u2713\n?\n\nWith your Tracking Plan complete, it\u2019s time to apply the Tracking Plan to one or more Sources. Select Connect Source from the right hand menu for your specific Tracking Plan.\n\nFrom this menu, you will be redirected to a workflow to select a Source from your workspace. Note that a Source can only have one tracking plan applied to it. You can\u2019t select a Source that already has a Tracking Plan connected to it, but you can apply a Tracking Plan to multiple sources.\n\nAfter selecting a Source, you will be shown the consequences of connecting your Tracking Plan.\n\nIMPORTANT: Make sure to read through the consequences of connecting a source!\n\nDisconnect Source from Tracking Plan\n\nTo disconnect the Source from the Tracking Plan, go to the Tracking Plan overview page, locate the column for the tracking plan you want to disconnect, then click the icon under the Connected Sources. In the settings that appear, click Disconnect next to the Source you want to disconnect.\n\nThis page was last modified: 03 Aug 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nDisconnect Source from Tracking Plan\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nJourneys\n/\nBuild a Journey\nBuild a Journey\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\nBefore you begin\n\nVerify that you\u2019ve connected at least one source to your Engage space, with events streaming in.\n\nFor more information, see Setting up your Sources.\n\nAdding the entry condition\nFrom your Engage space, click the Journeys tab.\nClick + New Journey to access the Journey builder.\nClick + Add Entry Condition. Define entry criteria with an entry condition, the first step in the Journey. Before publishing, you can also enable historical data and preview users who meet the entry criteria.\nAdd a name to describe the step, for example New users.\nAdd inclusion conditions, or import conditions from an existing audience to define users who will enter the Journey.\nCheck Use historical data to allow users who have already matched the entry criteria to enter the Journey. Otherwise, only users who meet the entry conditions after publication will enter the Journey.\nClick Preview to see the list of users who meet your criteria. Verify that you\u2019ve defined the right conditions.\nClick Save.\nSegment displays the entry condition on the Journey Builder canvas. It may take up to two minutes for Segment to estimate the number of users in the journey.\nClick + to add the next step and view available step types.\n\nSegment recommends that your entry condition\u2019s time window be shorter than or equal to any exit settings you have. This prevents users from repeating your journey an excessive amount of times.\n\nUsing historical data for the entry step\n\nIf you select the Use historical data option, Segment queries all historical data to generate a list of users who enter the Journey upon publication. If you don\u2019t select Use historical data, only users who satisfy the entry condition after you publish enter the Journey.\n\nYour Use historical data selection won\u2019t impact subsequent Journey steps. Only future events and existing trait memberships trigger post-entry Journey steps.\n\nAvailable step types\n\nOnce you\u2019ve created an entry condition, you can begin adding steps to your Journey.\n\nJourneys offers the following steps:\n\nAdd a condition, which defines conditions a user must satisfy to move from one step to the next\nAdd a delay, which defines the length of time in minutes, hours, days, or weeks that a user must wait before moving to the next step\nTrue/false splits, which divide the previous step\u2019s users into two branches\nMulti-branch splits, which divide the previous step\u2019s users into two or more branches\nRandomized splits, which divide users into random groups so that you can test the performance of a Journey branch\nConnect to existing steps, which joins two separate branches.\nSend an email, which sends a Channels email to a group of users\nSend an SMS, which sends a Channels SMS to a group of users\nSend a WhatsApp (Beta), which sends a Channels WhatsApp to a group of users\nSend to Destinations, which delivers information about the Journey to the selected Destination\n\nFor more details on each available Journey step, view the Journey step types documentation.\n\nPublishing a Journey\n\nOnce you\u2019ve added steps, you\u2019re ready to publish the Journey.\n\nTo publish and activate a Journey, click Publish Journey from the Journey Overview. You can also click Publish Journey in the bottom-right corner of the Journey Builder.\n\nSome Journey features can only be edited before publication. For more information, see the difference between Draft and Published Journeys below.\n\nYour Journey is now live. Next, you\u2019ll learn about making changes to a published Journey.\n\nWorking with a published Journey\n\nYou may find that you need to make changes to a published Journey, like adding new steps or pausing entry to the Journey. This section explains how to pause, resume, and clone a Journey so that you can modify it as needed.\n\nPausing and resuming a Journey\n\nPausing a published Journey prevents new users from joining your Journey. Users already in the Journey, however, will continue their progress.\n\nFollow these steps to pause a Journey:\n\nSelect the Journeys tab within your Engage space.\nSelect the More Options icon next to the Journey you want to pause.\nFrom the dropdown menu, select Pause Entry.\nA modal window appears. Select Pause Entry again to confirm.\n\nCompute Limits\n\nBecause pausing only affects new Journey members, paused Journeys still count towards compute credit limits.\n\nResuming a Journey\n\nYou can resume new user entries to a paused Journey at any time.\n\nAfter you resume a Journey, users who meet the Journey\u2019s entry conditions will join the Journey. New users will not enter the Journey, however, if they met its entry conditions while it was paused.\n\nFollow these steps to resume entry to a paused Journey:\n\nSelect the Journeys tab within your Engage space.\nSelect the More Options icon next to the Journey you want to resume.\nFrom the dropdown menu, select Resume Entry.\nA modal window appears. Select Resume Entry again to confirm. After the confirmation, editing locks until the Journey Resume process completes.\nCloning a Journey\n\nYou can duplicate a Journey by cloning it.\n\nFollow these steps to clone a Journey:\n\nIn the Journey List view, click the \u2026 icon at the end of a row.\nSelect Clone Journey.\n\nSegment then creates a draft of your Journey.\n\nYou can also clone a Journey from a Journey\u2019s Overview by clicking the \u2026 icon.\n\nArchive a Journey\n\nUse the Journey archive setting when you want to end a Journey but preserve its data.\n\nNo new users enter archived Journeys, and progress stops for any users already in the Journey. Archived Journeys no longer send data to Destinations.\n\nSteps in archived Journeys don\u2019t count towards your compute credits.\n\nJourney exits and re-entry\nJourney exits\n\nYou can apply exit settings to both single entry and re-entry Journeys. Users who exit a Journey leave all Journey steps and Destinations.\n\nConfigure exit settings during initial Journey setup by enabling exit settings and entering the number of days that should pass before users exit the Journey. Journeys exits users once this time passes.\n\nIf you don\u2019t apply exit settings to a Journey, users will remain in the Journey indefinitely.\n\nJourney re-entry\n\nThe Journeys re-entry setting allows users to repeat Journeys they\u2019ve already exited. Common use cases for Journeys re-entry include the following:\n\nRetargeting users who abandon multiple carts\nRecurring rewards and promotion offers\nNotifying users when to renew a subscription\nExit and re-entry times\n\nTo let users re-enter a Journey they\u2019ve exited, you\u2019ll need to enable two Journeys settings:\n\nJourneys exit time\nJourneys re-entry time\n\nJourneys exits users based off of the exit time you configure. Users can re-enter the Journey once they meet the Journey\u2019s entry condition again and your defined re-entry time has passed. You can configure re-entry time by hour, day, or week. Re-entry time begins once a user exits the Journey.\n\nSuppose, for example, you enable re-entry for an abandoned cart campaign. You set exit to seven days and re-entry to 30 days. A user who abandons their cart will progress through the journey and exit no later than seven days after entering. Once 30 days after exit have passed, the user will immediately re-enter the journey if the user still satisfies the journey\u2019s entry condition.\n\nAd-based exit settings\n\nExit settings you configure for the Show an ad step don\u2019t impact other Journey steps. Users can exit an ad step but remain in the Journey.\n\nSetting up re-entry\n\nTo enable Journey re-entry for a new Journey, follow these steps:\n\nSelect the Journeys tab within your Engage space, then click New Journey.\nUnder Entry settings, select Re-Entry and enter a re-entry time.\nUnder Exit settings, enter an exit time.\nClick Build Journey to complete Journey setup.\nDrafting a Journey\n\nWhen you\u2019ve finished creating your Journey, click Save as Draft in the bottom-right corner.\n\nWhen Journeys are in a draft state\nJourneys estimates user counts only for the entry step.\nJourneys doesn\u2019t send data to connected Destinations.\nAbout published Journeys\n\nKeep the following in mind when working with a published Journey:\n\nIt may take up to three hours for Journeys to compute user counts after publication.\nYou can edit a Journey\u2019s name, description, and Destination steps.\nYou can\u2019t add, edit, or delete other steps in the Journey.\nOnce Journeys computes and displays user counts, you\u2019ll see the list of users at each step of the Journey.\nClick a user profile to see the Journey list to which they belong.\nJourneys sends and updates data to Destinations in real-time.\n\nThis page was last modified: 04 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nBefore you begin\nAdding the entry condition\nAvailable step types\nPublishing a Journey\nWorking with a published Journey\nJourney exits and re-entry\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nDestinations\n/\nDestination Catalog\nDestination Catalog\n\nWant a simpler list?\n\nCheck out the list of all destinations.\n\n\n\n\nA/B Testing\n2mee\nAB Smartly\nAB Tasty client side\nABsmartly (Actions)\nAdobe Target Cloud Mode\nAdobe Target Web\nApptimize\nBraze\nCleverTap\nContentstack Cloud\n\nBETA\n\nContentstack Web\n\nBETA\n\nConvertFlow\nDynamic Yield by Mastercard Audiences\n\nBETA\n\nExperiments by GrowthHackers\nFlagship.io\nFreshmarketer\nFunnelEnvy\nGraphJSON\nKameleoon (Actions)\n\nBETA\n\nKana\nLaunchDarkly (Actions)\nLaunchDarkly Audiences\nLeanplum\nMarkettailor\nMonetate\nMutiny\nNinetailed by Contentful\nOptimizely Advanced Audience Targeting\n\nBETA\n\nOptimizely Data Platform\n\nBETA\n\nOptimizely Feature Experimentation (Actions)\nOptimizely Full Stack\nOptimizely Web\nPersonyze\nPostHog\nProof Experiences\nSplit\nStatsig\nTamber\nTaplytics\nTrackier\nVespucci\nVisual Website Optimizer\nVWO Cloud Mode (Actions)\nVWO Web Mode (Actions)\n\nBETA\n\nAdvertising\nAdikteev\nAdQuick\nAdRoll\nAdtriba\nAmazon Ads DSP and AMC\n\nBETA\n\nAngler AI\n\nBETA\n\nBing Ads\nByteGain\nCriteo App & Web Events\nCriteo Audiences\nDisplay and Video 360 (Actions)\nDoubleClick Floodlight\nEPICA\nEverflow\nFacebook App Events\nFacebook Conversions API (Actions)\nFacebook Offline Conversions\nFacebook Pixel\nFirebase\nFlurry\nGoogle Ads (Classic)\nGoogle Ads (Gtag)\nGoogle Ads Conversions\nGoogle Ads Remarketing Lists\nInnovid\nJivox\nKevel\nKitemetrics\nLinkedIn Audiences\nLinkedIn Conversions API\nLinkedIn Insight Tag\nLiveIntent Audiences\nLiveRamp Audiences\n\nBETA\n\nMillennial Media\nMoloco MCM\n\nBETA\n\nNanigans\nPerfect Audience\nPersonas Facebook Custom Audiences\nPinterest Audiences\nPinterest Conversions API\nPinterest Tag\nPodsights\nQuantcast\nQuanticMind\nQuora Conversion Pixel\nReddit Conversions API\nRevX Cloud (Actions)\n\nBETA\n\nRokt\nRokt Audiences (Actions)\n\nBETA\n\nShareASale\nSimpleReach\nSnapchat Audiences\nSnapchat Conversions API\nStackAdapt\n\nBETA\n\nTaboola (Actions)\n\nBETA\n\nThe Trade Desk Crm\n\nBETA\n\nTikTok Audiences\nTikTok Conversions\nTiktok Offline Conversions\n\nBETA\n\nTikTok Pixel\n\nBETA\n\nTopsort\n\nBETA\n\nTrafficGuard\nTwitter Ads\nYahoo Audiences\n\nBETA\n\nZaius\nAnalytics\n1Flow\n1Flow Mobile Plugin\n\nBETA\n\n1Flow Web (Actions)\n\nBETA\n\nAB Smartly\nAccoil Analytics\nActable Predictive\n\nBETA\n\nAdobe Analytics\nAdQuick\nAdtriba\nAggregations.io (Actions)\n\nBETA\n\nAkita Customer Success\nAlexa\nAlgolia Insights (Actions)\nAmazon Kinesis\nAmazon Kinesis Firehose\nAmberflo\nAmplitude\nAmplitude (Actions)\nAnodot\nAppFit\n\nBETA\n\nAsayer\nAstrolabe\n\nBETA\n\nAuryc\nAvo\nAWS S3\nBeamer\nBlend Ai\n\nBETA\n\nBlendo\nBlitzllama\nBloomreach Engagement\nBreyta CRM\nBucket\nBuzzBoard\nByteGain\nBytePlus\nCalixa\nCandu\nChartbeat\nChartMogul\n\nBETA\n\nClearBrain\nCleverTap\nClicky\nCliff\ncomScore\nConvertly\n\nBETA\n\nCorrelated\nCountly\nCrowdPower\nCruncher\nDreamdata\nEmarsys\nEmarsys (Actions)\n\nBETA\n\nEMMA\nEPICA\nEquals\n\nBETA\n\nevents.win\n\nBETA\n\nEverflow\nExperiments by GrowthHackers\nFacebook App Events\nFactorsAI\nFirebase\nFL0\nFlurry\nFoxMetrics\nFullStory\nFullstory (Actions)\nFullstory Cloud Mode (Actions)\nFunnelFox\nGainsight\nGainsight PX\nGainsight Px Cloud (Actions)\nGauges\nGoogle Ads Conversions\nGoogle Analytics 4 Cloud\nGoogle Analytics 4 Web\nGoSquared\nGraphJSON\nGroundswell\nGWEN (Actions)\n\nBETA\n\nHeap\nHitTail\nHouseware\nHubble (Actions)\n\nBETA\n\nHubSpot\nHubSpot Cloud Mode (Actions)\nHubSpot Web (Actions)\nHumanic AI\nhydra\nIndicative\nInleads AI\njourny io\nJune\nJune (Actions)\nKable\nKana\nKeen\nKissmetrics\nKitemetrics\nKoala\nKoala (Cloud)\nKubit\nLibrato\nLocalytics\nLogRocket\nLucky Orange\nLytics\nMadkudu\nMatomo\nMixpanel (Actions)\nMixpanel (Legacy)\nMoEngage\nMoengage (Actions)\nMoesif API Analytics\nMutiny\nNew Relic\nNielsen DCR\nOptimizely Feature Experimentation (Actions)\nOrb\nParsely\nPeaka\n\nBETA\n\nPendo\nPendo Web (Actions)\n\nBETA\n\nPlayerZero Web\nPointillist\nPostHog\nProfitWell\nProsperStack\nQuantcast\nRabble AI\n\nBETA\n\nRefersion\nRetina\nRichpanel\nRipe Cloud Mode (Actions)\nRipe Device Mode (Actions)\nRokt\nRupt\n\nBETA\n\nSaleswings (Actions)\nSchematic\n\nBETA\n\nScopeAI\nScreeb\nScreeb Web (Actions)\n\nBETA\n\nScuba Analytics\nSegment Profiles\nSegMetrics\nSerenytics\nSherlock\nSIGNL4 Alerting\nSingleStore\nSingular\nSkalin\nSlicingDice\nSmartlook\nSpideo\nSplit\nSprig Cloud\nStartdeliver\nStatsig\nStories\nStormly\nStrikedeck\nSurvicate\nSurvicate (Actions)\n\nBETA\n\nSwrve\nTamber\nToplyne Cloud Mode (Actions)\n\nBETA\n\nTopsort\n\nBETA\n\nTractionboard\nTrafficGuard\nTreasure Data\nUnwaffle\nUpollo\n\nBETA\n\nUpollo Web (Actions)\n\nBETA\n\nUserIQ\nUsermaven (Actions)\n\nBETA\n\nUserMotion (Actions)\n\nBETA\n\nUserpilot Cloud (Actions)\n\nBETA\n\nUserpilot Web (Actions)\n\nBETA\n\nVespucci\nVidora\nWalkMe\nWebEngage\nWigzo\nWindsor\nWoopra\nXtremepush\nYandex Metrica\nYoubora\nAttribution\nAdjust\nAdtriba\nAngler AI\n\nBETA\n\nAppsFlyer\nAttribution\nBranch Metrics\nButton\nConvertro\nDreamdata\nEMMA\nImpact Partnership Cloud\nKable\nKitemetrics\nKochava\nLocalytics\nLou\nMatcha\nPodscribe (Actions)\nRefersion\nRetina\nRevX Cloud (Actions)\n\nBETA\n\nRockerbox\nSaaSquatch v2\nScuba Analytics\nSingular\nStormly\nTrafficGuard\nTUNE\nUsermaven (Actions)\n\nBETA\n\nCRM\n1Flow Mobile Plugin\n\nBETA\n\nAampe\nActions Pipedrive\n\nBETA\n\nAkita Customer Success\nAmberflo\nAstrolabe\n\nBETA\n\nAttio (Actions)\n\nBETA\n\nBlackbaud Raiser's Edge NXT\n\nBETA\n\nBraze\nBraze Cohorts\nBraze Web Device Mode (Actions)\nBreyta CRM\nByteGain\nCalixa\nChartMogul\n\nBETA\n\nCleverTap\nClose\nCrisp\nCrowdPower\nCustify\nDrip (Actions)\n\nBETA\n\nEmarsys\nEnjoyHQ\nFirebase\nFreshsales\nFreshsales Suite - CRM\nFunnelFox\nGoSquared\nHouseware\nHubSpot\nHubSpot Cloud Mode (Actions)\nHubSpot Web (Actions)\nHumanic AI\njourny io\nJune\nJune (Actions)\nKoala\nKoala (Cloud)\nKustomer\nLiveIntent Audiences\nMatcha\nMoEngage\nMoengage (Actions)\nMoesif API Analytics\nNat\nNoora\nProductBird\nRichpanel\nSalescamp CRM\nSalesforce (Actions)\nSaleswings (Actions)\nSegMetrics\nStartdeliver\nStrikedeck\nThe Trade Desk Crm\n\nBETA\n\nUserlist\nVariance\nWebEngage\nWhale Alerts\nWigzo\nWindsor\nWishpond\nXtremepush (Actions)\n\nBETA\n\nCustomer Success\n1Flow Mobile Plugin\n\nBETA\n\n2mee\nAdQuick\nAkita Customer Success\nAsayer\nAstrolabe\n\nBETA\n\nBeamer\nBlitzllama\nBreyta CRM\nCalixa\nCandu\nChurned\nChurnZero\nClientSuccess\nCommandBar\nCorrelated\nCourier\nCrisp\nCrossing Minds\nCustify\nElevio\nEmarsys\nEngage Messaging\nEnjoyHQ\nevents.win\n\nBETA\n\nGainsight\nGainsight PX\nGainsight Px Cloud (Actions)\nGleap (Action)\n\nBETA\n\nGraphJSON\nGroundswell\nHawkei\nHelp Scout\nHouseware\nHumanic AI\nhydra\nInleads AI\nIntercom\nIntercom Cloud Mode (Actions)\nIntercom Web (Actions)\nJimo\nJimo (Actions)\n\nBETA\n\njourny io\nJune\nJune (Actions)\nKustomer\nLearndot\nLou\nMetronome (Actions)\nNat\nNatero\nNoora\nPlanhat\nProsperStack\nRamen\nRecombee AI\nRefiner\nRichpanel\nSaaSquatch v2\nSalescamp CRM\nSalesmachine\nSatisMeter\nSavio\nScopeAI\nScreeb\nScuba Analytics\nSherlock\nSkalin\nSlack\nSlack (Actions)\nSnapboard\nSprig Cloud\nStartdeliver\nStartdeliver-v2\n\nBETA\n\nStonly\nStrikedeck\nTalon.One\nTotango\nTrustpilot\nUnwaffle\nUpcall\nUserIQ\nUserVoice\nVitally\nVoucherify\nWalkMe\nWindsor\nZendesk\nZopim\nDeep Linking\nAdjust\nAmberflo\nAppsFlyer\nBranch Metrics\nButton\nEMMA\nSingular\nEmail\nUser.com\nEmail Marketing\nAcoustic (Actions)\nActiveCampaign\nAttentive Mobile\nAutopilotHQ\nBloomreach Engagement\nBraze\nBraze Cloud Mode (Actions)\nBraze Cohorts\nBraze Web Device Mode (Actions)\nBronto\nCleverTap\nCordial (Actions)\nCourier\nCrisp\nCrossing Minds\nCrowdPower\nCustomer.io\nCustomer.io (Actions)\nDrip\nEloqua\nEmarsys\nEmarsys (Actions)\n\nBETA\n\nEncharge (Actions)\nEngage Messaging\nExtole Platform\nFreshmarketer\nGainsight PX\nGainsight Px Cloud (Actions)\nGist\nHubSpot\nHubSpot Cloud Mode (Actions)\nHubSpot Web (Actions)\nhydra\nInflection\nIntercom\nIntercom Cloud Mode (Actions)\nIntercom Web (Actions)\nIterable\nIterable (Actions)\nJivox\nKahuna\nKissmetrics\nKlaviyo\nLeanplum\nListrak (Actions)\nLoops (Actions)\nLumen\nMailChimp\nMailjet\nMailmodo\nMarketo Static Lists (Actions)\nMarketo V2\nMoEngage\nMoengage (Actions)\nMoosend\nNudgespot\nOneSignal (New)\nOrtto\nPardot (Actions)\nPersistIQ\nPersonyze\nRecombee AI\nResponsys\nSailthru v2\nSalescamp CRM\nSalesforce Marketing Cloud (Actions)\nSelligent Marketing Cloud\nSendGrid\nSwrve\nTalon.One\nTrustpilot\nUserIQ\nUserlist\nVero\nVoucherify\nWebEngage\nWigzo\nWishpond\nXtremepush\nZaius\nEnrichment\nActable Predictive\n\nBETA\n\nAttio (Actions)\n\nBETA\n\nBreyta CRM\nBuzzBoard\nClearbit Enrichment\nClearbit Reveal\nCruncher\nDreamdata\nJimo\nMadkudu\nNoora\nRefiner\nScreeb\nUpollo\n\nBETA\n\nUpollo Web (Actions)\n\nBETA\n\nUserMotion (Actions)\n\nBETA\n\nVidora\nFeature Flagging\nABsmartly (Actions)\nApptimize\nKameleoon (Actions)\n\nBETA\n\nLaunchDarkly (Actions)\nLaunchDarkly Audiences\nOptimizely Feature Experimentation (Actions)\nOptimizely Full Stack\nOptimizely Web\nSchematic\n\nBETA\n\nSplit\nHeatmaps & Recordings\nAuryc\nCrazy Egg\nFreshmarketer\nFullStory\nFullstory (Actions)\nFullstory Cloud Mode (Actions)\nHotjar\nInspectlet\nLogRocket\nLucky Orange\nmabl\nMouseflow\nSmartlook\nStories\nVespucci\nVWO Cloud Mode (Actions)\nVWO Web Mode (Actions)\n\nBETA\n\nWalkMe\nLivechat\nCourier\nGist\nGleap (Action)\n\nBETA\n\nGoSquared\nIntercom\nIntercom Cloud Mode (Actions)\nIntercom Web (Actions)\nLiveChat\nLiveLike\n\nBETA\n\nOlark\nRichpanel\nSnapEngage\nZopim\nMarketing Automation\n1Flow Mobile Plugin\n\nBETA\n\nAampe\nAccoil Analytics\nAcoustic (Actions)\nActions Pipedrive\n\nBETA\n\nAirship\nAirship (Actions)\nAstrolabe\n\nBETA\n\nAttentive Mobile\nBraze Cloud Mode (Actions)\nBraze Cohorts\nByteGain\nCallingly\nChurned\nClearBrain\nCleverTap\nCleverTap (Actions)\nCordial (Actions)\nCorrelated\nCourier\nCrisp\nCrowdPower\nDreamdata\nDrip (Actions)\n\nBETA\n\nEmarsys\nEncharge (Actions)\nEPICA\nEverflow\nExperiments by GrowthHackers\nExtole Platform\nFreshmarketer\nFunnelEnvy\nGameball (Actions)\n\nBETA\n\nGist\nHumanic AI\nhydra\nInsider Audiences\nInsider Cloud Mode (Actions)\nIterate Web (Actions)\n\nBETA\n\nJimo\nJivox\njourny io\nKitemetrics\nListrak (Actions)\nLiveLike\n\nBETA\n\nLoops (Actions)\nLumen\nMarkettailor\nMoEngage\nMoengage (Actions)\nMutiny\nNat\nOneSignal (New)\nOrtto\nPardot (Actions)\nPersistIQ\nPersonyze\nPlotline\n\nBETA\n\nPodscribe (Actions)\nProof Experiences\nProsperStack\nPushwoosh\n\nBETA\n\nRecombee AI\nRegal.io\nRehook\n\nBETA\n\nRetina\nRipe Cloud Mode (Actions)\nRipe Device Mode (Actions)\nSaaSquatch v2\nSailthru v2\nSalescamp CRM\nSalesforce Marketing Cloud (Actions)\nSegMetrics\nSendGrid\nSingular\nStackAdapt\n\nBETA\n\nStartdeliver\nStories\nStrikedeck\nTalon.One (Actions)\nTamber\nUnwaffle\nUpcall\nUserlist\nVariance\nVero\nVidora\nVoucherify\nVoucherify (Actions)\n\nBETA\n\nWebEngage\nWisepops\nXtremepush\nXtremepush (Actions)\n\nBETA\n\nPerformance Monitoring\n1Flow Mobile Plugin\n\nBETA\n\nAdQuick\nAsayer\nAtatus\nBlitzllama\nBugHerd\nBugsnag\nButton\nCallingly\nCliff\nCrittercism\nCruncher\nErrorception\nExperiments by GrowthHackers\nFL0\nFlagship.io\nFunnelFox\nHawkei\nHumanic AI\nKeen\nLogRocket\nMatcha\nNew Relic\nOptimizely Feature Experimentation (Actions)\nPingdom\nPlayerZero Web\nProductBird\nRetina\nRollbar\nSegMetrics\nSentry\nSerenytics\nSIGNL4 Alerting\nSnapboard\nTrack JS\nTrafficGuard\nVariance\nPersonalization\n1Flow Mobile Plugin\n\nBETA\n\nAampe\nAmazon Personalize\nAppcues\nButton\nByteGain\nCandu\nChameleon\nClearBrain\nCleverTap\nCommandBar\nContentstack Cloud\n\nBETA\n\nContentstack Web\n\nBETA\n\nConvertFlow\nCorrelated\nCrossing Minds\nCruncher\nDynamic Yield by Mastercard Audiences\n\nBETA\n\nEPICA\nFlagship.io\nFunnelEnvy\nFunnelFox\nGainsight PX\nGainsight Px Cloud (Actions)\nGameball (Actions)\n\nBETA\n\nGWEN (Actions)\n\nBETA\n\nInflection\nInsider Audiences\nInsider Cloud Mode (Actions)\nJivox\nKana\nLaunchDarkly (Actions)\nLeanplum\nLumen\nLytics\nMailmodo\nMarkettailor\nMonetate\nMovable Ink (Actions)\nMutiny\nNat\nNinetailed by Contentful\nOneSignal (New)\nOptimizely Advanced Audience Targeting\n\nBETA\n\nOptimizely Data Platform\n\nBETA\n\nOptimizely Feature Experimentation (Actions)\nOptimizely Full Stack\nOptimizely Web\nPersonyze\nPlotline\n\nBETA\n\nProductBird\nProof Experiences\nPushwoosh\n\nBETA\n\nQualtrics\nRecombee AI\nRegal.io\nRehook\n\nBETA\n\nSailthru v2\nSelligent Marketing Cloud\nSnapboard\nSprig Cloud\nStartdeliver\nStonly\nTalon.One\nTamber\ntray.io\nUnwaffle\nUpcall\nUser.com\nUserGuiding\n\nBETA\n\nUserpilot Cloud (Actions)\n\nBETA\n\nUserpilot Web (Actions)\n\nBETA\n\nUserpilot Web Plugin\nVariance\nVoucherify\nWebEngage\nWigzo\nWisepops\nWishpond\nXtremepush\nRaw Data\nAggregations.io (Actions)\n\nBETA\n\nAlgolia Insights (Actions)\nAmazon EventBridge\nAmazon Kinesis\nAmazon Kinesis Firehose\nAmazon Lambda\nAnodot\nAsayer\nAstrolabe\n\nBETA\n\nAWS S3\nAzure Function\n\nBETA\n\nBlendo\nCalixa\nCruncher\nExperiments by GrowthHackers\nFunnelFox\nGoogle Cloud Function\n\nBETA\n\nGoogle Cloud PubSub\nGoogle Sheets\nGraphJSON\nIron.io\nKable\nKafka\n\nBETA\n\nKeen\nMammoth\nMatcha\nPeaka\n\nBETA\n\nRepeater\nScuba Analytics\nSegment Connections\nSegment Profiles\nSerenytics\nSingleStore\nSlicingDice\nStories\ntray.io\nTreasure Data\nVidora\nWebhooks (Actions)\nZapier\nReferrals\nAmbassador\nAttribution\nExtole Platform\nFriendbuy (Cloud Destination)\nFriendbuy (Legacy)\nFriendbuy (Web Destination)\nImpact Partnership Cloud\nPerkville\nRefersion\nSaaSquatch v2\nTalkable\nTalon.One\nTalon.One (Actions)\nTrustpilot\nVoucherify\nVoucherify (Actions)\n\nBETA\n\nSMS & Push Notifications\n2mee\nAampe\nAirship\nAirship (Actions)\nBatch\nBeamer\nBraze\nBraze Cloud Mode (Actions)\nBraze Cohorts\nBraze Web Device Mode (Actions)\nCallingly\nCleverTap\nCleverTap (Actions)\nCordial (Actions)\nCourier\nCustomer.io\nEmarsys\nEMMA\nFlurry\nIterable\nIterable (Actions)\nKahuna\nLeanplum\nLocalytics\nLumen\nMoEngage\nMoengage (Actions)\nOneSignal (New)\nPostscript\n\nBETA\n\nRegal.io\nSailthru v2\nSelligent Marketing Cloud\nSIGNL4 Alerting\nSwrve\nTamber\nUser.com\nWebEngage\nWigzo\nXtremepush\nZaius\nSecurity & Fraud\nCastle\nRupt\n\nBETA\n\nSIGNL4 Alerting\nSingular\nTrafficGuard\nSurveys\n1Flow\n1Flow Mobile Plugin\n\nBETA\n\n1Flow Web (Actions)\n\nBETA\n\nAuryc\nBeamer\nBlitzllama\nCanny (Actions)\nConvertFlow\nDelighted\nGainsight\nGainsight PX\nGainsight Px Cloud (Actions)\nHubble (Actions)\n\nBETA\n\nInMoment (formerly Wootric)\nIterate Web (Actions)\n\nBETA\n\nJimo\nJimo (Actions)\n\nBETA\n\nNoora\nPendo\nPendo Web (Actions)\n\nBETA\n\nProductBird\nProsperStack\nQualaroo\nQualtrics\nRamen\nRefiner\nSatisMeter\nSavio\nScreeb\nScreeb Web (Actions)\n\nBETA\n\nSprig (Actions)\nSprig Cloud\nStonly\nStrikedeck\nSurvicate\nSurvicate (Actions)\n\nBETA\n\nTrustpilot\nUserGuiding\n\nBETA\n\nUserIQ\nUserVoice\nWalkMe\nWebEngage\nTag Managers\nFunnelEnvy\nGoogle Tag Manager\nVideo\n1Flow Mobile Plugin\n\nBETA\n\n2mee\nAdobe Analytics\nChartbeat\ncomScore\nNielsen DCR\nParsely\nYoubora\n\nOn this page\n\nA/B Testing\nAdvertising\nAnalytics\nAttribution\nCRM\nCustomer Success\nDeep Linking\nEmail\nEmail Marketing\nEnrichment\nFeature Flagging\nHeatmaps & Recordings\nLivechat\nMarketing Automation\nPerformance Monitoring\nPersonalization\nRaw Data\nReferrals\nSMS & Push Notifications\nSecurity & Fraud\nSurveys\nTag Managers\nVideo\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSources\n/\nSegment-Managed Custom Domain\nSegment-Managed Custom Domain\nFREE X\nTEAM X\nBUSINESS \u2713\nADD-ON X\n?\n\nCustom Domain is a fully-managed service that enables you to configure a first-party subdomain over HTTPS. You can then track event requests through your own domain (for example, cdp.mydomain.com), instead of the default (segment.com). Tracking events through your own domain allows for more secure and complete first-party data collection by reclaiming first-party data lost to browser controls. With a more complete view of your customer behaviors, you can build more robust profiles for greater attribution and ROAS.\n\nCustom Domain is only available to Business Tier customers\n\nCustomers not on the Business Tier but who have interest in Custom Domain should contact Segment\u2019s sales team for assistance with upgrading to a Business Tier plan. Segment also offers an alternative DNS record service, Custom Proxy.\n\nSegment recommends configuring Custom Domain alongside Consent Management to ensure you are respectful of your end users\u2019 consent preferences.\n\nSegment\u2019s domain delegation solutions\n\nSegment offers two domain delegation solutions: Custom Proxy and Custom Domain. If you use Custom Domain, you can choose to use either DNS delegation or a Cannonical Name (CNAME). Segment recommends using Custom Domain with DNS delegation, which leads to easy setup, maintenance, and monitoring.\n\nSERVICE\tHOW IT WORKS\tINFRASTRUCTURE MANAGEMENT\tAVAILABILITY\nCustom Domain with DNS Delegation\tA Segment service that allows your website to use your own subdomain to load Analytics.js securely over HTTPS and send event data. It is not limited to Analytics.js and is also compatible with server libraries. It uses a DNS subdomain that you delegate to Segment.*\tSegment manages all related infrastructure, including applying security updates, managing the SSL certificate lifecycle, and monitoring.\tBusiness Tier\n\nRecommended for reliable data collection.\nCustom Domain with CNAME\tThis approach uses a Canonical Name (CNAME) to map an alias name on your domain name to Analytics.js. It is not limited to Analytics.js and is also compatible with server libraries.\tCustomers are responsible for maintaining CNAME.\tBusiness Tier\n\nNot recommended due to evolving and persistent browser privacy measures.\nCustom Proxy\tThis approach uses a proxy or wrapper where all data is first collected by a proxy on your domain and then forwarded to Segment.\tCustomers are responsible for maintaining their own proxy infrastructure.\tAvailable to all Segment users.\n\nNot recommended because it adds a point of failure, but remains an option if Custom Domain with sub-domain delegation is unavailable to you.\n\n*If it\u2019s not possible for you to delegate subdomains to Segment, you can use a CNAME instead. Segment encourages users to delegate a DNS subdomain rather than use use CNAME aliasing due to the evolving privacy standards in browsers, but CNAME aliasing remains an option for users not interested in using nameservers.\n\nHow DNS subdomain delegation works\n\nDNS subdomain delegation is a process where the control of a specific subdomain is assigned to another DNS server, allowing that server to manage the DNS records for the subdomain. This delegation is useful for distributing the management of DNS records and enables specialized handling of subdomain traffic.\n\nHow CNAME records work\n\nWhen a user tries to access the alias domain, the DNS resolver looks up the CNAME record, finds the canonical name, and resolves it to the IP address of the target. For example, you could alias your subdomain to point to the Segment domain. If a user accesses your site, they are redirected to the Segment domain, but their browser\u2019s address bar still shows the alias domain.\n\nCNAME records provide flexibility and centralized management, making it easier to handle domain redirections and subdomain configurations.\n\nImplementing a Custom Domain using CNAME delegation requires you to add a CNAME and record for two domains that Segment generates on your behalf: one for the Segment CDN and a second for the Tracking API. You must add a CNAME and DNS record for both domains.\n\nSupported sources\n\nCustom Domain supports the following sources:\n\nAnalytics.js\nClojure\nGo\nJava\nNode.js\nPHP\nPython\nRuby\n.NET\nGetting started\n\nCustom Domain configuration won\u2019t disrupt your event tracking. Default Segment domains will continue to function alongside your custom domains once the setup is complete.\n\nTo configure Custom Domain:\n\nSelect the subdomain you\u2019d like Segment to use for event request tracking (for example, cdp.domain.com).\nSign into the Segment app, select your user avatar, and click Contact Support.\nCreate a support request with the following fields:\nTopic: Select Custom Domain.\nSubject: Enter a subject line for your support request.\nDomain Name: Enter the subdomain that Segment should use for event request tracking.\nAdditional Domain Name: If applicable, add an additional subdomain. This field is optional.\nSource names: Select the sources you would like to use for Custom Domain. Segment recommends starting with a stage or dev source. For initial setup, an Analytics.js source is required. For a list of all sources that support Custom Domain, see Supported sources.\nIs the domain name enabled for Content Policy: Select either Yes or No. You are not required to create a Content Policy prior to requesting Custom Domain. If you\u2019ve enabled a Content Security Policy (CSP), you must add the new subdomains provided by Segment to your CSP once you\u2019ve enabled the Custom Domain feature. This ensures that the CSP does not block the subdomains when you load Segment.\nDescription: Enter an optional description for your service request. If you are requesting Custom Domain for multiple workspaces, enter any additional workspace slugs and source names into this field.\nSegment provides you with a list of nameservers you should add to your DNS. Once you receive the nameservers from Segment, update your DNS.\nAfter you\u2019ve updated your DNS, Segment verifies that you\u2019ve made all required updates and then provides you with two custom domains, one for the Tracking API and a second for your CDN.\nOnce Custom Domain is enabled for your workspace, the Segment app generates a new JavaScript source code snippet for your Analytics.js sources. Copy and paste this snippet into the header of your website. You can also use the subdomain provided for the Tracking API as the new endpoint for your server library sources.\nFAQ\nCan I set up multiple Custom Domains?\n\nSegment recommends creating a different subdomain (for example, mysubdomain.mydomain.com) for each source. You cannot connect multiple custom domains to the same source.\n\nWhat sources can I use with Custom Domain?\n\nFor initial setup, Segment requires an Analytics.js source. Custom Domain was largely developed to support JavaScript sources. It helps with comprehensive collection of first-party data from your website when accessed over any platform (desktop, mobile, and more). You can use the subdomain for all other non-JavaScript sources as well, for consistency, but it will have no impact on data collection for those sources.\n\nHow can I configure non-JavaScript sources to use Custom Domain?\n\nFor non-Analytics.js sources, you\u2019ll need to update your implementation to use the subdomain as an endpoint when using the Tracking API. For example:\n\nServer Sources: When sending data from server-side implementations, use the host configuration parameter to send data to your subdomain instead of the default Segment domain.\nMobile Sources: When sending data from mobile implementations, use the apiHost configuration parameter to send data to your subdomain instead of the default Segment domain.\nIs there a benefit in migrating server-side sources over to client-side with Custom Domain?\n\nServer-side tracking is generally more reliable than client-side tracking. For example, when tracking data client-side, you might lose data when users might block all cookies or use tools that interfere with network requests leaving the browser.\n\nFor business-critical events, Segment recommends server-side data tracking. This approach means that your data is less susceptible to disruptions from client-side variables, which can result in more accurate and reliable tracking.\n\nIs this a fully-managed solution? What servers or infrastructure do I need to set up on my side for this proxy?\n\nYes, Custom Domain is a fully-managed solution. However, you must set up the following infrastructure on your end:\n\nDelegate a DNS subdomain to Segment\nAdd the name servers Segment provides\u00a0to your DNS\n\nFirst, decide on your subdomain and then delegate it to Segment. Segment then asks you to add a DNS NS record to your DNS with specific values to complete the DNS delegation. From there on, Segment fully manages the infrastructure for serving Analytics.js and ingesting events data through the subdomain.\n\nCan I change my Segment subdomain after the initial setup?\n\nSegment doesn\u2019t recommend that you change the subdomain after the initial setup. If you change the subdomain, Segment must revoke the older certificates for your subdomain and you are required to redo the entire onboarding process, as several underlying components, like certificates, would need to be recreated and reassociated.\n\nWho is responsible for managing the SSL certificate for the Custom Domain?\n\nSegment hosts and manages SSL Certificate on the Custom Domain. At this time, Segment does not support importing a certificate you may already have, as Segment must request a SSL certificate on your behalf using AWS Certificate Manager (ACM) when initially setting up your Custom Domain.\n\nSegment also uses ACM to manage and renew certificates.\n\nCan you rename window.analytics with Custom Domain?\n\nYes, Custom Domain allows Segment to rename window.analytics to a unique name to avoid being blocked by some ad blocking software.\n\nCustomers who have access to the Custom Domain feature can rename analytics to <workspaceid>/<sourceid>.js by choosing an Alias for Analytics.js within the source settings that are available after the workspace is enabled for Custom Domain.\n\nWhat happens to the Analytics.js cookies already set on the user\u2019s browser prior to a Custom Domain implementation?\n\nAnalytics.js cookies are not lost in the transition to Custom Domain. When users revisit your website, the previous Analytics.js cookies continue to be fetched and added to events, if available.\n\nCan I use the same subdomain across multiple workspaces?\n\nNo, each workspace requires its own unique subdomain (for example, mysubdomain.mydomain.com).\n\nThis page was last modified: 18 Nov 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSegment\u2019s domain delegation solutions\nSupported sources\nGetting started\nFAQ\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGetting Started\n/\nHow Segment Works\nHow Segment Works\n\nIn a nutshell, the Segment libraries (Sources) generate messages about what\u2019s happening in your site or app, and send them to the Segment servers. Segment then translates the content of those messages into different formats for use by other tools (which Segment calls Destinations), and sends the translated messages to those tools. The Segment servers also archive a copy of the data, and can send data to your storage systems (such as databases, warehouses, or bulk-storage buckets).\n\nOverview\n\nSegment Spec methods are how you collect interaction data from your interfaces, and the Sources are what you package with your interfaces to collect and route the data.\n\nOnce you\u2019ve collected your interaction data, there are several different actions you can take:\n\nSend it to Destinations, which receive the data from any number of sources in real time\nSend it to Warehouses and other bulk storage tools, which hold your raw event schemas and update on regular intervals\nEnrich the customer data you collect by connecting data from your other tools, and then collect it in a warehouse to monitor performance, inform decision-making processes, and create uniquely customized user experiences.\nUse Engage, Twilio\u2019s marketing automation tool, to build marketing campaigns personalized to your audience.\nSources for collecting data\n\nYou can collect data by implementing Segment\u2019s tracking libraries as your Sources:\n\nAnalytics.js, the Segment JavaScript source, is the most powerful way to track customer data from a website. Segment recommends it as the default installation for any website.\nThe Segment Mobile SDKs are the best way to simplify tracking in your iOS, Android, and Xamarin apps. Segment recommends them over server-side sources as the default installation for any mobile app.\nServer-side sources let you send analytics data directly from your servers when client-side tracking doesn\u2019t work, or when you\u2019re sending mission-critical data like revenues.\nSources for unique cases\n\nSegment also offers these other source libraries to cover less straightforward cases:\n\nUse the HTTP Tracking API if Segment doesn\u2019t offer a library for your specific environment yet.\nThe Pixel Tracking API lets you track events from environments where you can\u2019t execute code - for example, tracking when an email was opened.\nThe Querystring API lets you use querystrings to load API methods when a user first visits a Segment-enabled site. Use this API for tracking events like email clicks and identifying users associated with those clicks on the destination page.\nCloud App Sources\n\nSegment also offers Cloud App Sources to integrate data from your third-party tools:\n\nObject Cloud Sources can import third party tool data directly into your Segment warehouse, but can\u2019t stream that data into your other Segment destinations. Make sure you enable a Segment warehouse before you enable an object cloud source.\nEvent Cloud Sources don\u2019t just import third party tool data into your Segment warehouse, they also send event data in real-time to your other Segment destinations. You don\u2019t need to set up a data warehouse to send Event Cloud Source data to your destinations.\nHow you can track data\n\nSegment supports several ways to implement tracking. The two most common are to use device-based or server-based libraries. You can use Segment\u2019s device-based libraries, such as JavaScript, iOS, and Android, to make calls on users\u2019 browsers or mobile devices. You can also track data with Segment\u2019s server-based libraries, such as Node, Python, or PHP, where the calls are triggered on your own servers and then sent to the Segment servers.\n\nWhen you collect data using device-based libraries, you can choose between these two different connection modes:\n\nCloud-mode is where the library sends the data directly to the Segment servers which then translate and forward it.\nDevice-mode is where the library sends the data both directly to the Segment servers, and also to the servers for the destination tool. Device-mode sometimes requires some additional set-up steps, but can unlock rich device data.\n\nAlthough there are some tradeoffs between the two approaches, neither is better than the other, and Segment recommends that you implement a mix of both. In general, more direct interaction data is available using a device-based library, but server-based collection is more secure, reliable, and can\u2019t be blocked by ad blockers.\n\nThe Segment Methods\n\nThe Segment libraries generate messages about what happens on your interface, translate those messages into different formats for use by destinations, and transmit the messages to those tools.\n\nThere are several tracking API methods, that you can call to generate messages. The four most important methods are:\n\nIdentify: Who is the user?\nPage and Screen: What web page or app screen are they on?\nTrack: What are they doing?\n\nEvery call shares the same common fields. When you use these methods as intended, it allows Segment to detect a specific type of data and correctly translate it to send it on to downstream destinations.\n\nWhere you can send data\n\nSegment maintains a catalog of destinations where you can send your data.\n\nBACK\nGetting Started Overview\nNEXT\nA simple Segment installation\n\nWalk through a disposable, demo implementation.\n\nThis page was last modified: 21 Apr 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nOverview\nSources for collecting data\nHow you can track data\nThe Segment Methods\nWhere you can send data\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nDestinations\n/\nDestination Filters\nDestination Filters\n\nDestination filters are only available to Business Tier customers.\n\nUse destination filters to prevent certain data from flowing into a destination. You can conditionally filter out event properties, traits, and fields, or even filter out the event itself.\n\nYou can configure destination filters on cloud-mode, mobile cloud-mode destinations, and web device-mode and actions-based destinations. With device-mode destinations, you can use the same user interface or API mechanism that you use for your cloud-mode destinations, and have those filters acted upon for device-mode destinations on web.\n\nCommon use cases for destination filters include:\n\nManaging PII (personally identifiable information) by blocking fields from reaching certain destinations\nControlling event volume by sampling or dropping unnecessary events for specific destinations\nIncreasing data relevance in your destinations by removing unused or unwanted data\nPreventing test or internally-generated events from reaching your production tools\nLimitations\n\nKeep the following limitations in mind when you use destination filters:\n\nDestination Filters aren\u2019t applied to events sent through the Event Tester.\nSegment applies destination filters in the following order: Sample, Drop (\u2018Only Sends\u2019 are Drops), Drop Properties, Allow Properties.\nYou can\u2019t apply destination filters to Warehouses or S3 destinations.\nEach filter can only apply to one source-destination pair.\n(For device-mode) Destination filters don\u2019t apply to items that are added to the payload server-side such as IP addresses.\n(For device-mode) Destination filters don\u2019t filter on native events that the destination SDK collects. Instead, you can use the load option to conditionally load relevant bundled JavaScript on the page. See the docs for load options.\n(For device-mode) Destination filters don\u2019t filter some fields that are collected by the destination SDK outside of Segment such as page.url and page.referrer.\n(For web device-mode) Destination filters for web device-mode only supports the Analytics.js 2.0 source. You need to enable device mode destination filters for your Analytics.js source. To do this, go to your Javascript source and navigate to Settings > Analytics.js and turn the toggle on for Destination Filters.\n(For web device-mode) Destination filters for device-mode only supports the Analytics.js 2.0 source.\n(For iOS and Android mobile device-mode) Destination filters aren\u2019t supported. Segment\u2019s Swift, Kotlin and React Native libraries do support device-mode destination filters.\n(For Kotlin, Swift, and React Native device-mode) You must enable the filters for your source. To do this, go to your source and navigate to Settings\u00a0\u00bb Advanced and turn on the toggle for Destination Filters. After you\u2019ve done that, you\u2019ll still need to add the Destination Filters plugin to your application. Instructions for this can be found here:\nKotlin\nSwift\nReact Native\nDestination Filters don\u2019t apply to events that send through the destination Event Tester.\n\nContact Segment if these limitations impact your use case.\n\nCreate a destination filter\n\nTo create a destination filter:\n\nGo to Connections > Destinations and select your destination.\nClick on the Filters tab of your destination.\nClick + New Filter.\nConfigure the rules for your filter.\n(Optional) Click Load Sample Event to see if the event passes through your filter.\nClick Next Step.\nName your filter (max. 64 length) and click the toggle to enable it.\nClick Save.\n\nEnable destination filters for Analytics.js sources\n\nIf you are currently using Analytics.js as your source and want to apply filters to device-mode destinations, you need to enable device mode destination filters for your Analytics.js source. To do this, go to your Javascript source, navigate to Settings > Analytics.js, and turn on the toggle for Destination Filters. This will ensure the filters are effectively applied to device-mode destinations.\n\nDestination filters API\n\nThe destination filters API provides more power than Segment\u2019s dashboard destination filters settings. With the API, you can create complex filters that are conditionally applied using Segment\u2019s Filter Query Language (FQL).\n\nThe destination filters API offers four different filter types:\n\nFILTER\tDETAILS\ndrop_event\tDoesn\u2019t send matched events to the destination.\nsample_event\tSends only a percentage of events through to the destination.\nwhitelist_fields\tOnly sends whitelisted properties to the destination.\nblocklist_fields\tDoesn\u2019t send blocklisted properties to the destination.\n\nTo learn more, read Segment\u2019s Destination Filters API docs.\n\nExamples\n\nThe following examples illustrate common destinations filters use cases:\n\nPII management\nControl event volume\nCleaner data\nRemove internal and test events from production tools\nSample a percentage of events\nDrop events\nOnly send events with userId\nRemove userId from payload\nPII management\n\nExample: Remove email addresses from context and properties:\n\nProperty-level allowlisting is available with Segment\u2019s API. Using destination filters, you can configure a rule that removes email addresses from context and properties. As a result, Segment only sends traits without PII to the destination.\n\nHealthcare and Life Sciences (HLS) customers can encrypt data flowing into their destinations\n\nHLS customers with a HIPAA eligible workspace can encrypt data in fields marked as Yellow in the Privacy Portal before they flow into an event stream, cloud-mode destination.\n\nTo learn more about data encryption, see the HIPAA Eligible Segment documentation.\n\nControl event volume\n\nThis example shows a filter that controls event volume by only sending User Signed Up and Demo Requested events.\n\nCleaner data\n\nThis example shows a rule that only sends track calls to Google Analytics.\n\nRemove internal and test events from production tools\n\nIn the example below, the rule targets email addresses with internal domains to stop test events from reaching Destinations.\n\nIn the example below, the rule prevents an event from sending if Order Completed and properties.email contain an internal @segment.com email address.\n\nSample a percentage of events\n\nUsing the destination filters API, you can create a rule to randomly sample video heartbeat events.\n\nDrop events\n\nWatch this destination filters walkthrough to learn how to use event names to filter events sent to destinations.\n\nOnly send events with userId\n\nUse the Public API to only send events to your destination if they contain a userId. Here\u2019s an example of how you might format this request:\n\n{\n    \"sourceId\": \"<SOURCE_ID>\",\n    \"destinationId\": \"<DESTIANTION_ID>\",\n    \"title\": \"Don't send event if userId is null\",\n    \"description\": \"Drop event if there is no userId on the request\",\n    \"if\": \"length( userId ) < 1\",\n    \"actions\": [\n      {\n        \"type\": \"DROP\"\n      }\n    ],\n    \"enabled\": true\n  }\n\nRemove userId from payload\n\nThere are certain destinations to which you may not want to send the userId. To accomplish this, you can use the Public API to create a Filter that will target and remove the userId (or any other top-level field) like this:\n\n{\n    \"sourceId\": \"<sourceId>\",\n    \"destinationId\": \"<destinationId>\",\n    \"title\": \"Don't send userId at all\",\n    \"description\": \"Drop userId on all requests\",\n    \"if\": \"all\",\n    \"actions\": [\n       {\n        \"type\": \"DROP_PROPERTIES\",\n          \"fields\": {\n            \"\":[\"userId\"]\n            }\n       }\n      ],\n      \"enabled\": true\n}\n\nFilter conditional operators\ncontains: checks whether the field\u2019s value includes the provided substring\nglob matches: case sensitive, can accept wildcard characters, checks whether the value matches provided string\nis (number): checks whether the value is exactly the provided integer\nis (string): checks whether the value is exactly the provided string\nis false: checks whether the value is type boolean and is false\nis not (number): checks whether the value isn\u2019t exactly the provided integer\nis not (string): checks whether the value isn\u2019t exactly the provided string\nis not null: checks that the existing field does not have a null value\nis null: check that the existing field has a null value\nis true: checks whether the value is type boolean and is true\nImportant notes\nConflicting settings\n\nSome destinations offer settings that also allow you to filter data. For example, the Facebook App Events destination allows you to map Screen events to Track events. Because destination filters are evaluated and applied before the destination settings are applied, they can conflict with your settings.\n\nFor example, if you have a destination filter that filters Track events and you have the Use Screen Events as Track Events setting enabled, Track events drop, but Screen events still process. The destination settings transform it into a Track event - after the filters.\n\nError handling\n\nSegment makes effort to ensure that destination filters can handle unexpected situations. For example, if you use the contains() FQL function on the null field, Segment returns false instead of returning an error. If Segment can\u2019t infer your intent, Segment logs an internal error and drops the event. Segment defaults to this behavior to prevent sensitive information, like a PII filter, from getting through.\n\nErrors aren\u2019t exposed in your Destination\u2019s Event Deliverability tab. For help diagnosing missing destination filter events, contact Segment.\n\nFAQs\nHow do destination filters work with array properties?\n\nDestination filters can filter properties out of objects nested in an array. For example, you can filter out the price property of every object in an array at properties.products. You can also filter out an entire array from the payload. However, you can\u2019t drop nested objects in an array or filter properties out of a single object in an array.\n\nTo block a specific property from all of the objects within a properties array, set the filter using the following the format: <propertyType>.<arrayName>.<arrayElementLabel>\u200b.\n\nFor example, the properties.products.newElement filter blocks all newElement property fields from each products object of an array within the properties object of a Track event.\n\nTo block the Identify event trait products.newElement, select the option under the User Traits list instead. To block the context object field products.newElement, select it from the Context Fields list.\n\nHow many filters can I create?\n\nSegment supports 10 filters per destination. If you need help consolidating filters or would like to discuss your use case, contact Segment.\n\nCan I set multiple Only Send destination filters?\n\nSegment evaluates multiple Only Send filters against each other and resolves destination filters in order. If multiple Only Send filters conflict with each other, Segment won\u2019t send information downstream.\n\nHow many properties can I view in the filter dropdown?\n\nSegment displays the most recent 15,000 properties. To find a property not in the filter dropdown, enter the property manually.\n\nHow can I filter out warehouse events?\n\nTo filter out events from warehouses, use Selective Sync.\n\nI don\u2019t see a name property at the top level of my events to filter on event name\u201d.\n\nGenerally, only Track calls have name properties, which correspond to the event field in an event.\n\nHow can I find out when new destination filters have been added or removed?\n\nThe Activity Feed shows the action, date, and user who performed the action when a destination filter is created, modified, enabled, disabled, or deleted. You can also subscribe to notifications for any of these changes in the Activity Feed settings page.\n\nWhy am I getting a permissions denied error when I try to save a filter?\n\nYou must have write access to save and edit filters. Read permission access only allows viewing and testing access.\n\nHow can I test my filter?\n\nUse the destination filter tester during setup to verify that you\u2019re filtering out the right events. Filtered events show up on the schema page but aren\u2019t counted in event deliverability graphs.\n\nCan I filter on properties/traits that have spaces in the name (for example, properties.test event field)?\n\nDestination Filters can\u2019t target properties or traits with spaces in the field name. As an alternative, use Insert Functions, which let you write code to take care of such filtering.\n\nCan I use destination filters to drop events unsupported by a destination?\n\nThe check for unsupported events types happens before any destination filter checks. As a result, Destination Filters can\u2019t prevent unsupported event type errors. To filter these events, use the Integrations Object.\n\nWhy do I see events sent through after I just added a destination filter?\n\nDestination filters only filter events sent after filter setup. If you just added a destination filter but still see some events going through, you\u2019re likely seeing retries from failed events that occurred before you set up the filter.\n\nWhen Segment sends an event to a destination but encounters a timeout error, it attempts to send the event again. As a result, if you add a destination filter while Segment is trying to send a failed event, these retries could filter through, since they reflect events that occurred before filter setup.\n\nHow do destination filters handle Protocols Transformations?\nSource-Scoped Transformations: If destination filters are enabled, Segment processes source scoped transformations before the events reach destination filters.\nDestination-Scoped Transformations: Segment processes destination-specific transformations after the events have passed through the destination filters.\nAre destination filter conditions case-sensitive?\n\nDestination filters are case-sensitive. Make sure to test your filter conditions with a test event before saving and enabling the filter.\n\nThis page was last modified: 23 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nCreate a destination filter\nDestination filters API\nExamples\nFilter conditional operators\nImportant notes\nFAQs\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nUser Subscriptions\n/\nSubscription Groups\nSubscription Groups\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE PREMIER \u2713\n?\n\nEngage Premier entered an End of Sale (EOS) period effective June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:\n\nTwilio Marketing Campaigns\n\nPreferred ISV Partners:\n\nAirship Blog\nBloomreach Blog\nBraze Blog\nInsider Blog\nKlaviyo Blog\nTwilio Engage Foundations Documentation\n\n\nSubscription groups let your users choose the emails they want to receive from you. This page introduces subscription groups and explains how you can use them with Engage email campaigns.\n\nAbout subscription groups\n\nA subscription group lets you send email campaigns to specific groups of users. Subscription groups also give your customers the ability to manage their email preferences, ensuring they only get emails they want to receive.\n\nFor example, you may want to create a subscription group that will receive only promotional email campaigns. Should a customer decide to opt out of your promotional campaigns, they\u2019ll still be able to receive email campaigns from other subscription groups you\u2019ve created and to which they\u2019ve subscribed.\n\nWhat your users experience\n\nWith subscription groups, your customers can opt in and out of groups on an individual basis instead of unsubscribing from all your campaigns.\n\nYour customers will have the chance to opt in and out of subscription groups on both a subscription preferences page and on the landing page that launches when they unsubscribe.\n\nCustomers can access these pages through the unsubscribe and manage preference links that you include in your email templates.\n\nUsing subscription groups\n\nTo use a subscription group, you\u2019ll need to first create the group, add subscribers, then create a new email template.\n\nCreate a subscription group\n\nFollow these steps to create a subscription group:\n\nNavigate to Engage > Engage settings > Subscriptions.\nClick +Create subscription group.\nAdd a name and description for the group, then click Next.\n(Optional:) Add subscribers to your group with a CSV file upload, then click Next.\nReview your new subscription group, then click Create Subscription Group.\nAdd group subscribers\n\nIn addition to adding group subscribers when you first create a subscription group, you can also add subscribers to existing groups with a CSV file upload with these steps:\n\nNavigate to Engage > Engage settings > Subscriptions.\nFrom the Subscription groups table, select the more options icon, then click Add group subscribers.\nDownload the template CSV file, then fill it out by entering email addresses and subscription groups. The subscription group should follow the format [group_name]_subscription_status.\nUpload the CSV file, then click Add Subscribers.\nValidation errors\n\nThe following table lists validation errors you may run into with your CSV upload:\n\nERROR\tERROR MESSAGE\nInvalid file types\tYou can upload only .csv files. Change your file format, then try again.\nEmpty files\tThis file contains no data. Add data to your CSV, then try again.\nCSV parsing error\tWe encountered an issue while parsing your CSV file. Validate the CSV file and try again.\nUnexpected/fallback\tSomething went wrong. Try again later.\nEmpty header row\tThis file contains empty header(s). Remove the empty header(s), then try again.\nFile exceeds one million rows\tToo many rows. You can upload up to 1000000 rows.\nFile exceeds 100 MB\tFiles can be up to 100 MB.\nExtraneous columns/column name typos\tThis file has columns that do not match the identifiers in your identity resolution configuration.\nView update history\n\nUse the Update History page to view CSV file uploads in your workspace over the last 30 days.\n\nTo view the Update History page:\n\nNavigate to Unify > Profile explorer or Engage > Engage settings > Subscriptions.\nFrom the Subscription groups table, click the three dots icon, then click View update history.\nFrom the Upload history table, click the file name link to download the error reports.\n\nEngage uses the following error codes on the report:\n\nERROR CODE\tDESCRIPTION\nINVALID_EMAIL\tThe email address isn\u2019t formatted correctly.\nINVALID_PHONE\tThe phone number is invalid.\nINVALID_SUBSCRIPTION_STATUS\tThe subscription status is invalid. Check the status or leave it blank.\nCONFIGURATION_ERROR\tYour SendGrid settings are not configured correctly. Contact Segment support for help.\nSYSTEM_ERROR\tSomething went wrong. Please try again.\nUNABLE_TO_SUBSCRIBE\tYou can\u2019t update the subscription status for this phone number because the user unsubscribed by replying STOP. The user must reply START to resubscribe.\nGLOBAL_STATE_NOT_SUBSCRIBED\tGlobal state isn\u2019t subscribed or set, so Segment can\u2019t update subscription states.\nSubscription group CSV upload limits\n\nPlease note the following limits as you upload CSV files to Twilio Engage:\n\nYou can only upload .csv files.\nFiles can\u2019t be empty and must have at least one header and one row.\nYou can\u2019t have multiple columns with the same header.\nUpload CSV files with up to 1 million rows (plus one header row).\nYou can only upload one file at a time.\nThe CSV file size can\u2019t exceed 100 MB.\nIf you upload the same email or phone number with different subscription states in a single CSV file, Engage doesn\u2019t guarantee the subscription status result.\nThe phone and email identifiers must be valid phone numbers and email addresses, otherwise they\u2019ll process as errors.\nThe subscription group CSV uploader only honors group subscriptions, so sms_subscription_status, whatsapp_subscription_status, and email_subscription_status aren\u2019t allowed.\nOther than [group_name]_subscription_status, you should set up all columns in your identity resolution configuration.\nCreate a new email template and send an email\n\nTo use subscription groups, you\u2019ll need to create a new email template with new unsubscribe and manage preference links.\n\nOnce you\u2019ve created a subscription group and added subscribers to it, follow these steps to send to the group:\n\nBuild a new email template. The template should include both unsubscribe and manage preferences links. For more on special links, view Add unsubscribe links.\nDuring email setup, select the subscription group you want to send to from the Which subscription states should receive this message? dropdown, then finish setting up and publishing your campaign.\nSet subscription group status with the Identify call\n\nSegment supports subscription groups for email. You can send statuses for email subscription groups using the Identify call.\n\nTo set susbcription groups with the Identify call, you\u2019ll need to include a key-value pair of \"type\": \"EMAIL\" and the groups object, like in the following sample payload:\n\n{\n  \"userId\": \"12aBCDeFg4hIjKlM5OPq67RS8Tu\",\n  \"context\": {\n    \"messaging_subscriptions\": [\n      {\n        \"key\": \"(123) 555-5555\",\n        \"type\": \"SMS\",\n        \"status\": \"SUBSCRIBED\" | \"UNSUBSCRIBED\" | \"DID_NOT_SUBSCRIBE\"\n      },\n      {\n        \"key\": \"(123) 555-5555\",\n        \"type\": \"WhatsApp\",\n        \"status\": \"SUBSCRIBED\" | \"UNSUBSCRIBED\" | \"DID_NOT_SUBSCRIBE\"\n      },\n      {\n        \"key\": \"test@example.com\",\n        \"type\": \"EMAIL\",\n        \"status\": \"SUBSCRIBED\" | \"UNSUBSCRIBED\" | \"DID_NOT_SUBSCRIBE\",\n        \"groups\": [\n            {\n               \"name\": \"newsletter\",\n               \"status\": \"SUBSCRIBED\" | \"UNSUBSCRIBED\" | \"DID_NOT_SUBSCRIBE\"\n            },\n            {\n               \"name\": \"marketing updates\",\n               \"status\": \"SUBSCRIBED\" | \"UNSUBSCRIBED\" | \"DID_NOT_SUBSCRIBE\"\n            }\n        ]\n      }\n    ],\n    \"externalIds\": [\n      {\n        \"id\": \"(123) 555-5555\",\n        \"type\": \"phone\",\n        \"collection\": \"users\",\n        \"encoding\": \"none\"\n      }\n    ],\n    \"traits\": {\n       \"email\": \"test@example.com\"\n    }\n  },\n  \"integrations\": {},\n  \"traits\": {}\n}\n\nFAQs\nHow many subscription groups can I have?\n\nYour Engage space includes up to 25 subscription groups.\n\nCan I use subscription groups with templates I\u2019ve already built?\n\nNo. Templates you\u2019ve previously created aren\u2019t compatible with subscription groups. To use subscription groups, you\u2019ll need to create new templates that include new unsubscribe and manage preference links.\n\nWhat happens if I delete a subscription group?\n\nIf you delete a subscription group, Engage will still maintain the preferences of the group\u2019s end users.\n\nWhat subscription group events does the Engage Channels Source send?\n\nThe Engage Events Source tracks four subscription group events: Email Unsubscribed, Email Group Unsubscribed, Channel Subscription Updated, and Group Subscription Updated.\n\nHow can users opt back in if they\u2019ve unsubscribed from all groups?\n\nIf a user unsubscribes from all of your subscription groups, they\u2019ll need to re-subscribe by explicitly opting back in to each group.\n\nDo subscription preference links work in test emails?\n\nYes. Test emails include fully functional unsubscribe and subscription preference links. If a test email recipient unsubscribes using a test email, Segment updates that user\u2019s subscription state.\n\n\n\nTest emails temporarily override an email subscription state. This means that an unsubscribed email address can receive a test email but won\u2019t receive regular email campaigns from which they\u2019ve unsubscribed.\n\nShould I follow any conventions when naming a subscription group?\n\nYes. Keep the following table in mind when you name a subscription group:\n\n\n\n\nFIELD\tCONVENTION\nGroup Name Character Limit\tLimited to 75 characters, including spaces\nGroup Description Character Limit\tLimited to 500 characters, including spaces\nSpaces in Group Names\tSpaces aren\u2019t allowed at the beginning and/or end of the Group name\nUnsupported characters for Group Names\t!@#$%^&*()_+\\-=\\[\\]{};':\"\\\\|,.<>\\/?\nUnsupported accent characters for Group Names\t\u00e1, \u00e9, \u00ed, \u00f3, \u00fa, \u00e0, \u00e8, \u00ec, \u00f2, \u00f9, \u00eb, \u00ef, \u00e3\n\nThis page was last modified: 15 Jul 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nAbout subscription groups\nUsing subscription groups\nSet subscription group status with the Identify call\nFAQs\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nProtocols\n/\nApis And Extensions\n/\nTypewriter\nTypewriter\n\nTypewriter for analytics.js and analytics-node will receive no new features and only critical maintenance updates from Segment. Typewriter for other libraries and SDKs are not actively maintained by Segment. Typewriter is available on GitHub under the MIT license for the open-source community to fork and contribute.\n\nTypewriter is a tool for generating strongly-typed Segment analytics libraries based on your pre-defined Tracking Plan spec.\n\nAt a high-level, Typewriter can take an event from your Tracking Plan like this \"Order Completed\" event:\n\nTypewriter uses the event to generate a typed analytics call in different languages:\n\n// Example client in your web app\nimport typewriter from './analytics'  \n\ntypewriter.orderCompleted({\n  orderID: 'ck-f306fe0e-cc21-445a-9caa-08245a9aa52c',\n  total:   39.99\n})\n\n// Example client in your iOS app\nSEGTypewriterAnalytics.orderCompleted(\n  orderID: \"ck-f306fe0e-cc21-445a-9caa-08245a9aa52c\",\n  total: 39.99\n)\n\n\nTypewriter can generate clients for analytics.js, analytics-node, analytics-swift and analytics-kotlin.\n\nThese generated clients are embedded with metadata from your Tracking Plan, which contextualizes your analytics instrumentation, and reduces (or entirely eliminates!) incorrect instrumentations in your production environments. In your editor, you can access event names, descriptions, property names, types and more:\n\nYou can also configure Typewriter to validate analytic events at runtime, which can alert you to instrumentation errors during development and testing. Typewriter can warn you about missing required properties, invalid enum values, regex mismatches, and any other advanced JSON Schema you configure in your Tracking Plan.\n\nYou can use this with a test suite to automatically fail your unit tests if the instrumentation generates any violations:\n\nIf you use a statically typed language (such as TypeScript, Java, Objective-C, or Swift), you get access to compile-time warnings about your instrumentation:\n\nTypewriter also helps teams adopt analytics best practices, such as avoiding autogenerated event names, and carefully considering what properties are tracked.\n\nTo get started, check out one of the quickstart guides below:\n\nBrowser Quickstart\nKotlin Quickstart\nNode.js Quickstart\nReact Native Quickstart\nSwift Quickstart\n\nFor use with the Analytics-iOS and Analytics-Android SDK, use Typewriter v7.\n\nHave feedback on Typewriter? Consider opening a GitHub Issue in the @segmentio/typewriter repository.\n\nPrerequisites\n\nTypewriter is built using Node.js, and requires node >= 14.x\n\nYou can check if you have Node and NPM installed by running the following commands in your command-line window:\n\n$ node --version\nv14.x\n\n\nIf you don\u2019t have these, you\u2019ll need to install node. Installing node also installs npm and npx for you. If you\u2019re on macOS, you can install it with Homebrew:\n\n$ brew install node\n\n\nOnce you\u2019ve installed Node and NPM, run the --version commands again to verify that they were installed correctly.\n\nBrowser Quickstart\n\nTo get started with Typewriter in your browser:\n\nMake sure you have node installed using the instructions in the prerequisites above.\nInstall analytics.js in your app. There are two methods.\nSnippet method (most common): Paste the snippet in theStep 1: Copy the Snippet from the analytics.js Quickstart Guide.\nNPM method: Load analytics.js with the npm library. Learn more about using the npm method in the @segmentio/analytics-next repository.\n\nOnce you\u2019ve got analytics.js installed, add Typewriter as a developer dependency in your project:\n\n $ npm install --save-dev typewriter\n\nIf you are a snippet user that uses TypeScript, you should also install the npm library as a dev dependency to get the typescript types.\n   $ npm install --save-dev @segment/analytics-next\n\n\nRun npx typewriter init to use the Typewriter quickstart wizard that generates a typewriter.yml configuration, along with your first Typewriter client. When you run the command, it creates a typewriter.yml file in your project. For more information on the format of this file, see the Typewriter Configuration Reference. The command also adds a new Typewriter client in ./analytics (or whichever path you configured). You can import this client into your project, like so:\n\n // Import your auto-generated Typewriter client:\n import typewriter from './analytics'\n\n // Issue your first Typewriter track call!\n typewriter.orderCompleted({\n   orderID: 'ck-f306fe0e-cc21-445a-9caa-08245a9aa52c',\n   total:   39.99\n })\n\n\n### Configuration for snippet + TypeScript users\n\n // Optional but recommended: you can improve your developer experience by adding typings for the global analytics object.\n import type { AnalyticsSnippet } from '@segment/analytics-next'\n    \n declare global {\n   interface Window {\n     analytics: AnalyticsSnippet;\n }\n\n\n### Configuration for NPM users\n\n // As an npm user, you *must* explicitly pass in your analytics instance.\n import { AnalyticsBrowser } from '@segment/analytics-next'\n    \n const analytics = AnalyticsBrowser.load({ writeKey: 'YOUR_WRITE_KEY' })\n\n typewriter.setTypewriterOptions({\n   analytics: analytics\n })\n\n\nRun npx typewriter to regenerate your Typewriter client. You need to do this each time you update your Tracking Plan.\n\nTo help you minimize your bundle size, Typewriter supports tree-shaking using named exports. All generated analytics calls generate and export automatically, so you can import them like so:\n\nimport { orderCompleted } from './analytics'\n\norderCompleted({\n  orderID: 'ck-f306fe0e-cc21-445a-9caa-08245a9aa52c',\n  total:   39.99\n})\n\n\nTypewriter wraps your analytics calls in an ES6 Proxy, which helps protect your application from crashing if you make analytics calls with a generated function that doesn\u2019t exist. For example, if an Order Completed event didn\u2019t exist in your Tracking Plan in the first example above, then your app would crash with a TypeError: typewriter.orderCompleted is not a function. However, since Typewriter dynamically proxies the underlying function calls, it can detect if a function doesn\u2019t exist, and handle it for you. Typewriter logs a warning message, then fires an Unknown Analytics Call Fired event into your source. This helps to prevent regressions when you migrate JavaScript projects to Typewriter in bulk. Keep in mind that proxying doesn\u2019t work with named exports.\n\nNode.js Quickstart\n\nTo get started with Node.js:\n\nMake sure you have node installed using the instructions in the prerequisites above.\nInstall @segment/analytics-node in your app. For now, you just need to complete Step 2: Install the Module from the analytics-node Quickstart Guide.\n\nOnce you have analytics-node installed, add Typewriter as a developer dependency in your project:\n\n $ npm install --save-dev typewriter\n\n\nRun npx typewriter init to use the Typewriter quickstart wizard that generates a typewriter.yml configuration, along with your first Typewriter client. When you run the command, it creates a typewriter.yml file in your repo. For more information on the format of this file, see the Typewriter Configuration Reference. The command also adds a new Typewriter client in ./analytics (or whichever path you configured). You can import this client into your project, like so:\n\n // Initialize analytics-node, per the analytics-node guide above.\n import { Analytics } from '@segment/analytics-node'\n\n const analytics = new Analytics({ writeKey: '<MY_WRITE_KEY>' })\n\n app.post('/login', (req, res) => {\n    analytics.identify({\n       userId: req.body.userId,\n       previousId: req.body.previousId\n   })\n   res.sendStatus(200)\n })\n    \n app.post('/cart', (req, res) => {\n   analytics.track({\n     userId: req.body.userId,\n     event: 'Add to cart',\n     properties: { productId: '123456' }\n   })\n    res.sendStatus(201)\n });\n\n\nRun npx typewriter to regenerate your Typewriter client. You need to do this each time you update your Tracking Plan.\n\nTypewriter wraps your analytics calls in an ES6 Proxy, which helps protect your application from crashing if you make analytics calls with a generated function that doesn\u2019t exist. For example, if an Order Completed event didn\u2019t exist in your Tracking Plan in the first example above, then your app would crash with a TypeError: typewriter.orderCompleted is not a function. However, since typewriter dynamically proxies the underlying function calls, it can detect if a function does not exist, and handle it for you. Typewriter logs a warning message, then fires an Unknown Analytics Call Fired event into your source. This helps to prevent regressions when you migrate JavaScript projects to Typewriter in bulk. Keep in mind that proxying doesn\u2019t work with named exports.\n\nSwift Quickstart\n\nFor use with the analytics-ios SDK, use Typewriter v7.\n\nTo get started using Typewriter with Swift:\n\nMake sure you have node installed using the instructions in the prerequisites above.\nInstall analytics-swift in your app. Follow the analytics-swift Quickstart Guide.\n\nRun npx typewriter init to use the Typewriter quickstart wizard that generates a typewriter.yml configuration, along with your first Typewriter client. When you run the command, it creates a typewriter.yml file in your repo. For more information on the format of this file, see the Typewriter Configuration Reference.\n\n\nNote: Run npx typewriter to regenerate your Typewriter client. You need to do this each time you update your Tracking Plan.\n\nImport your new Typewriter client into your project using XCode. If you place your generated files into a folder in your project, import the project as a group not a folder reference.\n\n\nWhen you add the generated client to your Xcode Project you can use as a Swift extension method on any Analytics client object:\n\n Analytics.main.orderCompleted(OrderCompleted(\n   orderID: \"ck-f306fe0e-cc21-445a-9caa-08245a9aa52c\",\n   total: 39.99\n ))\n\nKotlin Quickstart\n\nFor use with the analytics-android SDK, use Typewriter v7.\n\nTo get started using Typewriter with Kotlin:\n\nMake sure you have node installed. Use the instructions in the prerequisites above.\nInstall analytics-kotlin in your app. Follow the analytics-kotlin QuickStart Guide.\nRun npx typewriter init. This command enables you to use the Typewriter quickstart wizard that generates a typewriter.yml configuration, along with your first Typewriter client. The command creates a typewriter.yml file in your repo. For more information on the format of this file, see the Typewriter Configuration Reference.\n\nTypewriter creates the class file with the package name typewriter. Segment recommends you to enter the right package name during npx typewriter init by choosing to review the Advanced Options for Kotlin. You can also enter the right package name directly in typewriter.yml:\n\n  client:\n    language: kotlin\n    sdk: kotlin\n    languageOptions:\n      package: com.segment.typewriter\n\n\nRun npx typewriter to regenerate your Typewriter client. You need to do this each time you update your Tracking Plan.\n\nYou can now use your Typewriter client in your Android Kotlin or Java application as extensions to any Analytics object:\n\nKotlin:\n\n// Import your auto-generated Typewriter client:\nimport com.segment.generated.*\nanalytics.orderCompleted(OrderCompleted(orderID = \"110\", total = 39.98))\n\n\nJava:\n\n// Import your auto-generated Typewriter client:\nimport com.segment.generated.*\n\n// Issue your first Typewriter track call!\nTypewriterAnalytics.with(this).orderCompleted(\n  OrderCompleted.Builder()\n    .orderID(\"ck-f306fe0e-cc21-445a-9caa-08245a9aa52c\")\n    .total(39.99)\n    .build()\n);\n\nReact Native Quickstart\n\nTo get started with React Native:\n\nFollow the Getting Started guide for React Native.\n\nAdd typewriter as a dev dependency in your project once you have the library installed in your project.\n\n $ npm install --save-dev typewriter\n\n\nRun npx typewriter init to use the Typewriter quickstart wizard that generates a typewriter.yml configuration along with your first Typewriter client.\n\nThis command creates a typewriter.yml file in your repo. For more information on the format of this file, see the Typewriter Configuration Reference. The command also adds a new Typewriter / Segment client in ./analytics (or whichever path you configured). You can use this interchangeably as a normal React Native Segment client. It contains additional methods for your tracking plan:\n\n import {\n   createClient,\n   AnalyticsProvider,\n } from '../typewriter'; // Remember to import the methods from your typewriter generated file!\n const segmentClient = createClient({\n   writeKey: 'SEGMENT_API_KEY'\n });\n const App = () => (\n   <AnalyticsProvider client={segmentClient}>\n     <Content />\n   </AnalyticsProvider>\n );\n\n\nFrom there you can use it with hooks:\n\n import React from 'react';\n import { Text, TouchableOpacity } from 'react-native';\n import { useAnalytics } from '../typewriter'; // Important! To\n const Button = () => {\n   const { orderCompleted } = useAnalytics();\n   return (\n     <TouchableOpacity\n       style={styles.button}\n       onPress={() => {\n         orderCompleted({orderID: \"111\", total: 39.99});\n       }}\n     >\n       <Text style={styles.text}>Press me!</Text>\n     </TouchableOpacity>\n   );\n };\n\n\nOr directly through the client:\n\n segmentClient.orderCompleted({orderID: \"111\", total: 39.99});\n // Remember this is just an extended client with the typewriter methods so all the normal segment methods still work!\n segmentClient.track('Untyped event');\n\n\nRun npx typewriter to regenerate your Typewriter client. You need to do this each time you update your Tracking Plan.\n\nAdding Events\n\nTo update or add a new event to a Typewriter client, first apply your changes to your Tracking Plan. Then run the following:\n\n# Run this in the directory with your repo's `typewriter.yml`.\n$ npx typewriter\n\nAPI Token Configuration\n\nTypewriter requires a Segment API token to fetch Tracking Plans from the Segment Public API.\n\nYou must be a workspace owner to create Segment API tokens.\n\nTo create an API token:\n\nClick on the Tokens tab on the Access Management page and click Create Token.\nChoose Segment\u2019s Public API.\nAdd a description for the token and assign access. If you choose Workspace Member, you only need to select Tracking Plan Read-Only for the Resource Role, as Typewriter only needs the Tracking Plan Read-Only role.\nClick Create.\n\nTypewriter looks for an API token in three ways, in the following order:\n\nIf a token is piped through, it will use that token. For example, echo $TW_TOKEN | typewriter build.\nTypewriter executes a token script from the typewriter.yml. See Token Script for more information.\nTypewriter reads the contents of the ~/.typewriter file.\n\nThe quickstart wizard prompts you for an API token and stores it in ~/.typewriter for you.\n\nSegment recommends you use a Token Script to share an API token with your team. When you use a token script, you can supply your API token as an environment variable (echo $TYPEWRITER_TOKEN), from an .env. file (source .env; echo $TYPEWRITER_TOKEN) or using any other CLI tool for providing secrets.\n\nSegment also recommends you to pipe through your API Token as this will let you keep your token secret, but it also allows you to share it across your team.\n\nSegment is temporarily keeping the Token Script execution for compatibility purposes. Segment might deprecate this feature in the future, and encourages you to execute your script and pipe in the token. For example, echo $TW_TOKEN | typewriter build.\n\nEditor Configuration\n\nTo make the most of Typewriter, Segment recommends installing a few extensions:\n\nJavaScript\n\nTypewriter clients include function documentation adhering to the JSDoc specification. Install the relevant extension below for JSDoc support in your editor:\n\nVSCode: Supports JSDoc out-of-the-box.\nAtom: Install the official atom-ide-ui and ide-typescript plugins (the latter provides JavaScript support).\nSublime Text: Install tern_for_sublime. And then follow this guide\u2019s advice on configuring Tern.\n\nTypeScript\n\nFor intellisense in TypeScript clients, install the relevant extension below for TypeScript support in your editor. If your project is a mix between JavaScript and TypeScript, then you should also install the plugins in the JavaScript section above so that your editor will also support JSDoc intellisense.\n\nVSCode: Supports TypeScript out-of-the-box.\nAtom: Install the official atom-ide-ui and ide-typescript plugins.\nSublime Text: Install the TypeScript plugin from Package Control.\n\niOS\n\nXCode does not require any extra configuration and shows intellisense out-of-the-box.\n\nAndroid\n\nAndroid Studio does not require any extra configuration and shows intellisense out-of-the-box.\n\nBest Practices\n\nSegment strongly recommends that you store your Tracking Plan (plan.json) in a version control system. This guarantees that Typewriter will generate the same client, regardless of any changes you make to your Tracking Plan in the Segment app. Otherwise, changes to your Tracking Plan could lead to broken builds.\n\nSegment recommends that you only check in the plan.json, and generate your Typewriter client during the application build step (by calling npx typewriter). You can do this in git with the following .gitignore:\n\n# Make sure to update `analytics` to the full path to your Typewriter client.\nanalytics/*\n!analytics/plan.json\n\n\nIf this isn\u2019t possible you can also check in the full generated client. Segment, however, doesn\u2019t recommend this method.\n\nConfiguration Reference\n\nTypewriter stores its configuration in a typewriter.yml file in the root of your repo. A sample configuration might look like this:\n\n# Segment Typewriter Configuration Reference (https://github.com/segmentio/typewriter)\n# Just run `npx typewriter` to re-generate a client with the latest versions of these events.\n\nscripts:\n  # You can supply a Segment API token using a `script.token` command. See `Token Script` below.\n  token: source .env; echo $TYPEWRITER_TOKEN\n  # You can format any of Typewriter's auto-generated files using a `script.after` command.\n  # See `Formatting Generated Files` below.\n  after: ./node_modules/.bin/prettier --write analytics/plan.json\n\nclient:\n  # Which Segment SDK you are generating for.\n  # Valid values: analytics.js, analytics-node, analytics-react-native, swift, kotlin.\n  sdk: analytics-node\n  # The target language for your Typewriter client.\n  # Valid values: javascript, typescript, kotlin, swift.\n  language: typescript\n\ntrackingPlans:\n  # The Segment Protocols Tracking Plan that you are generating a client for.\n  # Provide your workspace slug and Tracking Plan id, both of which can be found\n  # in the URL when viewing the Tracking Plan editor. For example:\n  # https://app.segment.com/segment-demo/protocols/tracking-plans/rs_QhWHOgp7xg8wkYxilH3scd2uRID\n  # You also need to supply a path to a directory to save your Typewriter client.\n  - id: rs_QhWHOgp7xg8wkYxilH3scd2uRID\n    workspaceSlug: segment-demo\n    path: ./analytics\n\n\nAt any time, you can regenerate this file by running the Typewriter quickstart wizard:\n\n$ npx typewriter init\n\nToken Script\n\nSegment is keeping the Token Script execution for compatibility purposes only in v8 of Typewriter. Segment might deprecate this feature in the future, and encourages you to execute your script and pipe in the token. For example, echo $TW_TOKEN | typewriter build.\n\nIf your team has a standard way to supply secrets (passwords and tokens) in development environments, whether that\u2019s an .env file or an AWS-backed secret store, you can configure Typewriter to use it to get a Segment API token.\n\nTo configure this, create a token script called scripts.token in your typewriter.yml. This script is a string that contains a shell command that, when executed, outputs a valid Segment API token. Here\u2019s an insecure, example:\n\nscripts:\n  # NOTE: NEVER commit a Segment API token to your version control system.\n  token: echo \"OIEGO$*hf83hfh034fnosnfiOEfowienfownfnoweunfoiwenf...\"\n\n\nTo give a real example, Segment stores secrets in segmentio/chamber which is backed by AWS Parameter Store. Providing access to a token in chamber looks like this:\n\nscripts:\n  token: aws-okta exec dev-privileged -- chamber export typewriter | jq -r .typewriter_token\n\n\nTo learn more about the typewriter.yml configuration format, see the Configuration Reference.\n\nFormatting Generated Files\n\nIn your typewriter.yml, you can configure a script (scripts.after) that fires after generating a Typewriter client. You can use this to apply your team\u2019s style guide to any of Typewriter\u2019s auto-generated files.\n\nFor example, if you want to apply your prettier formatting to plan.json (the local snapshot of your Tracking Plan), you can use an after script like this:\n\nscripts:\n  after: ./node_modules/.bin/prettier --write ./analytics/plan.json\n\n\nTo learn more about the typewriter.yml configuration format, see the Configuration Reference.\n\nConnecting to CI\n\nAs mentioned in the Best Practices section above, Segment recommends that you only check in the plan.json, and not the generated clients, into your version control. Instead, Segment recommends building these clients as part of the build step for your application.\n\nIn your CI environment, this usually involves a step to build the Typewriter client. Make sure to build the production client before deploying the application, as explained in the Tracking Plan Violation Handling section below.\n\n# An example (simplified) CircleCI configuration:\njobs:\n  test:\n    steps:\n      - npx typewriter development\n      - yarn run test\n\n  deploy:\n    steps:\n      - npx typewriter production\n      - yarn run deploy\n\nTracking Plan Violation Handling\n\nYou can also configure Typewriter to validate analytic events at runtime, which can alert you to instrumentation errors during development and testing. By default, Typewriter generates a \u201cdevelopment\u201d build, which means that it includes this logic. You can generate a \u201cproduction\u201d build that omits this logic:\n\n# To build a development client (the default, if not supplied):\n$ npx typewriter development\n# To build a production client:\n$ npx typewriter production\n\n\nNot all languages support run-time validation. Currently, analytics.js and analytics-node support it using AJV (both for JavaScript and TypeScript projects) while analytics-ios and analytics-android do not yet support run-time validation. Typewriter also doesn\u2019t support run-time validation using Common JSON Schema. For languages that don\u2019t support run-time validation, the development and production clients are identical.\n\nSegment recommends you to use a development build when testing your application locally, or when running tests. Segment generally recommends against using a development build in production, since this includes a full copy of your Tracking Plan which can increase the size of the application.\n\nYou can provide a custom handler that fires whenever a violation is seen. By default, this handler logs a warning.\n\nFor analytics.js and analytics-node clients, you can configure this handler with setTypewriterOptions:\n\nimport typewriter from './analytics'\n\nconst yourViolationHandler = (message, violations) => {\n  console.error(`Typewriter Violation found in ${message.event}`, violations)\n}\n\ntypewriter.setTypewriterOptions({\n  onViolation: yourViolationHandler\n})\n\n\nA common use case for this handler is to configure Typewriter to detect when your tests are running and if so, throw an error to fail your unit tests. For example:\n\nconst typewriter = require('./analytics')\n\nconst yourViolationHandler = (message, violations) => {\n  if (process.env.IS_TESTING === 'true') {\n    throw new Error(`Typewriter Violation found in ${message.event}`)\n  }\n}\n\ntypewriter.setTypewriterOptions({\n  onViolation: yourViolationHandler\n})\n\n\nTypewriter is preconfigured in analytics-node environments to throw an error if NODE_ENV=test, which is set by most Node.js testing libraries such as ava and jest.\n\nAnother common use case is to customize how violations are reported to your team. For example, Segment customized this handler to show a toast notification to developers in-app:\n\nimport typewriter from './analytics'\nimport { toaster } from 'evergreen-ui'\n\ntypewriter.setTypewriterOptions({\n  // Note that this handler only fires in development mode, since we ship the production build\n  // of Typewriter to customers.\n  onViolation: (msg, violations) => {\n    toaster.warning(`\"${msg.event}\" Fired with Tracking Plan Violation`, {\n      description: violations[0].message\n    })\n  }\n})\n\nKnown Limitations\n\nTypewriter only supports track calls. However, you can continue to use the underlying (untyped) analytics instance to perform identify, group, page, screen, and alias calls.\n\nNot all languages support run-time validation. Currently, analytics.js and analytics-node support it using AJV (both for JavaScript and TypeScript projects) while analytics-swift and analytics-kotlin don\u2019t support run-time validation. Typewriter also does not support event validation using the Common JSON Schema.\n\nContributing\n\nIf you\u2019re interested in contributing, open an issue on GitHub and Segment can help provide you pointers to get started.\n\nFeedback\n\nSegment welcomes feedback you may have on your experience with Typewriter. To contact Segment, open an issue on GitHub.\n\nThis page was last modified: 12 Aug 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nPrerequisites\nBrowser Quickstart\nNode.js Quickstart\nSwift Quickstart\nKotlin Quickstart\nReact Native Quickstart\nAdding Events\nAPI Token Configuration\nEditor Configuration\nBest Practices\nConfiguration Reference\nToken Script\nFormatting Generated Files\nConnecting to CI\nTracking Plan Violation Handling\nKnown Limitations\nContributing\nFeedback\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nPrivacy\n/\nConsent Management\n/\nConfigure Consent Management\nConfigure Consent Management\nFREE X\nTEAM X\nBUSINESS \u2713\nADDON X\n?\n\nAfter setting up your consent management platform (CMP), you can enforce the consent collected from your users by adding the consent object to your events.\n\nOnce you\u2019ve configured consent in the Segment app and updated your sources to contain consent preference in every event, your events are routed only to the categories your end users consented to share data with. Events without the consent preference will continue to flow to destinations without consent enforcement.\n\nPrerequisites\n\nConsent management edit and update capabilities limited to Workspace Owners\n\nOnly users with the Workspace Owner role are able to create, edit, and disable consent categories. All other users have read-only access to Consent Management features.\n\nBefore you can configure consent in Segment, take the following steps:\n\nSet up your third-party consent management tool and create consent categories. Take note of your consent categories and the key or ID associated with each category.\nKnow how your company uses each destination. You need to know which destinations to map to each category.\nAccess to your web and mobile libraries. After you set up consent categories in the Segment app, you need to integrate your CMP and your Segment sources using a wrapper or other solution.\nFor Analytics.js sources only : Navigate to your Analytics.js source. Select Settings > Analytics.js and enable Destination Filters.\nStep 1: Create consent categories in the Segment app\n\nLimited availability of destinations\n\nAWS S3 and Engage destinations do not enforce consent preferences.\n\nFrom the Segment homepage, select the Privacy tab and click Consent Management.\nOn the Consent management page, click Create categories.\nConfirm that you have completed the required prerequisites, and click Next.\nOn the Create consent categories page, add the following information to the category form:\nCategory name: Enter a name that describes your use case for the data sent to this destination.\nCategory ID: In OneTrust, this is a string of up to five alphanumeric characters, but other CMPs may have a different format. This field is case sensitive, cannot start with a number, and must have fewer than 35 characters.\nMapped destinations: Select one or more of your destinations to map to this category. Category mappings apply to all instances of a destination.\nAfter you\u2019ve finished setting up your category or categories, click Save.\n\nSegment recommends mapping all destinations to a category\n\nSegment assumes all destinations without a mapping do not require user consent and will receive all events containing a consent object. If a destination is mapped to multiple categories, a user must consent to all categories for data to flow to the destination.\n\nStep 2: Integrating your CMP with Segment\n\nOnce you\u2019ve created consent categories in the Segment app, you need to integrate your CMP with Segment.\n\nSegment supports the following CMPs:\n\nCONSENT MANAGEMENT PLATFORM\tSUPPORTED WEB LIBRARIES\tSUPPORTED MOBILE LIBRARIES\tCONTACT\nOneTrust\t Analytics.js*\t Kotlin\n Swift\n React Native\tFor support and troubleshooting, contact Segment.\nTrustArc\t Analytics.js\t\tFor support and troubleshooting, contact TrustArc.\nKetch\t Analytics.js\t\tFor support and troubleshooting, contact Ketch.\n\n*If you send data to device-mode destinations connected to your Analytics.js source, you must navigate to your Analytics.js source in the Segment app, select Settings > Analytics.js, and enable Destination Filters.\n\nFor more information about Segment\u2019s Analytics.js OneTrust wrapper, see the Analytics.js OneTrust Wrapper documentation.\n\nIf you\u2019d like to integrate with any other CMP, Segment requires you to build your own wrapper or use any mechanism provided it meets the following requirements for data and event generation:\n\nReads the end user consent preference from your CMP and includes the consent object in every event\nIf using Unify and Engage, generates the Segment Consent Preference Updated event every time a user provides or updates their consent preferences with their anonymousId and userId\n\nTo get started building your own wrapper, follow the instructions in the @segment/analytics-consent-tools repository.\n\nConsent Management is not backwards compatible with Segment's legacy iOS and Android libraries\n\nIf you are using one of Segment\u2019s legacy mobile libraries (iOS or Android,) you will need to upgrade to Swift or Kotlin before using Consent Management.\n\nValidate your CMP integration\n\nCustomers with Analytics.js 2.0 sources can use the Segment Inspector to confirm that events from their source contain the consent object. Unify and Engage users can also verify that the Segment Consent Preference Updated event emits every time end users update their consent preferences.\n\nAll users can validate that events contain the consent object and that the Segment Consent Preference Updated event is present using Segment\u2019s Source Debugger.\n\nYou can also confirm your events flow to destinations or are blocked from destinations according to the consent categories you created in Step 1: Create consent categories in the Segment App, if already connected to the destination.\n\nEdit consent categories\n\nIf you need to make changes to your consent categories, you can edit them on the Consent Management page. You may experience some latency between making the changes and having the changes take effect.\n\nFrom the Segment homepage, select the Privacy tab and click Consent Management.\nOn the Consent Management page, navigate to the consent category you\u2019d like to edit and click Edit.\nOn the Edit consent category page, you can make changes to the consent category name, ID, and the destinations connected to a category.\nWhen you\u2019ve made your changes, click Save.\n\nThe Audit Trail surfaces information about when a consent category is created, modified, or disabled, and when consent mappings are created or removed.\n\nDisable consent categories\n\nDisabling a consent category means that Segment no longer enforces end user consent preferences for the destinations in the disabled category. Other consent categories aren\u2019t affected.\n\nFrom the Segment homepage, select the Privacy tab and click Consent Management.\nOn the Consent Management page, disable the toggle for the category you\u2019d like to disable.\nOn the \u201cDisable [category-name]?\u201d popup, enter the category name in the Consent category name field and click Disable category.\n\nThis page was last modified: 16 Oct 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nPrerequisites\nStep 1: Create consent categories in the Segment app\nStep 2: Integrating your CMP with Segment\nEdit consent categories\nDisable consent categories\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nApi\n/\nConfig Api\n/\nAuthentication\nAuthentication\n\nThe Segment Public API is available\n\nSegment\u2019s Public API is available for Team and Business tier customers to use. You can use the Public API and Config APIs in parallel, but moving forward any API updates will come to the Public API exclusively.\n\nPlease contact your account team or friends@segment.com with any questions.\n\nYou can access the Config API programmatically using access tokens. When you authenticate with an access token, you have access to any resource and permission assigned to the token.\n\nCreate an Access Token\n\nAs a workspace owner, you can create access tokens from the Access Management page in Admin settings. You can assign the same granularity of permissions as you can for a logged-in user. As best practice, tokens should be assigned the least permissions needed to perform a required API action. All tokens must have a description.\n\nSecret Token\n\nYou can not retrieve the plain-text token later, so you should save it in a secret manager. If you lose the token you can generate a new one.\n\nUse an Access Token\n\nNow that you have an access token, you can use this token to access the Config API by setting it in the Authorization header of your requests, for example:\n\n$ ACCESS_TOKEN=qiTgISif4zprgBb_5j4hXfp3qhDbxrntWwwOaHgAMr8.gg9ok4Bk7sWlP67rFyXeH3ABBsXyWqNuoXbXZPv1y2g\n\n$ curl \\\n  -X GET \\\n  -H \"Authorization: Bearer $ACCESS_TOKEN\" \\\n  https://platform.segmentapis.com/v1beta/workspaces\n\n\nExample response:\n\n{\n  \"workspaces\": [\n    {\n      \"name\": \"myworkspace\",\n      \"display_name\": \"My Space\",\n      \"id\": \"e5bdb0902b\",\n      \"create_time\": \"2018-08-08T13:24:02.651Z\"\n    }\n  ]\n}\n\nScopes\n\nYou cannot use access tokens created with the workspace:read scope to create or update resources. If you do so, you\u2019ll get the following error:\n\n{\n  \"error\": \"insufficient scope\",\n  \"code\": 7\n}\n\n\nSee Config API Errors for error codes.\n\nThis page was last modified: 27 Sep 2022\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nCreate an Access Token\nUse an Access Token\nScopes\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nContent\n/\nOrganizing Your Templates\nOrganizing Your Templates\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE PREMIER \u2713\n?\n\nEngage Premier entered an End of Sale (EOS) period effective June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:\n\nTwilio Marketing Campaigns\n\nPreferred ISV Partners:\n\nAirship Blog\nBloomreach Blog\nBraze Blog\nInsider Blog\nKlaviyo Blog\nTwilio Engage Foundations Documentation\n\n\nTo add structure to your marketing content, you can organize templates into folders and duplicate them within your Segment space.\n\nOrganize with folders\n\nUse folders to organize your Email, SMS/MMS, Push, and WhatsApp content templates. Group related content together to better help you manage and find your marketing resources.\n\nFrom the Templates overview page you can create, update, view, and delete template folders.\n\nYou must have both read and write workspace permissions to create or make changes to folders.\n\nTo create a folder:\n\nNavigate to Engage > Content.\nSelect the tab for the template type (Email, SMS, WhatsApp, or Push) you\u2019d like to create the folder for.\nClick Create, then select Folder.\nAdd a folder name, then click Create.\n\nYou can also rename, add templates, or disband your folder from the Templates overview page. Disbanding a folder returns all templates from the folder to the main template list, without deleting any of the templates.\n\nYou can only organize templates in your folders according to template type. For example, you can\u2019t group email and SMS templates in the same folder.\n\nMove templates to your folders\n\nFrom the Templates overview page, you can select individual template(s) to move to your folders.\n\nAfter you select the templates you\u2019d like to move:\n\nClick Actions, and select Move Templates.\nSelect the destination folder, then click Move templates to folder.\n\nUse the Actions button in your folder to remove templates or move them to a different location. When you remove a template, Engage returns the template to the Templates overview page, without deleting it.\n\nDuplicate an Engage template\n\nYou can clone existing Engage templates to edit and use in your message campaigns.\n\nDuplicate email, SMS, and push templates\n\nTo duplicate an email, SMS, or push template:\n\nNavigate to Engage > Content.\nSelect the tab for the template type (Email, SMS, or Push) you\u2019d like to clone.\nSelect the \u2026 icon next to your template, then click Duplicate.\nConfigure your duplicate template:\nFor SMS and push, edit your template, and save the duplicate once you\u2019re finished.\nFor email, add a template name on the Duplicate Template popup screen, then click Duplicate. You can then edit your email template from the Templates page.\n\nLearn more about configuring email, SMS, and push templates.\n\nDuplicate WhatsApp templates\n\nTo duplicate WhatsApp templates:\n\nNavigate to Engage > Content > WhatsApp.\nSelect the template you want to duplicate.\nClick Duplicate.\nAdd a template name and language, then click Duplicate.\nConfigure and save your WhatsApp template.\n\nLearn more about configuring WhatsApp templates.\n\nThis page was last modified: 15 Jul 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nOrganize with folders\nDuplicate an Engage template\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nContent\n/\nEmail\n/\nEmail Template\nEmail Template\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE PREMIER \u2713\n?\n\nEngage Premier entered an End of Sale (EOS) period effective June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:\n\nTwilio Marketing Campaigns\n\nPreferred ISV Partners:\n\nAirship Blog\nBloomreach Blog\nBraze Blog\nInsider Blog\nKlaviyo Blog\nTwilio Engage Foundations Documentation\n\n\nUse Twilio Engage to build personalized email templates to store and use throughout marketing campaigns.\n\nBuild an email template from scratch using the Drag and Drop Editor or the HTML Editor. Include personalized content in the subject line, preview text, and email body to engage with users based on their profile traits and actions.\n\nBuild an Email template\n\nNavigate to Engage > Content and use the Email Templates page to preview and edit existing templates.\n\nTo configure an email template, click Create Template.\n\nSelect Email, and click Configure.\n\nYou must first connect a SendGrid subuser account to your Segment space to build email templates in Engage. Visit the onboarding steps for more information.\n\nConfigure the email template.\nAdd a template name for internal reference.\nAdd an internal description.\nEnter the sender email address. You can optionally include profile traits.\nEmails can only be sent from verified domains.\nEnter the sender name.\nIndicate if you want replies sent back to the initial sender. If not, add a \u201creply to\u201d email and name, or include profile traits.\nAdd email addresses to receive a blind carbon copy of your email.\nAdd preview text and the subject line. Use merge tags to personalize the email template with real-time profile traits.\nSelect the design method to build your email template:\nDrag and Drop Editor is a drag and drop WYSIWYG tool with customizable content blocks.\nHTML Editor contains both a code and visual editor from a single view. This editor provides complete HTML editing access with error flagging.\nDesign the email template, then click Create Email Template.\n\nEngage content validation\n\nFor all content editors in Engage, you\u2019ll see alerts for any issues in your template, such as invalid profile traits or incorrect liquid syntax. Engage both flags template issue(s), and displays recommended next steps. While you can save these templates, you must fix any issues before using them in Engage campaigns.\n\nTest the Email template\n\nYou can send test emails before you include a template in marketing campaigns.\n\nSelect the email template you want to test on the Templates screen.\nFrom the Template Settings page, click Test email.\nIf your template has profile traits, enter a trait value for the test email. This ensures that your merge tags work as expected.\nTo test a default value, leave the profile traits field blank. Default values must be assigned in your merge tags. For example, loyal customer would be the default for the following merge tag: {{profile.traits.first_name | default: \"loyal customer\"}}.\nTo test traits with arrays, you must input the array in JSON format. For example, [\u201citem1\u201d, \u201citem2\u201d] or [{\u201citemName\u201d: \u201cshoes\u201d}]. If you don\u2019t use JSON format for arrays in the test message side sheet, your template might not render as expected.\nEnter recipient email addresses for the test message.\nProfiles that you send test messages to must have a userId in Segment.\nSelect Send test email.\n\nYou can also test email templates directly from a Send an Email step in Journeys.\n\nDynamic sender using merge tags\n\nEngage supports dynamic sending using merge tags. Personalize email content by adding real-time profile traits in merge tags to the following fields:\n\nSubject line\nPreview text\nMessage body\nFrom sender\nReply to email\n\nAs you configure the template, click Merge Tags and select the profile traits to include. Engage inserts the merge tags based on cursor placement.\n\nFor all merge tags, you must add a default value inside a single quote. For example: {{profile.traits.traits | default: 'Default'}}\nOnly use variable tags in liquid sytax.\n\nThe following table contains a description and some best practices for all fields in the email template. Asterisks indicate required fields.\n\nFIELD\tDESCRIPTION\nTemplate Name*\tThe email template name.\nDescription\tA description for the template.\nFrom sender*\tThe email address users will see in the from field of the email campaign.\n\nFor the profile trait and default value, use a valid username. For example:\n- default: 'jsmith' is valid\n- default: 'j smith' is invalid\nSender name*\tThe name users will see next to the sender email.\nReply to email*\tThe email address that will receive any replies users send. You can use different Sender and reply-to email addresses. Email recipients will see this address if they reply to your campaign.\n\nThe profile trait and default value must be one of the following:\n- A valid email address\n- A valid username for the email address (the input field needs to end with a valid domain for the email address)\nReply to name*\tThe name users will see next to the reply-to email address.\nBCC\tEmail address that will receive a blind carbon copy of your email campaign.\nPreview text\tA brief message that displays next to the email subject.\nSubject*\tThe email subject.\n\nYou can also add merge tags in the heading or body text as you design an email with the Drag and Drop or HTML editors. Engage supports liquid templating to create dynamic content in the email design editor.\n\nUse liquid statements with an image URL\n\nIf you\u2019re using the image content module in the Drag and Drop Editor, you can\u2019t use liquid statements in the Image URL field. To use liquid statements with an image, Segment recommends using an HTML block with the following syntax:\n<img src=\u201c{{profile.traits.imageLink | default: '<insert your default URL here>'}}\u201d, where profile.traits.imageLink is an example profile trait representing personalized image links for each recipient.\n\nTo learn more about profile traits, visit Segment\u2019s Computed Traits and SQL Traits documentation.\n\nInclude unsubscribe and manage preference links\n\nWhen you build an email template, you\u2019ll need to include links that your customers can access to unsubscribe and manage their email preferences. You\u2019ll find both in the Special Links dropdown menu of the Insert/Edit link window.\n\nUnsubscribe links\n\nWhen you build email templates, it\u2019s your responsibility to include an unsubscribe link in your message. Add unsubscribe links to an email template from the Drag and Drop or HTML editors.\n\nWhen a recipient clicks on an unsubscribe link, they\u2019ll see a confirmation page and the recipient\u2019s subscription state is updated.\n\nLearn more about User Subscriptions in Twilio Engage.\n\nManage preference links\n\nThe manage preference link lets your customers opt in and out of email groups on an individual basis instead of unsubscribing from all your campaigns.\n\nFor more information, see subscription groups.\n\nArrays and objects in Broadcasts\n\nSegment doesn\u2019t support profile traits in object and array datatypes in Broadcasts, but you cam use them in Journeys.\n\nNext steps\n\nView some email deliverability tips and tricks from SendGrid.\n\nYou can also use the Templates screen in Engage to build SMS templates.\n\nFAQs\nDo updates to an email template automatically apply to Journey steps using it?\n\nWhen you add a template to a Journey step, it becomes a copy specific to that step. Changes made to the original template won\u2019t update the Journey version, and edits made in the Journey step won\u2019t affect the original template. This keeps your Journey changes separate while preserving the original for reuse.\n\nThis page was last modified: 26 Nov 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nBuild an Email template\nTest the Email template\nDynamic sender using merge tags\nInclude unsubscribe and manage preference links\nNext steps\nFAQs\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nEngage and Warehouses\nEngage and Warehouses\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nEngage provides a complete, up-to-date view of your users customer journey as it unfolds, and one of the best ways to understand the data produced by this journey is by analyzing the data in your data warehouse using SQL.\n\nWith Engage, you can send Computed Traits and Audiences to a data warehouse like Redshift, BigQuery, or Snowflake. This allows you to perform analysis and reporting around key customer audiences and campaigns, as well set up your user data as input into predictive models.\n\nSegment makes it easy to load your customer profile data into a clean schema, so your analysts can help answer some of your toughest business questions.\n\nSet up\n\nWhen you build an audience or computed trait, you can configure it to send an identify call or a track call to your data warehouse, and additionally include mobile ids.\n\nIdentify calls for audiences\n\nIf you chose to send your Engage data as an identify call, Engage usually sends one call per user.\n\nWhen you send audiences as an identify call, Engage includes a boolean trait that matches the audience name. When a user enters an audience the boolean is set to true, and when they exit, the boolean is set to false.\n\nIn the example below, you can see that the identify payload includes a trait of the audience first_time_shopper with the value of true.\n\n{\n  \"type\": \"identify\",\n  \"userId\": u123,\n  \"traits\": {\n     \"first_time_shopper\": true // false when a user exits the audience\n  }\n}\n\nIdentify calls for computed traits\n\nWhen you send computed traits as an identify call, Engage sends a similar call with the computed value for that trait. In the example below, the trait total_revenue_180_days includes the calculated value of 450.00.\n\n{\n  \"type\": \"identify\",\n  \"userId\": u123,\n  \"traits\": {\n     \"total_revenue_180_days\": 450.00\n  }\n}\n\nWarehouse schema for Engage identify calls\n\nEngage identify calls appear in your warehouse using a similar format as normal Connections identify calls. Identify calls appear in two tables per Engage space. These tables are named with a prefix of engage_, then the Engage space name, followed by identifies or users. The identifies table contains a record of every identify call, and the users table contains one record per user_id with the most recent value.\n\nThe engage_ schema name is specific to the Engage space and cannot be modified. Additional audiences and computed traits appear as additional columns in these tables.\n\nengage_default.identifies\n\nUSER_ID\tFIRST_TIME_SHOPPER\tTOTAL_REVENUE_180_DAYS\nu123\ttrue\t\u00a0\nu123\t\u00a0\t450.0\n\nengage_default.users\n\nUSER_ID\tFIRST_TIME_SHOPPER\tTOTAL_REVENUE_180_DAYS\nu123\ttrue\t450.00\nTrack calls for audiences\n\nWhen you send audiences using track calls, Engage sends an Audience Entered event when a user enters, and an Audience Exited event when the user exits, by default. These event names are configurable.\n\nEngage also sends two event properties about the audience: the audience_key, which records the name of the audience that the event modifies, and the audience name and its value, as a separate key and value pair. The value of the audience key is populated with a boolean value.\n\nIn the example below, you can see that the audience_key is set to record a modification to the first_time_shopper audience, and the first_time_shopper value is set to true.\n\n{\n  \"type\": \"track\",\n  \"userId\": u123,\n  \"event\": \"Audience Entered\",\n  \"traits\": {\n     \"audience_key\": \"first_time_shopper\",\n     \"first_time_shopper\": true\n  }\n}\n\nTrack calls for computed traits\n\nWhen you send computed traits, Engage sends a Trait Computed event that records which computed trait it updates, then records the updated key and value. You can also customize this event name.\n\nIn the example below, the Trait Computed event contains the trait_key which records which computed trait is being modified, and then includes the key total_revenue_180_days with the updated value of 450.00.\n\n{\n  \"type\": \"track\",\n  \"userId\": u123,\n  \"event\": \"Trait Computed\",\n  \"traits\": {\n     \"trait_key\": \"total_revenue_180_days\",\n     \"total_revenue_180_days\": 450.00\n  }\n}\n\nWarehouse schema for Engage track calls\n\nSimilar to track calls in Connections, Engage track calls appear in your warehouse as one table per event name. For example, if you configure your events called Audience Entered, Audience Exited, and Trait Computed, Engage would create tables like the following examples in your warehouse:\n\nengage_default.audience_entered\n\nUSER_ID\tAUDIENCE_KEY\tFIRST_TIME_SHOPPER\nu123\tfirst_time_shopper\ttrue\n\nengage_default.audience_exited\n\nUSER_ID\tAUDIENCE_KEY\tFIRST_TIME_SHOPPER\nu123\tfirst_time_shopper\tfalse\n\nengage_default.trait_computed\n\nUSER_ID\tTOTAL_REVENUE_180_DAYS\tTRAIT_KEY\nu123\t450.00\ttotal_revenue_180_days\nUsers table\n\nThe users table is an aggregate view based on the user_id field. This means that anonymous profiles with just an anonymous_id identifier aren\u2019t included in this view. You can still view identify calls for anonymous audiences and computed traits in the identifies table.\n\nThe users table is synced as soon as the warehouse is connected as a destination in Engage, if you\u2019ve previously created Engage computations. As a result, the table might contain data from computations not directly connected to the warehouse.\n\nSync frequency\n\nAlthough Engage can compute audiences and traits in real-time, these calculations are subject to the sync schedule allowed by your warehouses plan, which is usually hourly. You can check the warehouse sync history to see details about past and upcoming syncs. When you look at the sync schedule, sources with the engage_ prefix sync data from Engage.\n\nCommon questions\nCan I prevent a table, a computed trait, or audience from syncing to my warehouse?\n\nYes. You can use Warehouses Selective Sync to manage which traits, audiences, and tables get synced from Engage.\n\nWhy are there multiple schemas prefixed with engage_ in my warehouse when I only have one space?\n\nSegment can only connect a source to one instance of each destination. For example, one source cannot send to two different Amplitude instances. As a workaround, Engage creates multiple sources to send events to the destinations connected to your space.\n\nFor example, if you have three webhook destinations in your space, Engage creates three different sources to send events to them. This creates three different warehouse schemas, and is usually the reason you have more schemas than spaces.\n\nThis approach doesn\u2019t apply to messaging destinations, however. Messaging destinations connected from journeys and broadcasts don\u2019t generate multiple background sources.\n\nThis page was last modified: 30 Jan 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSet up\nIdentify calls for audiences\nIdentify calls for computed traits\nWarehouse schema for Engage identify calls\nWarehouse schema for Engage track calls\nUsers table\nSync frequency\nCommon questions\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nSegment App\n/\nIam\n/\nManage Workspace Access\nManage Workspace Access\nFREE X\nTEAM X\nBUSINESS \u2713\nADD-ON X\n?\n\nThis page explains how to add Team Members and User Groups to your team\u2019s workspace, how to assign them roles, and how to remove them.\n\nNote: Only Workspace Owners can change a workspace\u2019s Access Management settings.\n\nInvite a new team member\n\nNote: Workspaces that are on Free or Team plans can only grant the workspace owner and source admin roles.\n\nNavigate to the Workspace settings then to Access management. (Only Workspace owners can access this section.)\nClick Invite Team Member.\nEnter the user\u2019s email address and choose their roles, OR assign them to a user group (if available).\nSelect the roles to grant to the user, and choose any specific resources to grant them access to. See the Roles documentation for more information.\nChange a team member\u2019s access\nNavigate to the Workspace settings > Access management and click the Users tab.\nEnter the user\u2019s name or email to search for their member record.\nReview the current permissions in the table.\nClick the member to see details.\nSelect or deselect roles, and choose the specific resources to grant access to.\n\nTip: You can also grant the role for all current and future resources. For admin roles, this includes the ability to create new resources.\n\nCreate a new User Group\n\nWorkspace Owners can manage permissions for groups of team members who should have similar access using \u201cuser groups\u201d.\n\nNavigate to the Workspace settings > Access management and click the Groups tab.\nClick Create User Group.\nEnter a name for the group.\nSelect or deselect roles, and choose the specific resources to grant the group access to.\nAdd team members to the group. You can search by name or email to locate the team member(s).\nReview the group permissions in the panel on the right, and click Create User Group to save the new group.\nAdd a team member to a User Group\n\nYou can make changes to group membership from two places in the Segment App: From the Edit Team Member page (the user\u2019s individual access page), and from the Edit User Group page, where you can see all members of the group.\n\nTo add a team member from the Edit Team Member page:\n\nNavigate to the Workspace settings > Access management and click the Members tab.\nSelect the team member you would like to add to the group.\nClick Add User Group.\nSelect the user group(s) to add the team member to.\nClick Save.\n\nTip: This method is best when adding a single team member to one or more user groups.\n\nTo add a team member from the Edit User Group page:\n\nNavigate to the Workspace settings > Access management and click the Groups tab.\nSelect the User Group you want to add new users to.\nClick Edit User Group in the panel on the right.\nClick the Members tab and click + Add.\nAdd team members to the group. Search by name or email to locate the team member(s).\nClick Save.\n\nTip: This method is best when adding more than one user to a single user group at the same time\n\nRemove a team member from a User Group\nNavigate to the Workspace settings > Access management and click the Groups tab.\nSelect the User Group you would like to modify.\nClick Edit User Group in the panel on the right.\nNavigate to the Members tab and select the team members you would like to remove.\nClick Save.\nRemove a team member from your workspace\n\nOpen the member details and click Remove Team Member at the top.\n\nTeam Management with Single Sign On\n\nIf you are on a Business plan and choose to use Single Sign On (SSO), you grant implicit access to your workspace by assigning team members access to Segment from your identity provider. By default, you grant these users minimal workspace access.\n\nSegment supports \u201cJust-In-Time\u201d user provisioning using SSO. Any users with access to the application as defined in your IDP can seamlessly create a Segment account when they first log in. By default, all automatically-provisioned users created this way are created as Workspace Members with Minimal Workspace Access.\n\nOnce they have been created, Workspace Owners can update these users\u2019 access from the Access Management page in the Segment App.\n\nSegment does not support programmatic de-provisioning at this time. However, if your workspace uses SSO, a user that cannot authenticate to your IDP cannot view or edit any of your workspaces or their contents.\n\nRequest Access\n\nIf you are a workspace member, you might encounter a section of the Segment App that you do not have access to view. If you need expanded permissions, you can request access directly in the Segment App. Once submitted, Access requests are sent to all workspace owners by email.\n\nTo review an access request, workspace owners click the link in the access request email to go to their workspace\u2019s Access Management Settings. The requestor\u2019s access request message appears on the Segment Access Management page, and the workspace owner can adjust the user\u2019s permissions. The access request message disappears after the permissions are updated.\n\nRemove invalid or expired Invite\n\nWhen you send an invitation to an incorrect email address or the token included in the email invite link expires, the invite might still show up as \u201cInvite Pending\u201d in the Segment App. In these cases, you can revoke the invite to remove it and resend the invite if needed. The invitation will expire within a few days. Therefore, if a user accepts an invite with an expired link, Segment does not grant them access.\n\nTo revoke invite:\n\nNavigate to the Workspace settings > Access management\nClick on the invite that you would like to remove, which also shows Invite Pending\nClick Edit Invite in the panel on the right.\nClick the Revoke Invite on the top right side to remove the invite\n\nThis page was last modified: 03 Aug 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nInvite a new team member\nChange a team member\u2019s access\nCreate a new User Group\nAdd a team member to a User Group\nRemove a team member from a User Group\nRemove a team member from your workspace\nTeam Management with Single Sign On\nRequest Access\nRemove invalid or expired Invite\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSpec\n/\nSpec: Common Fields\nSpec: Common Fields\n\nIn the Segment Spec all the API calls have a common structure, and a few common fields.\n\nHowever, not all destinations accept all fields included in the Spec. Not sure which fields a destination accepts? Refer to the destination\u2019s documentation page, or check out the open-source destination code on GitHub.\n\nSegment University: The Segment Methods\n\nCheck out our high-level overview of these APIs in Segment University. (Must be logged in to access.)\n\nStructure\n\nEvery API call has the same core structure and fields. These fields describe user identity, timestamping, and mechanical aides like API version.\n\nHere\u2019s an example of these common fields in raw JSON:\n\n{\n  \"anonymousId\": \"507f191e810c19729de860ea\",\n  \"context\": {\n    \"active\": true,\n    \"app\": {\n      \"name\": \"InitechGlobal\",\n      \"version\": \"545\",\n      \"build\": \"3.0.1.545\",\n      \"namespace\": \"com.production.segment\"\n    },\n    \"campaign\": {\n      \"name\": \"TPS Innovation Newsletter\",\n      \"source\": \"Newsletter\",\n      \"medium\": \"email\",\n      \"term\": \"tps reports\",\n      \"content\": \"image link\"\n    },\n    \"device\": {\n      \"id\": \"B5372DB0-C21E-11E4-8DFC-AA07A5B093DB\",\n      \"advertisingId\": \"7A3CBEA0-BDF5-11E4-8DFC-AA07A5B093DB\",\n      \"adTrackingEnabled\": true,\n      \"manufacturer\": \"Apple\",\n      \"model\": \"iPhone7,2\",\n      \"name\": \"maguro\",\n      \"type\": \"ios\",\n      \"token\": \"ff15bc0c20c4aa6cd50854ff165fd265c838e5405bfeb9571066395b8c9da449\"\n    },\n    \"ip\": \"8.8.8.8\",\n    \"library\": {\n      \"name\": \"analytics.js\",\n      \"version\": \"2.11.1\"\n    },\n    \"locale\": \"en-US\",\n    \"network\": {\n      \"bluetooth\": false,\n      \"carrier\": \"T-Mobile US\",\n      \"cellular\": true,\n      \"wifi\": false\n    },\n    \"os\": {\n      \"name\": \"iPhone OS\",\n      \"version\": \"8.1.3\"\n    },\n    \"page\": {\n      \"path\": \"/academy/\",\n      \"referrer\": \"\",\n      \"search\": \"\",\n      \"title\": \"Analytics Academy\",\n      \"url\": \"https://segment.com/academy/\"\n    },\n    \"referrer\": {\n      \"id\": \"ABCD582CDEFFFF01919\",\n      \"type\": \"dataxu\"\n    },\n    \"screen\": {\n      \"width\": 320,\n      \"height\": 568,\n      \"density\": 2\n    },\n    \"groupId\": \"12345\",\n    \"timezone\": \"Europe/Amsterdam\",\n    \"userAgent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36\",\n    \"userAgentData\": {\n      \"brands\": [\n        {\n          \"brand\": \"Google Chrome\",\n          \"version\": \"113\"\n        },\n        {\n          \"brand\": \"Chromium\",\n          \"version\": \"113\"\n        },\n        {\n          \"brand\": \"Not-A.Brand\",\n          \"version\": \"24\"\n        }\n      ],\n      \"mobile\": false,\n      \"platform\": \"macOS\"\n    }\n  },\n  \"integrations\": {\n    \"All\": true,\n    \"Mixpanel\": false,\n    \"Salesforce\": false\n  },\n  \"event\": \"Report Submitted\",\n  \"messageId\": \"022bb90c-bbac-11e4-8dfc-aa07a5b093db\",\n  \"receivedAt\": \"2015-12-10T04:08:31.909Z\",\n  \"sentAt\": \"2015-12-10T04:08:31.581Z\",\n  \"timestamp\": \"2015-12-10T04:08:31.905Z\",\n  \"type\": \"track\",\n  \"userId\": \"97980cfea0067\",\n  \"version\": 2\n}\n\n\n\nIn more detail these common fields for every API call are:\n\nFIELD\t\tTYPE\tDESCRIPTION\nanonymousId\trequired; optional if userID is set instead\tString\tA pseudo-unique substitute for a User ID, for cases when you don\u2019t have an absolutely unique identifier. A userId or an anonymousId is required. See the Identities docs for more details.\ncontext\toptional\tObject\tDictionary of extra information that provides useful context about a message, but is not directly related to the API call like ip address or locale See the Context field docs for more details.\nintegrations\toptional\tObject\tDictionary of destinations to either enable or disable See the Destinations field docs for more details.\nmessageId\timplicit\tString\tAutomatically collected by Segment, a unique identifier for each message that lets you find an individual message across the API. This field is limited to 100 characters.\nreceivedAt\timplicit\tDate\tAutomatically set by Segment, the timestamp of when a message is received by Segment It is an ISO-8601 date string. See the Timestamps fields docs for more detail.\nsentAt\toptional\tDate\tTimestamp of when a message is sent to Segment, used for clock skew correction It is set automatically by the Segment tracking libraries. It is an ISO-8601 date string. See the Timestamps fields docs for more detail.\ntimestamp\toptional\tDate\tTimestamp when the message itself took place, defaulted to the current time by the Segment Tracking API, as a ISO-8601 format date string. If the event just happened, leave it out and we\u2019ll use the server\u2019s time. If you\u2019re importing data from the past, make sure you to provide a timestamp.See the Timestamps fields docs for more detail.\ntype\timplicit\tString\tType of message, corresponding to the API method: 'identify', 'group', 'track', 'page', 'screen' or 'alias'.\nuserId\trequired; optional if anonymousID is set instead\tString\tUnique identifier for the user in your database. A userId or an anonymousId is required. See the Identities docs for more details.\nversion\timplicit\tNumber\tVersion of the Tracking API that received the message, automatically set by Segment.\n\nBeyond this common structure, each API call adds a few specialized top-level fields.\n\nContext\n\nContext is a dictionary of extra information that provides useful context about a datapoint, for example the user\u2019s ip address or locale. You should only use Context fields for their intended meaning.\n\nFIELD\tTYPE\tDESCRIPTION\nactive\tBoolean\tWhether a user is active.\n\nThis is usually used to flag an .identify() call to just update the traits but not \u201clast seen.\u201d\napp\tObject\tdictionary of information about the current application, containing name, version, and build.\n\nThis is collected automatically from the mobile libraries when possible.\ncampaign\tObject\tDictionary of information about the campaign that resulted in the API call, containing name, source, medium, term, content, and any other custom UTM parameter.\n\nThis maps directly to the common UTM campaign parameters.\ndevice\tObject\tDictionary of information about the device, containing id, advertisingId, manufacturer, model, name, type, and version.\n\nNote: If you collect information about iOS devices, note that the model value set by Apple might not exactly correspond to an iPhone model number. For example, an iPhone 15 Pro Max has a model value of iPhone16,2.\nip\tString\tCurrent user\u2019s IP address.\nlibrary\tObject\tDictionary of information about the library making the requests to the API, containing name and version.\nlocale\tString\tLocale string for the current user, for example en-US.\nnetwork\tObject\tDictionary of information about the current network connection, containing bluetooth, carrier, cellular, and wifi. If the context.network.cellular and context.network.wifi fields are empty, then the user is offline.\nos\tObject\tDictionary of information about the operating system, containing name and version.\npage\tObject\tDictionary of information about the current page in the browser, containing path, referrer, search, title and url. This is automatically collected by Analytics.js.\nreferrer\tObject\tDictionary of information about the way the user was referred to the website or app, containing type, name, url, and link.\nscreen\tObject\tDictionary of information about the device\u2019s screen, containing density, height, and width.\ntimezone\tString\tTimezones are sent as tzdata strings to add user timezone information which might be stripped from the timestamp, for example America/New_York.\ngroupId\tString\tGroup / Account ID.\n\nThis is useful in B2B use cases where you need to attribute your non-group calls to a company or account. It is relied on by several Customer Success and CRM tools.\ntraits\tObject\tDictionary of traits of the current user.\n\nThis is useful in cases where you need to track an event, but also associate information from a previous Identify call. You should fill this object the same way you would fill traits in an identify call.\nuserAgent\tString\tUser agent of the device making the request.\nuserAgentData\tObject\tThe user agent data of the device making the request. This always contains brands, mobile, platform, and may contain bitness, model, platformVersion,uaFullVersion, fullVersionList, wow64, if requested and available.\n\nThis populates if the Client Hints API is available on the browser.\n\nThis may contain more information than is available in the userAgent in some cases.\nchannel\tString\twhere the request originated from: server, browser or mobile\nContext fields automatically collected\n\nBelow is a chart that shows you which context variables are populated automatically by the iOS, Android, and analytics.js libraries.\n\nOther libraries only collect context.library, any other context variables must be sent manually.\n\nCONTEXT FIELD\tANALYTICS.JS\tANALYTICS-IOS\tANALYTICS-ANDROID\napp.name\t\u00a0\t\u2705\t\u2705\napp.version\t\u00a0\t\u2705\t\u2705\napp.build\t\u00a0\t\u2705\t\u2705\ncampaign.name\t\u2705\t\u00a0\t\u00a0\ncampaign.source\t\u2705\t\u00a0\t\u00a0\ncampaign.medium\t\u2705\t\u00a0\t\u00a0\ncampaign.term\t\u2705\t\u00a0\t\u00a0\ncampaign.content\t\u2705\t\u00a0\t\u00a0\ndevice.type\t\u00a0\t\u2705\t\u2705\ndevice.id\t\u00a0\t\u2705\t\u2705\ndevice.advertisingId\t\u00a0\t\u2705\t\u2705\ndevice.adTrackingEnabled\t\u00a0\t\u2705\t\u2705\ndevice.manufacturer\t\u00a0\t\u2705\t\u2705\ndevice.model\t\u00a0\t\u2705\t\u2705\ndevice.name\t\u00a0\t\u2705\t\u2705\nlibrary.name\t\u2705\t\u2705\t\u2705\nlibrary.version\t\u2705\t\u2705\t\u2705\nip*\t\u2705\t\u2705\t\u2705\nlocale\t\u2705\t\u2705\t\u2705\nnetwork.bluetooth\t\u00a0\t\u00a0\t\u2705\nnetwork.carrier\t\u00a0\t\u2705\t\u2705\nnetwork.cellular\t\u00a0\t\u2705\t\u2705\nnetwork.wifi\t\u00a0\t\u2705\t\u2705\nos.name\t\u00a0\t\u2705\t\u2705\nos.version\t\u00a0\t\u2705\t\u2705\npage.path\t\u2705\t\u00a0\t\u00a0\npage.referrer\t\u2705\t\u00a0\t\u00a0\npage.search\t\u2705\t\u00a0\t\u00a0\npage.title\t\u2705\t\u00a0\t\u00a0\npage.url\t\u2705\t\u00a0\t\u00a0\nscreen.density\t\u00a0\t\u00a0\t\u2705\nscreen.height\t\u00a0\t\u2705\t\u2705\nscreen.width\t\u00a0\t\u2705\t\u2705\ntraits\t\u00a0\t\u2705\t\u2705\nuserAgent\t\u2705\t\u00a0\t\u2705\nuserAgentData*\t\u2705\t\u00a0\t\u00a0\ntimezone\t\u2705\t\u2705\t\u2705\nIP Address isn\u2019t collected by Segment\u2019s libraries, but is instead filled in by Segment\u2019s servers when it receives a message for client side events only.\n\nIPv6\n\nSegment doesn\u2019t support automatically collecting IPv6 addresses.\n\nThe Android library collects screen.density with this method.\n\nuserAgentData is only collected if the Client Hints API is available on the browser.\n\nSegment doesn\u2019t collect or append to the context of subsequent calls in the new mobile libraries (Swift, Kotlin, and React Native).\n\nTo pass the context variables which are not automatically collected by Segment\u2019s libraries, you must manually include them in the event payload. The following code shows how to pass groupId as the context field of Analytics.js\u2019s .track() event:\n\nanalytics.track(\"Report Submitted\", {}, {\n  context: {\n    groupId: \"1234\"\n  }\n});\n\n\nTo add fields to the context object in the new mobile libraries, you must utilize a custom plugin. Documentation for creating plugins for each library can be found here:\n\nReact Native\nSwift\nKotlin\nIntegrations\n\nA dictionary of destination names that the message should be sent to. 'All' is a special key that applies when no key for a specific destination is found.\n\nIntegrations defaults to the following:\n\n{\n  All: true,\n  Salesforce: false,\n}\n\n\nThis is because Salesforce has strict limits on API calls.\n\nSending data to the rest of Segment\u2019s destinations is opt-out so if you don\u2019t specify the destination as false in this object, it will be sent to rest of the destinations that can accept it.\n\nTimestamps\n\nEvery API call has four timestamps, originalTimestamp, timestamp, sentAt, and receivedAt. They\u2019re used for very different purposes.\n\nAll timestamps are ISO-8601 date strings, and are in the UTC timezone. To see the user\u2019s timezone information, check the timezone field that\u2019s automatically collected by client-side libraries.\n\nYou must use ISO-8601 date strings that include timezones when you use timestamps with Engage. If you send custom traits without a timezone, Segment doesn\u2019t save the timestamp value.\n\nTimestamp overview\nTIMESTAMP\tCALCULATED\tDESCRIPTION\noriginalTimestamp\tTime on the client device when call was invoked\nOR\nThe timestamp value manually passed in through server-side libraries.\tUsed by Segment to calculate timestamp.\n\nNote: originalTimestamp is not useful for analysis since it\u2019s not always trustworthy as it can be easily adjusted and affected by clock skew.\nsentAt\tTime on client device when call was sent.\nOR\nsentAt value manually passed in.\tUsed by Segment to calculate timestamp.\n\nNote: sentAt is not useful for analysis since it\u2019s not always trustworthy as it can be easily adjusted and affected by clock skew.\nreceivedAt\tTime on Segment server clock when call was received\tUsed by Segment to calculate timestamp, and used as sort key in Warehouses.\n\nNote: For max query speed, receivedAt is the recommended timestamp for analysis when chronology does not matter as chronology is not ensured.\ntimestamp\tCalculated by Segment to correct client-device clock skew using the following formula:\nreceivedAt - (sentAt - originalTimestamp)\tUsed by Segment to send to downstream destinations, and used for historical replays.\n\nNote: Recommended timestamp for analysis when chronology does matter.\noriginalTimestamp\n\nThe originalTimestamp tells you when call was invoked on the client device or the value of timestamp that you manually passed in.\n\nNote: The originalTimestamp timestamp is not useful for any analysis since it\u2019s not always trustworthy as it can be easily adjusted and affected by clock skew.\n\nsentAt\n\nThe sentAt timestamp specifies the clock time for the client\u2019s device when the network request was made to the Segment API. For libraries and systems that send batched requests, there can be a long gap between a datapoint\u2019s timestamp and sentAt. Combined with receivedAt, Segment uses sentAt to correct the original timestamp in situations where a user\u2019s device clock cannot be trusted (mobile phones and browsers). The sentAt and receivedAt timestamps are assumed to occur at the same time (maximum a few hundred milliseconds), and therefore the difference is the user\u2019s device clock skew, which can be applied back to correct the timestamp.\n\nNote: The sentAt timestamp is not useful for any analysis since it\u2019s tainted by user\u2019s clock skew.\n\nSegment now adds `sentAt` to a payload when the batch is complete and initially tried to the Segment API for the Swift, Kotlin, and C# mobile libraries\n\nThis update changes the value of the Segment-calculated timestamp to align closer with the receivedAt value rather than the originalTimestamp value. For most users who are online when events are sent, this does not significantly impact their data. However, if your application utilizes an offline mode where events are queued up for any period of time, the timestamp value for those users now more closely reflects when Segment received the events rather than the time they occurred on the users\u2019 devices.\n\nreceivedAt\n\nThe receivedAt timestamp is added to incoming messages as soon as they hit the API. It\u2019s used in combination with sentAt to correct clock skew, and also to aid with debugging libraries and systems that deliver events in batches.\n\nThe receivedAt timestamp is most important as the sort key in Segment\u2019s Warehouses product. Use this for max query speed when retrieving data from your Warehouse.\n\nNote: Chronological order of events is not ensured with receivedAt.\n\ntimestamp\n\nThe timestamp timestamp specifies when the data point occurred, corrected for client-device clock skew. This is the timestamp that is passed to downstream destinations and used for historical replays. It is important to use this timestamp for importing historical data to the API.\n\nIf you are using the Segment server Source libraries, or passing calls directly to the HTTP API endpoint, you can manually set the timestamp field. This change updates the originalTimestamp field of the Segment event. If you use a Segment Source in device mode, the library generates timestamp and you cannot manually set one directly in the call payload.\n\nSegment calculates timestamp as timestamp = receivedAt - (sentAt - originalTimeStamp).\n\nFor client-side tracking it\u2019s possible for the client to spoof the originalTimeStamp, which may result in a calculated timestamp value set in the future.\n\nFAQ\nWhy Are Events Received with Timestamps Set in the Past or Future?\n\nIf you\u2019re using one of Segment\u2019s client-side libraries, please note that several factors can cause timestamp discrepancies in your event data.\n\nOverriding Timestamp Value:\nWhen a manual timestamp is set in the payload with a date in the past, it can cause events to appear as if they were sent earlier than they actually were.\nAnalytics.js Source with Retries Enabled:\nThe Retries feature supports offline traffic by queuing events in Analytics.js. These events are sent or retried later when an internet connection is available, keeping the original timestamp intact.\nMobile App Backgrounded or Closed:\nIf a user closes the app, events may be queued within the app. These queued events won\u2019t be sent until the app is re-opened, potentially in the future, leading to timestamp discrepancies.\nInaccurate Browser/Device Clock Settings:\nTimestamps can be incorrect if the client\u2019s device time is inaccurate, as the originalTimestamp relies on the client device\u2019s clock, which can be manually adjusted.\nTraffic from Internet Bots:\nInternet Bots can sometimes send requests with unusual timestamps, either intentionally or due to incorrect settings, leading to discrepancies.\n\nThis page was last modified: 30 Oct 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nStructure\nContext\nContext fields automatically collected\nIntegrations\nTimestamps\nFAQ\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nJourneys\n/\nUnderstanding Journeys Logic\nUnderstanding Journeys Logic\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nJourneys are powered by a series of Audiences and Computed Traits. This guide defines the logic used to create sequential campaigns.\n\nBy the end of this guide, you\u2019ll understand how and why users progress through your Journey. You\u2019ll also gain familiarity with the following key Journeys concepts:\n\nJourney entry conditions and step behavior\nHow Segment evaluates Journeys step membership\nHow real-time step membership works\nEntry conditions and step behavior\n\nJourneys begin with an entry condition that computes like standard Engage Audiences. This entry condition queries your customer data in Segment to find users who meet your specified criteria.\n\nAfter users meet the Journey\u2019s entry condition, their progress through the Journey depends on satisfying the criteria of subsequent Journey steps.\n\nJourney steps operate based on the following behaviors:\n\nOnly the entry condition can backfill event data from before Journey publication.\nThe entry condition requires no previous step membership.\nPost-entry condition step membership relies on users at some point entering the preceding step.\nWhen a user first joins a step, Segment adds a step_joined_time trait to their profile.\nMembership is calculated using Segment\u2019s Real-Time Compute System.\nSegment doesn\u2019t calculate Waits and Splits in real-time.\n\nThe combination of these traits, audiences, and business rules allows you to create an enforced funnel with the following implications:\n\nUsers enter the Journey when they fulfill the entry conditions.\nUsers can\u2019t re-enter the same Journey at an earlier step.\nUsers can only move forward through a Journey.\nUsers remain in a step indefinitely until they fulfill the next step\u2019s criteria.\nStep membership\n\nTo enter a Journey, users must satisfy the entry conditions.\n\nTo enter each subsequent step, three conditions must be true:\n\nThe user previously joined the parent step.\nThe user meets the next step\u2019s conditions.\nThe users satisfies wait conditions.\nCondition steps\n\n\u201cAdd a condition\u201d steps operate like Engage Audiences. The defined conditions provide criteria for each step\u2019s membership.\n\nWait times\n\nWhen you add a \u201cWait\u201d step to a Journey, Segment automatically includes wait times in the membership criteria of the next condition step. Journeys represents wait times in relation to the preceding_step_joined_time trait, which must be at least N time ago.\n\nThe following table summarizes the three step membership conditions and their equivalents in written logic:\n\nSEMANTIC LOGIC\tWRITTEN LOGIC CONDITION\nHas the user ever joined the previous step?\tDoes preceding_step_audience_member trait exist?\nDoes the user meet the specified step conditions?\tDefined conditions in \u201cAdd a condition\u201d step\nHas the user met preceding N wait time conditions?\tTrait preceding_step_joined_time at least N time ago\nReal-Time step membership\n\nFor every step after the entry step, Journeys leverages the Engage real-time compute system.\n\nWhen a user\u2019s traits change or they exceed time-based conditions (for example, \u201cwithin 7 days\u201d), they may no longer fulfill the conditions of a previously joined step. If a user joins a step but no longer meets its conditions, Journeys removes them from that step\u2019s preview and analytics. The user does, however, continue to progress through the Journey.\n\nConsider the following example of Journey conditions for a cart abandonment campaign:\n\nEntry Condition: User has clicked add to cart and purchases = 0 within the last 7 days.\nWait Time Condition: 5 days.\nStep Condition: User is member of Example Audience A\nSend to Destination\n\nIf a user makes a purchase during the wait time of 5 days, the system would automatically update membership to false for the audience created from the entry condition, Step 1. However, the user could still satisfy Step Condition 3 based on the three step membership conditions:\n\nSEMANTIC LOGIC\tWRITTEN LOGIC CONDITION\nHas the user ever joined the previous step?\tTrue; preceding_step_audience_member remains true.\nDoes the user meet the specified step conditions?\tTrue; assuming user is member of Example Audience A.\nHas the user met preceding N wait time conditions?\tTrue; once 5 days has passed from initial entry.\n\nTo maintain best practices and enforce your funnel, re-check or modify audience conditions that follow wait steps. For example, adding a purchases = 0 condition to Step 3 results in Segment not advancing users who made a purchase during the wait time:\n\nEntry Condition: User has clicked add to cart and purchases = 0 in the last 7 days\nWait Time Condition: 5 days\nStep Condition: User is member of Example Audience A and purchases = 0 in the last 7 days.\nSend to Destination\nSend to Destination steps\n\nUnless a Journey has an exit condition configured, Journey members permanently remain in Destination sync steps. Segment neither sends Audience Exit events to Destinations nor removes users from Destinations lists. Exit conditions will lead to users being removed from all Journey steps and Destinations.\n\nFAQs\nWhat happens when a user reaches a single or Multi-Split Condition step and the conditions evaluates to false?\n\nEach step\u2019s membership conditions evaluate in real time, which means that users remain in a step until the immediate next step\u2019s conditions becomes true.\n\nCan users exit and re-enter a Journey?\n\nYes. To allow users to re-enter Journeys that they\u2019ve exited, enable re-entry during initial Journey setup.\n\nWhat happens to traits and audiences when I delete a Journey?\n\nDeleting a Journey removes its underlying audiences from profile views in the Profile explorer. However, the Journey\u2019s True/False traits remain in the user\u2019s last recorded state.\n\nNote\n\nCloning a Journey generates new, unique traits and sync keys. Deleting the original Journey won\u2019t impact any cloned Journeys.\n\nAre splits mutually exclusive?\n\nTrue/false splits enforce mutual exclusivity by ensuring that once users enter either side of a split, they can\u2019t enter the other.\n\nMulti-branch splits don\u2019t enforce mutual exclusivity. Users can enter multiple branches of a split if they satisfy the split conditions.\n\nHow does \u201cUse Historical Data\u201d backfill work?\n\nUse Historical Data backfills the entry condition to \u201cprime\u201d the Journey. Future events and existing trait memberships trigger all Journey conditions, except for entry. As a result, event-based conditions only evaluate events that occurred after you published the Journey.\n\nIf you want to check for events that occurred before you published your Journey, base your conditions on computed traits instead.\n\nFor example, to evaluate if a user already in a Journey has ever used a discount code, create a Computed Trait for discount_used, and set it to true or false.\n\nHow do time windows within step conditions work?\n\nWith time windows within step conditions, you can designate a timeframe for Segment to evaluate whether or not a user has met the condition. Segment calculates the time window from the current point in time, not relative to any other steps in your Journey.\n\nThis page was last modified: 09 Jan 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nEntry conditions and step behavior\nStep membership\nReal-Time step membership\nFAQs\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nUsage And Billing\n/\nBilling and Account FAQs\nBilling and Account FAQs\nWhat is a billing cycle?\n\nOn the Segment monthly Team plan, your billing cycle starts the day after your 14-day trial ends. You\u2019re billed on this day for each month while you\u2019re on this plan.\u00a0\n\nOn the Segment annual Team plan, you\u2019re billed at the end of your 14-day trial for the amount of an entire year of service including a specific number of MTUs. Annual plan subscribers are billed for MTU overages at the end of each monthly cycle.\n\nHow do I change my plan?\n\nIf you already have a Segment workspace, you can change which plan your workspace is on by navigating to Settings > Usage & Billing > Plans.\n\nIf you cancel or downgrade your plan during the 2-week trial period, you don\u2019t incur any charges.\n\nWhat if I cancel my paid plan before the end of the month?\nCancellation on the monthly Team plan\n\nIf you cancel your plan or downgrade to a free account before the end of your official billing period on the monthly Team plan, you\u2019ll receive a final bill for the prorated amount for the $120 base + a charge for any MTUs you\u2019ve used over the allotted 10,000 at the rates posted on the pricing page.\n\nCancellation on the annual Team plan\n\nSegment doesn\u2019t issue refunds for the pre-paid portion of your annual bill after your trial ends.\n\nBe aware that if you notify Segment of wanting to cancel your annual plan, but continue to send data to Segment\u2019s servers, you may incur overage charges in any given month. You should fully delete your workspace or cycle your write keys to stop all data flow into Segment to avoid future charges.\n\nWill Segment charge sales tax on my invoice?\n\nAll Segment customers with a US business address may be subject to state and local sales taxes. The applicable tax law applies based on your business location address, which may be different from your billing address. Customers who purchase a taxable product or service, and are located in a jurisdiction where Segment currently charges sales tax, will see the calculated sales tax on their invoice.\n\nSegment collects Value Added Tax (VAT) and Goods and Services Tax (GST) on the services sold to its international customers located in certain foreign jurisdictions.\n\nFor more information about sales tax, VAT, and GST, see the Segment VAT/GST FAQs.\n\nDo I qualify for a tax exemption?\n\nIf you believe your organization qualifies for a sales tax exemption (for example, because of a nonprofit or government status), you can contact tax@segment.com with the appropriate form.\n\nI submitted a form for tax exemption, why am I still charged sales tax?\n\nTax might still be charged on your bill if either:\n\nThe exemption certificate was still in review while the invoice was issued; or\nThe exemption certificate covers a state that is different from the billing address\nDo you offer refunds?\n\nIn most cases Segment doesn\u2019t offer refunds, as noted in the Terms of Service. Contact support if you feel that you\u2019re in a unique situation.\n\nIs there a free trial for paid plans?\n\nSegment offers a 2-week trial on the Team plan to let you try the plan before you purchase it.\n\nSegment also offers the Free plan, which includes up to 1,000 MTUs, at no cost to you.\u00a0\n\nFind out more about the different plans and which one suits your needs best. \u00a0\n\nWhat happens when I exceed the Free plan limit?\n\nThe Free plan includes up to 1,000 MTUs at no cost. If you exceed the 1,000 MTU limit once in a 6-month period, Segment locks your account but data is still able to flow through Segment. To unlock your account, you can choose from these options:\n\nOption 1: Wait for a full billing cycle (1 month) to go by with any overages. This will automatically unlock your account if the MTU numbers are able to go back down on their own.\nOption 2: Upgrade to the Team plan. This starts a 2-week free trial that gives you 14 days to fix your implementation to decrease the traffic.\n\nIf you exceed the 1,000 MTU limit twice in a 6-month period, Segment locks your account and also stops sending and receiving data. You can unlock your account by following option 2 above to upgrade to the Team plan 2-week free trial.\n\nTeam Trial FAQ\n\nWhat is the Team trial?\n\nThe Team trial is a 14-day free trial of Segment\u2019s Team plan, and it includes all the features associated with a Team plan, including unlimited sources, two warehouse syncs per day, 10 seats, and 10,000 MTUs (with the ability to track more MTUs as needed).\n\nHow do I get a two-week Team trial?\n\nYou automatically receive a 2-week trial when you sign up for a Team plan.\u00a0\n\nDo I have to be a \u201cnew\u201d customer to receive the free Team trial?\n\nThe free trial is available to all customers who have never had a Team plan. This includes new customers as well as customers who have previously been on the Free plan.\u00a0\n\nDo you have to include your payment information when signing up for a Team trial?\n\nIf you\u2019re upgrading from a Free Plan to a Team Plan, you\u2019re required to add your payment information.\u00a0\n\nIf you\u2019re signing up for a new Team plan, you don\u2019t have to add your payment information during sign up. If you would like to continue to use the Team plan after the 14-day trial, add your credit card information on the \u201cPayment Information\u201d page in your workspace before the trial ends.\u00a0\n\nWhat happens when the two-week trial ends?\n\nIf you added your payment information, your subscription automatically continues at the regular rate after the trial period expires. You can delete your workspace or downgrade to a Free plan any time during the trial to avoid charges.\n\nTo activate the free trial, add your payment information.\n\nThis page was last modified: 24 Oct 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nWhat is a billing cycle?\nHow do I change my plan?\nWill Segment charge sales tax on my invoice?\nDo I qualify for a tax exemption?\nDo you offer refunds?\nIs there a free trial for paid plans?\nWhat happens when I exceed the Free plan limit?\nTeam Trial FAQ\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nJourneys\n/\nExample Journeys Use Cases\nExample Journeys Use Cases\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nTo help you get underway, you can reference these sample Journeys.\n\nE-commerce use cases\nRepeat purchase campaign\n\nThis journey focuses on converting one-time buyers into repeat purchasers by delivering communications in their preferred channels.\n\nCreate the entry condition with the step name One-Time Purchasers.\nAll users who performed Order Completed exactly 1 time\nAdd a wait duration of 14 days\nAdd a condition called No New Transactions\nAll users who performed Order Completed exactly 1 time\nAdd a multi-branch split\nBranch 1: Customers who have the trait Most Frequent UTM Source equals Email\nSend to destination: Email\nBranch 2: Customers who have the trait Most Frequent UTM Source equals SMS\nSend to destination: SMS\nLow recency purchase winback\n\nThis journey represents a campaign designed to drive returning purchases based on intent and lifetime value goals.\n\nCreate the entry condition with the step name Low recency purchasers.\nAll users who have performed the Order Completed event zero times within the last 180 days.\nAdd a True/false split.\nSplit the audience around a computed trait of Customer Lifetime Value > 100.\nFor the True branch, send the list of users to Email and Advertising destinations.\nFor the False branch, send the list of users to an Email destination.\nAdd a wait duration of 1 day to the True branch from step 2.\nAdd a Wait for condition step to wait for a Page Viewed event at least 1 time and where utm_source is equal to the ad or email campaign, within 1 day.\nSend this list of users to an email destination, as they are more likely to accept a discount and complete the purchase.\nB2B use cases\nTrial to paid conversion\n\nThis journey creates an acquisition campaign designed to convert trial accounts to paid accounts with a unified owned and paid media strategy.\n\nCreate the entry condition with the step name Trial started.\nAll users who performed Trial Started at least once and who performed Subscription Started exactly 0 times.\nAdd a wait duration of 5 days.\nAdd a True/false split.\nSplit the audience around users who have performed Subscription Started\nFor the True branch, send the list of users to Email and Support destinations.\nFor the False branch, send the list of users to an Email destination, Support, and Advertising destinations.\nOnboarding flow\n\nThis journey creates an onboarding flow designed to maintain new user engagement through the onboarding experience.\n\nCreate the entry condition with the step name Account created. Set the condition to all users who performed Account Created at least 1 time.\nAdd a wait duration of 1 hour.\nAdd a True/false split.\nSplit the audience based on those who have performed Tutorial Completed.\nFor the True branch, send the list of users to Email, Support, and In-App destinations.\nFor the False branch, send the list of users to Email, Support, In-App, and Advertising destinations.\nMedia use cases\nPaid subscription acquisition\n\nThis journey creates an acquisition campaign designed to convert trial subscriptions to paid subscriptions with a unified owned and paid media strategy.\n\nCreate the entry condition with the step name Free trial. Set the condition to all users who performed Subscription Started at least 1 time, and where Subscription Plan Type is Free.\nAdd a wait duration of 1 hour.\nSend the list of users to an Email destination.\nAdd a wait duration of 7 days.\nAdd a True/false split.\nSplit the audience based on those who have performed Subscription Started where Subscription Plan Type is paid.\nFor the True branch, send to an email destination.\nFor the False branch, send to both email and advertising destinations.\nRe-engagement Campaign\n\nThis journey aims to bring back users with personalized messaging while conserving ad spend based on user preferences.\n\nCreate the entry condition with the step name Low Recency Engagement. Set the condition to all users who performed Page Viewed exactly 0 times within 60 days.\nAdd a True/false split.\nSplit the audience based on those who have performed Subscription Started where Subscription Plan Type is paid\nFor the True branch, add a multi-branch split\nFor users who have the Computed Trait user_favorite_article-category = Engineering\nSend to email and ads destinations\nFor users who have the Computed Trait user_favorite_article-category = Marketing\nSend to email and ads destinations\nFor the False branch, send to an email destination only.\n\nThis page was last modified: 27 Sep 2022\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nE-commerce use cases\nB2B use cases\nMedia use cases\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nApi\n/\nPublic Api\n/\nDestination Filter Query Language\nDestination Filter Query Language\n\nThe Segment Public API is available\n\nSegment\u2019s Public API is available for Team and Business tier customers to use. You can use the Public API and Config APIs in parallel, but moving forward any API updates will come to the Public API exclusively.\n\nPlease contact your account team or friends@segment.com with any questions.\n\nThis reference provides a comprehensive overview of the Segment Destination Filter query language. For information on the Destination Filters API (including information on migrating from the Config API), visit the Destination Filters API reference.\n\nThe Transformations API uses Filter Query Language (FQL) to filter JSON objects and conditionally apply transformations. You can use FQL statements to:\n\nApply filters that evaluate to true or false based on the contents of each Segment event. If the statement evaluates to true, the transformation is applied, and if it is false the transformation is not applied.\nDefine new properties based on the result of an FQL statement.\n\nIn addition to boolean and equality operators like and and >=, FQL has built-in functions that make it more powerful such as contains( str, substr ) and match( str, pattern ).\n\nExamples\n\nGiven the following JSON object:\n\n{\n  \"event\": \"Button Clicked\",\n  \"type\": \"track\",\n  \"context\": {\n    \"library\": {\n      \"name\": \"analytics.js\",\n      \"version\": \"1.0\"\n    }\n  },\n  \"properties\": {\n    \"features\": [\"discounts\", \"dark-mode\"]\n  }\n}\n\n\nThe following FQL statements will evaluate as follows:\n\nFQL\tRESULT\nevent = 'Button Clicked'\ttrue\nevent = 'Screen Tapped'\tfalse\ncontext.path.path = '/login'\tfalse\ntype = 'identify' or type = 'track'\ttrue\nevent = 'Button Clicked' and type = 'track'\ttrue\nmatch( context.library.version, '1.*' )\ttrue\nmatch( context.library.version, '2.*' )\tfalse\ntype = 'track' and ( event = 'Click' or match( event, 'Button *' ) )\ttrue\n!contains( context.library.name, 'js' )\tfalse\n'dark-mode' in properties.features\ttrue\n'blink' in properties.features\tfalse\nField Paths\n\nFQL statements may refer to any field in the JSON object including top-level properties like userId or event as well as nested properties like context.library.version or properties.title using dot-separated paths. For example, the following fields can be pointed to by the associated field paths:\n\n{\n  \"type\": \"...\",       // type\n  \"event\": \"...\",      // event\n  \"context\": {         // context\n    \"library\": {       // context.library\n      \"name\": \"...\"    // context.library.name\n    },\n    \"page\": {          // context. page\n      \"path\": \"...\",   // context.page.path\n    }\n  }\n}\n\nEscaping Field Paths\n\nIf your field name has a character not in the set of {a-z A-Z 0-9 _ -}, you must escape it using a \\ character. For example, the nested field below can be referred to by properties.product\\ 1.price:\n\n{\n  \"properties\": {\n    \"product 1\": {\n      \"price\": \"19.99\"\n    }\n  }\n}\n\nOperators\nBoolean\nOPERATOR\tLEFT SIDE\tRIGHT SIDE\tRESULT\nand\tbool or null\tbool or null\ttrue if the left and right side are both true, false otherwise.\nor\tbool or null\tbool or null\ttrue if at least one side is true, false if either side is false or null.\nUnary\nOPERATOR\tRIGHT SIDE\tRESULT\n!\tbool\tNegates the right-hand side.\nComparison\nOPERATOR\tLEFT SIDE\tRIGHT SIDE\tRESULT\n=\tstring, number, list, bool, or null\tstring, number, list, bool, or null\ttrue if the left and right side are the same type and are strictly equal, false otherwise.\n!=\tstring, number, list, bool, or null\tstring, number, list, bool, or null\ttrue if the left and right side are different types or if they are not strictly equal, false otherwise.\n>\tnumber\tnumber\ttrue if the left side is greater than the right side.\n>=\tnumber\tnumber\ttrue if the left side is greater than or equal to the right side.\n<\tnumber\tnumber\ttrue if the left side is less than the right side.\n<=\tnumber\tnumber\ttrue if the left side is less than or equal to the right side.\nin\tstring, number, bool, or null\tlist\ttrue if the left side is contained in the list of values.\nSubexpressions\n\nYou can use parentheses to group subexpressions for more complex \u201cand / or\u201d logic as long as the subexpression evaluates to true or false:\n\nFQL\ntype = 'track' and ( event = 'Click' or match( 'Button *', event ) )\n( type = 'track' or type = 'identify' ) and ( properties.enabled or match( traits.email, '*@company.com' ) )\n!( type in ['track', 'identify'] )\nFunctions\nFUNCTION\tRETURN TYPE\tRESULT\ncontains( s string, sub string )\tbool\tReturns true if string s contains string sub.\nlength( list or string )\tnumber\tReturns the number of elements in a list or number of bytes (not necessarily characters) in a string. For example, a is 1 byte and\u30a2 is 3 bytes long. Please note that you can\u2019t use this function with JSON as the argument. Using JSON may result in the function not working.\nlowercase( s string )\tstring\tReturns s with all uppercase characters replaced with their lowercase equivalent.\nuppercase( s string )\tstring\tReturns s with all lowercase characters replaced with their uppercase equivalent.\nsnakecase( s string )\tstring\tReturns s with all space characters replaced by underscores. For example, kebabcase(\"test string\") returns test_string.\nkebabcase( s string )\tstring\tReturns s with all space characters replaced by dashes. For example, kebabcase(\"test string\") returns test-string.\ntitlecase( s string )\tstring\tReturns s with all space characters replaced by dashes. For example, titlecase(\"test string\") returns Test String.\ntypeof( value )\tstring\tReturns the type of the given value: \"string\", \"number\", \"list\", \"bool\", or \"null\".\nmatch( s string, pattern string )\tbool\tReturns true if the glob pattern pattern matches s. See below for more details about glob matching.\nbool( list or string or number or nil )\tbool\tConverts the value to a boolean value.\nstring( list or string or number or nil )\tstring\tConverts the value to a string value.\nnumber( number or string )\tnumber\tConverts the value to a number value.\n\nFunctions handle null with sensible defaults to make writing FQL more concise. For example, you can write length( userId ) > 0 instead of typeof( userId ) =\n'string' and length( userId ) > 0.\n\nFUNCTION\tRESULT\ncontains( null, string )\tfalse\nlength( null )\t0\nlowercase( null )\tnull\ntypeof( null )\t\"null\"\nmatch( null, string )\tfalse\nmatch( string, pattern )\n\nThe match( string, pattern ) function uses \u201cglob\u201d matching to return true if the given string fully matches a given pattern. Glob patterns are case sensitive. If you only need to determine if a string contains another string, you should use contains().\n\nPATTERN\tSUMMARY\n*\tMatches zero or more characters.\n?\tMatches one character.\n[abc]\tMatches one character in the given list. In this case, a, b, or c will be matched.\n[a-z]\tMatches a range of characters. In this case, any lowercase letter will be matched.\n\\x\tMatches the character x literally. This is useful if you need to match *, ? or ] literally. For example, \\*.\nPATTERN\tRESULT\tREASON\nmatch( 'abcd', 'a*d' )\ttrue\t* matches zero or more characters.\nmatch( '', '*' )\ttrue\t* matches zero or more characters.\nmatch( 'abc', 'ab' )\tfalse\tThe pattern must match the full string.\nmatch( 'abcd', 'a??d' )\ttrue\t? matches one character only.\nmatch( 'abcd', '*d' )\ttrue\t* matches one or more characters even at the beginning or end of the string.\nmatch( 'ab*d', 'ab\\*d' )\ttrue\t\\* matches the literal character *.\nmatch( 'abCd', 'ab[cC]d' )\ttrue\t[cC] matches either c or C.\nmatch( 'abcd', 'ab[a-z]d' )\ttrue\t[a-z] matches any character between a and z.\nmatch( 'abcd', 'ab[A-Z]d' )\tfalse\t[A-Z] matches any character between A and Z but c is not in that range because it is lowercase.\nError Handling\n\nIf your FQL statement is invalid (for example userId = oops\"), your Segment event will not be sent on to downstream Destinations. Segment defaults to not sending the event to ensure that invalid FQL doesn\u2019t cause sensitive information like PII to be incorrectly sent to Destinations.\n\nFor this reason, Segment recommends that you use the Destination Filters \u201cPreview\u201d API to test your filters without impacting your production data.\n\nThis page was last modified: 04 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nExamples\nField Paths\nOperators\nSubexpressions\nFunctions\nError Handling\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nSegment App\n/\nIam\n/\nSystem for Cross-domain Identity Management (SCIM) Configuration Guide\nSystem for Cross-domain Identity Management (SCIM) Configuration Guide\nFREE X\nTEAM X\nBUSINESS \u2713\nADD-ON X\n?\n\nThe SCIM specification is designed to make managing user identities in cloud-based applications like Segment easier. SCIM allows your Identity Provider (IdP) to manage users and groups within your Segment workspace.\n\nMost IdPs offer SCIM, and it complements SAML. You can think of SAML as a way for your employees to authenticate and SCIM as a way to make sure they have the appropriate permissions.\n\nRequirements\n\nBefore you start, remember that SSO is only available to Business Tier customers, and that only workspace owners may configure SSO connections.\n\nTo set up SCIM, you must first create an SSO connection. Once you create your SSO connection, log back in to Segment using SSO.\n\nConfiguration Instructions\n\nSegment officially supports Okta, Microsoft Entra ID, and OneLogin. Each link includes specific setup instructions for that IdP. You should read the features section of this page to understand which features of SCIM Segment supports.\n\nYou may still be able to use SCIM with another Identity Provider (IdP) by adapting the following instructions.\n\nBase URL\n\nYour IdP needs to know where to send SCIM requests. The Segment base URL is: https://scim.segmentapis.com/scim/v2\n\nAPI Key\n\nThe other value you need is an API key (sometimes referred to as an Authorization Header). To generate one, go to Settings > Advanced Settings in the Segment app, and find the SSO Sync section. Click Generate SSO Token and copy the generated token. Use this token for the API key or Authorization Header in your IdP.\n\nYou can find this page in the settings sidebar of your Segment app.\n\nFeatures\n\nIt\u2019s important to remember that Segment has a multi-tenant user/workspace relationship, meaning that users can be part of more than one workspaces. In most cases these workspaces are all related to a single customer (for example, a single company might have individual workspaces for different brands or subsidiaries). However, some users can be members of workspaces for different Segment customers, such as with contractors or consultants.\n\nBecause of this, Segment must balance the autonomy of users with the desired level of control of a Workspace Owner.\n\nCreating Users\n\nEven though Segment users exist separately from workspaces, your IdP can create a new Segment user or add an existing Segment user to your workspace using the same IdP workflow. This process is transparent to the IdP and to you as the customer. In other words, you don\u2019t need to know if a user exists before adding them to your workspace.\n\nIf the person you want to add does not have a Segment account, your IdP will create one. If the person already has a Segment account, you can still add them to your Workspace using your IdP, but it does not create a new Segment account.\n\nYou can create new users and set their userName (email) and displayName (single value field that represents a user\u2019s full name) using your IdP.\n\nIf a user already has a Segment account, you can add them using their email address using your IdP. However, Segment ignores the displayName sent by the IdP, and instead uses the name chosen by the user when they created their account.\n\nUpdating User Attributes\n\nSegment user profiles only contain a userName (email) and displayName. Once you create a user, you cannot update these attributes using SCIM. They can only be updated by the user through the Segment UI.\n\nDeleting or Deactivating Users\n\nSegment workspace owners cannot delete Segment workspace member accounts using SCIM, the web UI, or the Segment API. A user must delete their own account using the Segment app. Workspace owners can remove members from the workspace using SCIM, the web UI, or the Segment API.\n\nSome IdPs want to set users as \u201cinactive\u201d or \u201cactive.\u201d Segment does not have an \u201cinactive\u201d state for user accounts. Similar functionality can be achieved by removing a user from your workspace. Setting an existing Segment user to \u201cactive\u201d is similar to adding that user to the workspace.\n\nWhen your IdP updates a user to set active: false, or attempts to delete a user, Segment removes the user from your Segment workspace. If your IdP attempts to create a user with an existing email, or set active: true on an existing user, Segment adds that existing user account to your workspace.\n\nAny Segment group memberships must be reassigned when a user is removed and re-added from your workspace. Newly added workspace users have the \u201cMinimal Workspace Access\u201d permission by default. The \u201cMinimal Workspace Access\u201d role does not have access to any sources, destinations, etc.\n\nThis reassignment might happen automatically depending on how you configured your IdP. If the user was assigned groups using your IdP, your IdP should automatically re-add the user within Segment. For this reason, Segment strongly recommends that you create groups in your IdP, push them into Segment, and maintain an active link between your IdP and Segment.\n\nCreating Groups\n\nYour IdP can create new groups in Segment using SCIM. All groups created using SCIM start with \u201cMinimal Workspace Access.\u201d The \u201cMinimal Workspace Access\u201d permission does not have access to any sources, destinations, etc. To add more permissions to a group you must use the Segment web app.\n\nUpdating Groups\n\nYour IdP can add or remove workspace members from existing groups using SCIM. Your IdP can also update Segment group names.\n\nDeleting Groups\n\nYour IdP can use SCIM to delete groups from your Segment workspace. Deleting a group in Segment does not remove its members from your workspace. To remove members from the workspace, unassign the users from Segment from your IdP, then Segment removes them from the workspace.\n\nAttribute Mapping\n\nWhen you integrate Segment SCIM and your IdP you might need to map attributes for users. The only attributes that Segment SCIM supports are userName and displayName. You should leave any existing mapping for the email SAML attribute, which you might have set up during your initial SSO setup. This mapping supports SAML authentication, and is separate from setting up SCIM, but may be within the same page depending on your IdP.\n\nYou\u2019ll need to map an email (IdP) to userName (Segment). Depending on your IdP this attribute might be called email or mail. If your IdP uses emails for usernames, you can map userName (IdP) to userName (Segment).\n\nIf your IdP supports the displayName attribute, you can map it directly to the Segment displayName attribute. If it does not, most IdPs can create a \u201cmacro mapping\u201d which allows you to map more than one field to a single field in Segment.\n\nFor example, you might map {firstName} {lastName} from your IdP to displayName in Segment. If your IdP doesn\u2019t support this, you can map firstName (IdP) to displayName (Segment).\n\nOkta Setup Guide\nComplete the Okta setup guide for SSO\nClick Provisioning, then click Configure API Integration and select Enable API Integration.\n\nGenerate an API key, then copy and paste this value into the API Token field in Okta, and click Save.\n\nNext, select To App in the left sidebar of the Provisioning tab. Click Edit and select both Create Users and Deactivate Users. Click Save.\n\nClick the Assignments tab. You can now assign people or groups. Before you continue, read through the features section in this doc to make sure you understand how groups work. Segment recommends that you assign users to the Segment app by Okta group. This allows you to manage which groups in your organization can authenticate to Segment. You can also assign users individually.\n\nOnce you assign your users, push the assigned Okta groups to Segment.\n\nNext, go to the Segment app and assign permissions to these groups.\n\nTip: You can also link Okta groups to an existing group from in the Segment app using the Okta UI.\n\nMicrosoft Entra ID Setup Guide\n\nInstructions for configuring Microsoft Entra ID can be found on the Microsoft Docs website.\n\nComplete the Microsoft Entra ID setup guide for SSO\n\nComplete the Microsoft Entra ID setup guide for SCIM\n\nTo make Azure compatible with Segment\u2019s SCIM v2 implementation, append the flag ?aadOptscim062020 to the tenant URL as explained in the Microsoft Entra ID documentation. By appending the flag to your tenant URL, your request has the correct structure when you remove a user from a group.\n\nOneLogin Setup Guide\n\nInstructions for configuring OneLogin can be found on the OneLogin Docs website.\n\nAdd and configure the Segment SSO integration from within the OneLogin application.\n\nComplete the OneLogin setup Guide for SCIM\n\nThis page was last modified: 20 Jun 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nRequirements\nConfiguration Instructions\nFeatures\nCreating Users\nUpdating User Attributes\nDeleting or Deactivating Users\nCreating Groups\nUpdating Groups\nDeleting Groups\nAttribute Mapping\nOkta Setup Guide\nMicrosoft Entra ID Setup Guide\nOneLogin Setup Guide\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nReverse Etl\n/\nReverse Etl Source Setup Guides\n/\nBigQuery Reverse ETL Setup\nBigQuery Reverse ETL Setup\n\nTo set up your BigQuery source with Reverse ETL, you must construct a BigQuery role and service account and create a BigQuery source in the Segment app.\n\nBigQuery Reverse ETL sources support Segment's dbt extension\n\nIf you have an existing dbt account with a Git repository, you can use Segment\u2019s dbt extension to centralize model management and versioning, reduce redundancies, and run CI checks to prevent breaking changes.\n\nConstructing your own role or policy\n\nYou need to be an account admin to set up the Segment BigQuery connector as well as write permissions for the __segment_reverse_etl dataset.\n\nThere are two approaches you can take when granting Segment access to your BigQuery resources:\n\nGrant Full Access: This option allows Segment to automatically complete the setup for you after you provide Segment with all the necessary permissions. This option requires less time and engineering effort on your part.\nGrant Limited Access: This option is more secure, as it restricts the permissions Segment has access to. However, due to the limited access, you must complete a few additional setup steps. These are one-time steps, and the documentation provides you with the information required to complete this process.\n\nYou can choose the approach that best suits your needs.\n\nGrant Full Access\n\nWith this approach, use BigQuery predefined roles to create a service account for Segment to assume.\n\nIn BigQuery, navigate to IAM & Admin > Service Accounts.\nClick + Create Service Account to create a new service account.\n\nEnter your Service account name and a description of what the service account will do.\n\nClick Create and Continue.\nClick + Add another role and add the BigQuery User role.\nClick + Add another role and add the BigQuery Data Editor role.\nClick Continue, then click Done.\nGrant Limited Access\n\nWith this approach, you can set up a custom role with the following permissions:\n\nPERMISSION\tDETAILS\nbigquery.datasets.get\tThis allows Segment to determine if the __segment_reverse_etl dataset exists.\nbigquery.jobs.create\tThis allows Segment to execute queries on any datasets or tables your model query references, and also allows Segment to manage tables used for tracking.\nbigquery.tables.getData\tThis allows Segment to run SELECT queries on tables that will be defined in the model.\nIn BigQuery, navigate to IAM & Admin > Roles.\nClick + CREATE ROLE to create a new role.\nAdd Title and Description as you like.\nClick ADD PERMISSIONS and add the permission listed in the above tables. Repeat this step until you\u2019ve added all required permissions.\nClick CREATE.\nNavigate to IAM & Admin > Service Accounts.\nClick + Create Service Account to create a new service account.\nEnter your Service account name and a description of what the account will do.\nClick Create and Continue.\nIn the Grant this service account access to project section, select the role you just created.\nClick Continue.\nClick Done. Copy and keep the Service Account email handy for the next steps.\nNavigate to the BigQuery SQL editor and create a dataset that will be used by Segment:\nCREATE SCHEMA IF NOT EXISTS `__segment_reverse_etl`;\n\nGrant limited access to the Segment Reverse ETL dataset\nGRANT `roles/bigquery.dataEditor` ON SCHEMA `__segment_reverse_etl` TO \"serviceAccount:<YOUR SERVICE ACCOUNT EMAIL>\";\n\nBigQuery resource location\n\nWhen connecting your BigQuery warehouse to Segment, you\u2019ll need to know the location of your resources.\n\nYou can find the location of your BigQuery resources using the following method:\n\nIn the BigQuery console, navigate to your dataset. In the explorer panel on the left, expand the project and dataset to view the tables.\nClick on the name of the dataset, and it opens a page showing its details.\nThe Location of the dataset (like US or EU) is displayed in the Dataset Info.\nSet up BigQuery as your Reverse ETL source\nIn the BigQuery console, search for the service account you created.\nWhen your service account pulls up, click the 3 dots under Actions and select Manage keys.\nClick Add Key > Create new key.\nIn the pop-up window, select JSON for the key type and click Create. The file will be downloaded.\nCopy all the content in the JSON file you created in the previous step.\nOpen the Segment app and navigate to Connections > Sources.\nOn the My sources page, click + Add source.\nSearch for \u201cBigQuery\u201d and select the BigQuery source from the sources catalog. On the BigQuery overview page, click Add Source.\nOn the Set up BigQuery page, enter a name for your source and paste all the credentials you copied from previous step into the Enter your credentials section.\nEnter the location of your BigQuery warehouse in the Data Location field.\nClick Test Connection to test to see if the connection works. If the connection fails, make sure you have the right permissions and credentials and try again.\nIf the test connection completes successfully, click Add source to complete the setup process.\n\nAfter you\u2019ve added BigQuery as a source, you can add a model and follow the rest of the steps in the Reverse ETL setup guide.\n\nThis page was last modified: 22 Oct 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nConstructing your own role or policy\nSet up BigQuery as your Reverse ETL source\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nProfile API\nProfile API\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\n\nThe Segment Profile API provides a single API to read user-level and account-level customer data. Segment now allows you to query the entire user or account object programmatically, including the external_ids , traits , and events that make up a user\u2019s journey through your product.\n\nYou can use this API to:\n\nBuild an in-app recommendation engine to show users or accounts the last five products they viewed but didn\u2019t purchase\nEmpower your sales and support associates with the complete customer context by embedding the user profile in third-party tools like Zendesk\nPower personalized marketing campaigns by enriching dynamic / custom properties with profile traits in marketing tools like Braze\nQualify leads faster by embedding the user event timeline in Salesforce\n\nThis document has four parts:\n\nProduct Highlights\nQuickstart: Walks you through how to get started querying your user profile in <1 min\nAPI Reference: Retrieve a list of users sorted by recent activity or find a particular user\nBest Practices: Recommended implementation and example Profile API workflow\nProduct highlights\nFast response times \u2014 fetch traits from a user profile under 200ms\nReal-time data \u2014 query streaming data on the user profile\nOne identity \u2014 query an end user\u2019s interactions across web, mobile, server, and third party touch-points\nRich data \u2014 query user traits, audiences, and events\nAny external ID \u2014 the API supports query from user_id, advertising IDs, anonymous_id, and custom external IDs.\nQuickstart\n\nImportant: The Profile API is intended to be used server-side. You should not implement directly in client applications. See the Best Practices section for more details.\n\nConfigure access\n\nYour access token enables you to call the Profile API and access customer data.\n\nEuropean Union requirements\n\nTo implement the Profile API in the European Union, you must complete the following steps within an EU workspace. View the regional Segment documentation for more information.\n\nNavigate to the API access settings page Unify > Unify settings > API access.\n\nCreate your Access Token with a name that describes your use case, for example testing/development. Take note of the space ID value, you\u2019ll pass this into the Profile API request URL in a later step.\n\nClick Generate token. Copy the resulting Access Token and store it in a file on your computer. You\u2019ll pass in the Access Token into the Profile API for authorization as an HTTP Basic Auth username in a later step.\n\nFind a user\u2019s external id\nNavigate to Unify > Profile explorer and select the user you want to query through the API.\nTake note of the user\u2019s available identifiers. For example, this user has an anonymous_id with the value eml_3bca54b7fe7491add4c8d5d4d9bf6b3e085c6092. The Profile API requires both the type of ID and the value separated by a colon. For example, anonymous_id:eml_3bca54b7fe7491add4c8d5d4d9bf6b3e085c6092. Click the duplicate icon to copy the identifier to your clipboard.\n\n\nTo query phone numbers that contain a plus sign (+), insert the escape characters %2B in place of the plus sign. For example, if a phone_number identifier has the value +5555550123, enter phone_number:%2B5555550123 in your query.\n\nIf the field you\u2019re using within the Profile API endpoint contains a value with a non-alphanumeric character, then the Profile API may respond with 500 error. In this case, see W3\u2019s ASCII Encoding Refernece, which lists the escape characters you can use to replace the non-alphanumeric character in the Profile API endpoint so that the Profile API will respond with a 200 Success.\n\nQuery the user\u2019s event traits\nFrom the HTTP API testing application of your choice, configure the authentication as described above.\n\nPrepare the request URL by replacing <space_id> and <external_id> in the request URL: https://profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles/<external_id>/traits\n\nIf you\u2019re using the Profile API in the EU, use the following URL for all requests:\n\nhttps://profiles.euw1.segment.com/v1/spaces/<space_id>/collections/users/profiles/<external_id>/traits\n\nSend a GET request to the URL.\nExplore the user\u2019s traits in the response\n\nThe response is returned as a JSON object which contains the queried user\u2019s assigned traits.\n\n{\n    \"traits\": {\n        \"3_product_views_in_last_60_days\": false,\n        \"Campaign Name\": \"Organic\",\n        \"Campaign Source\": \"Organic\",\n        \"Experiment Group\": \"Group A\",\n        \"Invited User?\": \"Invited User?\",\n        \"Referrering Domain\": \"http://duckduckgo.com\",\n        \"all_users_order_completed\": true,\n        \"big_spender\": false\n    },\n    \"cursor\": {\n        \"url\": \"https://profiles.segment.com/v1/spaces/kNU0gh7EVl/collections/users/profiles/user_id:1413639574/traits?%3Acollection=users&%3Aid=user_id%3A1413639574&%3Anamespace=kNU0gh7EVl&next=browser\",\n        \"has_more\": true,\n        \"next\": \"browser\",\n        \"limit\": 10\n    }\n}\n\nExplore more of the API\n\nSearch by an External ID: You can query directly by a user\u2019s user_id or other external_id.\n\nhttps://profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles/<user_identifier>/events\n\nExternal IDs: You can query all of a user\u2019s external IDs such as anonymous_id or user_id.\n\nhttps://profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles/<user_identifier>/external_ids\n\nTraits You can query a user\u2019s traits (such as first_name, last_name, and more):\n\nhttps://profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles/<external_id>/traits\n\nBy default, the response includes 20 traits. You can return up to 200 traits by appending ?limit=200 to the querystring. If you wish to return a specific trait, append ?include={trait} to the querystring (for example ?include=age). You can also use the ?class=audience\u200b or ?class=computed_trait\u200b URL parameters to retrieve audiences or computed traits specifically.\n\nMetadata You can query all of a user\u2019s metadata (such as created_at, updated_at, and more):\n\nhttps://profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles/<external_id>/metadata\n\nSearch an account profile\n\nIf you\u2019re sending group calls to Segment, you can now access your account profiles as well. Retrieve your account traits, computed traits, and audience traits by querying the group_id you are interested in:\n\nhttps://profiles.segment.com/v1/spaces/<space_id>/collections/accounts/profiles/group_id:12345/traits\n\nSearch for linked users or accounts\n\nIf you\u2019re looking to find all the users linked to an account, you can search for an account\u2019s linked users, or a user\u2019s linked accounts.\n\nhttps://profiles.segment.com/v1/spaces/<space_id>/collections/accounts/profiles/group_id:12345/links\n\nThe return limit for the /links endpoint is 20 records, which you can request by appending ?limit=20 to the query string.\n\ncURL\n\nYou can also request using cURL:\n\nexport SEGMENT_ACCESS_SECRET=\"YOUR_API_ACCESS_TOKEN_SECRET_HERE\"\n\ncurl https://profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles/<external_id>/traits -u $SEGMENT_ACCESS_SECRET:\n\nAPI reference\n\nThe Segment API is organized around REST. The API has predictable, resource-oriented URLs, and uses HTTP response codes to indicate API errors. Segment uses standard HTTP features, like HTTP authentication and HTTP verbs, which are understood by off-the-shelf HTTP clients. JSON is returned by all API responses, including errors.\n\nEndpoint\n\nhttps://profiles.segment.com\n\n\nEuropean Union endpoint\n\nhttps://profiles.euw1.segment.com\n\nAuthentication\n\nThe Profile API uses basic authentication for authorization \u2014 with the Access Token as the authorization key. Your Access Token carries access to all of your customer data, so be sure to keep them secret. Don\u2019t share your Access Token in publicly accessible areas such as GitHub or client-side code.\n\nYou can create your Access Secret in your Unify settings page. Segment recommends that you name your tokens with the name of your app and its environment, such as marketing_site/production. Access tokens are shown once \u2014 you won\u2019t be able to see it again. In the event of a security incident, you can revoke and cycle the access token.\n\nWhen you make requests to the Profile API, use the Access Token as the basic authentication username and keep the password blank. Base64 is a requirement for authentication. If you use a tool like Postman, or if you use the -u flag in a cURL request, this encoding occurs automatically. Otherwise, you\u2019ll need to use Base64 to manually encode your Access Token.\n\ncurl https://profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles\n  -u $SEGMENT_ACCESS_TOKEN:\n\n\nIf you\u2019re using a Segment Function or Node.js you can format your header object to include authentication, like so:\n\nheaders: {\n          'Content-Type': 'application/json',\n          Authorization:\n            `Basic ${btoa('<access_token>' + ':')}`,\n        }\n\nErrors\n\nSegment uses conventional HTTP response codes to indicate the success or failure of an API request. In general, codes in the 2xx range indicate success, codes in the 4xx range indicate an error that failed given the information provided (for example, a required parameter was omitted), and codes in the 5xx range indicate an error with Segment\u2019s servers.\n\nHTTP Status\n\nHTTP STATUS\tDESCRIPTION\n200 - OK\tEverything worked as expected.\n400 - Bad Request\tThe request was unacceptable, often due to missing a required parameter.\n401 - Unauthorized\tNo valid Access Token provided.\n404 - Not Found\tThe requested resource doesn\u2019t exist.\n429 - Too Many Requests\tToo many requests hit the API too quickly. Segment recommends an exponential backoff of your requests. By default, each space has a limit of 100 requests/sec. Please contact friends@segment.com if you need a higher limit with details around your use case.\n500, 502, 503, 504 - Server Errors\tSomething went wrong on Segment\u2019s side.\n\nError Body\n\n{\n  \"error\": {\n    \"code\": \"validation_error\",\n    \"message\": \"The parameter `collection` has invalid character(s) `!`\"\n  }\n}\n\nCODE\tMESSAGE\nauthentication_error\tFailure to properly authenticate yourself in the request.\ninvalid_request_error\tInvalid request errors arise when your request has invalid parameters.\nrate_limit_error\tToo many requests hit the API too quickly.\nvalidation_error\tErrors triggered when failing to validate fields (for example, when a collection name has invalid characters).\nRate limit\n\nTo ensure low response times, every Space has a default rate limit of 100 requests/sec. Please contact friends@segment.com if you need a higher limit with details around your use case. For more information about rate limits, see the Product Limits documentation.\n\nPagination\n\nAll top-level API resources have support for bulk fetches using \u201clist\u201d API methods. For instance you can list profiles, a profile\u2019s events, a profile\u2019s traits, and a profile\u2019s external_ids. These list API methods share a common structure, taking at least two parameters: next and limit.\n\nRequest arguments\nARGUMENT\tDESCRIPTION\nlimit\tA limit on the number of objects to be returned, between 1 and 100.\nnext\tThe string cursor that indexes the next page of requests.\nResponse arguments\nARGUMENT\tDESCRIPTION\nhas_more\tWhether or not there are more elements available after this set. If false, this set comprises the end of the list.\nnext\tThe string cursor that indexes the next page of requests.\nurl\tThe URL for accessing this list.\nRequest IDs\n\nEach API request has an associated request identifier. You can find this value in the response headers, under Request-Id.\n\ncurl -i https://profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles/<identifier>/metadata\nHTTP/1.1 200 OK\nDate: Mon, 01 Jul 2013 17:27:06 GMT\nStatus: 200 OK\nRequest-Id: 1111-2222-3333-4444\n\n\nIf you need to contact Segment regarding a specific API request, please capture and provide the Request-Id.\n\nRoutes\n\nThe Profile API supports the following routes. These routes are appended the Profile API request URL:\n\nhttps://profiles.segment.com/v1/spaces/<space_id>/\n\nNAME\tROUTE\nGet a Profile\u2019s Traits\tcollections/users/profiles/<identifier>/traits\nGet a Profile\u2019s External IDs\tcollections/users/profiles/<identifier>/external_ids\nGet a Profile\u2019s Events\tcollections/users/profiles/<identifier>/events\nGet a Profile\u2019s Metadata\tcollections/users/profiles/<identifier>/metadata\nGet a Profile\u2019s Links\tcollections/users/profiles/<identifier>/links\nGet a profile\u2019s traits\n\nRetrieve a single profile\u2019s traits within a collection using an external_id. For example, two different sources can set a different first_name for a user. The traits endpoint will resolve properties from multiple sources into a canonical source using the last updated precedence order.\n\nGET /v1/spaces/<space_id>/collections/users/profiles/<id_type:external_id>/traits\n\nQuery parameters\nARGUMENT\tDESCRIPTION\tEXAMPLE\nclass\tSupports returning all audiences, or all computed traits\tclass=audience or class=computed_trait\ninclude\tA comma-separated list of property keys to include\tfirst_name,city\nlimit\tDefines how many traits are returned in one call\t100\nverbose\tTrue for verbose field selection\ttrue,false\nExamples\n\nThis example retrieves a profile\u2019s traits by an external id, like an anonymous_id:\n\nGET /v1/spaces/lg8283283/collections/users/profiles/anonymous_id:a1234/traits\n\n\nOr a user_id:\n\nGET /v1/spaces/lg8283283/collections/users/profiles/user_id:u1234/traits\n\n\nRequest\n\n    curl https://profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles/<id_type:ext_id>/traits\n      -X GET\n      -u $SEGMENT_ACCESS_SECRET:\n\n\n404 Not Found\n\n{\n  \"error\": {\n    \"code\": \"not_found\",\n    \"message\": \"Profile was not found.\"\n  }\n}\n\n\n200 OK\n\n{\n  \"traits\": {\n    \"first_name\": \"Bob\",\n    \"emails_opened_last_30_days\": 33,\n  },\n  \"cursor\": {\n    \"url\": \"/v1/spaces/lgJ4AAjFN4/collections/users/profiles/use_RkjG0kW53igMijEISMH0vKBF4sL/traits\",\n    \"has_more\": false,\n    \"next\": \"\"\n  }\n}\n\n\n\nWith ?verbose=true enabled:\n\n{\n  \"traits\": {\n    \"first_name\": {\n      \"value\": \"Bob\",\n      \"source_id\": \"..\",\n      \"updated_at\": \"..\"\n    }\n    \"emails_opened_last_30_days\": {\n      \"value\": 33,\n      \"source_id\": \"..\",\n      \"updated_at\": \"..\"\n    }\n  },\n  \"cursor\": {\n    \"url\": \"/v1/spaces/lgJ4AAjFN4/collections/users/profiles/use_RkjG0kW53igMijEISMH0vKBF4sL/traits\",\n    \"has_more\": false,\n    \"next\": \"\"\n  }\n}\n\nGet a Profile\u2019s External IDs\n\nGet a single profile\u2019s external ids within a collection using an external_id.\n\nGET /v1/spaces/<space_id>/collections/users/profiles/<id_type:ext_id>/external_ids\n\n\nRequest\n\ncurl https://profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles/<id_type:ext_id>/external_ids\n  -X GET\n  -u $SEGMENT_ACCESS_TOKEN:\n\n\n404 Not Found\n\n{\n  \"error\": {\n    \"code\": \"not_found\",\n    \"message\": \"Profile was not found.\"\n  }\n}\n\n\n200 OK\n\n{\n  \"data\": [\n      {\n        \"source_id\": \"GFu4AJc2bE\"\n        \"collection\": \"users\",\n        \"id\": \"1d1cd931-bc7d-4e39-a1a7-61563296fb15\",\n        \"type\": \"cross_domain_id\",\n        \"created_at\": \"2017-11-30T06:05:01.40468Z\",\n        \"encoding\": \"none\",\n        \"first_message_id\": \"ajs-0af8675aa114c759210a76b2baea0a03-clean\",\n      }\n    ],\n    \"cursor\": {\n      \"url\": \"/v1/spaces/lgJ4AAjFN4/collections/users/profiles/use_RkjG0kW53igMijEISMH0vKBF4sL/external_ids\",\n      \"has_more\": true,\n      \"next\": \"map_0vKouKs2XyirgwMO4SmnDGaps7j\"\n    }\n}\n\nQuery parameters\nARGUMENT\tDESCRIPTION\tEXAMPLE\ninclude\tA comma-separated list of external ids to include\tuser_id, anonymous_id\nlimit\tDefines how many external ids are returned in one call\t25\nverbose\tTrue for verbose field selection\ttrue,false\nGet a profile\u2019s events\n\nGet up to 14 days of a profile\u2019s historical events within a collection using an external_id.\n\n    GET /v1/spaces/<space_id>/collections/users/profiles/<external_id>/events\n\n\nRequest\n\n    curl https://profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles/<external_id>/events\n      -X GET\n      -u $SEGMENT_ACCESS_SECRET:\n\n\n404 Not Found\n\n{\n  \"error\": {\n    \"code\": \"not_found\",\n    \"message\": \"Profile was not found.\"\n  }\n}\n\n\n200 OK\n\n{\n  \"data\": [\n    {\n      \"external_ids\": [\n        {\n          \"collection\": \"users\",\n          \"type\": \"user_id\",\n          \"id\": \"c0HN02fNe1\",\n          \"encoding\": \"none\"\n        },\n        {\n          \"collection\": \"users\",\n          \"type\": \"cross_domain_id\",\n          \"id\": \"1d1cd931-bc7d-4e39-a1a7-61563296fb15\",\n          \"encoding\": \"none\"\n        }\n      ],\n      \"context\": {\n        \"ip\": \"73.92.233.78\",\n        \"library\": {\n            \"name\": \"analytics.js\",\n            \"version\": \"3.2.5\"\n        },\n        \"page\": {\n            \"path\": \"/docs/connections/spec/ecommerce/v2/\",\n            \"referrer\": \"https://www.google.com/\",\n            \"search\": \"\",\n            \"title\": \"Spec: V2 Ecommerce Events Documentation - Segment\",\n            \"url\": \"https://segment.com/docs/connections/spec/ecommerce/v2/\"\n        },\n        \"traits\": {\n            \"crossDomainId\": \"1d1cd931-bc7d-4e39-a1a7-61563296fb15\"\n        },\n        \"userAgent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36\"\n      },\n      \"type\": \"track\",\n      \"message_id\": \"ajs-1a6064a677b3c16a01b8055c18f16e0b-clean\",\n      \"source_id\": \"CRx5M9uk2p\",\n      \"timestamp\": \"2018-01-05T00:16:35.663Z\",\n      \"properties\": {\n        \"name\": \"Docs\",\n        \"page_name\": \"Docs\",\n        \"path\": \"/docs/connections/spec/ecommerce/v2/\",\n        \"referrer\": \"https://www.google.com/\",\n        \"search\": \"\",\n        \"section\": \"Spec\",\n        \"title\": \"Spec: V2 Ecommerce Events Documentation - Segment\",\n        \"topic\": \"Spec: V2 Ecommerce Events\",\n        \"url\": \"https://segment.com/docs/connections/spec/ecommerce/v2/\"\n      },\n      \"event\": \"Page Viewed\",\n      \"related\": {\n        \"users\": \"use_RkjG0kW53igMijEISMH0vKBF4sL\"\n      }\n    }\n  ],\n  \"cursor\": {\n    \"url\": \"/v1/spaces/lgJ4AAjFN4/collections/users/profiles/use_RkjG0kW53igMijEISMH0vKBF4sL/events\",\n    \"has_more\": true,\n    \"next\": \"MTUxMzc1NTQzNjg2NzAwMDAwMDo6YWpzLTcyMWFhNzFjNDM2ZWJhOTUyYmI1ZmNiMzJlZWI3MWMzLWNsZWFu\"\n  }\n}\n\nQuery parameters\nARGUMENT\tDESCRIPTION\tEXAMPLE\nend\tReturns all the events that end before end (in ISO 8601).\t2018-01-02\nexclude\tA comma-separated list of event keys to exclude.\tPage Viewed,Experiment Viewed\ninclude\tA comma-separated list of event keys to include.\tPage Viewed,Experiment Viewed\nlimit\tDefines how many events are returned in one call.\t100\nsort\tDetermines whether the result is ascending or descending. Defaults to descending.\tasc,desc\nstart\tReturns all the events that start after start (in ISO 8601).\t2006-01-02\nGet a profile\u2019s metadata\n\nGet a single profile\u2019s metadata within a collection using an external_id.\n\n    GET /v1/spaces/<space_id>/collections/users/profiles/<external_id>/metadata\n\n\nRequest\n\n    curl https://profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles/<external_id>/metadata\n      -X GET\n      -u $SEGMENT_ACCESS_SECRET:\n\n\n404 Not Found\n\n{\n  \"error\": {\n    \"code\": \"not_found\",\n    \"message\": \"Profile was not found.\"\n  }\n}\n\n\n200 OK\n\n{\n  \"metadata\": {\n    \"created_at\": \"2017-10-23T00:22:42.78Z\",\n    \"updated_at\": \"2018-01-05T00:16:36.919Z\",\n    \"expires_at\": null,\n    \"first_message_id\": \"ajs-32ed8dea3980c0c92ed2b8c9c8c5dfb5-clean\",\n    \"first_source_id\": \"GFu4AJc2bE\",\n    \"last_message_id\": \"ajs-1a6064a677b3c16a01b8055c18f16e0b-clean\",\n    \"last_source_id\": \"CRx5M9uk2p\",\n  },\n}\n\nQuery parameters\nARGUMENT\tDESCRIPTION\tEXAMPLE\nverbose\tTrue for verbose field selection\ttrue,false\nGet a profile\u2019s linked users or accounts\n\nGet the users linked to an account, or accounts linked to a user, using an external_id.\n\nGET /v1/spaces/<space_id>/collections/users/profiles/<external_id>/links\n\n\nRequest\n\n    curl https://profiles.segment.com/v1/spaces/<space_id>/collections/users/profiles/<external_id>/links\n      -X GET\n      -u $SEGMENT_ACCESS_SECRET:\n\n\n404 Not Found\n\n{\n  \"error\": {\n    \"code\": \"not_found\",\n    \"message\": \"Profile was not found.\"\n  }\n}\n\n\n200 OK\n\n{\n    \"data\": [\n        {\n            \"to_collection\": \"accounts\",\n            \"external_ids\": [\n                {\n                    \"id\": \"ADGCJE3Y8H\",\n                    \"type\": \"group_id\",\n                    \"source_id\": \"DFAAJc2bE\",\n                    \"collection\": \"accounts\",\n                    \"created_at\": \"2018-10-06T03:43:26.63387Z\",\n                    \"encoding\": \"none\"\n                }\n            ]\n        },\n        {\n            \"to_collection\": \"accounts\",\n            \"external_ids\": [\n                {\n                    \"id\": \"ghdctIwnA\",\n                    \"type\": \"group_id\",\n                    \"source_id\": \"DFAAJc2bE\",\n                    \"collection\": \"accounts\",\n                    \"created_at\": \"2018-10-07T06:22:47.406773Z\",\n                    \"encoding\": \"none\"\n                }\n            ]\n        }\n    ]\n}\n\nBest practices\nRecommended implementation\n\nThe Profile API doesn\u2019t support CORS because it has access to the sum of a customer\u2019s data. Segment also requests that you prevent the Access Token to the public, for example in a client-side application. Engineers implementing this API are advised to create a personalization service in their infrastructure, which other apps, websites, and services communicate with to fetch personalizations about their users.\n\nExample workflow\n\nIf you want to display the most relevant blog posts given a reader\u2019s favorite blog category:\n\nCreate a computed trait favorite_blog_category in the Engage UI [Marketer or Engineer]\nCreate /api/recommended-posts in customer-built personalization service [Engineer]\nAccept user_id, anonymous_id to fetch favorite_blog_category using API\nReturn array of most recent posts of that category to render in recommended section\nAdd recommended section to the blog [Engineer]\nClient-side by making a request to /recommended-posts if it accepts CORS (recommended for static blogs, WordPress plugin, or other CMS solutions)\nServer-side by collecting all the personalizations you want to make on the blog in a single request to increase the total time to load (recommended for custom blog setup)\n\nUsers who take a few minutes to read through an article on the blog will find posts recommended using their historical reading pattern including the post they just read.\n\nExternal IDs\n\nSegment does not recommend using external_ids as a lookup field that might contain personally identifiable information (PII), because this can make its way into your server logs that can be hard to find and remove. For this reason, Segment recommends against using email as an external_id for Profile API use cases.\n\nPerformance\n\nSegment typically sees p95 response times under 200ms for the /traits endpoint, based on an in-region test in us-west to retrieve 50 traits. However, if you know which traits you\u2019re looking for, Segment suggests you use the /traits?include= parameter to provide a list of traits you want to retrieve.\n\nAnother best practice to optimize performance in high-throughput applications is to use connection pooling. Your personalization service should share existing connections when making a request to the Profile API, instead of opening and closing a connection for each request. This additional TLS handshake is a common source of overhead for each request.\n\nSegment recommends against blocking the page render to wait for a third party API\u2019s response, as even small slow down can impact the page\u2019s conversion performance. Instead, Segment recommends you to asynchronously request the data from after the page loads and use a server-to-server request for the necessary computed traits. Resulting computed traits can be cached for the second page load.\n\nThis page was last modified: 15 Jul 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nProduct highlights\nQuickstart\nAPI reference\nBest practices\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nUsage And Billing\n/\nDiscounts or Coupons\nDiscounts or Coupons\n\nSegment currently offers coupons on an ongoing basis for:\n\nEarly-stage startups\nParticipants and alumni of our Accelerator partners\nNon-profits\n\nEarly-stage startups Segment offers a Startup Program to enable early startups to track data correctly and easily test the marketing and analytics tools necessary to grow their business. Participating startups receive $25,000 in annual credit toward our monthly Team plan for as long as they meet our eligibility requirements (up to 2 years).\n\nLearn more about the Segment Startup Program and eligibility requirements here.\n\nParticipants and Alumni of Accelerator Partners We currently partner with various accelerator programs around the globe to offer promotions for both current and alumni participants. Contact your accelerator administrator to see if they partner with Segment and for more information on how to redeem the coupon.\n\nIf your accelerator does not participate in our program, have them apply here or contact us to us with the details of your program and the best point of contact for consideration.\n\nNon-profits We offer non-profit customers a $120 per month discount on our monthly Team plan, which typically covers 10,000 MTUs per month. Contact us to our support team with proof of your non-profit status for more details.\n\nSpecial Promotions We occasionally offer special promotions. Customers will be notified directly if they are eligible for a special promotion.\n\nCoupon FAQ\n\nHow do coupons work? Coupons are applied to your monthly (or annual) bill, which reduces the corresponding charge to your credit card. Coupons can either take the format of a percent-off or a dollar value-off your bill. If your coupon is a percentage-off your bill, the dollar value of the coupon may change as your bill may fluctuate month-to-month.\n\nHow do I redeem a coupon? Eligible startups can apply directly for the Segment Startup Program. Other coupons can be redeemed by reaching out to Segment\u2019s support team, who will apply the promotion to your account.\n\nWhere can I view which coupons are applied to my account? The Startup Program credits are reflected in the Workspace usage and billing page. Other coupons applied to your workspace are not currently reflected in the Segment application. If you are curious about a promotion you are currently on, or if you workspace has a coupon applied, contact the Segment support team.\n\nDo I have to be a \u201cnew\u201d customer to receive a coupon? The Segment Startup Program is only for customers that have not previously received any other coupon. Both the non-profit and accelerator promotion can be redeemed regardless if you\u2019re a new customer or have been with us for years. A user/workspace can only receive any coupon once.\n\nWhat happens when my coupon expires? When your promotion expires, your bill returns to the normal, non-discounted rate. Certain promotions may include a follow-up discount immediately after the promotion expires.\n\nThis page was last modified: 14 Jul 2021\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nCoupon FAQ\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSpec\n/\nSpec: Screen\nSpec: Screen\n\nThe Screen call lets you record whenever a user sees a screen, the mobile equivalent of Page, in your mobile app, along with any properties about the screen. Calling Page or Screen in one of Segment\u2019s sources is one of the first steps to getting started with Segment.\n\nSegment University: The Screen Method\n\nCheck out our high-level overview of the Screen method in Segment University. (Must be logged in to access.)\n\nHere\u2019s the payload of a typical Screen call, with most common fields removed:\n\n{\n  \"type\": \"screen\",\n  \"name\": \"Home\",\n  \"properties\": {\n    \"Feed Type\": \"private\"\n  }\n}\n\n\nAnd here\u2019s the corresponding Objective-C event that would generate the above payload:\n\n[[SEGAnalytics sharedAnalytics] screen:@\"Home\"\n                            properties:@{ @\"Feed Type\": @\"private\" }];\n\n\nBased on the library you use, the syntax in the examples might be different. You can find library-specific documentation on the Sources Overview page.\n\nBeyond the common fields, the Screen call takes the following fields:\n\n_\nFIELD\t\tTYPE\tDESCRIPTION\nname\toptional\tString\tName of the screen See the Name field docs for more details.\nproperties\toptional\tObject\tFree-form dictionary of properties of the screen, like name See the Properties field docs for a list of reserved property names.\nExample\n\nHere\u2019s a complete example of a Screen call:\n\n{\n  \"anonymousId\": \"3a12eab0-bca7-11e4-8dfc-aa07a5b093db\",\n  \"channel\": \"mobile\",\n  \"context\": {\n    \"ip\": \"8.8.8.8\"\n  },\n  \"integrations\": {\n    \"All\": true,\n    \"Mixpanel\": false,\n    \"Salesforce\": false\n  },\n  \"messageId\": \"022bb90c-bbac-11e4-8dfc-aa07a5b093db\",\n  \"name\": \"Home\",\n  \"properties\": {\n    \"variation\": \"blue signup button\"\n  },\n  \"receivedAt\": \"2015-02-23T22:28:55.387Z\",\n  \"sentAt\": \"2015-02-23T22:28:55.111Z\",\n  \"timestamp\": \"2015-02-23T22:28:55.111Z\",\n  \"type\": \"screen\",\n  \"userId\": \"97980cfea0067\",\n  \"version\": \"1.1\"\n}\n\nCreate your own Screen call\n\nUse the following interactive code pen to see what your Screen calls would look like with user-provided information:\n\nSample Screen\nVariation:\nBlue\nGreen\nRed\nSample Screen Call\nSample output goes here!\n\nIdentities\n\nThe User ID is a unique identifier for the user performing the actions. Check out the User ID docs for more detail.\n\nThe Anonymous ID can be any pseudo-unique identifier, for cases where you don\u2019t know who the user is, but you still want to tie them to an event. Check out the Anonymous ID docs for more detail.\n\nNote: In our browser and mobile libraries a User ID is automatically added from the state stored by a previous identify call, so you do not need to add it yourself. They will also automatically handle Anonymous IDs under the covers.\n\nName\n\nEach screen can be tagged with a name. For example, many apps have a \u201cSignup\u201d screen that can be useful to tag so that you can see users as they move through your funnel.\n\nProperties\n\nProperties are extra pieces of information that describe the screen. They can be anything you want.\n\nSegment has reserved some properties with semantic meanings and handles them in special ways. You should only use reserved properties for their intended meaning.\n\nReserved properties that Segment has standardized:\n\nPROPERTY\tTYPE\tDESCRIPTION\nname\tString\tName of the screen. This is reserved for future use.\n\nThis page was last modified: 21 Nov 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nExample\nIdentities\nName\nProperties\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nSegment Documentation\n\nLearn how to use Segment to collect, responsibly manage, and integrate your customer data with hundreds of tools.\n\nGetting started with Segment\n\nLearn about Segment, plan and work through a basic implementation, and explore features and extensions.\n\nHow can Segment help you?\nSimplify data collection\n\nIntegrate the tools you need for analytics, growth, marketing, and more.\n\nProtect data integrity\n\nPrevent data quality issues with a tracking schema and enforcement with Protocols.\n\nPersonalize experiences\n\nBuild audiences and journeys from real-time customer data to personalize experiences on every channel.\n\nRespect users' privacy\n\nKeep customer data private with Segment's data discovery and policy enforcement tools.\n\nGet Data into Segment\n\nThe Segment Spec helps you identify, capture, and format meaningful data for use with Segment libraries and APIs as well as downstream tools.\n\nSegment calls\n\nUse Track, Page, Identify, and other Segment tracking calls.\n\nCommon traits\n\nSave time by letting Segment calls collect information for you.\n\nUse case specs\n\nUse our business-case specs to ensure that your tools get the most from your data.\n\nLearning about Segment\nSegment for Developers\n\nThe basics of your Segment implementation.\n\nHow-To Guides\n\nOver a dozen how-to guides that help you accomplish common tasks.\n\nConnect your app to Segment\nJavaScript\nSwift\nAll other Sources\nAdditional Resources\nTotally new to Analytics?\n\nSegment's Analytics Academy walks you through the wide world of analytics, including best practices, an overview of the most popular tools, and case studies of how other developers have achieved success.\n\nWant more hands-on guidance?\n\nFor a more hands-on tutorial of Segment, check out Segment University. It offers step-by-step instructions, starting with first steps and going through some of our more advanced features.\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nInternet Bots\nInternet Bots\nWhat\u2019s a bot?\n\nIf you stumbled onto this page by accident and don\u2019t know what a bot is or are just curious to learn more, the following Wikipedia article provides an awesome summary: https://en.wikipedia.org/wiki/Internet_bot.\n\nSurprisingly, more than half of all web traffic is made up of bots. While a fraction of them are good bots with a regulated pattern, and therefore beneficial to all online businesses, the majority of them have malicious intents and are mostly unregulated.\n\nIs it possible to ignore bad bots?\n\nSegment doesn\u2019t offer an out-of-the-box solution to filter or ignore bot traffic.\n\nAs such, you generally have two options:\n\nHandle the filtering at a destination-level: Some of Segment\u2019s destination partners, like Mixpanel, filter bots automatically. Whereas others such as Hubspot allow you to set up bot filtering manually. The advantage of filtering bots at a destination level is that it allows you to implement a robust, easy-to-maintain solution. However, as it pertains to Segment, the downside is that bot traffic will still make it to Segment, affecting your MTU count.\n\nWrite custom logic that suppresses bot activity from being sent to Segment: if you want to prevent bot traffic from making it to Segment in the first place, another option is to write your own custom code. The logic, in pseudo-code, would look something like this if you know a particular characteristic of the bot traffic to filter out, such as the userAgent:\n\nvar robots = [useragent1, useragent2]\nif ! window.navigator.userAgent in robots\n  // send analytics calls\n \u00a0analytics.track\n\n\nThe benefit here is that you would be able to limit the impact that bots have on your MTU count. On the flip side, it\u2019s much harder to implement and maintain a custom filter.\n\nIf I see a massive MTU spike because of bots, can I apply for a refund?\n\nAs a matter of policy, Segment doesn\u2019t provide refunds for bot-related MTU spikes, as bot traffic is out of Segment\u2019s control. However for extenuating circumstances, you can petition for a refund, assuming you\u2019re able to provide proof of the bot\u2019s effect.\n\nI\u2019m seeing a lot of browser traffic from Boardman; is that from Segment or a bot?\n\nSegment uses Amazon\u2019s hosting services, which are based in Boardman, Oregon. However many bots also originate from AWS in Boardman as well.\n\nOne way you can confirm whether or not traffic is coming from Segment vs. a bot is to check the userAgent of the inbound call. Segment\u2019s is:\n\n'Mozilla/5.0 (' + deviceModel.slice(0, -3) + '; CPU ' + osName + ' ' +\nosVersion.replace(/\\./g, '_') + ' like Mac OS X) AppleWebKit/600.1.4 (KHTML,\nlike Gecko) Version/' + osVersion.charAt(0) + '.0 Mobile/10B329 Safari/8536.25'\n\n\nThis page was last modified: 28 Oct 2022\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nWhat\u2019s a bot?\nIs it possible to ignore bad bots?\nIf I see a massive MTU spike because of bots, can I apply for a refund?\nI\u2019m seeing a lot of browser traffic from Boardman; is that from Segment or a bot?\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSources\n/\nSchema\n/\nUsing Schema Controls\nUsing Schema Controls\n\nOnce you have enabled destinations for a given source, all of the data you track will be routed to your connected tools and warehouses. If you no longer wish to send all data to a particular destination, you can disable the destination from the Source overview page.\u00a0\n\nSegment gives you the power to control exactly what data is allowed into your destinations, so you can protect the integrity of your data, and the decisions you make with it. You can send all of your data to a warehouse and only two specific events to an analytics tool. You can also block rogue events from all of your warehouses and end tools.\n\nFilter specific events from being sent to specific destinations\n\nAn integrations object may be passed in the options of \u00a0group, identify, page and track methods, allowing selective destination filtering. By default all destinations are enabled.\n\nAll customers can filter specific events from being sent to specific destinations (except for warehouses) by updating their tracking code. Here is an example showing how to send a single message only to Intercom and Google Analytics:\n\nanalytics.identify('user_123', {\n  email: 'jane.kim@example.com',\n  name: 'Jane Kim'\n}, {\n  integrations: {\n    'All': true,\n    'Intercom': true,\n    'Google Analytics': true,\n    'Mixpanel': false\n  }\n});\n\n\nDestination flags are case sensitive and match the Destination\u2019s name in the docs (for example, \u201cAdLearn Open Platform\u201d, \u201cawe.sm\u201d, \u201cMailChimp\u201d, etc.).\n\nIf you\u2019re on Segment\u2019s Business plan, you can filter track calls right from the Segment UI on your Source Schema page by clicking on the field in the Integrations column and then adjusting the toggle for each tool. Segment recommends using the UI if possible since it\u2019s a much simpler way of managing your filters and can be updated with no code changes on your side.\n\nBlock or disable specific events and properties from being sent to all destinations\n\nIf you no longer want to track an event, you can either remove it from your code or, if you\u2019re on the Business plan, you can block track calls right from the Segment UI on your Source Schema page by adjusting the toggle for each event.\n\nOnce you block an event in Segment, Segment stops forwarding it to all of your destinations, including your warehouses. You can remove it from your code at your leisure. In addition to blocking track calls, Business plan customers can block all Page and Screen calls, as well as Identify traits and Group properties.\u00a0\n\nAdd a new event using the New Event button\n\nThe New Event button in your source schema adds the event to the source schema only. It does not add any events to your tracking code. If you want to track an event, you still need to manually add it to your source code.\n\nA use case for this feature might be to enable schema filtering for a new event before it arrives in the source to prevent it from reaching specific downstream destinations.\n\nExport your Source Schema\n\nSegment allows users with Source Read-only permissions to download Source Schemas as a CSV file, maximizing portability and access to event data. You can download a copy of your schema by visiting the Source Schema page.\n\nYou can export Track, Identify, and Group Source Schemas.\n\nDownload a CSV\n\nYou can only download one Source Schema CSV schema type (Track, Identify, or Group) per source at the same time.\n\nTo download a Source Schema CSV file:\n\nSign in to Segment and select a source.\nClick the Schema tab in the source header.\nOn the Source Schema page, select a schema type (Track, Identify, or Group) and a timeframe (7 days or 30 days).\nClick the Download CSV button.\nA toast pops up on the top of the page, with the message \u201cYour file is processing. When your file is ready it will be available to download from the Download History page.\u201d\nOpen the Download History page by clicking the link in the toast or following the instructions in the view download history section.\nOnce the file status column indicates that the download was successful, click the Download CSV link to download your CSV to your computer. If the file status column shows that the download has failed, return to the Source Schema page and try the download again.\nThe Source Schema CSV name has the following format:\nworkspaceSlug-sourceSlug-schemaType--yyyy-mm-dd--hh-mm-utc\n\nAll events and properties are now included in the CSV file\n\nWhen you export a Source Schema, all events and properties are included in the CSV file regardless of the filters or search parameters currently applied to the Source Schema view.\n\nView download history\n\nYou can view the Source Schema exports from the last 14 days on the Download History page.\n\nTo access the Download History page:\n\nSign in to Segment and select a source.\nClick the Schema tab in the source header.\nClick the View Download History link.\nTrack event CSV format\n\nThe Track event CSV file contains the following columns:\n\nEvent Name\nLast Seen At (UTC)\nIf greater than your selected timeframe (7 days or 30 days) the value is \u201cmore than 7 days ago\u201d or \u201cmore than 30 days ago\u201d\nProperty Name\nAllowed\nBlocked\nTotal\nPlanned (available for Protocols customers with a connected Tracking Plan)\nValues are \u201cplanned\u201d or \u201cunplanned\u201d\n\nLabels in your exported CSV\n\nIf you use labels, they appear as columns in your CSV. The column headers are keys, and the column data contains values.\n\nIdentity and Group event CSV format\n\nThe Identify and Group CSV files contain the following columns:\n\nTrait Name\nLast Seen At (UTC)\nIf greater than your selected timeframe (7 days or 30 days) the value is \u201cmore than 7 days ago\u201d or \u201cmore than 30 days ago\u201d\nAllowed\nBlocked\nTotal\nPlanned (available for Protocols customers with a connected Tracking Plan)\nValues are \u201cplanned\u201d or \u201cunplanned\u201d\n\nThe exported schema doesn\u2019t include actual values (for example, personal data) for the events, properties, and traits you are tracking for a specific source.\n\nSee the Segment Schema Limits for more information on how to manage the Source Schema.\n\nThis page was last modified: 22 Jun 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nFilter specific events from being sent to specific destinations\nBlock or disable specific events and properties from being sent to all destinations\nAdd a new event using the New Event button\nExport your Source Schema\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSources\n/\nSources Catalog\nSources Catalog\n\nBelow is a list of the available sources on the Segment platform.\n\nWebsite\nJavascript\nShopify (by Littledata)\nMobile\nApple\nFactual Engine\n\nBETA\n\nFlutter\n\nBETA\n\nKotlin (Android)\nReact Native\nUnity\nXamarin\nServer\n.NET\nClojure\nGo\nHTTP API\nJava\nKotlin\nPHP\nPixel Tracking API\nPython\nRuby\nA/B Testing\nConfigCat\n\nBETA\n\nInsider\nLaunchDarkly\nLeanplum\nStatsig\n\nBETA\n\nAdvertising\nFacebook Ads\nGoogle Ads\nProveSource\nAnalytics\nAmplitude Cohorts\nBeamer\nCandu\nGWEN Webhooks\n\nBETA\n\nLooker\nMixpanel Cohorts\nMoesif API Analytics\nNavattic\n\nBETA\n\nPendo\nShopify - Powered by Fueled\nUpollo\n\nBETA\n\nAttribution\nShopify - Powered by Fueled\nCRM\nAircall\nFreshchat\n\nBETA\n\nHubSpot\nMoesif API Analytics\nOneTrust\n\nBETA\n\nPaytronix\n\nBETA\n\nSalesforce\nCustom\nAmazon S3\nCustomer Success\nAircall\nBeamer\nCandu\nCommandBar\nGladly\nMoesif API Analytics\nProveSource\nRefiner\nEmail Marketing\nActiveCampaign\nAutopilotHQ\nBlueshift\nBraze\nCustomer.io\nDrip\nFacebook Lead Ads\n\nBETA\n\nInflection\nInsider\nIntercom\nIterable\nKlaviyo\nKlenty\nLeanplum\nListrak\nMailchimp\nMailjet\nMailmodo\n\nBETA\n\nMandrill\nMarketo\nMoEngage (Source)\nNudgespot\nPaytronix\n\nBETA\n\nSalesforce Marketing Cloud\nSelligent Marketing Cloud\nSendGrid\nSendGrid Marketing Campaigns\n\nBETA\n\nVero\nYotpo\n\nBETA\n\nEnrichment\nAlloy Flow\n\nBETA\n\nBluedot\nFactual Engine\n\nBETA\n\nFoursquare Movement\nHerow\nLiveLike (Source)\n\nBETA\n\nOne Creation\n\nBETA\n\nQuin AI\n\nBETA\n\nRadar\nRefiner\nFeature Flagging\nConfigCat\n\nBETA\n\nLaunchDarkly\nHelpdesk\nAircall\nGladly\nZendesk\nLearning Management System\nSynap\n\nBETA\n\nWorkRamp\nLivechat\nBlip\n\nBETA\n\nChatlio\nFreshchat\n\nBETA\n\nMarketing Automation\nAirship\nAntavo\n\nBETA\n\nBluedot\nCleverTap\nFoursquare Movement\nFriendbuy\nInflection\nInsider\nListrak\nMailmodo\n\nBETA\n\nMoEngage (Source)\nPaytronix\n\nBETA\n\nProveSource\nPushwoosh Source\n\nBETA\n\nSelligent Marketing Cloud\nSendGrid Marketing Campaigns\n\nBETA\n\nVoucherify\n\nBETA\n\nWhite Label Loyalty\n\nBETA\n\nYotpo\n\nBETA\n\nOtt\nRoku (alpha)\nPayments\nStripe\nPerformance Monitoring\nMoesif API Analytics\nSynap\n\nBETA\n\nPersonalization\nBluedot\nCandu\nCleverTap\nCommandBar\nElastic Path\n\nBETA\n\nElastic Path CX Studio\n\nBETA\n\nFoursquare Movement\nGWEN Webhooks\n\nBETA\n\nInsider\nLeanplum\nLiveLike (Source)\n\nBETA\n\nOne Creation\n\nBETA\n\nPaytronix\n\nBETA\n\nProveSource\nPushwoosh Source\n\nBETA\n\nQualtrics\nQuin AI\n\nBETA\n\nUpollo\n\nBETA\n\nUserGuiding\n\nBETA\n\nWhite Label Loyalty\n\nBETA\n\nRaw Data\nAlloy Flow\n\nBETA\n\nAuthvia\n\nBETA\n\nElastic Path\n\nBETA\n\nElastic Path CX Studio\n\nBETA\n\nReferrals\nFriendbuy\nVoucherify\n\nBETA\n\nSMS & Push Notifications\nAirship\nBeamer\nBraze\nInsider\nLeanplum\nOneSignal\n\nBETA\n\nSelligent Marketing Cloud\nTwilio\nSurveys\nBeamer\nDelighted\nJebbit\nQualtrics\nRefiner\nUserGuiding\n\nBETA\n\nVirtual Assistant\nIBM Watson Assistant\n\nOn this page\n\nWebsite\nMobile\nServer\nA/B Testing\nAdvertising\nAnalytics\nAttribution\nCRM\nCustom\nCustomer Success\nEmail Marketing\nEnrichment\nFeature Flagging\nHelpdesk\nLearning Management System\nLivechat\nMarketing Automation\nOtt\nPayments\nPerformance Monitoring\nPersonalization\nRaw Data\nReferrals\nSMS & Push Notifications\nSurveys\nVirtual Assistant\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGetting Started\n/\nUse Cases\n/\nUse Cases Reference\nUse Cases Reference\n\nThis reference guide provides detailed information on the suggested events, sources, and destinations for each Segment use case. Use this guide to ensure you\u2019re tracking the right events and connecting the best sources and destinations for your specific needs.\n\nUse Cases by business goal\n\nThe business goal you select during onboarding determines the use cases that Segment shows you.\n\nThis table lists each business goal and each of its corresponding use cases:\n\nBUSINESS GOAL\tUSE CASES\nOptimize advertising\tBuild high-value lookalikes\nBuild lookalikes for app install\nIncrease signups with lookalikes\nMitigate cart abandonment\nMitigate high value churn\nSuppress based on time\nSuppress with purchase\nPersonalize first conversion\tAccelerate app install\nAccelerate onboarding\nAccelerate signup\nAcquire paid subscriptions\nConvert trials to paid subscriptions\nMitigate cart abandonment\n\nBoost retention, upsell, and cross-sell\tBuild high value lookalikes\nIncrease repeat purchases\nMitigate high value churn\nNurture with content\nPersonalize upsell content\nPersonalize winback\n\nPersonalize communications and product experiences\tAccelerate onboarding\nIncrease repeat purchases\nMitigate high value churn\nNurture with content\nPersonalize upsell content\nPersonalize winback\n\nSuggested events, sources, and destinations\n\nThis section contains tables for the different events, sources, and destinations that Segment recommends for each use case.\n\nOptimize advertising\n\nClick on each use case in this section to view Segment\u2019s recommendations for the Optimize advertising business goal, which helps you improve return on ad spend.\n\nBuild high value lookalikes\nBuild lookalikes for app install\nIncrease signups with lookalikes\nMitigate cart abandonment\nMitigate high value churn\nSuppress based on time\nSuppress with purchase\nPersonalize first conversion\n\nClick on each use case in this section to view Segment\u2019s recommendations for the Personalize first conversion business goal, which helps you convert prospective or free customers.\n\nAccelerate app install\nAccelerate onboarding\nAccelerate signup\nAcquire paid subscriptions\nConvert trials to paid subscriptions\nMitigate cart abandonment\nBoost retention, upsell, and cross-sell\n\nClick on each use case in this section to view Segment\u2019s recommendations for the Boost retention, upsell, and cross-sell business goal, which helps you increase repeat visits or purchases.\n\nBuild high value lookalikes\nIncrease repeat purchases\nMitigate high value churn\nNurture with content\nPersonalize upsell content\nPersonalize winback\nPersonalize communications and product experiences\n\nClick on each use case in this section to view Segment\u2019s recommendations for the Personalize communications and product experiences business goal, which helps you engage your customers with relevant content.\n\nAccelerate onboarding\nIncrease repeat purchases\nMitigate high value churn\nNurture with content\nPersonalize upsell content\nPersonalize winback\n\nThis page was last modified: 08 Oct 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nUse Cases by business goal\nSuggested events, sources, and destinations\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nProtocols\n/\nTracking Plan\n/\nData Collection Best Practices\nData Collection Best Practices\n\nFiguring out what events to track in Segment can feel overwhelming. Fortunately, Segment has helped thousands of customers through this process and has amassed a ton of resources to help you get started. Whether you\u2019re a small team just getting your app off the ground or a highly complex enterprise with hundreds of stakeholders, these resources can help.\n\nThat being said, be prepared to invest time defining how you want to track data. Any investment in improving data quality will reap massive rewards, and compound over time by allowing your analytics teams to produce better insights, your marketing teams to run better campaigns and so much more.\n\nData tracking philosophy\n\nTracking is about learning and taking action. Think about what you want to know about your product or customers. Think about what assumptions need to be tested or invalidated. Think about the unknowns. Here are some helpful questions to get started:\n\nWhat kind of events or data will shed light on how your customers use your product?\nHow do people discover, pay for, and start using your product?\nWhat are the most important steps in a customer\u2019s journey?\nDefine business objectives\n\nSegment recommends documenting your high-level business objectives. What measurable business outcomes do you want to achieve? Do you want to acquire new customers, activate new signups, drive incremental revenues among your current customer base? You can best answer this question by interviewing stakeholders who would consume the data in your organization.\n\nWith your business goals documented, you now need to map user actions to those business goals. For example, if one of your goals is to activate new signups, you want to think about which activities are related to a signup. Ask yourself what actions people take before signing up. Do specific actions predict user signups?\n\nAs an example, you may end up with a list like the following:\n\nAd Campaign Clicked\nLink Clicked\nArticle Completed\nCampaign Opened\nForm Initiated\nForm Submitted\nUser Signed Up\n\nWhile these may only represent a portion of the total user actions you will track, focusing on business objectives helps make data collection more manageable.\n\nFormalize your naming and collection standards\n\nWith your business objectives documented, it\u2019s time to build a set of standards that you and your team will use when determining what to track. Segment\u2019s most successful customers limit their tracking plan to a minimal number of core events with rich properties that provide context. While some customers find success with the \u201cless is more\u201d philosophy of tracking data, others take a more liberal \u201ctrack more and analyze later\u201d approach. Both options have pros and cons you should take into account when you consider your company\u2019s needs.\n\nRegardless of your approach, keep the following tips in mind:\n\nPick a casing convention. Segment recommends Title Case for event names and snake_case for property names. Make sure you pick a casing standard and enforce it across your events and properties.\n\nPick an event name structure. As you may have noticed from the Segment specs, Segment uses the Object (Blog Post) + Action (Read) framework for event names. Pick a convention and stick to it.\n\nDon\u2019t create event names dynamically. Avoid creating events that pull a dynamic value into the event name (like User Signed Up (11-01-2019)).\n\nDon\u2019t create events to track properties. Avoid adding values to event names that could be a property. Instead, add values a property (like \"blog_post_title\":\"Best Tracking Plans Ever\").\n\nDon\u2019t create property keys dynamically. Avoid creating property names like \"feature_1\":\"true\",\"feature_2\":\"false\", as these are ambiguous and difficult to analyze.\n\nCreate a tracking plan\n\nA tracking plan clarifies what events to track, where those events live in the code base, and why those events are necessary from a business perspective. Prior to Protocols, tracking plans typically lived in a spreadsheet. The tracking plan served as a project management tool to align an entire organization around data as the basis on which to make decisions. The tracking plan helps marketers, product managers, engineers, and analysts get on the same page.\n\nThe tracking plan has been so instrumental in helping organizations reclaim their own data efforts that Segment invested years of product development to create Protocols. Whatever tool you choose to build your tracking plan, make sure that it represents a single source of truth for your data collection efforts.\n\nIdentify your users\n\nThe Identify call is important because it updates all records of the user with a set of traits. But how do you choose which traits to include?\n\nHere is a sample Identify call (with analytics.js) for Segment:\n\nanalytics.identify({\n\u00a0 name: 'Jane Doe',\n\u00a0 email: 'janedoe@iamawesome.com',\n\u00a0 login: 'janedoe',\n\u00a0 type: 'user',\n\u00a0 created: '2016-11-07T16:40:52.238Z',\n});\n\n\nThe traits represent dimensions in your data that you can group or pivot on. For example, in the previous sample call, you can easily create cohorts of all types that are users or accounts created within a time window of your choosing.\n\nDefine your Track events\n\nAfter you\u2019ve documented your event naming and collection standards, it\u2019s time to add events to your tracking plan. Segment recommends starting with fewer events that are directly tied to one of your business objectives. This focused effort helps avoid a situation where you become overwhelmed by endless possible actions to track. As you get more comfortable, you can add more events to your tracking plan that can answer peripheral questions.\n\nSegment began by tracking these events:\n\nUser Signed Up\nSource Data Sent\nSubscription Started\n\nNext, Segment added some of the following peripheral events that helped monitor performance:\n\nUser Invited; When users invite more people to their organization, it\u2019s a good indicator that they\u2019re engaged and serious about using the product. This helps measure organizational growth.\nDestination Enabled; Turning on a destination is a key value driver for Segment\u2019s customers.\nDebugger Call Expanded; When Segment sees that a certain customer has used the live event stream feature a number of times, Segment can contact them to see if they need help debugging.\n\nFor an ecommerce company, however, the main events might be something like:\n\nAccount Created\nProduct Added\nOrder Completed\n\nNote that Segment has a set of \u201creserved\u201d event names specifically for ecommerce, called the Segment ecommerce spec. Check it out to see which events Segments covers and how they are used in our downstream destinations.\n\nFor a community, on the other hand, an entirely different set of actions indicate engagement, listed in the following pyramid. For example, a community like GrowthHackers may want to track actions like:\n\nContent Viewed\nContent Shared\nComment Submitted\nContent Produced\nContent Curated\n\nWith this, they\u2019re able to measure key metrics around engagement and understand how users are moving towards their ultimate conversion event: curation content for others. For more information, check out this article from GrowthHackers about the events they track and why.\n\nDefine your Track event properties\n\nEach Track call can accept an optional dictionary of properties, which can contain any key-value pair you want. These properties act as dimensions that allow your end tool to group, filter, and analyze the events. They give you additional detail on broader events.\n\nAs mentioned earlier, events should be generic and high level, whereas properties are specific and detailed. For example, at Segment, Business Tier Workspace Created works poorly as an event name. Instead, Segment used Workspace Created with a property of account_tier and value of business:\n\nanalytics.track('Workspace Created', {\n\u00a0 account_tier: 'business'\n})\n\n\nSimilar to the traits in the Identify call, the properties provide you a column that you can pivot against or filter on in your analytics tools or allow you to create a cohort of users in email tools.\n\nAvoid dynamically generated key\u2019s in the properties dictionary, as each key will create a new column in your downstream tools. Dynamically generated key\u2019s will clutter your tools with data that will make it difficult and confusing to use later.\n\nHere is Segment\u2019s Lead Captured Track call:\n\nanalytics.track(userId, 'Lead Captured', {\n\u00a0 email: 'email',\n\u00a0 location: 'header navbar'\n\u00a0 url: 'https://segment.com/'\n});\n\n\nThe high level event is Lead Captured and all of the details are tucked into the properties dictionary. In its downstream tools, Segment can easily look at how many leads were captured in different locations on the Segment website.\n\nIf you want to learn more about how properties are used by downstream tools, check out The Anatomy of a Track Call.\n\nWant a free consultation from our Customer Success Managers on how they simplify customer\u2019s analytics? Request a demo of Segment.\n\nThis page was last modified: 28 Feb 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nData tracking philosophy\nDefine business objectives\nFormalize your naming and collection standards\nCreate a tracking plan\nIdentify your users\nDefine your Track events\nDefine your Track event properties\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nSegment for Workspace Administrators\nSegment for Workspace Administrators\n\nIf your job is to set up or maintain a Segment Workspace for your organization, or assist other people using the Segment Web App, this guide is for you. If you\u2019re more interested in Segment implementation details, see the developer intro guide.\n\nWhat is Segment?\n\nIf you\u2019ve already read an Introduction to Segment, you can skip ahead.\n\nSegment is a system for sending messages from your websites, mobile apps, and servers. These messages contain event and user data that you can send to other tools or collect in warehouses for further analysis. Segment also gathers information about your users from external systems, like help desk software or CRMs. You can use this collated information to analyze data, build user audiences, and personalize your users\u2019 experiences.\n\nWhat\u2019s a Workspace?\n\nA workspace is a group of sources that can be administered and billed together. Workspaces help companies manage access for multiple users and data sources. Workspaces let you collaborate with team members, add permissions, and share sources across your whole team using a shared billing account.\n\nWhen you first log in to your Segment account, you can create a new workspace, or choose to log into an existing workspace if your account is part of an existing organization.\n\nThe Workspace Administrator\u2019s Role\n\nYou don\u2019t have to be a developer to be a Workspace administrator for an organization, and this guide only covers tasks specifically related to managing a Workspace in the Segment App.\n\nHowever, many Workspace admins are also involved in the Segment implementation process as there are usually some tasks that must be performed in the Workspace to complete an implementation. If you think you might develop a Segment implementation or help out other developers, first read Segment for developers.\n\nNote: Workspace roles are only available to Business Tier customers. If you\u2019re on a Free or Team plan, all workspace members are granted workspace administrator access.\n\nIn addition, Workspace administrators set up and maintain the organization\u2019s workspace settings, which include:\n\nBilling information and billing contacts\nIncident contacts - the people who get notified in the event of an outage or incident\nThe Workspace name and slug - the display name and namespace of the workspace in the Segment system\n\nChanging a workspace name and slug won\u2019t impact configured sources or destinations, which connect using an internal ID and writeKey.\n\nWorkspace administrators might also maintain:\n\nThe organization\u2019s authentication settings. This can include login settings, multi-factor authentication enforcement, Identity provider (IDP) settings (including SAML and OAuth), and other related settings.\nAccess Management settings. Business-tier plans include object-, group-, and role-based access management settings, Segment workspace \u201cenvironments\u201d and labels, roles and groups, and the general permissions model.\nBilling information. If your Workspace is on a Team plan, you might have access to a billing page, where you can update the credit card on file or change other billing details.\nTasks in Connections\n\nAs an administrator, you might be asked to help other members of your organization with tasks related to setting up and troubleshooting your Segment implementation.\n\nSetting up destinations\n\nDestinations are the endpoints to which Segment sends data flowing from your Sources. Destinations can be third-party external tools, like Google Analytics or Mixpanel, or bulk-storage resources, like warehouses.\n\nYou can set up a Destination from within the Segment App by navigating to the Destination Catalog and selecting the tool you want to set up. In most cases, you\u2019ll need an existing API key or token so that Segment can send the data to the correct account. If you\u2019re setting up a Warehouse or other storage destination, more steps might be required; see the Warehouses documentation for more details.\n\nTroubleshooting\n\nUse these Segment features to keep tabs on your Workspace:\n\nWorkspace Health - if there are any problems with sources or destinations in your workspace, they\u2019ll show up here.\nEvent Tester - The Event tester allows you to troubleshoot your Sources, their configuration, and their downstream destinations. The Event Tester shows a sample of the data available, so you can check that it\u2019s being sent, and that it\u2019s in the correct format.\nEvent Delivery - Event Delivery is a bit like the Event Tester, but specifically for determining if rules or filters within Segment are preventing data from getting to a destination.\nCheck out Segment\u2019s list of integration error codes for insight into what might cause an error.\n\nStill stumped? Contact support for more help troubleshooting.\n\nHave suggestions for this guide? Reach out with your feedback.\n\nThis page was last modified: 07 Jun 2022\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nWhat is Segment?\nWhat\u2019s a Workspace?\nThe Workspace Administrator\u2019s Role\nTasks in Connections\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nData Graph\n/\nSetup Guides\n/\nDatabricks Data Graph Setup\nDatabricks Data Graph Setup\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\n\nOn this page, you\u2019ll learn how to connect your Databricks data warehouse to Segment for the Data Graph.\n\nDatabricks credentials\n\nSegment assumes that you already have a workspace that includes the datasets you\u2019d like to use for the Data Graph. Sign in to Databricks with admin permissions to create new resources and provide the Data Graph with the necessary permissions.\n\nStep 1: Create a new Service Principal user\n\nSegment recommends setting up a new Service Principal user and only giving this user permissions to access the required catalogs and schemas.\n\nIf you already have a Service Principal user you\u2019d like to use, grant it \u201cCan use\u201d permissions for your data warehouse and proceed to Step 2.\n\n1a) Create a new Service Principal user\nLog in to the Databricks UI as an Admin.\nClick User Management.\nSelect the Service principals tab.\nClick Add Service Principal.\nEnter a Service Principal user name and click Add.\nSelect the Service Principal user you just created and click Generate secret.\nSave the Secret and Client ID to a safe place. You\u2019ll need these values to connect your Databricks warehouse to Segment.\nNavigate to Workspaces and select your Workspace.\nSelect the \u201cPermissions\u201d tab and click Add Permissions.\nAdd the newly created Service Principal user and click Save.\n1b) Add your Service Principal user to Warehouse User Lists\nLog in to the Databricks UI as an Admin.\nNavigate to SQL Warehouses.\nSelect your warehouse and click Permissions.\nAdd the Service Principal user and grant them \u201cCan use\u201d access.\nClick Add.\nStep 2: Create a catalog for Segment to store checkpoint tables\n\nSegment requires write access to this catalog for internal bookkeeping and to store checkpoint tables for the queries that are executed. Therefore, Segment recommends creating a new catalog for this purpose. This is also the catalog you\u2019ll be required to specify when connecting Databricks with the Segment app.\n\nSegment recommends creating a new database for the Data Graph. If you choose to use an existing database that has also been used for Segment Reverse ETL, you must follow the additional instructions to update user access for the Segment Reverse ETL catalog.\n\nCREATE CATALOG IF NOT EXISTS `SEGMENT_LINKED_PROFILES_DB`;\n-- Copy the saved Client ID from previously generated secret\nGRANT USAGE ON CATALOG `SEGMENT_LINKED_PROFILES_DB` TO `${client_id}`;\nGRANT CREATE ON CATALOG `SEGMENT_LINKED_PROFILES_DB` TO `${client_id}`;\nGRANT SELECT ON CATALOG `SEGMENT_LINKED_PROFILES_DB` TO `${client_id}`;\n\nStep 3: Grant read-only access to the Profiles Sync catalog\n\nRun the following SQL to grant the Data Graph read-only access to the Profiles Sync catalog:\n\nGRANT USAGE, SELECT, USE SCHEMA ON CATALOG `${profiles_sync_catalog}` TO `${client_id}`;\n\nStep 4: Grant read-only access to additional catalogs for the Data Graph\n\nRun the following SQL to grant your Service Principal user read-only access to any additional catalogs you want to use for the Data Graph.\n\n-- ********** REPEAT THIS COMMAND FOR EACH CATALOG YOU WANT TO USE FOR THE DATA GRAPH **********\nGRANT USAGE, SELECT, USE SCHEMA ON CATALOG `${catalog}` TO `${client_id}`;\n\n(Optional) Step 5: Restrict read-only access to schemas\n\nRestrict access to specific schemas by running the following SQL:\n\nGRANT USAGE ON CATALOG `${catalog}` TO `${client_id}`;\nUSE CATALOG `${catalog}`;\nGRANT USAGE, SELECT ON SCHEMA `${schema_1}` TO `${client_id}`;\nGRANT USAGE, SELECT ON SCHEMA `${schema_2}` TO `${client_id}`;\n...\n\nStep 6: Validate the permissions of your Service Principal user\n\nSign in to the Databricks CLI with your Client ID secret and run the following SQL to verify the Service Principal user has the correct permissions for a given table.\n\nIf this command succeeds, you can view the table.\n\nUSE DATABASE ${linked_read_only_database} ;\nSHOW SCHEMAS;\nSELECT * FROM ${schema}.${table} LIMIT 10;\n\nStep 7: Connect your warehouse to Segment\n\nTo connect your warehouse to the Data Graph:\n\nNavigate to Unify > Data Graph. This should be a Unify space with Profiles Sync already set up.\nClick Connect warehouse.\nSelect Databricks as your warehouse type.\nEnter your warehouse credentials. You can find these details in your Databricks workspace by navigating to SQL Warehouse > Connection details. Segment requires the following settings to connect to your Databricks warehouse:\nHostname: The address of your Databricks server\nHttp Path: The address of your Databricks compute resources\nPort: The port used to connect to your Databricks warehouse. The default port is 443, but your port might be different\nCatalog: The catalog you designated in Step 2\nService principal client ID: The client ID used to access to your Databricks warehouse\nOAuth secret: The OAuth secret used to connect to your Databricks warehouse\nTest your connection, then click Save.\nUpdate user access for Segment Reverse ETL catalog\n\nIf Segment Reverse ETL has ever run in the catalog you are configuring as the Segment connection catalog, a Segment-managed schema is already created and you need to provide the new Segment user access to the existing catalog. Run the following SQL if you run into an error on the Segment app indicating that the user doesn\u2019t have sufficient privileges on an existing _segment_reverse_etl catalog.\n\nGRANT ALL PRIVILEGES ON SCHEMA ${segment_internal_catalog}.__segment_reverse_etl TO `${client_id}`;\n\n\nThis page was last modified: 05 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nDatabricks credentials\nStep 1: Create a new Service Principal user\nStep 2: Create a catalog for Segment to store checkpoint tables\nStep 3: Grant read-only access to the Profiles Sync catalog\nStep 4: Grant read-only access to additional catalogs for the Data Graph\n(Optional) Step 5: Restrict read-only access to schemas\nStep 6: Validate the permissions of your Service Principal user\nStep 7: Connect your warehouse to Segment\nUpdate user access for Segment Reverse ETL catalog\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nDelivery Overview\nDelivery Overview\n\nDelivery Overview is a visual observability tool designed to help Segment users diagnose event delivery issues for any cloud-streaming destination receiving events from cloud-streaming sources.\n\nDelivery Overview for RETL destinations and Engage Audience Syncs currently in development\n\nThis means that Segment is actively developing Delivery Overview features for RETL destinations and Engage Audience syncs. Some functionality may change before Delivery Overview for these integrations becomes generally available.\n\nDelivery Overview is generally available for streaming connections (cloud-streaming sources and cloud-streaming destinations) and in public beta for storage destinations. Some metrics specific to storage destinations, like selective syncs, failed row counts, and total rows seen, are not yet available. All users of Delivery Overview have access to the Event Delivery tab, and can configure delivery alerts for their destinations.\n\nKey features\n\nDelivery Overview has three core features:\n\nPipeline view: a visual overview of each step your data takes during the delivery process\nBreakdown table: contains more detail about the events that were processed at each pipeline step\nDiscard table: contains details about the events that failed or were filtered out of your process and allows you to inspect samples of them\n\nYou can refine these tables using the time picker and the metric toggle, located under the destination header. With the time picker, you can specify a time period (last 10 minutes, 1 hour, 24 hours, 7 days, 2 weeks, or a custom date range over the last two weeks) for which you\u2019d like to see data. With the metric toggle, you can switch between seeing metrics represented as percentages (for example, 85% of events or a 133% increase in events) or as counts (13 events or an increase of 145 events.) Delivery Overview shows percentages by default.\n\nPipeline view\n\nThe pipeline view provides insights into each step your data is processed by enroute to the destination, with an emphasis on the steps where data can be discarded due to errors or your filter preferences. Each step provides details into counts, change rates, and event details (like the associated Event Type or Event Names), and the discard steps (Failed on ingest, Filtered at source, Filtered at destination, & Failed delivery) provide you with the reasons events were dropped before reaching the destination. Discard steps also include how to control or alter that outcome, when possible. The pipeline view also includes a label between the Filtered at destination and Failed delivery steps indicating how many events are currently pending retry.\n\nLookback window\n\nDelivery Overview applies a 5-minute lookback period to provide stable, accurate metrics across all pipeline steps. This interval accounts for processing delays and ensures the data Segment displays reflects a reliable snapshot of recent events.\n\nClassic destinations\n\nThe pipeline view for classic destinations includes the following steps:\n\nSuccessfully received: Events that Segment ingested from your source.\nFailed on ingest: Events that Segment received, but were dropped due to internal data validation rules.\nFiltered at source: Events that were discarded due to schema settings or Protocols Tracking Plans.\nFiltered at destination: Events that were discarded due to Destination Filters, filtering in the Integrations object, Destination Insert functions, or per source schema integration filters. Actions destinations also have a filtering capability: for example, if your Action is set to only send Identify events, all other event types will be filtered out. Actions destinations with incomplete triggers or disabled mappings are filtered out at this step. Consent Management users also see events discarded due to consent preferences.\nFailed delivery: Events that have been discarded due to errors or unmet destination requirements.\nSuccessful delivery: Events that were successfully delivered to the destination.\nActions destinations\n\nThe pipeline view for Actions destination includes the following steps:\n\nSuccessfully received: Events that Segment ingested from your source. You can filter these events by event type, event name, app version, and enrichment status.\nFailed on ingest: Events that Segment received, but were dropped due to internal data validation rules.\nFiltered at source: Events that were discarded due to schema settings or Protocols Tracking Plans.\nMapping dropdown: Select a mapping to filter the events in the Filtered at destination, Failed delivery and Successful delivery pipeline steps.\nFiltered at destination: Events that were discarded due to Destination Filters, filtering in the Integrations object, Destination Insert functions, or per source schema integration filters. Actions destinations also have a filtering capability: for example, if your Action is set to only send Identify events, all other event types will be filtered out. Actions destinations with incomplete triggers or disabled mappings are filtered out at this step. Consent Management users also see events discarded due to consent preferences.\nRetry count: The number of events currently pending retry.\nFailed delivery: Events that have been discarded due to errors or unmet destination requirements.\nSuccessful delivery: Events that were successfully delivered to the destination.\n\nStorage destinations\n\nThe pipeline view for storage destination includes the following steps:\n\nSuccessfully received: Events that Segment ingested from your source.\nFailed on ingest: Events that Segment received, but were dropped due to internal data validation rules.\nFiltered at source: Events that were discarded due to schema settings or Protocols Tracking Plans.\nFiltered at destination: Events that were discarded due to Destination Filters, filtering in the Integrations object, Destination Insert functions, or per source schema integration filters. Actions destinations also have a filtering capability: for example, if your Action is set to only send Identify events, all other event types will be filtered out. Actions destinations with incomplete triggers or disabled mappings are filtered out at this step. Consent Management users also see events discarded due to consent preferences.\nEvents to warehouse rows: A read-only box that shows the point in the delivery process where Segment converts events into warehouse rows.\nFailed to sync: Syncs that either failed to sync or were partially successful. Selecting this step takes you to a table of all syncs with one or more failed collections. Select a sync from the table to view the discard reason, any collections that failed, the status, and the number of rows that synced for each collection. For information about common errors, see Ware\nSuccessfully synced: A record of all successful or partially successful syncs made with your destination. To view the reason a partially successfully sync was not fully successful, see the Failed to sync step.\n\nThe following image shows a storage destination with 23 partially successful syncs:\n\nBreakdown table\n\nThe breakdown table provides you with greater detail about the selected events.\n\nTo open the breakdown table, select either the first step in the pipeline view, the last step in the pipeline view, or select a discard step and then click on a discard reason.\n\nThe breakdown table displays the following details:\n\nEvent type: The Segment spec event type (Track call vs. Identify call, for example)\nEvent name: The event name, provided by you or the source (not available for inspection at all steps)\nApp version: The app/release version, provided by you or the source (not available for inspection at all steps)\nEvent count: How many of each event either successfully made it through this pipeline step (in the case of the first or last steps in the pipeline view) or were filtered out (if you access it from a discard table)\n% Change: Insight into how the event counts differ from the last comparable time range as a percentage1\n\n1: Segment calculates the related change percentage by subtracting the percent of events impacted in the previous time period from the percent of impacted events in the current time period. For example, if last week 15% of your events were filtered at a source, but this week, 22% of your events were filtered at a source, you would have a related change percentage of 7%.\n\nDiscard table\n\nThe discard table provides you with greater detail about the events that failed to deliver or were filtered out of your sources and destinations.\n\nTo open the discard table, click on one of the discard steps. If you click on a row in the discard table, you can see the breakdown table for the discarded events.\n\nThe discard table displays the following details:\n\nDiscard reason: Any relevant error code, message, or description associated with the event\u2019s failure. When possible, Delivery Overview links to any troubleshooting information you can use to get your events up and running again. Clicking on a discard reason brings you to the breakdown table where you can see more detail about discarded events. For more context about discard reasons, see the Troubleshooting documentation.\nDetails & Samples: View up to ten samples over the selected time range. Examine the error message and reason for the error or discard and inspect the payloads involved with the attempted transaction (not available for inspection at all steps)\nEvent count: How many of each event were discarded in this pipeline step\n% Change: Insight into how the event counts differ from the last comparable time range as a percentage1\n\n1: Segment calculates the related change percentage by subtracting the percent of events impacted in the previous time period from the percent of impacted events in the current time period. For example, if last week 15% of your events were filtered at a source, but this week, 22% of your events were filtered at a source, you would have a related change percentage of 7%.\n\nWhen should I use Delivery Overview?\n\nDelivery Overview is useful to diagnose delivery errors in the following scenarios:\n\nWhen setting up a destination, tracking plan, or filter for the first time: With Delivery Overview, you can verify that the data you\u2019re sending to a new destination, a new tracking plan, or a new filter arrives in your destination as expected.\nWhen data is missing from your destination: The pipeline view can help you see where your data is getting \u201cstuck\u201d on the way to your destination, which can help you quickly diagnose and address problems in your data pipeline.\nWhen emission or delivery volume fluctuates out of expected norms: Delivery Overview will highlight where the largest rate change(s) occurred and what events were associated with the change.\n\nDelivery Overview in Engage Destinations\n\nBecause Engage uses sources for multiple purposes, you can expect to see filtered at destination events with the integrations object in destinations linked to Engage. Engage uses the integrations object to route events to destinations you\u2019ve added to your audiences, traits, and journey steps. As a result, some events aren\u2019t meant to be delivered by the destination, so the integrations object filters them.\n\nWhere do I find Delivery Overview?\n\nTo view the Delivery Overview page:\n\nSign into Segment.\nFrom the homepage, navigate to Connection > Destinations and click on the destination you\u2019d like to investigate.\nSelect the Delivery Overview tab from the destination header.\nHow do I use Delivery Overview?\n\nTo use Delivery Overview:\n\nNavigate to the destination you\u2019d like to review, and select Delivery Overview from the destination header.\nOn the Delivery Overview tab, select a time period from the time picker. The time picker reflects data in the user\u2019s local time.\nOptional: Turn the metric toggle off if you\u2019d like to see the quantity of events as counts instead of percentages. Delivery Overview shows percentages by default.\nSelect a success or discard step to view additional context about the events that passed through that step.\nHow does Delivery Overview differ from other Segment monitoring and observability tools?\n\nWith Source Debugger or Event Delivery, you can only verify that events are successfully making it from your source or to your destination. If events fail, you have to troubleshoot to see where in the pipeline your events are getting stuck. With Event Tester, you can verify that your event makes it from your source to your destination, but if the results aren\u2019t what you expected, you\u2019re stuck troubleshooting your source, filters, tracking plans, and destinations.\n\nWith Delivery Overview, you can verify that your source receives your events, that any filters and tracking plans work as expected, and that events successfully make it to your destination. Any errors or unexpected behavior can be identified using the pipeline view, leading to quicker resolution.\n\nHow can I configure alerts?\n\nYou can use the Event Delivery alerting features (Delivery Alerts) by selecting the Alerts tab in the destination header. Once you enable alerts, if the successful delivery rate of all events is less than the threshold percentage in the last 24 hours, you\u2019ll be notified through in-app notification and/or workspace email.\n\nNote that this is dependent on your notification settings. For example, if the threshold is set to 99%, then you\u2019ll be notified each time less than 100% of events fail.\n\nYou can also use Connections Alerting, a feature that allows Segment users to receive in-app, email, and Slack notifications related to the performance and throughput of an event-streaming connection.\n\nConnections Alerting allows you to create two different alerts:\n\nSource volume alerts: These alerts notify you if your source ingests an abnormally small or large amount of data. For example, if you set a change percentage of 4%, you would be notified when your source ingests less than 96% or more than 104% of the typical event volume.\nSuccessful delivery rate alerts: These alerts notify you if your destination\u2019s successful delivery rate falls outside of a percentage that you set. For example, if you set a percentage of 99%, you would be notified if you destination had a successful delivery rate of 98% or below.\nHow \u201cfresh\u201d is the data in Delivery Overview?\n\nThe data in Delivery Overview has an expected latency of approximately 30 seconds after event ingestion, but this may vary, depending on the features you\u2019ve enabled in your workspace and spikes in volume. Segment delays the data visible in the Delivery Overview UI by 5 minutes to allow for more precise metric correlation. Segment does not impose the 5 minute delay if you access data using the Public API.\n\nWhy is the Delivery Overview page only available for cloud-mode destinations?\n\nSimilar to Segment\u2019s Event Delivery feature, the Delivery Overview page is only available for server-side integrations (also known as cloud-mode destinations). You won\u2019t be able to use the Delivery Overview page for client side integrations (also known as device-mode destinations) because device-mode data is sent directly to the destination tool\u2019s API. In order to report on deliverability, data must be sent to destinations using a server-side connection.\n\nTroubleshooting\n\nThe Delivery Overview pipeline steps Failed on Ingest, Filtered at Source, Filtered at Destination, and Failed Delivery display a discard table with information about why your events failed or were discarded.\n\nThis table provides a list of all possible discard reasons available at each pipeline step.\n\n Show all All Failed on Ingest Filtered at Source Filtered at Destination Failed Delivery\n\nDISCARD REASON\tERROR CODE\tWHAT HAPPENED?\tREMEDY\nFAILED ON INGEST: EVENTS THAT SEGMENT RECEIVED, BUT WERE DROPPED DUE TO INTERNAL DATA VALIDATION RULES\nEmpty batch result\tempty_batch_result\tNo messages found for batch result. After processing messages within batch, no messages returned\tCheck the event payload and client instrumentation\nSource disabled\tsource_disabled\tSource is not enabled\tCheck the source settings\nBatch is empty\tempty_batch\tThe batch request contained no messages\tCheck the event payload and client instrumentation.\n\nFor more information, see the HTTP API Batch documentation\nMulti user error\tmulti_user_error\tOne or more messages within the batch had an error. Only messages without errors were published\tReview individual payloads for each error.\n\nFor more information, see the HTTP API Errors documentation\nNo userID or anonymousID\tno_user_anon_id\tThe userID or anonymousID was not provided\tCheck the event payload and client instrumentation.\n\nFor more information, see the Anatomy of a Segment message documentation\nEvent not defined\tevent_not_defined\tTrack event did not have event name\tCheck the event payload and client instrumentation.\n\nFor more information, see the Spec: Track documentation\nTrack event not a string\tevent_not_string\tTrack event name is not a string\tCheck the event payload and client instrumentation.\n\nFor more information, see the Spec: Track documentation\nProperties field not an object\tproperties_not_object\tThe properties field must be an object type\tCheck the event payload and client instrumentation.\n\nFor more information, see the Spec: Track documentation\nTraits must be an object\ttraits_not_object\tThe traits field must be an object type\tCheck the event payload and client instrumentation.\n\nFor more information, see the Spec: Track documentation\nName must be non-empty string\tname_not_string\tFor Page or Screen calls, name field was an empty string or not a string\tCheck the event payload and client instrumentation.\n\nFor more information, see the Spec: Page documentation\nCategory field must be a string\tcategory_not_string\tFor Page or Screen calls, category field was an empty string or not a string\tCheck the event payload and client instrumentation.\n\nFor more information, see the Spec: Page documentation\nIdentifier missing from payload\tid_required\tAll payloads require a userId and/or an anonymousId\tEnsure all payloads have a userId and/or anonymousId.\n\nFor more information, see the Anatomy of a Segment message documentation\nIdentifier not a string\tid_not_string\tThe userID or anonymousId was an empty string or not a string\tCheck the event payload and client instrumentation.\n\nFor more information, see the Anatomy of a Segment message documentation\nConsent categoryPreference object does not exist\tconsent_\ncategorypreferences\n_should_exist\tcontext.consent.\ncategoryPreferences object is required\tCheck the event payload and instrumentation for the Segment Consent Preference Updated Track event.\n\nFor more information, see the Segment Consent Preference Updated event documentation\nConsent Categories field must be an object for \"Segment Consent Preference Updated\" event\tconsent_\ncategorypreferences\n_fields_should_be_object\tcontext.consent.\ncategoryPreferences must be an object\tCheck the event payload and instrumentation for the Segment Consent Preference Updated Track event.\n\nFor more information, see the Segment Consent Preference Updated event documentation\nConsent category preferences not boolean\tconsent_\ncategorypreferences\n_fields_should_be_bool\tConsent preferences for the categories must be boolean\tCheck the event payload and instrumentation for the Segment Consent Preference Updated Track event.\n\nFor more information, see the Segment Consent Preference Updated event documentation\nDevice advertisingId not a string\tdevice_advertisingid\n_should_be_string\tadvertisingId must be a string\tCheck the event payload and instrumentation for the Segment Consent Preference Updated Track event.\nConsent version not a number\tconsent_version_\nshould_be_number\tVersion must be a number\tCheck the event payload and instrumentation for the Segment Consent Preference Updated Track event.\nCould not decode payload\tbad_request\tThe payload could have an incorrect content type, body, or something else\tFix the payload and include any missing details.\n\nFor more information, see the Source Functions documentation\nCould not read write key from url\tunknown_source\tFailed to find source with [write_key]\tVerify and use the appropriate function webhook URL.\n\nFor more information, see the Source Functions documentation\nCould not find source from write key\tunknown_source\tFailed to find source with [write_key]\tVerify and use the appropriate function webhook URL.\n\nFor more information, see the Source Functions documentation\nSource missing write key\tunknown_source\tFailed to find source with [write_key]\tVerify and use the appropriate function webhook URL.\n\nFor more information, see the Source Functions documentation\nCould not decode internal settings of the source\tinvalid_settings\tFunction internal settings are invalid\tFix the function settings. If you need more information to troubleshoot the function settings, contact support.\n\nFor more information, see the Source Functions documentation\nCould not parse content-type\tBAD_REQUEST\tThe payload has an incorrect content type\tFix the payload and include any missing details.\nMake sure you're using 'application/json' or 'application/x-www-form-urlencoded' as your content-type header value.\n\nFor more information, see the Source Functions documentation\nCould not parse request body\tBAD_REQUEST\tThe payload has an incorrect body\tEnsure the payload uses accurate JSON.\n\nFor more information, see the Source Functions documentation\nUnsupported content-type\tBAD_REQUEST\tThe payload has an incorrect content type\tFix the payload and include any missing details. Make sure you're using 'application/json' or 'application/x-www-form-urlencoded' as your content-type header value.\n\nFor more information, see the Source Functions documentation\nSource/project is disabled\tsource_disabled\nor\nSOURCE_DISABLED\tThe source/project instance is disabled\tEnable the source/project instance.\n\nFor more information, see the Source Functions documentation\nWorkspace is locked out\tlocked_workspace\tThe workspace is disabled\tContact support for more details\nFunction not deployed\tinternal\tThe function is not deployed properly\tRe-deploy the function. Contact support if the issue persists.\n\nFor more information, see the Source Functions documentation\nUnexpected deploy type\tinternal\tThe function must be deployed as an AWS lambda type\tRe-deploy the function. Contact support if the issue persists.\n\nFor more information, see the Source Functions documentation\nInvalid deploy ID\tinternal\tThe function is missing the lambda ARN\tRe-deploy the function. Contact support if the issue persists.\n\nFor more information, see the Source Functions documentation\nCould not call tracking API\tTRACKING_API_FAILED\tFailed to call tracking API\tCheck the payload. Contact support if issue persists.\n\nFor more information, see the HTTP API documentation\nCould not call set API\tSET_API_FAILED\tFailed to call set API because the client was closed\tCheck the payload. Contact support for more details.\n\nFor more information, see the Source Functions documentation\nCould not call set API\tSET_API_FAILED\tFailed to call set API\tCheck the payload. Contact support for more details.\n\nFor more information, see the Source Functions documentation\nFailed to encode into lambda input format\tlambda_err\nor\ninternal\tInternal encoding error\tContact support for more details\nFailed to create lambda client\tlambda_err\tUnexpected lambda error\tContact support for more details\nLambda API permanent error\tlambda_err\tUnexpected lambda error\tContact support for more details\nLambda API temporary error\tlambda_err\tUnexpected lambda error\tContact support for more details\nToo many lambda API requests\ttoo_many_requests\tThe incoming event traffic rate exceeds the expected rate.\tContact support for more details.\n\nFor more information, see the Rate Limits documentation\nFunction timeout\tFUNCTION_TIMEOUT\tThe function timed out\tOptimize the function code.\n\nFor more information, see the Functions Usage documentation\nFunction retry error\tRETRY_ERROR\tRetry error from function code. Retry attempt will be done\tFunction will be retried. Segment's systems have a retry mechanism where an event will be retried 6 times over a four-hour period with exponential backoff.\n\nFor more information, see the Source Functions documentation\nFunction execution error\tINVOKE_ERROR\tThe function failed to execute\tCheck the function code for syntax and config issues. Contact support if issue persists.\nFailed to decode function output\tinternal\tInternal error\tReach out to support for more details\nFailed is not deployed\tBAD_DEPLOY\tThe function is not deployed properly\tRe-deploy the function and then reach out to support if issue persists.\n\nFor more information, see the Source Functions documentation.\nUnexpected DeployType. Supported is aws::lambda\tBAD_DEPLOY\tThe function is not deployed properly\tReach out to support if issue persists.\n\nFor more information, see the Source Functions documentation.\nInvalid deploy ID, missing lambda ARN\tBAD_DEPLOY\tThe function is not deployed properly\tRe-deploy the function and then reach out to support if issue persists.\n\nFor more information, see the Source Functions documentation.\nFILTERED AT SOURCE: EVENTS THAT WERE DISCARDED DUE TO SCHEMA SETTINGS OR PROTOCOLS TRACKING PLANS\nCommon schema violation\tcommon_schema_violation\tEvent violated common JSON schema of Tracking Plan\tCheck event payload against the connected Tracking Plan Common JSON Schema. If the event passes the correct information, then update the tracking plan common JSON schema with the new information.\nor:\nUpdate the source configurations settings to allow events that violate the connected Tracking Plan JSON schema:\nSource > Settings > Schema Configurations > Advanced Blocking Controls\n\nFor more information, see the Common JSON schema documentation\nEvent discard setting\tevent_setting\tThe Source is configured to discard events of this type\tCheck source schema filters.\n\nFor more information, see the Source Schema Integrations Filters documentation\nSchema violation\tschema_violation\tSource schema is configured to block events that violate the connected Tracking Plan JSON schema.\tCheck event payload against the connected Tracking Plan Common JSON Schema. If the event passes the correct information, then update the tracking plan common JSON schema with the new information.\nor:\nUpdate the source configurations settings to allow events that violate the connected Tracking Plan JSON schema:\nSource > Settings > Schema Configurations > Advanced Blocking Controls\n\nFor more information, see the Customize your schema controls documentation\nUnplanned event\tunplanned\tSource schema configured to block events not defined in the connected Tracking Plan\tCheck source Configurations:\nSettings > Schema Configurations > to allow unplanned events.\nOR:\nAdd the new event in the connected tracking plan so it's recognized as a planned event. For more information, see the Customize your schema controls documentation\nUnplanned and schema violation\tunplanned_and_\nschema_violation\tSource schema configured to block events not defined in the connected tracking plan. The event also violated the connected tracking plan JSON schema\tUpdate the source schema configurations to allow unplanned events\nOR:\nAdd the new event in the connected tracking plan so it's recognized as a planned event.\nFILTERED AT DESTINATION: EVENTS THAT WERE DISCARDED DUE TO DESTINATION FILTERS, FILTERING IN THE INTEGRATIONS OBJECT, OR PER SOURCE SCHEMA INTEGRATION FILTERS\nFiltered by rules\tFILTERED_BY_RULES\tEvent matched a Destination Filter rule\tTo include events like this, change the Destination Filter to be more specific or exclude this event.\n\nFor more information, see the Destination Filters documentation\nFiltered by integrations object\tFILTERED_BY_\nINTEGRATIONS_OBJECT\tThe event was filtered because sending to the destination in the integrations object is disabled\tTo include events like this, remove filtering from the integrations object.\n\nFor more information, see the Filtering with the integrations object documentation\nUnkown integration\tUNKNOWN_INTEGRATION\tDestination not registered in the integrations info\tCheck the event payloads integrations object to ensure all listed destinations are valid. Refer to the destination's documentation for acceptable names.\n\nFor more information, see the Filtering with the integrations object documentation\nMessage sent client side\tMESSAGE_SENT_CLIENT_SIDE\tThe message was already sent client side\tThese events are being sent client side in Device Mode and will not be sent from Segment's servers. Events in this category are sent directly from your website or app to the downstream destination.\n\nFor more information, see the Destination methods comparison documentation\nUnsupported event type\tUNSUPPORTED_EVENT_TYPE\tThe destination does not support this event type\tFor more information about the events your destination can consume, see the Filtering with the integrations object documentation\nInvalid settings\tINVALID_SETTINGS\tThe event type is missing one or more required settings\tCheck your integration type.\n\nFor more information, see the Destination settings documentation\nFunctions lock out\tFUNCTIONS_LOCK_OUT\tThe function wasn't executed because the workspace reached its paid limit for functions\tTo increase your functions limits, upgrade your workspace plan.\n\nFor more information, see the Functions usage limits documentation\nInternal error\tINTERNAL\tSomething went wrong\tContact support for more information\nAction missing mapping or trigger\tNO_MATCHING_MAPPING\tThe Actions destination is missing either a mapping or trigger for this event\tYour event does not meet any trigger cases for your Actions mappings. Please add a mapping with a trigger that accepts this event.\n\nFor more information, see the Actions destination FAQ\nFiltered at mapping\tFILTERED_AT_MAPPING\tThe event was filtered because it did not match the Actions destination mapping\tContact support for more information\nFailed data encryption\tFAILED_DATA_ENCRYPTION\tMessage delivery failed due to data encryption; either there was an issue encrypting data or failed to deliver data with encrypted values\tCheck if the destination can accept encrypted data in the fields being encrypted\nInvalid request\tbad_request\tThe request is either malformed or the function threw an unknown exception\tReview the payload to ensure that it aligns with Segment's expectations.\n\nFor more information, see the HTTP API Errors documentation\nInvalid settings\tinvalid_settings\tThe function's internal settings are invalid\tFix the function's settings or reach out to support for more details.\nInvalid event\tmessage_rejected\tThe function threw exceptions such as InvalidEventPayload or ValidationError\tFix the payload to contain the data needed by the function\nUnsupported content-type\tunsupported_event_type\tEventNotSupported or Missing event handler\tAdd the missing handler to the function code\nFailed to process request\tinternal\tSegment failed to process the request\tContact support for more information\nToo many incoming lambda API requests\ttoo_many_requests\tThe incoming event traffic rate exceeds the expected rate\tContact support for more information\nFunction timeout\tgateway_timeout\tThe function timed out\tCheck the function code or contact support to increase the timeout\nFunction retry error\tretry\tRetry errror from function code\tFunction will be retried automatically\nFiltered by end user consent\tFILTERED_BY_\nEND_USER_CONSENT\tThe message was dropped due to the user's consent preferences\tContact support for more information\nFAILED DELIVERY: EVENTS THAT HAVE BEEN DISCARDED DUE TO ERRORS OR UNMET DESTINATION REQUIREMENTS\nInvalid settings\tINVALID_SETTINGS\tThe event is missing some required settings as configured for that integration per event type\tReview your Segment settings and make any necessary updates.\n\nFor more information, see the Integration Error Codes documentation\n429\t429\tToo many requests were sent in a time frame\tThese events will be retried automatically.\nIf the events eventually fail due to too much volume, contact the partner to raise your rate limit.\nIf the destination allows batching in Segment, you may be able to reduce the total number of requests.\n\nFor more information, see the Integration Error Codes documentation\nErefused\tEREFUSED\tThere was a temporary problem connecting to the destination's API\tThis event will be retried automatically.\nIf the event eventually fails, your Segment configuration settings may contain some invalid settings, or the integration may not be operational.\nIf your configurations are valid, consider disabling this integration or conact the intagration partner.\n\nFor more information, see the Integration Error Codes documentation\nUnsupported event type\tUNSUPPORTED_EVENT_TYPE\tThe destination does not support this event type\tContact support for more information\nMessage rejected\tMESSAGE_REJECTED\tRequest was blocked\tCheck the event payload for required fields and data types for all fields and compare it to your destination configuration.\n\nFor more information, see the Integration Error Codes documentation\n400/Bad request\t400\nor\nBAD_REQUEST\tThe downstream API rejected the payload\tReview the Response from Destination tab for more information.\n\nFor more context, see the Integration Error Codes documentation\nEtimedout\tETIMEDOUT\nor\netimedout\tThe downstream destination did not send an API response back to Segment in a reasonable amount of time\tNo action is needed. The delivery will be retried automatically\nEnotfound\tENOTFOUND\tThe endpoint URL cannot be found or does not exist\tCheck the Request from Segment tab to see which URL the request is being sent to and verify that the URL there is correct\nInternal\tINTERNAL\tThere was a problem connecting to the destination's server\tNo action needed. Events will be retried when there's a successful connection\n404\t404\tThe server cannot find the requested resource\tThe server cannot find the requested resource. This can happen for a number of reasons, such as:\nThe requested resource does not exist.\nThe requested resource has been moved or deleted.\nThere is a typo in the URL.\nThe server is experiencing technical difficulties.\n307\t307\tThe requested resource was temporarily redirected\tSegment will automatically retry the request using the redirected URL\n502\t502\tThe server recieved an invalid response from the upstream server\tNo action needed. Segment will try to send the payload again.\n\nFor more information, see the Integration Error Codes documentation\n503\t503\tServer could not handle the request\tNo action needed. Segment will try to send the payload again.\n\nFor more information, see the Integration Error Codes documentation\nRetry\tRETRY\tThe intitial request was unsuccessful. The request was sent again\tNo action needed. Segment will continue to retry sending the payload.\n\nFor more information, see the Integration Error Codes documentation\n401\t401\tThe request could not be completed because the authentication credentials are either invalid or expired\tRe-authenticate your account with the partner and update your authentication settings in Segment.\n\nFor more information, see the Integration Error Codes documentation\nEconnreset\tECONNRESET\tSegment could not establish a connection to the partner server\tYour Segment configurations might contain some invalid settings or the integration may no longer be operational. If your configurations are valid, disable this integration or contact the integration partner.\n\nFor more information, see the Integration Error Codes documentation\n\nThis page was last modified: 14 Nov 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nKey features\nWhen should I use Delivery Overview?\nWhere do I find Delivery Overview?\nHow do I use Delivery Overview?\nHow does Delivery Overview differ from other Segment monitoring and observability tools?\nHow can I configure alerts?\nHow \u201cfresh\u201d is the data in Delivery Overview?\nWhy is the Delivery Overview page only available for cloud-mode destinations?\nTroubleshooting\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSpec\n/\nPackaging SDKs for Mobile Destinations\nPackaging SDKs for Mobile Destinations\n\nWhen it comes to Mobile SDKs, we know that minimizing size and complexity is a priority for our customers. That\u2019s why our core Mobile SDKs are small and offload as much work as possible in handling destinations to our servers. When you install our light-weight SDK, you have access to our entire suite of server-side destinations.\n\nWhy do some destinations require bundling their SDKs?\n\nWe bundle certain SDKs, instead of just proxying your data to them through our servers, so that you have access to their deeper features that requires direct client manipulation (A/B testing, user surveys, touch heatmapping, etc) or access to rich data such as CPU usage, network data, or raised exceptions. For those types of features, we still need to bundle their native SDK for you so you can make the most of them.\n\nWe\u2019ve worked hard to make our mobile SDKs as modular as possible so that you only need to include the SDKs for tools you plan to use.\n\nCustom builds allow us to offer the native functionality of all of our destinations without having to include hefty third-party SDKs by default. This gives you control over size and method bloat. Check out how to use custom builds for both\u00a0Android\u00a0and\u00a0iOS.\n\nWhich SDKs are bundled?\n\nTo check if a destination is bundled or not, take a look at our\u00a0documentation\u00a0for that specific destination.\n\nThis page was last modified: 14 Jul 2021\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nHow To Guides\n/\nSegment's Role in Attribution\nSegment's Role in Attribution\n\nAt a higher level, attribution tools allow you to connect a specific campaign to user acquisition, giving you more visibility into campaign performance. \u00a0See the destination catalog for a list of attribution tools that Segment supports.\u00a0\n\nThere are three stages of mobile attribution as it relates to Segment.\u00a0\n\nCustomer installs your app\n\nThe install is attributed by an attribution provider (Adjust, AppsFlyer, etc)\n\nAttribution information is sent back to Segment\n\nHere is a bit more information on what is happening at each of those stages.\u00a0\n\nCustomer installs your app\n\nWhen lifecycle events are enabled, the Application Installed and Application Opened events are triggered on the first app open after the app is installed. \u00a0Note, if the app is deleted and then later reinstalled on the device, these events will be triggered again on first app open.\u00a0\n\nSituations where install counts look lower in Segment than in other tools.\u00a0\n\nSome tools, like iTunes or Google Play, count install on download rather than on app open like Segment.\u00a0 iTunes and Google Play is able to easily collect data on download but not as easily able to collect first-party data on app open. Whereas other tools, such as Segment, need their SDK to be loaded in app and initialized on app open before they are able to collect the install information. For example, if a user downloads your app but does not open it, the install will be counted in iTunes/Google Play but not counted in Segment or other tools.\n\nSituations where install counts look higher in Segment than in other tools\n\nMany tools deduplicate install data. Some tools only allow one install event per lifetime of deviceId. Others deduplicate by deviceId accepting only one install per UTC day. \u00a0Each and every tool is different. \u00a0\n\nSegment, on the other hand, does not deduplicate. \u00a0We don\u2019t believe our role in your data pipeline should be deduping particular events. \u00a0In fact, there may be situations where you may want to account for multiple Application Installed events such as: user sells their phone, user uninstalls and later decides to reinstall, etc. It is better to think about the Application Installed data in your Segment warehouse as the raw source of data, giving you flexibility to query\u00a0\n\nFor more information on how installs are counted in different tools, here are a few resources from our partners:\u00a0\n\nAdjust - Discrepancies and Why Data Does not Always Match Up\n\nThe install is attributed by an attribution provider\nDevice-Mode Connection\n\nWhen you enable an attribution destination in device-mode, our integration code will also load that tool\u2019s SDK. Upon app launch, the destination\u2019s SDK will send install information which is then use to attribute that install to a campaign on their backend. \u00a0Segment loads the destination\u2019s SDK, but attribution happens outside of Segment.\u00a0\n\nCloud-Mode Connection\n\nDestination receives the Application Installed event and attributes the installation on their backend.\u00a0\n\nAttribution information is sent back to Segment\nDevice-Mode Connection\n\nFor tools that support this, if you have enabled \u201cTrack Attribution Data\u201d in your Segment dashboard, our integration listens to the attribution tool\u2019s SDK for a change in attribution state. Note: Not all device-mode attribution tools offer \u201cTrack Attribution Data\u201d functionality. \u00a0See the settings section for a particular tool in your Segment dashboard for confirmation.\u00a0\n\nWhen there is a change in attribution state, the integration code triggers an Install Attributed call to be sent back to your Segment source (and on to all other enabled destinations - in device and cloud-mode). \u00a0\n\nHere is an example of how that call is triggered in the AppsFlyer integration code. This is the similar for other attribution providers such as Adjust.\u00a0\n\nCloud-Mode Connection\n\nFor tools that support server-side postback, after install is attributed, an Install Attributed event is triggered and sent server-side to your Segment source and forwarded on to all enabled cloud-mode destinations.\u00a0\n\nExample Install Attributed event:\u00a0\n\nanalytics.track('Install Attributed', {\n\u00a0 provider: 'Tune/Adjust/AppsFlyer',\n\u00a0 campaign: {\n\u00a0 \u00a0 source: 'Network/FB/AdWords/MoPub/Source',\n\u00a0 \u00a0 name: 'Campaign Name',\n\u00a0 \u00a0 content: 'Organic Content Title',\n\u00a0 \u00a0 ad_creative: 'Red Hello World Ad',\n\u00a0 \u00a0 ad_group: 'Red Ones'\n\u00a0 }\n});\n\n\nFor more detailed information on a particular attribution destination and functionality, see our Destinations docs.\n\nThis page was last modified: 07 Feb 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nCustomer installs your app\nThe install is attributed by an attribution provider\nAttribution information is sent back to Segment\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nStorage\n/\nWarehouses\n/\nChoosing a Warehouse\nChoosing a Warehouse\nComparing Redshift and Postgres\n\nIn most cases, you will get a much better price-to-performance ratio with Redshift for typical analyses.\n\nRedshift lacks some features, datatypes, and functions supported by Postgres and also implements some features differently. If you need any of the features or functions missing in Redshift and BigQuery, choose Postgres. If not (or you\u2019re not sure), Segment recommends choosing Redshift.\n\nIf you\u2019d like more information, Amazon wrote about this in their documentation.\n\nComparing Redshift and BigQuery\n\nBoth Redshift and BigQuery are attractive cloud-hosted, affordable, and performant analytical databases. The differences between the two are around their architecture and pricing.\n\nArchitecture\n\nWhen you provision a Redshift cluster, you\u2019re renting a server from Amazon Web Services. Your cluster consists of nodes, each with dedicated memory, CPU, and disk storage. These nodes handle data storage, query execution, and - if your cluster contains multiple nodes - a leader node will handle coordination across the cluster.\n\nRedshift performance and storage capacity is a function of cluster size and cluster type. As your storage or performance requirements change, you can scale up or down your cluster as needed.\n\nWith BigQuery, you\u2019re not constrained by the storage capacity or compute resources of a given cluster. Instead, you can load large amounts of data into BigQuery without running out of memory, and execute complex queries without maxing out CPU.\n\nThis is possible because BigQuery takes advantage of distributed storage and networking to separate data storage from compute power. Google\u2019sColossus distributed file system distributes data across many servers in the Google cloud. When you execute a query, the Dremel query engine splits the query into smaller sub-tasks, distributes the sub-tasks to computers across Google data centers, and then re-assembles them into your results.\n\nPricing\n\nThe difference in architecture translates into differences in pricing.\n\nRedshift prices are based on an hourly rate determined by the number and types of nodes in your cluster. They offer dense storage - optimized for storage - and dense compute nodes - optimized for query performance.\n\nBigQuery has two pricing options: variable and fixed pricing. With the variable, pay-as-you-go plan, you pay for the data you load into BigQuery, and then pay for the amount of data you query. BigQuery allows you to set up Cost Controls and Alerts to help control and monitor costs.\n\nFixed-price plans are more for high-volume customers and allow you to rent a fixed amount of compute power.\n\nResource Management\n\nRedshift does require you to create a cluster, choose sort and distribution keys, and resize your cluster as storage and performance needs change over time.\n\nBigQuery is \u201cfully-managed\u201d, which means that you\u2019ll never have to resize or adjust distribution or sort keys. BigQuery handles all of that.\n\nThis page was last modified: 20 Mar 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nComparing Redshift and Postgres\nComparing Redshift and BigQuery\nArchitecture\nPricing\nResource Management\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nTraits\n/\nSQL Traits\nSQL Traits\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY PLUS \u2713\n?\n\nSQL Traits End of Sale\n\nSQL Traits entered End of Sale as of March 31, 2024. Existing Segment customers will continue to have access to SQL Traits, but Segment will no longer offer SQL Traits to new customers. Segment recommends using Reverse ETL to sync your data into Segment.\n\nUse SQL Traits to import user or account traits from your data warehouse back into Unify or Engage to build audiences or to enhance data that you send to other Destinations.\n\nSQL Traits are only limited by the data in your warehouse. Because anything you can write a query for can become a SQL Trait, you can add detail to your user and account profiles, resulting in more nuanced personalization.\n\nThis unlocks some interesting possibilities to help you meet your business goals.\n\nTo improve your support team\u2019s customer satisfaction score (CSAT), create a SQL Trait of the most common ticket requests for a customer\u2019s industry by joining data from cloud sources like Zendesk and Salesforce. The resulting SQL Trait helps you anticipate the user\u2019s problems and accelerate potential solutions.\nTo determine if a user resides in a specific area, query address data in your warehouse and send it as a true or false Trait to an Engage audience.\nTo fill gaps in your customer profiles to include information before you implemented Segment, import historical Traits from your warehouse.\nTo predict a customer\u2019s lifetime value (LTV), generate a complex query based on demographic and customer data in your warehouse. You can then use that information in an Engage audience to send personalized offers or recommend specific products.\nTo inform your outreach efforts, use complex queries to build churn or product adoption models.\n\nCheck out Segment\u2019s SQL Traits blog post for more customer case studies.\n\nTo view SQL Traits in a user profile, you must have PII access. Without this access, Segment redacts all SQL traits in a profile.\n\nNote that after you bring in data with SQL Traits, changing data types for fields may not be compatible with all destinations.\n\nExample: cloud sources sync\n\nSQL Traits allow you to import data from object cloud sources like Salesforce, Stripe, Zendesk, Hubspot, Marketo, Intercom, and more. For example, bring in Salesforce Leads or Accounts, Zendesk ticket behavior, or Stripe LTV calculations.\n\nThe two examples below show SQL queries you can use to retrieve cloud-source information from your warehouse.\n\nSalesforce lead import\n\nIf you want to import data from the Salesforce leads and contacts table, you can use SQL similar to the following query:\n\n    select external_id_c as user_id,\n    lead_score_c,\n    lead_age_c,\n    lead_status\n    -- \u2026more properties\n    from salesforce.leads\n\n\nHas Open Ticket in Zendesk\n\nThis query computes whether a user has an open ticket:\n\n    select distinct u.external_id as user_id, true as has_open_ticket\n    from zendesk.tickets t\n    join zendesk.users u\n    on u.id = t.requester_id\n    where t.status in ('pending','open','hold','new')\n\nComparing trait types\n\nView the table below to better understand how Segment collects custom, computed, and SQL traits.\n\nYou can use the Profile explorer (Unify > Profile explorer) to view traits attached to a profile.\n\nTRAIT TYPE\tDESCRIPTION\nCustom traits\tTraits created from source events you pass into Segment. From your sources, send custom traits as pieces of information that you know about a user in an Identify call.\nComputed traits\tTraits collected from computations off of event and event property data from your sources. Create user or account-level calculations like most_viewed_page or total_num_orders for a customer. Learn more by viewing types of computed traits.\nSQL traits\tTraits created by running SQL queries on data in your warehouse. SQL traits are a type of computed trait. SQL traits help you import traits from your data warehouse back into Segment to build audiences or enhance data that you send to other destinations.\nConfigure SQL Traits\n\nTo use SQL Traits, you need the following:\n\na warehouse connected to Segment\na Segment workspace\na user account with access to Unify in that workspace\nStep 1. Set up a warehouse source\n\nSegment supports Redshift, Postgres, Snowflake, Azure SQL, and BigQuery as data warehouse sources for SQL Traits. Note that the BigQuery setup process requires a service user.\n\nSafeguard your data\n\nFor any warehouse, Segment recommends that you create a separate read-only user for building SQL Traits.\n\nRedshift, Postgres, Snowflake, Azure SQL setup\n\nIf you don\u2019t already have a data warehouse, use one of the following guides to get started:\n\nRedshift Getting Started\nPostgres Getting Started\nSnowflake Getting Started\nAzure SQL Getting Started\nBigQuery setup\n\nTo connect BigQuery to Segment SQL Traits, follow these instructions to create a service account for Segment to use:\n\nNavigate to the Google Developers Console.\n\nClick the drop down to the left of the search bar and select the project that you want to connect.\n\nNote: If you don\u2019t see the project you want in the menu, click the account switcher in the upper right corner, and verify that you\u2019re logged in to the right Google account for the project.\n\nClick the menu in the upper left and select IAM & Admin, then Service accounts.\n\nClick Create Service Account.\n\nGive the service account a name like segment-sqltraits.\n\nUnder Project Role, add only the BigQuery Data Viewer and BigQuery Job User roles.\n\nIMPORTANT: Do not add any other roles to the service account. Adding other roles can prevent Segment from connecting to the account.\n\nClick Create Key.\n\nSelect JSON and click Create.\n\nA file with the key is saved to your computer. Save this; you\u2019ll need it to set up the warehouse source in the next step.\n\nYou\u2019re now ready to create a new BigQuery warehouse source, upload the JSON key you just downloaded, and complete the BigQuery setup.\n\nStep 2. Add the warehouse as a Source\n\nOnce your warehouse is up and running, follow these steps:\n\nNavigate to the Engage settings (Engage > Engage Settings > Warehouse Sources), and click Add Warehouse Source.\n\nSelect the type of warehouse you\u2019re connecting.\n\nIn the next screen, provide the connection credentials, and click Save.\n\nIf you\u2019re connecting a BigQuery warehouse, use the JSON key file that you downloaded as the last step.\n\nCreate a SQL Trait\n\nBefore you create a SQL Trait, you must first preview it to validate your query. If you\u2019re new to SQL, try out one of the templates Segment offers.\n\nPreview the SQL Trait\n\nFrom the Audiences viewer, go to the Computed Traits tab, and click New Computed Trait. Next, choose SQL, and click Configure. Select the data warehouse that contains the data you want to query.\n\nIf you\u2019re sending data from object cloud sources to your warehouse, the SQL Traits UI has some pre-made templates you can try out.\n\nWhen you\u2019re building your query, keep the following requirements in mind for the data your query returns.\n\nThe query must return a column with a user_id, email, or anonymous_id (or group_id for account traits, if you have Engage for B2B enabled). The query cannot include values for both user_id and anonymous_id.\nThe query must return at least one trait in addition to user_id/anonymous_id/email/group_id, and no more than 25 total columns.\nThe query must not return any user_ids, anonymous_ids, or group_ids with a null value.\nThe query must not return any records with duplicate user_ids.\nThe query must not return more than 25 million rows.\nEach record must be less than 16KB in size to adhere to Segment\u2019s maximum request size.\n\nA successful preview returns a sample of users and their traits. If Segment recognizes a user already in Engage, it displays a green checkmark on their profile. Clicking the checkmark displays the user\u2019s profile. If a user has a question mark, Segment hasn\u2019t detected this user_id in Engage before.\n\nConfigure SQL Trait options\n\nOnce you\u2019re ready to import the SQL Trait, select the Destinations to which you want to send the data. If you prefer to build Engage audiences directly from the data instead of sending it to a Destination, click Skip.\n\nGive your SQL Trait a descriptive name. If you\u2019re importing multiple Traits, use a name like \u201cZendesk Traits\u201d. The Trait names you use in audience-building or in your downstream tools correspond to the column names from the query.\n\nIf you\u2019re building Engage audiences from this data, select \u201cCompute without enabled destinations\u201d.\n\nClick Create Computed Trait to save the Trait.\n\nCheck Compute without destinations if you only want to send to Engage.\n\nWhen you create a SQL Trait, Segment runs the query on the warehouse twice a day by default. You can customize the time at which Segment queries the data warehouse and the frequency, up to once per hour, from the SQL Trait\u2019s settings.\n\nFor each row (user or account) in the query result, Engage sends an identify or group call with all the columns that were returned as Traits. For example, if you write a query that returns user_id, has_open_ticket, num_tickets_90_days, avg_zendesk_rating_90days Segment sends an identify call with the following payload:\n\n    {\n      type: 'identify',\n      userId: 'u123',\n      traits: {\n        has_open_ticket: true,\n        num_tickets_90_days: 3,\n        avg_zendesk_rating_90_days: 8\n      }\n    }\n\nFAQs\nIs there a limit to the result set that can be queried and imported?\n\nYes. The result set is capped at 25 million rows.\n\nHow often does Segment query the customer\u2019s data warehouse?\n\nFor each SQL Trait you create, you can set a compute schedule to query the data warehouse up to once per hour. Your query may run at any given time during the hour you select.\n\nWhat identifiers can I use to query a list?\n\nYou can query based on email, user_id, or anonymous_id. If Segment doesn\u2019t locate a match based on the chosen identifier, it creates a new profile. See more below.\n\nCan I use SQL Traits to create users in Segment? Or do SQL Traits only append Traits to existing users?\n\nYes. The Engage engine sends an identify call if there is no match between the identifier you chose and an existing record. When this happens, Segment creates a new user profile. This identify call takes place in the back-end and doesn\u2019t show up in your Debugger.\n\nDoes Engage send identify/track/group calls on every run?\n\nNo. Engage only sends an identify/track/group call if the values in a row have changed from previous runs.\n\nI have a large (1M+) query of users to import, should I be worried?\n\nIf you\u2019re importing a large list of users and traits, you\u2019ll need to consider your API call usage as well as volume among the partners receiving your data. These vary depending on Segment\u2019s partners, contact support for more information.\n\nIs there a limit on the size of a SQL Trait\u2019s payload?\n\nYes, Segment limits request sizes to a maximum of 16KB. Records larger than this are discarded.\n\nDo SQL Traits support arrays?\n\nNo, SQL Traits supports string and numeric data types. You can cast arrays as a comma-separated string. In this case, if you used this trait to build an audience, you could check if the array contains a certain value with the \u201ccontains\u201d operator, but the value is sent to any connected destinations as a string.\n\nCan I change the Warehouse Source after a SQL trait has been created?\n\nAfter a SQL trait has been created, you can\u2019t change its Warehouse Source. You\u2019ll need to create a new trait if you want to change the Warehouse source.\n\nWhat happens if a user is no longer returned by the SQL trait?\n\nIf a user was present in one computation, but it is no longer present in the following one, the SQL trait will detect this difference and nullify all trait values for the user. Contact Segment if you have a use case which calls for an exemption from this default behavior.\n\nTroubleshooting\nI\u2019m getting a permissions error.\n\nYou might encounter a permission denied for schema error, like the following:\n\nSegment usually displays this error because you\u2019re querying a schema and table that the current user cannot access. To check the table privileges for a specific grantee (user), view the credentials of the stored warehouse user.\n\nTo grant access to a table, an admin usually needs to grant access to both a schema and table through the following similar commands:\n\n    GRANT USAGE ON SCHEMA ecommerce TO segment_user;\n    GRANT SELECT ON TABLE ecommerce.users TO segment_user;\n\n\nLearn more about granting permissions using the following links:\n\nPostgreSQL Grants\nWhat does \u2018Grant usage on schema\u2019 do?\nI\u2019m seeing a maximum columns error.\n\nSegment supports returning only 25 columns. Contact Segment with a description of your use case if you need access to more than 25 columns.\n\nI\u2019m seeing a duplicate user_id error.\n\nEach query row must correspond to a unique user. Segment displays this error if it detects multiple rows with the same user_id. Use a distinct or group by statement to ensure that each row has a unique user_id.\n\nI\u2019m seeing some users/accounts in my preview with question marks. What does that mean?\n\nQuestion marks in previews indicate one of two things:\n\n1. Segment doesn\u2019t recognize this user_id/group_id in Engage.\n\nIn this case, for sources connected to Engage, Segment hasn\u2019t received any event (for example, identify, track, or page) with this user_id. This could still be a legitimate user_id for a number of reasons, but before syncing, make sure you rule out option two (below), as sending a different identifier as the user_id can corrupt your identity graph.\n\n2. You have the wrong user_id column.\n\nYou might be returning a value for user_id that\u2019s inconsistent with how you track user_id elsewhere. Some customers want to return email as the user_id, or a partner\u2019s tool ID as the user_id. These conflict with Segment best practices and corrupt the identity graph if you then track user_id differently elsewhere in your apps.\n\nIf you see only question marks in the preview, and have already tracked data historically with Segment, then you likely have the wrong column. If your cloud source doesn\u2019t have the database user_id, Segment recommends using a JOIN clause with an internal users table before sending the results back to Segment.\n\nWhy do some SQL Trait settings not have the \u201cCompute schedule\u201d option?\n\nSegment added the compute schedule feature on Feb 8, 2021, so traits created prior to this date will not have this option. If your trait lacks this feature, recreating it will make it available.\n\nWhy doesn\u2019t the value of a SQL trait show in a user profile after a successful sync?\n\nCheck that you\u2019ve configured the identifier that uniquely identifies users in a SQL query (user_id, anonymous_id, email, or group_id for account traits) in Identity Resolution settings as an identifier. This ensures the trait is added to the user\u2019s profile with the correct identifier. If you don\u2019t configure the identifier in Identity Resolution settings, the trait\u2019s value is not added to the user profile.\n\nWhy doesn\u2019t the identifier updated by a SQL trait show the correct value found in the column?\n\nEnsure that the name given to the SQL trait is not the same name as the identifier or column name from the query. To use SQL traits to update an identifier, the identifier will need to be a column in the query of your SQL trait. The column name in the query of the SQL trait should be the one that Identity Resolution uses to generate the identifier.\n\nAre there any errors in the browser\u2019s Network or Console tab?\n\nIf you experience issues saving the SQL Trait query or previewing the results of the SQL Trait query, open the browser\u2019s Console and Network tabs to see if any errors occurred upon clicking the Save/Preview buttons. If you find any errors, please expand the error and take a screenshot of it. You can then share these details when creating a support ticket.\n\nWhy can\u2019t I see error messages in SQL traits while other users can?\n\nTo see error messages in SQL traits, you will need to have PII Access.\n\nIf I edit the SQL Trait query, when will that edit apply those changes?\n\nThe SQL Trait edit will apply to its next scheduled computational run. If the edit was made too closely to its next scheduled run, then its changes will be applied to the subsequent scheduled run, at which point you\u2019ll see those updates reflected on its user\u2019s profiles.\n\nIf I request a resync for my SQL Trait, when will that resync run?\n\nThe SQL Trait resync will apply to its next scheduled computational run. If the resync was made too closely to its next scheduled run, then its changes will be applied to the subsequent scheduled run, at which point you\u2019ll see those updates reflected on its user\u2019s profiles.\n\nThis page was last modified: 08 Aug 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nComparing trait types\nConfigure SQL Traits\nCreate a SQL Trait\nFAQs\nTroubleshooting\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nIdentity Resolution\n/\nIdentity Resolution Use-Cases\nIdentity Resolution Use-Cases\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\n\nIdentity Resolution helps create a unified view of the user across devices, apps, and unique identifiers. Identity resolution is critical to understanding the customer journey at multiple touch points, which allows brands to deliver personalized experiences to its customers at scale.\n\nAnonymous to known identification\n\nIdentity Resolution allows a company to link a customer\u2019s journey from pre-account creation to post-account activity. This is important to help a brand understand the behaviors that lead a user to convert from a window shopper in the discovery stage to a buyer with intent in the consideration and decision stage to the loyal return customer in the conversion and retention stage.\n\nBy linking any anonymous events a user had before creating an account to a user\u2019s logged-in activity, a marketing team can now have a complete understanding of a user\u2019s past interactions with your app.\n\nThis can lead to invaluable insights into the behaviors and triggers in an app that motivate a user to register for an account.\n\nCross-device identification\n\nUsers can have multiple touch points with an app ecosystem through more than one device. For example, users might view an eCommerce site through a mobile native app, a mobile web browser, or a desktop web browser.\n\nBy tracking a user\u2019s activity across all platforms, brands will be able to more efficiently target campaigns to users as they\u2019ll have the knowledge of funnels that complete across devices.\n\nFor example, a user who adds a product to a cart on the iPhone app but completes the checkout on the Android app shouldn\u2019t be targeted with abandoned cart push notifications on the iPhone app.\n\nCross-app identification\n\nA company\u2019s product ecosystem may also spread out across multiple apps.\n\nIf a company needs to understand a user\u2019s activity across all apps, Segment recommends connecting all sources to the same Space. This provides a comprehensive view of a user\u2019s activity across the entire app ecosystem.\n\nIf, however, each app should maintain its own metrics and LTV analysis, regardless of the overlap of users between apps, Segment recommends creating a separate Space per app and only connecting sources related to each app to its space. This will give a siloed view of how users interact with each individual app.\n\nEach workspace has two spaces by default. Contact your CSM to enable additional spaces.\n\nTo learn more, visit Segment\u2019s eCommerce Example doc.\n\nCross-Channel identification\n\nA user can interact with a brand through multiple channels and departments. A user might have touch points with a sales team, a marketing team, and a customer support team throughout their customer journey. It\u2019s important for companies to have insights into these cross-functional activities to ensure they understand the complete customer experience.\n\nFor example, if a user has logged a complaint with a customer support team, the marketing team should exclude this user from an automatic follow-up email asking for them to leave a public product review on their site.\n\nThis page was last modified: 28 Mar 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nAnonymous to known identification\nCross-device identification\nCross-app identification\nCross-Channel identification\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nProfiles Sync\n/\nProfiles Sync Sample Queries\nProfiles Sync Sample Queries\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\n\nOn this page, you\u2019ll find queries that you can run with Profiles Sync to address common use cases.\n\nThe examples in this guide are based on a Snowflake installation. If you\u2019re using another warehouse, you may need to adjust the syntax.\n\nAbout example schemas\n\nThe queries on this page use two example schemas:\n\nps_segment, a schema where Segment lands data\nps_ materialize, a schema with your produced materializations\n\nThese schema names may not match your own.\n\nMonitor and diagnose identity graphs\n\nThese queries let you view and manage identity graphs, which give you insight into unified customer profiles generated by identity resolution.\n\nShow how many profiles Segment creates and merges per hour\n\nThis example queries the id_graph_udpates table to measure the rate at which Segment creates and merges profiles, as well as the type of event that triggered the profile change:\n\nSELECT\n    DATE_TRUNC('hour',timestamp) as hr,\n    CASE\n      WHEN canonical_segment_id=segment_id\n      THEN 'profile creation' ELSE 'profile merge'\n    END as profile_event,\n    triggering_event_type,\n    COUNT(DISTINCT triggering_event_id) as event_count\nFROM ps_segment.id_graph_updates\nGROUP BY 1,2,3\n\nIsolate profiles that have reached an identifier\u2019s maximum configured value\n\nSegment\u2019s configurable identifier limits let you set maximum values for identifiers like email. These maximum configured values help prevent two separate users from being merged into a single Profile.\n\nThe following query lets you view Profiles that have reached a configured limit for the email identifier:\n\nWITH agg AS (\n    SELECT\n        canonical_segment_id,\n        COUNT(LOWER(TRIM(external_id_value))) as value_count,\n        LISTAGG(external_id_value,', ') as external_id_values\n    FROM ps_materialize.external_id_mapping\n    WHERE external_id_type='email'\n    GROUP BY 1\n)\nSELECT\n    canonical_segment_id,\n    external_id_values,\n    value_count\nFROM agg\nWHERE value_count > 5 -- set to your configured limit\n\nReconstruct a profile\u2019s traits\nIdentify the source that generated the value for a particular trait for a canonical profile as well as its child profiles\n\nWhen a merge occurs, Segment selects and associates a single trait value with a profile. This logic depends on how you materialize the profile_traits table.\n\nYou can break out a profile, though, to see the trait versions that existed before the merge. As a result, you can identify a particular trait\u2019s origin.\n\nThe following example inspects a particular profile, use_XX, and trait, trait_1. The query reports the profile\u2019s last observed trait, its source ID, and any profiles Segment has since merged into the profile:\n\nSELECT * FROM (\n  SELECT\n    ids.canonical_segment_id,\n    ident.segment_id,\n    ident.event_source_id,\n    ident.trait_1,\n    row_number() OVER(PARTITION BY ident.segment_id ORDER BY ident.timestamp DESC) as rn\n  FROM ps_segment.identifies as ident\n  INNER JOIN ps_materialize.id_graph as ids\nON ids.segment_id = ident.segment_id\nAND ids.canonical_segment_id = 'use_XXX'\nAND ident.trait_1 IS NOT NULL\n) WHERE rn=1\n\nMeasure and model your customer base\nPull a complete list of your customers, along with their merges, external identifiers, or traits\n\nThe following three snippets will provide a full list of your customers, along with:\n\nThe profile IDs merged into that customer:\nSELECT\n  canonical_segment_id,\n  LISTAGG(segment_id, ', ') as associated_segment_ids\nFROM ps_materialize.id_graph\nGROUP BY 1\n\nThe external IDs associated with that customer:\nSELECT\n  canonical_segment_id,\n  LISTAGG(external_id_value || '(' || external_id_type || ')', ', ') as associated_segment_ids\nFROM ps_materialize.external_id_mapping\nGROUP BY 1\n\nThe customer\u2019s traits:\nSELECT * FROM ps_materialize.profile_traits WHERE merged_to IS NULL\n\nPull the latest subscription status set for every profile identifier in the space\n\nProvides the latest subscription status set for all identifiers in the space. This query will not include identifiers that have no subscription status ever set.\n\nSELECT evt1.user_id,  evt1.channel, evt1._id id, evt1.status, evt1.received_at\nFROM ps_segment.CHANNEL_SUBSCRIPTION_UPDATED evt1\nJOIN (\n  SELECT _id, MAX(received_at) AS max_received_at\n  FROM ps_segment.CHANNEL_SUBSCRIPTION_UPDATED\n  GROUP BY _id\n) evt2\nON evt1._id = evt2._id AND evt1.received_at = evt2.max_received_at\nORDER BY 1\n\nShow all pages visited by a user\n\nTo get complete user histories, join event tables to the identity graph and aggregate or filter with id_graph.canonical_segment_id:\n\nSELECT\n    id_graph.canonical_segment_id,\n    pages.*\nFROM ps_segment.pages\nLEFT JOIN ps_materialize.id_graph\n    ON id_graph.segment_id = pages.segment_id\nWHERE canonical_segment_id = \u2018use_XX..\u2019\n\nShow the complete history of a trait or audience membership associated with a customer\n\nSuppose you want to track a user\u2019s entrances and exits of the audience aud_1. Running the following query would return all qualifying entrance and exits:\n\nSELECT\n    id_graph.canonical_segment_id,\n    identifies.aud_1,\n    identifies.timestamp\nFROM ps_segment.identifies\nINNER JOIN ps_materialize.id_graph\n    ON id_graph.segment_id = identifies.segment_id\n    AND identifies.aud_1 IS NOT NULL\n\n\nThis query works with any Trait or Audience membership, whether computed in Engage or instrumented upstream.\n\nFAQs\nCan I view Engage Audience membership and Computed Trait values in my Warehouse?\n\nYes. Engage sends updates to Audience membership (as a boolean) and computed trait value updates as traits on an Identify call that Segment forwards to your data warehouse.\n\nThe column name corresponds to the Audience or Trait key shown on the settings page:\n\nSurface these values the same way as any other trait value:\n\nThe Trait\u2019s complete history will be in identifies\nThe Trait\u2019s current state for each customer will be in profile_traits\nWhat is the relationship between segment_id and canonical_segment_id? Are they unique?\n\nIdentity merges change Segment\u2019s understanding of who performed historical events.\n\nFor example, if profile_b completed a \u201cProduct Purchased\u201d event but Segment understands that profile_b should be merged into profile_a, Segment deduces that profile_a performed that initial \u201cProduct Purchased\u201d event.\n\nWith that in mind, here\u2019s how to differentiate between segment_id and canonical_segment_id:\n\nsegment_id is a unique identifier representing Segment\u2019s understanding of who performed an action at the time the action happened.\ncanonical_segment_id is a unique identifier representing Segment\u2019s current understanding of who performed that action.\n\nThe mapping between these two identifiers materializes in your id_graph table. If a profile has not been merged away, then segment_id is equivalent to canonical_segment_id. If a profile has been merged away, id_graph reflects that state.\n\nAs a result, you can retrieve a customer\u2019s complete event history by joining an event table, like product_purchased to id_graph.\n\nFor more information, view the Profiles Sync tables guide.\n\nDoes Profiles Sync data ever differ from Unify data?\n\nProfiles Sync mimics the materialization performed by Segment Unify. A user\u2019s merges, external IDs, and traits should be expected whether they\u2019re queried in the warehouse, Profile API, or viewed in the UI.\n\nThe following edge cases might drive slight (<0.01%) variation:\n\nData processed by Unify hasn\u2019t yet landed in Profiles Sync.\nIf you rebuild or use non-incremental materialization for profile_traits, Profiles Sync will fully calculate traits against a user. As a result, Profiles Sync would ensure that all traits reflect the most recently observed value for fully-merged users.\n\nBy contrast, Segment Unify and incrementally-built Profiles Sync materializations won\u2019t combine already-computed traits across two merged profiles at the moment of merge. Instead, one profile\u2019s traits will be chosen across the board.\n\nWhat hash function is used for the external_id_hash field by Profiles Sync?\n\nThe external_id_hash is a hash of the external_id_type and external_id_value using SHA-1. This field corresponds to the primary_key for the table: hash (external_id_type and external_id_value). For example, in BigQuery the logic is: TO_HEX(SHA1(concat(external_id_type, external_id_value))) as seg_hash.\n\nThis page was last modified: 04 Jul 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nAbout example schemas\nMonitor and diagnose identity graphs\nReconstruct a profile\u2019s traits\nMeasure and model your customer base\nFAQs\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSpec\n/\nSpec: Group\nSpec: Group\n\nThe Group API call is how you associate an individual user with a group, such as a company, organization, account, project, or team.\n\nThe Group call enables you to identify what account or organization your users are part of. There are two IDs that are relevant in a Group call: the userId, which belongs and refers to the user, and the groupId, which belongs and refers to the specific group. A user can be in more than one group which would mean different groupIds, but the user will only have one userId that is associated to each of the different groups. Keep in mind that not all platforms support multiple groups for a single user.\n\nSegment University: The Segment Methods\n\nCheck out our high-level overview of these APIs in Segment University. (Must be logged in to access.)\n\nIn addition to the groupId, which is how you\u2019d identify the specific group or company, the group method receives traits that are specific to the group, like industry or number of employees for example, that belong to that specific account. Like the traits of an identify call, you can update these when you call the same trait with a different value.\n\nWhen using the Group call, it\u2019s helpful if you have accounts with multiple users.\n\nSegment doesn't have an ungroup call\n\nIf you\u2019re using a device-mode destination that has a method for ungrouping users, you can invoke it directly on the client side using Segment\u2019s ready() method.\n\nFor cloud-mode destinations, you can create a Destination Function to ungroup users.\n\nHere\u2019s the payload of a typical Group call, with most common fields removed:\n\n{\n  \"type\": \"group\",\n  \"groupId\": \"0e8c78ea9d97a7b8185e8632\",\n  \"traits\": {\n    \"name\": \"Initech\",\n    \"industry\": \"Technology\",\n    \"employees\": 329,\n    \"plan\": \"enterprise\",\n    \"total billed\": 830\n  }\n}\n\n\nAnd here\u2019s the corresponding JavaScript event that would generate the above payload:\n\nanalytics.group(\"0e8c78ea9d97a7b8185e8632\", {\n  name: \"Initech\",\n  industry: \"Technology\",\n  employees: 329,\n  plan: \"enterprise\",\n  \"total billed\": 830\n});\n\n\nBased on the library you use, the syntax in the examples might be different. You can find library-specific documentation on the Sources Overview page.\n\nBeyond the common fields, the Group call takes the following fields:\n\nFIELD\t\tTYPE\tDESCRIPTION\ngroupId\trequired\tString\tA unique identifier for the group in your database. See the Group ID field docs for more detail.\ntraits\toptional\tObject\tFree-form dictionary of traits of the group, like email or name See the Traits field docs for a list of reserved trait names.\nuserId\trequired; optional if anonymousID is set instead\tString\tUnique identifier for the user in your database. A userId or an anonymousId is required. See the Identities docs for more details.\nanonymousId\trequired; optional if userID is set instead\tString\tA pseudo-unique substitute for a User ID, for cases when you don\u2019t have an absolutely unique identifier. A userId or an anonymousId is required. See the Identities docs for more details.\nExample\n\nHere\u2019s a complete example of a Group call:\n\n{\n  \"anonymousId\": \"507f191e810c19729de860ea\",\n  \"channel\": \"browser\",\n  \"context\": {\n    \"ip\": \"8.8.8.8\",\n    \"userAgent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36\"\n  },\n  \"integrations\": {\n    \"All\": true,\n    \"Mixpanel\": false,\n    \"Salesforce\": false\n  },\n  \"messageId\": \"022bb90c-bbac-11e4-8dfc-aa07a5b093db\",\n  \"receivedAt\": \"2015-02-23T22:28:55.387Z\",\n  \"sentAt\": \"2015-02-23T22:28:55.111Z\",\n  \"timestamp\": \"2015-02-23T22:28:55.111Z\",\n  \"traits\": {\n    \"name\": \"Initech\",\n    \"industry\": \"Technology\",\n    \"employees\": 329,\n    \"plan\": \"enterprise\",\n    \"total billed\": 830\n  },\n  \"type\": \"group\",\n  \"userId\": \"97980cfea0067\",\n  \"groupId\": \"0e8c78ea9d97a7b8185e8632\",\n  \"version\": \"1.1\"\n}\n\nCreate your own Group call\n\nUse the following interactive code pen to see what your Group calls would look like with user-provided information:\n\nSample Group call\nName:\nIndustry:\nEmployees:\nPlan:\nTotal Billed:\nSample Group Call\nSample output goes here!\n\nIdentities\n\nThe User ID is a unique identifier for the user performing the actions. Check out the User ID docs for more detail.\n\nThe Anonymous ID can be any pseudo-unique identifier, for cases where you don\u2019t know who the user is, but you still want to tie them to an event. Check out the Anonymous ID docs for more detail.\n\nNote: In our browser and mobile libraries a User ID is automatically added from the state stored by a previous identify call, so you do not need to add it yourself. They will also automatically handle Anonymous IDs under the covers.\n\nGroup ID\n\nA Group ID is the unique identifier which you recognize a group by in your own database. For example, if you\u2019re using MongoDB it might look something like 507f191e810c19729de860ea.\n\nTraits\n\nTraits are pieces of information you know about a group that are passed along with the Group call, like employees or website.\n\nSegment has reserved some traits that have semantic meanings for groups, and handles them in special ways. You should only use reserved traits for their intended meaning.\n\nThe following are the reserved traits Segment has standardized:\n\nTRAIT\tTYPE\tDESCRIPTION\naddress\tObject\tStreet address of a group. This should be a dictionary containing optional city, country, postalCode, state, or street.\navatar\tString\tURL to an avatar image for the group.\ncreatedAt\tDate\tDate the group\u2019s account was first created. Segment recommends ISO-8601 date strings.\ndescription\tString\tDescription of the group, like their personal bio.\nemail\tString\tEmail address of group.\nemployees\tString\tNumber of employees of a group, typically used for companies.\nid\tString\tUnique ID in your database for a group.\nindustry\tString\tIndustry a user works in, or a group is part of.\nname\tString\tName of a group.\nphone\tString\tPhone number of a group.\nwebsite\tString\tWebsite of a group.\nplan\tString\tPlan that a group is in.\n\nNote: You might be used to some destinations recognizing special properties differently. For example, Mixpanel has a special track_charges method for accepting revenue. Luckily, you don\u2019t have to worry about those inconsistencies. Just pass along revenue. Segment handles all of the destination-specific conversions for you automatically. Same goes for the rest of the reserved properties.\n\nIf you pass these values, on null will throw a NullPointerException. You may continue to set values inside the trait. If you do so, this would work the same as the rules do with NoSQL data. If you had set a value previously for a user and on the next request you sent the same value of that property as on null, it will be replaced by null, but if you do not send that property, the original value is persisted.\n\nTraits are case-insensitive, so in JavaScript you can match the rest of your camel-case code by sending createdAt, and in Ruby you can match your snake-case code by sending created_at. That way the API never seems alien to your code base.\n\nThis page was last modified: 23 Jul 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nExample\nIdentities\nGroup ID\nTraits\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nReverse Etl\n/\nReverse Etl Source Setup Guides\n/\nPostgres Reverse ETL Setup\nPostgres Reverse ETL Setup\n\nSet up Postgres as your Reverse ETL source.\n\nAt a high level, when you set up Postgres for Reverse ETL, the configured user/role needs read permissions for any resources (databases, schemas, tables) the query needs to access. Segment keeps track of changes to your query results with a managed schema (__SEGMENT_REVERSE_ETL), which requires the configured user to allow write permissions for that schema.\n\nPostgres Reverse ETL sources support Segment's dbt extension\n\nIf you have an existing dbt account with a Git repository, you can use Segment\u2019s dbt extension to centralize model management and versioning, reduce redundancies, and run CI checks to prevent breaking changes.\n\nSegment supports the following Postgres database providers:\n\nHeroku\nRDS\n\nSegment only supports these Postgres database providers. Postgres databases from other providers aren\u2019t guaranteed to work. For questions or concerns about Segment-supported Postgres providers, contact Segment Support.\n\nSet up guide\n\nTo set up Postgres with Reverse ETL:\n\nLog in to your Postgres account.\nConfigure the correction network and security settings for your Postgres database.\nIf you\u2019re using RDS Postgres, follow this guide.\nMake sure the following IP addresses can access the database.\n\nRun the SQL commands below to create a user named segment.\n\n -- create a user named \"segment\" that Segment will use when connecting to your Postgres cluster.\n CREATE USER segment PASSWORD '<enter password here>';\n\n -- allows the \"segment\" user to create new schemas on the specified database. (this is the name you chose when provisioning your cluster) \n GRANT CREATE ON DATABASE \"<enter database name here>\" TO \"segment\";\n\nMake sure the user has correct access permissions to the database.\nFollow the steps listed in the Add a source section to finish adding Postgres as a source.\nExtra permissions\n\nGive the segment user read permissions for any resources (databases, schemas, tables) the query needs to access.\n\nGive the segment user write permissions for the Segment managed schema (__SEGMENT_REVERSE_ETL), which keeps track of changes to the query results.\n\nAfter you\u2019ve successfully added your Postgres source, add a model and follow the rest of the steps in the Reverse ETL setup guide.\n\nThis page was last modified: 10 Jun 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSet up guide\nExtra permissions\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nIdentity Resolution\n/\nIdentity Resolution eCommerce Example\nIdentity Resolution eCommerce Example\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\n\nIdentity Resolution helps to create a unified view of the user across devices, apps, and unique identifiers.\n\nTake the example of a sneaker company called SegmentKicks which has an eCommerce app called SegKicks as well as a running app called SegRuns. This example follows Jane Doe through her customer journey from an anonymous user to a registered buyer on one app, SegKicks, to her use of the same app on a different device, and finally to her use of a different app belonging to the same company, SegRuns.\n\nAnonymous to known identification\n\nIdentity Resolution can connect a user\u2019s anonymous behaviors to a user\u2019s post-account registration activity.\n\nTake this example using the eCommerce app, SegKicks:\n\nJane Doe downloads the app on her iPhone but doesn\u2019t yet register for an account.\n{\n  \"anonymousId\": \"anon_123\",\n  \"context\": {\n \"app\": \"SegKicks\",\n \"device\": {\n   \"id\": \"ios_abc123\",\n   \"type\": \"ios\"\n },\n  },\n  \"event\": \"App Opened\",\n  \"type\": \"track\"\n}\n\nShe then clicks on a few different types of shoes, ShoeA, ShoeB, and ShoeC but doesn\u2019t add them to a cart. Because she hasn\u2019t yet registered for an account, all of these events will be sent through with an anonymousID and an ios deviceID.\n{\n  \"anonymousId\": \"anon_123\",\n  \"context\": {\n \"app\": \"SegKicks\",\n \"device\": {\n   \"id\": \"ios_abc123\",\n   \"type\": \"ios\"\n },\n  },\n  \"event\": \"ShoeA Clicked\",\n  \"type\": \"track\"\n}\n\nShe then decides to add ShoeD to her cart. Upon checkout, she creates a new user profile with her email and purchases the shoe. At the point of account creation she is assigned a userID and the events of her purchase are sent through with an email.\n{\n  \"anonymousId\": \"anon_123\",\n  \"context\": {\n    \"app\": \"SegKicks\",\n    \"device\": {\n      \"id\": \"ios_abc123\",\n      \"type\": \"ios\"\n    },\n  },\n  \"userId\": \"abc123def\",\n  \"type\": \"identify\"\n}\n\n\nBy linking the original anonymous events to Jane\u2019s logged-in activity, the app\u2019s marketing team can now begin to map out her customer journey on a single app, understand her preferences, and re-target her with highly personalized emails about the shoes she didn\u2019t complete purchasing.\n\nHer identifiers will now contain the original anonymous_id, her email, and her user_id:\n\nCross-device identification\n\nUsers can have multiple touch points with an app ecosystem through more than one device. For example, users might interact with an eCommerce app through both a native app, a mobile browser, and a web browser.\n\nContinuing with the example of Jane Doe, she now views the same mobile app SegKicks on her Android phone.\n\nJane logs into the Android phone with the same email janedoe@example.com.\n\n{\n  \"anonymousId\": \"anon_456\",\n  \"context\": {\n    \"app\": \"SegKicks\",\n    \"device\": {\n      \"id\": \"and_1a2b3c4d\",\n      \"type\": \"android\"\n    },\n  },\n  \"type\": \"identify\",\n  \"userId\": \"abc123def\"\n}\n\n\nHer new User Profile identities will now contains an android.id:\n\nCross-app identification\n\nA company\u2019s product ecosystem may also spread out across multiple apps. For example, SegmentKicks also has a running app SegRuns.\n\nWhen Jane downloads the Android app SegRuns and views a workout:\n\n{\n  \"anonymousId\": \"anon_789\",\n  \"context\": {\n    \"app\": \"SegRuns\",\n    \"device\": {\n      \"id\": \"and_1a2b3c4d\",\n      \"type\": \"android\"\n    },\n  },\n  \"type\": \"identify\",\n  \"userId\": \"abc123def\"\n}\n\n\nHer final identifiers now have a new anonymous_id from the SegRuns app:\n\nConclusion\n\nBy combining the events throughout Jane\u2019s entire customer journey from anonymous to known user, cross-device, and cross-app identification, SegKicks and SegRuns can now work together to understand how to give Jane the best customer experience possible while increasing her LTV across the entire SegmentKicks ecosystem.\n\nFor example, if Jane looked at ShoeC on her iPhone and completed checkout for ShoeC on her Android, SegKicks will now know to exclude her from a cart abandonment email for ShoeC. This wouldn\u2019t be possible if SegKicks had only looked at her activity on the iPhone.\n\nAdditionally, most shoes need to be replaced every 300 to 400 miles. By understanding her activity on SegRuns, SegKicks will now be able to more effectively remind Jane to repurchase ShoeC or ShoeD once she\u2019s reached that mileage.\n\nThis page was last modified: 28 Mar 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nAnonymous to known identification\nCross-device identification\nCross-app identification\nConclusion\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nData Graph\n/\nSetup Guides\n/\nBigQuery Data Graph Setup\nBigQuery Data Graph Setup\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\n\nBigQuery for Data Graph is in beta and Segment is actively working on this feature. Some functionality may change before it becomes generally available. This feature is governed by Segment\u2019s First Access and Beta Preview Terms.\n\nSet up your BigQuery data warehouse to Segment for the Data Graph.\n\nStep 1: Roles and permissions\n\nYou need to be an account admin to set up the Segment BigQuery connector as well as write permissions for the __segment_reverse_etl dataset.\n\nTo set the roles and permissions:\n\nNavigate to IAM & Admin > Service Accounts in BigQuery.\nClick + Create Service Account to create a new service account.\nEnter your Service account name and a description of what the account will do.\nClick Create and Continue.\nClick + Add another role and add the BigQuery User role.\nClick Continue, then click Done.\nSearch for the service account you just created.\nFrom your service account, click the three dots under Actions and select Manage keys.\nNavigate to Add Key > Create new key.\nIn the pop-up window, select JSON for the key type, and click Create. The file will download.\nCopy all the content in the JSON file you created in the previous step, and save it for Step 5.\nStep 2: Create a dataset for Segment to store checkpoint tables\n\nCreate a new dataset as Segment requires write access to the dataset for internal bookkeeping and to store checkpoint tables for the queries that are executed.\n\nSegment recommends you to create a new dataset for the Data Graph. If you choose to use an existing dataset that has also been used for Segment Reverse ETL, you must follow the additional instructions to update user access for the Segment Reverse ETL catalog.\n\nTo create your dataset, navigate to the BigQuery SQL editor and create a dataset that will be used by Segment.\n\nCREATE SCHEMA IF NOT EXISTS `__segment_reverse_etl`;\nGRANT `roles/bigquery.dataEditor` ON SCHEMA `__segment_reverse_etl` TO \"serviceAccount:<YOUR SERVICE ACCOUNT EMAIL>\";\n\nStep 3: Grant read-only access for the Data Graph\n\nGrant the BigQuery Data Viewer role to the service account at the project level. Make sure to grant read-only access to the Profiles Sync project in case you have a separate project.\n\nTo grant read-only access for the Data Graph:\n\nNavigate to IAM & Admin > IAM in BigQuery.\nSearch for the service account you just created.\nFrom your service account, click the Edit principals pencil.\nClick ADD ANOTHER ROLE.\nSelect the BigQuery Data Viewer role.\nClick Save.\n(Optional) Step 4: Restrict read-only access\n\nIf you want to restrict access to specific datasets, grant the BigQuery Data Viewer role on datasets to the service account. Make sure to grant read-only access to the Profiles Sync dataset.\n\nTo restrict read-only access:\n\nIn the Explorer pane in BigQuery, expand your project and select a dataset.\nNavigate to Sharing > Permissions.\nClick Add Principal.\nEnter your service account in the New principals section.\nSelect the BigQuery Data Viewer role in the Select a role section.\nClick Save.\n\nYou can also run the following command:\n\nGRANT `roles/bigquery.dataViewer` ON SCHEMA `YOUR_DATASET_NAME` TO \"serviceAccount:<YOUR SERVICE ACCOUNT EMAIL>\";\n\nStep 5: Validate permissions\nNavigate to IAM & Admin > Service Accounts in BigQuery.\nSearch for the service account you\u2019ve just created.\nFrom your service account, click the three dots under Actions and select Manage permissions.\nClick View Access and click Continue.\nSelect a box with List resources within resource(s) matching your query.\nClick Analyze, then click Run query.\nStep 6: Connect your warehouse to Segment\nNavigate to Unify > Data Graph in Segment. This should be a Unify space with Profiles Sync already set up.\nClick Connect warehouse.\nSelect BigQuery as your warehouse type.\nEnter your warehouse credentials. Segment requires the following settings to connect to your BigQuery warehouse:\nService Account Credentials: JSON credentials for a GCP Service Account that has BigQuery read/write access. This is the credential created in Step 1.\nData Location: This specifies the primary data location. This can be either region or multi-region.\nTest your connection, then click Save.\nUpdate user access for Segment Reverse ETL dataset\n\nIf you ran Segment Reverse ETL in the project you are configuring as the Segment connection project, a Segment-managed dataset is already created and you need to provide the new Segment user access to the existing dataset.\n\nIf you run into an error on the Segment app indicating that the user doesn\u2019t have sufficient privileges on an existing __segment_reverse_etl dataset, grant the BigQuery Data Editor role on the __segment_reverse_etl dataset to the service account . Note that the __segment_reverse_etl dataset is hidden in the console. Run the following SQL command:\n\nGRANT `roles/bigquery.dataEditor` ON SCHEMA `__segment_reverse_etl` TO \"serviceAccount:<YOUR SERVICE ACCOUNT EMAIL>\";\n\n\nThis page was last modified: 05 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nStep 1: Roles and permissions\nStep 2: Create a dataset for Segment to store checkpoint tables\nStep 3: Grant read-only access for the Data Graph\n(Optional) Step 4: Restrict read-only access\nStep 5: Validate permissions\nStep 6: Connect your warehouse to Segment\nUpdate user access for Segment Reverse ETL dataset\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nAudiences\n/\nProduct Based Audiences Nutrition Facts Label\nProduct Based Audiences Nutrition Facts Label\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nTwilio\u2019s AI Nutrition Facts provide an overview of the AI feature you\u2019re using, so you can better understand how the AI is working with your data. Twilio outlines AI qualities in Product Based Audiences in the Nutrition Facts label below. For more information, including the AI Nutrition Facts label glossary, refer to the AI Nutrition Facts page.\n\nAI Nutrition Facts\n\nCustomer AI Product Based Audiences\n\n\n\n\nDescription\n\nCustomerAI Product Based Audiences lets customers improve marketing campaigns by segmenting users based on preferences like product, category, or brand to automate the creation and maintenance of personalized recommendations for businesses in the retail, media, and entertainment industries.\n\n\n\n\nPrivacy Ladder Level\n2\n\n\n\n\nFeature is Optional\nYes\n\n\n\n\nModel Type\nPredictive\n\n\n\n\nBase Model\nAWS Personalize - Hierarchical recurrent neural network\n\n\n\n\nTrust Ingredients\n\n\n\n\nBase Model Trained with Customer Data\nN/A\n\n\n\n\nCustomer Data is Shared with Model Vendor\nNo\n\n\n\n\nTraining Data Anonymized \u00a0\nNo\n\n\n\n\nData Deletion\nYes\n\n\n\n\nHuman in the Loop\nN/A\n\n\n\n\nData Retention\n30 days\n\n\nCompliance \u00a0 \u00a0\nLogging & Auditing\nYes\n\nGuardrails\nN/A\n\n\n\nInput/Output Consistency\nN/A\n\n\n\n\nOther Resources\n\nThis page was last modified: 19 Sep 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nTrait Activation Overview\nTrait Activation Overview\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nUse Trait Activation to configure sync payloads that you send from Engage Audiences and Journeys to a Destination or Destination Function. Map traits from user profiles to Destinations, configure identifiers to sync, and choose a sync strategy that fits your use cases.\n\nTrait Activation includes both Trait Enrichment and ID Sync. With Trait Enrichment, use custom, SQL, computed, and predictive traits to enrich the data you map to your destinations or destination functions. Use ID Sync to select identifiers and a sync strategy for each identifier when syncing Engage Audiences to Destinations.\n\nTrait Activation setup\n\nTo get started with Trait Activation, you\u2019ll need to set up the destination that you\u2019ll use with Trait Enrichment and ID Sync.\n\nSet up your destination\n\nSelect your destination, view its Segment documentation, then follow the corresponding required setup steps.\n\nDESTINATION\tTYPE\tCOMPATIBLE WITH TRAIT ENRICHMENT\tCOMPATIBLE WITH ID SYNC\nFacebook Custom Audiences\tList\t\t\nGoogle Ads Remarketing Lists\tList\t\t\nDestination Actions\tActions\t\t\nDestination Functions\tFunction\t\t\nClassic Destinations\tClassic\t\t\nResyncs\n\nSegment recommends creating a new audience for Trait Enrichment and ID Sync. For existing audience destinations, both Trait Enrichment and ID Sync won\u2019t resync the entire audience. Only new data flowing into Segment will adhere to new trait settings.\n\nContact Segment support if you\u2019d like your Audience resynced with Trait Enrichment and ID Sync.\n\nFor Audiences larger than 50 million users, it may take several hours, or even days, to sync. Only one resync is allowed at a time for each workspace.\n\nUse cases\n\nTrait Enrichment and ID Sync can help you:\n\nIncrease advertising match rates: Expand the pool of users you advertise to and increase match rates by using traits and identifiers to find the right customers.\n\nInclude more personalized message content: Include traits in your payload for more up-to-date, accurate data.\n\nConfigure how you send identifiers to Destinations: Send the right identifiers to your destinations. For profiles with multiple identifiers, choose a strategy to select identifiers and send them downstream.\n\nNext steps\n\nTo learn more about Trait Activation, visit the following docs:\n\nLearn more about how to access Segment profile traits when you sync Audiences and Journeys to Destinations or Destination Functions with Trait Enrichment.\nLearn how to sync select identifiers and create a sync strategy with ID Sync.\n\nThis page was last modified: 10 Sep 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nTrait Activation setup\nUse cases\nNext steps\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nSegment App\n/\nIam\n/\nRoles\nRoles\n\nA role gives a user access to resources within a workspace. Roles are additive, and can combine to configure a custom policy for a Team Member or a Group. A policy is at least one role plus one resource applied to an individual user or group.\n\nWhen a user has both User Permissions and Group Permissions, they will have the highest access given to either of those roles.\n\nGlobal Roles\n\nAll Segment workspaces have the following roles, regardless of account type.\n\nROLE\tDETAILS\nWorkspace Owner\tOwners have full read and edit access to everything in the workspace, including sources, destinations, add-on products, and settings. Owners have full edit access to all team permissions.\nWorkspace Member\tMembers inherit custom permissions based on individual roles assigned.\nSource Admin\tSource admins have edit access to:\n- assigned source(s)\n- the settings for that source\n- any connected streaming destinations\n- Schema\n- live data from the source in the debugger\n- the source\u2019s write key\n\nA user with the Source Admin role can get access to either all current and future sources, or a specific list of sources, or (if you\u2019re on a Business plan) to sources with a specific Label. Source Admins can create new sources when the \u201cAll sources in Workspace including future sources\u201d option is selected.\nFunction Admin\tFunction admins can create, edit and delete access to assigned function(s). When you assign a user the Functions Admin role, you can grant them access to either all current and future functions, or to a specific list of functions.\nFunction Read-only\tThe Function read-only role grants users the ability to read an assigned function(s). When you assign a user the Functions Read-only role, you can grant them access to either all current and future functions, or to a specific list of functions.\nBusiness Tier Roles\n\nThe following roles are only available to Segment Business Tier accounts.\n\nEnd User Privacy Admin\nEdit access to End User Privacy Settings. Includes access to Data Privacy Agreement, and user suppression and deletion workflows.\nScope: Grants access to only End User Privacy Settings in the App.\nIdentity Admin\nEdit access to Identity settings in Unify.\nScope: Grants access to all Identity settings.\nSource Read-only\nRead access to assigned source(s), source settings, connected streaming destinations, schema, transformations, and live data in the debugger. Reverse ETL sources are also included.\nScope: Grants access to either: all current and future Sources, or only specific Sources, or Sources with a specific Label (BT only).\nSource Admin\nEdit access to assigned source(s), source settings, connected streaming destinations, schema, transformations, the source\u2019s write key and live data in the debugger. Reverse ETL sources are also included.\nScope: Grants access to either: all current and future Sources, or only specific Sources, or Sources with a specific Label (BT only).\nUnify and Engage Admin\nEdit access to Unify settings and if purchased, Engage Audiences, Traits, Journeys, Content, and settings.\nScope: Grants access to either: all current and future Spaces, or a specific list of Spaces, or Spaces with a specific Label (BT only).\nUnify and Engage Read-only\nRead-only access to Unify settings and if purchased, Engage audiences, traits, journeys, and content. Cannot download PII or edit settings in Unify or Engage.\nScope: Grants access to either: all current and future Spaces, or a specific list of Spaces, or Spaces with a specific Label (BT only).\nUnify Read-only, Engage User\nRead-only access to Unify settings and if purchased, edit access to Engage audiences, traits, journeys, and content. Cannot download PII or edit settings in Unify or Engage.\nScope: Grants access to either: all current and future Spaces, or a specific list of Spaces, or Spaces with a specific Label (BT only).\nTracking Plan Admin\nEdit access to all Tracking Plans in Protocols.\nScope: Grants access to all Tracking Plans.\nTracking Plan Read-only\nRead access to all Tracking Plans in Protocols.\nScope: Grants access to all Tracking Plans.\nWarehouse Destination Admin\nEdit access to warehouse destinations and warehouse destination settings. (For example, Redshift, Postgres, BigQuery)\nScope: Grants access to all warehouses.\nWarehouse Destination Read-only\nRead-only access warehouse destination and warehouse destination settings. (For example, Redshift, Postgres, BigQuery)\nScope: Grants access to all warehouses.\nEntities Admin\n\nFull edit and view access to all entity models and connection details.\n\nEntities Read-only\n\nRead-only access, with the ability to view entity models.\n\nPII Access\n\nThe Segment App doesn\u2019t show detected Personally Identifiable Information (PII) to workspace members if the information matches specific expected formats for PII. When PII Access turns off, detected PII is masked based on red or yellow default matchers and any custom matchers defined in the Privacy Portal.\n\nWorkspace Owners can grant specific individuals or groups access to PII from their Access Management settings. PII Access only applies to the resources a user or user group has access to; it doesn\u2019t expand a user\u2019s access beyond the original scope. All Workspace Owners have PII access by default.\n\nFor example, users with PII Access and Source Admin/Read-Only permissions can view any PII present in the Source Debugger. However, users with the PII Access role don\u2019t have Privacy Portal access.\n\nOnly users with the Workspace Owner role can access the Privacy Portal.\n\nRoles for managing Engage destinations\n\nWhen managing destination connections in an Engage space, you may require additional permissions.\n\nConnecting or disconnecting destinations to Engage spaces: To allow a user to connect or disconnect destination instances to your Engage space, grant Unify and Engage Admin access for the specific Engage space, and Source Admin access for the source(s) linked to that Engage space, named Engage (space name).\nManaging connections to Engage features (Computed Traits/Audiences/Journeys): To allow a user to attach or detach a destination in your Engage space to specific Engage features like Audiences or Journeys, grant these users Unify and Engage Admin access on the selected Engage space. The Source Admin role is not necessary for this action.\nRoles for connecting resources\n\nTo connect two resource instances, you must have access to both. You can either grant this access to all resources, or to the specific resources you want to connect.\n\nTo connect a source to warehouse you must have Source Admin and Warehouse Admin access for the source and the warehouse.\n\nTo connect source to tracking plan requires Source Admin and Tracking Plan Admin access for the source and the tracking plan.\n\nRoles for Protocols Transformations\n\nTo view transformations, you need Source Read-only, either for all Sources or the specific Sources using Protocols.\n\nTo create or edit transformations you must have either Source Admin for all Sources, or for the specific Sources used with Protocols.\n\nRoles for Privacy Portal\n\nThe Privacy Portal is only accessible by Workspace owners. To view, create or edit any section of the Privacy Portal, you need to have the Workspace Owner role.\n\nThis page was last modified: 13 Nov 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nGlobal Roles\nBusiness Tier Roles\nPII Access\nRoles for managing Engage destinations\nRoles for connecting resources\nRoles for Protocols Transformations\nRoles for Privacy Portal\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nThe page you were looking for doesn't exist.\n\nYou may have mistyped the address or the page may have moved. Double-check the URL and try again or search the term.\n\n404",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nStorage\n/\nData Lakes\n/\nComparing Data Lakes and Warehouses\nComparing Data Lakes and Warehouses\nFREE X\nTEAM X\nBUSINESS \u2713\nADD-ON X\n?\n\nAs Segment builds new data storage products, each product evolves from prior products to best support the needs of customers. Segment Data Lakes is an evolution of the Warehouses product that meets the changing needs of customers.\n\nData Lakes and Warehouses are not identical, but are compatible with a configurable mapping. This mapping helps you to identify and manage the differences between the two storage solutions, so you can easily understand how the data in each is related.\n\nData freshness\n\nData Lakes and Warehouses offer different sync frequencies:\n\nWarehouses can sync up to once an hour, with the ability to set a custom sync schedule and selectively sync collections and properties within a source to Warehouses.\nData Lakes offers 12 syncs in a 24 hour period, and doesn\u2019t offer custom sync schedules or selective sync.\nDuplicates\n\nSegment\u2019s 99% guarantee of no duplicates for data within a 24 hour look-back window applies to data in Segment Data Lakes and Warehouses.\n\nWarehouses and Data Lakes also have a secondary deduplication system to further reduce the volume of duplicates to ensure clean data in your Warehouses and Data Lakes.\n\nObject vs event data\n\nWarehouses support both event and object data, while Data Lakes supports only event data.\n\nSee the table below for information about the source types supported by Warehouses and Data Lakes.\n\n\tWarehouses\tData Lakes\nWebsite Libraries\t\u2705\t\u2705\nMobile\t\u2705\t\u2705\nServer\t\u2705\t\u2705\nObject Cloud Sources\t\u2705\t\u2b1c\ufe0f\nEvent Cloud Sources\t\u2705\t\u2705\nHTTP\t\u2705\t\u2705\nPixel\t\u2705\t\u2705\nSchema\nData types\n\nWarehouses and Data Lakes both infer data types for the events each receives. Since events are received by Warehouses one by one, Warehouses look at the first event received every hour to infer the data type for subsequent events. Data Lakes uses a similar approach, however because it receives data every hour, Data Lakes is able to look at a group of events to infer the data type.\n\nThis approach leads to a few scenarios where the data type for an event may be different between Warehouses and Data Lakes. Those scenarios are:\n\nSchema evolution - Events reach Warehouses and Data Lakes at different times, due to their differing sync schedules. As a result, there is no way to guarantee that data types do not change since the field may have varying data types.\nDifferent data type inferred based on sample size - Warehouses and Data lakes use a different number of events to infer the schema. Warehouses receive one event at a time, and use the first received event to infer the data type. Data Lakes receive events in batches, and use a larger number of events to more accurately infer the data type.\n\nVariance in data types between Warehouses and Data Lakes don\u2019t happen often for booleans, strings, and timestamps, however it can occur for decimals and integers.\n\nIf a bad data type is seen, such as text in place of a number or an incorrectly formatted date, Warehouses and Data Lakes attempt a best effort conversion to cast the fields to the target data type. Fields that cannot be casted may be dropped. Contact Segment Support if you want to correct data types in the schema and perform a replay to ensure no data is lost.\n\nTables\n\nTables between Warehouses and Data Lakes will be the same, except for in these two cases:\n\ntracks - Warehouses provide one table per specific event (track_button_clicked) in addition to a summary table listing all track method calls. Data Lakes also creates one table per specific event, but does not provide a summary table. Learn more about the tracks table in the Warehouses schema docs.\nusers - Both Warehouses and Data Lakes create an identifies table (as seen in the Warehouses schema docs), however Warehouses also create a users table just for user data. Data Lakes does not create this, since it does not support object data. The users table is a materialized view of users in a source, constructed by data inferred about users from the identify calls.\naccounts - Group calls generate the accounts table in Warehouses. However because Data Lakes does not support object data (Groups are objects not events), there is no accounts table in Data Lakes.\n(Redshift only) Table names which begin with numbers - Table names are not allowed to begin with numbers in the Redshift Warehouse, so they are automatically given an underscore ( _ ) prefix. Glue Data Catalog does not have this restriction, so Data Lakes don\u2019t assign this prefix. For example, in Redshift a table name may be named _101_account_update, however in Data Lakes it would be named 101_account_update. While this nuance is specific to Redshift, other warehouses may show similar behavior for other reserved words.\nColumns\n\nSimilar to tables, columns between Warehouses and Data Lakes will be the same, except for in a few specific scenarios:\n\nevent and event_text - Each property within an event has its own column, however the naming convention for these columns differs between Warehouses and Data Lakes. Warehouses snake case the original payload value and preserves the original text within the event_text column. Data Lakes use the original payload value as-is for the column name, and does not need an event_text column.\nchannel, metadata_*, project_id, type, version - These columns are Segment internal data which are not found in Warehouses, but are found in Data Lakes. Warehouses is intentionally very detailed about it\u2019s transformation logic and does not include these. Data Lakes does include them due to its more straightforward approach to flatten the whole event.\n(Redshift only) uuid, uuid_ts - Redshift customers will see columns for uuid and uuid_ts, which are used for de-duplication in Redshift; Other warehouses may have similar columns. These aren\u2019t relevant for Data Lakes so the columns won\u2019t appear there.\nsent_at - Warehouses computes the sent_at value based on timestamps found in the original event in order to account for clock skews and timestamps in the future. This was done when the Segment pipeline didn\u2019t do this on it\u2019s own, however it now calculates for this so Data Lakes does not need to do any additional computation, and will send the value as-is when computed at ingestion.\nintegrations - Warehouses does not include the integrations object. Data Lakes flattens and includes the integrations object. You can read more about the integrations object in the filtering data documentation.\n\nThis page was last modified: 03 Aug 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nData freshness\nDuplicates\nObject vs event data\nSchema\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nApi\n/\nConfig API Overview\nConfig API Overview\n\nThe Segment Public API is available\n\nSegment\u2019s Public API is available for Team and Business tier customers to use. You can use the Public API and Config APIs in parallel, but moving forward any API updates will come to the Public API exclusively.\n\nPlease contact your account team or friends@segment.com with any questions.\n\nThe Config API enables you to programmatically manage Segment workspaces, sources, destinations and more. With the API you can:\n\nList all your workspace Sources and Destinations to see how data flows through Segment\nCreate new Destinations - or delete them - with a few lines of code\nCreate new users and assign them to scoped roles\nConfigure, disable, or view Sources and manage connected Destinations\nGet a complete view of all the Sources and Destinations available in Segment\u2019s catalog\nConfigure a Tracking Plan to see how data conforms to your expected schema\nQuery Event Delivery metrics to build custom dashboards and alerts to monitor delivery of your events to destinations\nFilter entire events or individual fields from reaching specific destinations\n\nThe Config API is a set of REST services under segmentapis.com:\n\nSERVICE\tDESCRIPTION\nAccess Tokens\tManage access tokens\nSource Catalog\tGet info about all event and cloud sources\nDestination Catalog\tGet info about all destinations\nWorkspaces\tGet info about workspaces\nSources\tManage workspace sources\nDestinations\tManage workspace destinations\nTracking Plans\tManage workspace tracking plans\nEvent Delivery Metrics\tGet event delivery metrics for cloud-mode destinations\nDestination Filters\tManage destination filters\nIAM\tManage workspace users and roles\nFunctions\tManage Functions\n\nTo see all the API methods and models see the Segment Config API Reference.\n\nAt this time there are no language-specific clients. However the API Reference also contains example code snippets for cURL, Go, Node, Python and more.\n\nQuick Start\n\nYou can interact with the API from the command line. First install the curl tool.\n\n$ brew install curl\n\nAccess Tokens\n\nYou can use the Config API with an access token to programmatically access Segment resources that the token can access. Access tokens are created by workspace owners using the Access Management page, and can only access resources that the token has permission to.\n\nThese are currently only suitable for first party, trusted applications, such as your personal local scripts and server side programs. Partners should not prompt Segment users for their username and password and save an access token as a way to delegate access. See the Authentication doc for more information.\n\nWhen you create an access token, you\u2019ll give it a description, a workspace, and determine whether it has workspace owner or member access.\n\nSecret Token\n\nYou can not retrieve the plain-text token later, so you should save it in a secret manager. If you lose the token you can generate a new one.\n\ninfo As of February 1, 2024, new Config API tokens cannot be created in the app as Segment moves toward exclusive support for the Public API. Migrate your implementation to the Public API to access the latest features and available endpoints. To create a new Config API token, reach out to friends@segment.com for support.\n\nAPI Requests\n\nNow that you have an access token, you can use this token to access the rest of the Config API by setting it in the Authorization header of your requests, for example:\n\n$ ACCESS_TOKEN=qiTgISif4zprgBb_5j4hXfp3qhDbxrntWwwOaHgAMr8.gg9ok4Bk7sWlP67rFyXeH3ABBsXyWqNuoXbXZPv1y2g\n\n$ curl \\\n  -X GET \\\n  -H \"Authorization: Bearer $ACCESS_TOKEN\" \\\n  https://platform.segmentapis.com/v1beta/workspaces\n\n\nExample response:\n\n{\n  \"workspaces\": [\n    {\n      \"name\": \"workspaces/myworkspace\",\n      \"display_name\": \"My Space\",\n      \"id\": \"e5bdb0902b\",\n      \"create_time\": \"2018-08-08T13:24:02.651Z\"\n    }\n  ],\n  \"next_page_token\": \"\"\n}\n\nReference\n\nFor an overview of the API\u2019s common design patterns and important information about versioning and compatibility, see the API Design document.\n\nTo see all the API methods and models see the Segment Config API Reference.\n\nThis page was last modified: 01 Feb 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nQuick Start\nReference\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nCampaigns\n/\nSMS Campaigns\nSMS Campaigns\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE PREMIER \u2713\n?\n\nEngage Premier entered an End of Sale (EOS) period effective June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:\n\nTwilio Marketing Campaigns\n\nPreferred ISV Partners:\n\nAirship Blog\nBloomreach Blog\nBraze Blog\nInsider Blog\nKlaviyo Blog\nTwilio Engage Foundations Documentation\n\n\nWith Twilio Engage, you can send email and SMS campaigns to users who have opted in to receive your marketing materials. On this page, you\u2019ll learn how to create and send an SMS campaign.\n\nSome knowledge of the Journeys product will benefit you as you read through this guide. If you\u2019re new to Journeys, the Journeys documentation will bring you up to speed.\n\nHow Engage campaigns work\n\nTwilio Engage uses Journeys to send email and SMS campaigns. With Journeys, you add conditions and steps that trigger actions like sending an email or an SMS.\n\nYou\u2019ll build and then send your campaign in three stages:\n\nCreate a Journey.\nAdd a Journey condition.\nCreate, test, and send your SMS campaign.\nCreate a Journey\n\nBecause Engage campaigns exist within Journeys, begin by creating a Journey:\n\nIn Engage, select Journeys, then click New Journey.\nName your Journey and select its entry settings.\nClick Build Journey to create the Journey.\nAdd a Journey condition\n\nWith your Journey created, you\u2019ll now create a condition that will trigger your SMS campaign:\n\nWithin the Journey builder, click + Add Entry Condition.\nIn the Add Entry Condition pane, give the step a name.\nClick + Add Condition, select your desired condition, then click Save.\n\nWith your entry condition added, you\u2019re now ready to create your SMS campaign.\n\nCreate, test, and publish your SMS campaign\n\nFollow these steps to create an SMS campaign:\n\nWithin the Journey builder, click the + node below your new condition.\nFrom the Select a Step window, click Send an SMS.\nIn the Send SMS window, select Build a new SMS or Use a template to choose an existing SMS template.\nEnter your campaign content into the Body field.\nAdd a STOP/unsubscribe line to the end of your SMS.\n\nUnsubscribe options are required by law. Your SMS campaign must contain \u201cReply STOP to unsubscribe.\u201d\n\nTest your SMS campaign\n\nAt this point, you can send a test SMS before publishing your campaign. Testing the SMS confirms that your content and merge tags appear as expected.\n\nAs part of your test send, you can enter custom values to populate the profile traits in your SMS message.\n\nFollow these steps to test your campaign:\n\nIn the Send an SMS pane, click Test SMS.\nIf your template has profile traits, enter a trait value for the test SMS. This ensures that your merge tags work as expected.\nTo test a default value, leave the profile traits field blank. Default values must be assigned in your merge tags. For example, loyal customer would be the default for the following merge tag: {{profile.traits.first_name | default: \"loyal customer\"}}.\nIn the Recipients field, enter the phone number(s) that will receive your test SMS.\nClick Send test SMS.\nPublish your SMS campaign\n\nWith your SMS created and tested, you\u2019re now ready to save the campaign and publish your Journey with the following steps:\n\nVerify that all Send SMS fields are correct.\nClick Save.\nIn the Journey builder, click Publish.\n\nYour SMS campaign is now live. Users who trigger the SMS step\u2019s parent Journey condition will receive your SMS campaign.\n\nSMS campaign fields\n\nThe following table contains descriptions of the three fields in the Journeys Send SMS builder. All SMS fields are required.\n\nFIELD\tDESCRIPTION\nStep name\tThe name for the SMS campaign\u2019s parent Journey step. SMS recipients won\u2019t see this field.\nSender number\tThe phone number from which you\u2019ll send the SMS campaign. Format the number with a + followed by the country code. Do not use hyphens, spaces, or periods.\nBody\tThe SMS message content. Engage SMS campaigns are limited to 1600 characters.\nNext steps\n\nUsing Journeys, you can create multi-channel customer engagement with both email and SMS campaigns. Having published an SMS, learn how Engage email campaigns can help you market to customers through email.\n\nThis page was last modified: 15 Jul 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nHow Engage campaigns work\nSMS campaign fields\nNext steps\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections Overview\nConnections Overview\n\nConnections is Segment\u2019s core product offering: you can collect event data from your mobile apps, websites, and servers with one API, then pull in contextual data from cloud apps like your CRM, payment systems, and internal databases to build a unified picture of your customers.\n\nSources\n\nIn Segment, you create a source (or more than one!) for each website or app you want to track. We highly recommend creating a Source for each unique source of data (each site, app, or server), though this isn\u2019t required.\n\nSources belong to a workspace, and the URL for a source looks something like this: https://segment.com/<my-workspace>/sources/<my-source-name>/\n\nYou can create new sources using the button in the workspace view. Each source you create has a write key, which is used to send data to that source. For example, to load\u00a0analytics.js, the Segment JavaScript library\u00a0on your page, the snippet on the\u00a0Quickstart Guide\u00a0includes:\n\nanalytics.load(\"YOUR_WRITE_KEY\");\n\n\nLearn more about sources from the sources overview page.\n\nDestinations\n\nDestinations are business tools or apps that you can connect to the data flowing through Segment. Some of Segment\u2019s most popular destinations are Google Analytics, Mixpanel, Kissmetrics, Customer.io, Intercom, and KeenIO.\n\nAll of these tools run on the same data: who are your customers and what are they doing? But each tool requires that you send that data in a slightly different format, which means that you\u2019d have to write code to track all of this information, again and again, for each tool, on each page of your app or website.\n\nEnter Segment. Do it once.\n\nSegment eliminates this process by introducing an abstraction layer. You send your data to Segment, and Segment understands how to translate it so we can send it along to any destination. You enable destinations from the catalog in the Segment App, and user data immediately starts flowing into those tools. No extra code required!\n\nSegment supports many categories of destinations, from advertising to marketing, email to customer support, CRM to user testing, and even data warehouses. You can view a complete list of available\u00a0destinations\u00a0or check out the\u00a0destination page\u00a0for a searchable list broken down by category.\n\nWarehouses\n\nA warehouse is a central repository of data collected from one or more sources. This is what commonly comes to mind when you think about a relational database: structured data that fits neatly into rows and columns.\n\nIn Segment, a Warehouse is a special type of destination. Instead of streaming data to the destination all the time, we load data to them in bulk at regular intervals. When we load data, we insert and update events and objects, and automatically adjust their schema to fit the data you\u2019ve sent to Segment.\n\nReverse ETL\n\nWith Reverse ETL, your data warehouse acts as your source, enabling you to send data from your warehouse to your destinations.\n\nInformation on sources and destinations pages\n\nThe Sources and Destinations pages allow each user to decide what information appears in their personal view for each page.\n\nOn both pages, you can click the stack icon in the upper right-hand corner of the table to see and select Source properties to show. You can select up to five columns of properties.\n\nThe following information is available for Sources:\n\nStatus\nEnvironment\nDestinations\nType\nCategory\nCreated At\nCreated By\n\nOn the Destinations page, you can choose among the following properties:\n\nStatus\nCreated At\nType\nSources\nCategory\n\nYou can then sort or filter each column to just the values you care about, by clicking on the arrow next to each displayed column.\n\nFAQs\nMy source was disabled and it wasn\u2019t done by anyone in my workspace\n\nSources without any enabled destinations are auto-disabled after 14 days. However, the workspace owner is notified by email before Segment disables the source. Data that flows into Segment but does not flow to any downstream tools is not valuable to you and unnecessarily takes up space.\n\nSegment understands there may be cases to keep a source active. If you\u2019d like to add your sources to an exception list, you can do so by filling out this Airtable form.\n\nCan I request Segment add an integration tool?\n\nYes, you are able to submit an integration request here https://segment.com/requests/integrations/.\n\nThis page was last modified: 29 Feb 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSources\nDestinations\nWarehouses\nInformation on sources and destinations pages\nFAQs\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nProtocols Overview\nProtocols Overview\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nPROTOCOLS \u2713\n?\n\nProtocols is only available for event stream (website, mobile, and server sources) and Engage sources.\n\nSegment helps customers collect and integrate customer data across a wide range of tools and Destinations. To do so reliably, the data Segment receives must be clean, consistent and adhere to a well thought out tracking plan.\n\nProtocols was built to automate and scale the data quality best practices developed over years of helping customers implement Segment. Investing in data quality will improve trust in your data, reduce time spent by your engineering and business teams navigating and validating data, and ultimately allow your business to grow faster.\n\nProtocols is a premium add-on feature available to Business Tier customers. If your plan includes Protocols, you can access it from the protocols path in your workspace. If your plan doesn\u2019t include Protocols, contact your Segment account executive.\n\nThere are four steps to using Protocols\n1. Align teams with a Tracking Plan\n\nGood data quality starts with a well thought out Tracking Plan. With Protocols, you can define your events and corresponding properties in a Tracking Plan. This tracking plan becomes a central source of truth for product, engineering, analytics, and business teams.\n\n2. Validate data quality with violations\n\nWith your tracking plan living in Segment, you can apply it to 1 or more data sources. Any event or property that does not match the tracking plan will generate a violation. Violations are displayed in aggregated form to spot trends, and detailed form to help you quickly find and resolve discrepancies.\n\n3. Enforce data standards with controls\n\nTo maintain a high degree of quality over time, we offer strict controls to block non-conforming events. Blocked events can be forwarded to a separate quarantined Segment source for analysis and review.\n\n4. Resolve data issues with transformations\n\nEven the most exacting data collection processes are subject to human error and organizational complexity. Transformations can be applied from within Protocols to change event and property names without touching code.\n\nLearn more\nBest Practices\n\nLearn more about tracking plans and why you need them.\n\nTracking Plan\n\nCreate a Tracking Plan to standardize your collected data.\n\nFAQ\n\nGet answers to Protocols questions that come up the most.\n\nGet Started: Learn more about Tracking Plans ->\n\nThis page was last modified: 13 Jul 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nThere are four steps to using Protocols\nLearn more\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nReverse Etl\n/\nReverse Etl Source Setup Guides\n/\nRedshift Reverse ETL Setup\nRedshift Reverse ETL Setup\n\nSet up Redshift as your Reverse ETL source.\n\nRedshift Reverse ETL sources support Segment's dbt extension\n\nIf you have an existing dbt account with a Git repository, you can use Segment\u2019s dbt extension to centralize model management and versioning, reduce redundancies, and run CI checks to prevent breaking changes.\n\nTo set up Redshift with Reverse ETL:\n\nLog in to Redshift and select the Redshift cluster you want to connect with Reverse ETL.\nFollow the networking instructions to configure the correct network and security settings.\n\nRun the SQL commands below to create a user named segment.\n\n -- create a user named \"segment\" that Segment will use when connecting to your Redshift cluster.\n CREATE USER segment PASSWORD '<enter password here>';\n\n -- allows the \"segment\" user to create new schemas on the specified database. (this is the name you chose when provisioning your cluster)\n GRANT CREATE ON DATABASE \"<enter database name here>\" TO \"segment\";\n\nFollow the steps listed in the Add a source section to finish adding Redshift as your source.\nExtra Permissions\n\nGive the segment user read permissions for any resources (databases, schemas, tables) the query needs to access.\n\nGive the segment user write permissions for the Segment managed schema (__segment_reverse_etl), which keeps track of changes to the query results.\n\nTroubleshooting\nExtraction failures: relation does not exist\n\nIf you are able to run the query in the Query Builder, but the sync fails with the relation does not exist error, please make sure the schema name is included before the database table name, and check that the schema name is correct:\n\nSELECT id FROM <schema_name>.<table_name>\n\n\nAfter you\u2019ve successfully added your Redshift source, add a model and follow the rest of the steps in the Reverse ETL setup guide.\n\nThis page was last modified: 10 Jun 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nExtra Permissions\nTroubleshooting\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nUsing Engage Data\nUsing Engage Data\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nYou can send your Computed Traits and Audiences to your Segment Destinations, which allows you to personalize messages across channels, optimize ad spend, and improve targeting. This page provides an overview of different ways to activate Engage data in Segment Destinations.\n\nYou can also use the Profile API to activate Engage data programmatically.\n\nEngage Destination types: Event vs. List\n\nThere are two ways to send data to Engage Destinations: as Events and as Lists.\n\nEvent Destinations receive data one by one, on a streaming basis as events, which are behaviors or traits tied to a user and a point in time. Every time a piece of data (such as a track event or identify call) is received in Segment \u2014 for example, from your website or your mobile app \u2014 Segment then sends this piece of data to the Destination right away.\n\nList Destinations periodically receive data in batches, and these batches contain lists of users. In most cases, Segment sends data to a list destination every hour, and sends all data accumulated since the last batch was sent.\n\nSome Destinations, such as Salesforce Marketing Cloud have both \u201cevent\u201d and \u201clist\u201d destination types that you can use.\n\nEngage sends computed traits and audiences to destinations in different ways depending on whether the destination is an Event or List type:\n\nComputed Traits are always sent to Event destinations either using an identify call for user traits, a group call for account-level computed traits, or a track event.\n\nWith Audiences, Engage sends the audience either as a boolean (true or false) user property to Event Destinations, or as a user list to List Destinations. If you are a B2B company creating account audiences (where each account represents a group of users, like employees at a business) and sending them to list destinations, Engage sends the list of all users within an account that satisfies the audience criteria.\n\nEvent Destinations\n\nEvent Destinations and Computed traits Computed traits can only be sent to Event destinations. When Engage sends a computed trait to an Event destination, it uses an identify call to send user traits, or a group call to send account-level computed traits.\n\nEvent Destinations and Audiences\n\nidentify call as a user trait. When you use identify calls, the trait name is the snake_cased version of the audience name you provided, and the value is \u201ctrue\u201d if the user is part of the audience. For example, when a user first completes an order in the last 30 days, Segment sends an identify call with the property order_completed_last_30days: true, and when this user no longer satisfies that criteria (for example if 30 days elapses and they haven\u2019t completed another order), Segment sets that value to false.\ntrack call as two events: Audience Entered and Audience Exited, with the event property order_completed_last_30days equal to true and false, respectively.\n\nSegment sends an identify or track call for every user in the audience when the audience is first created. Later syncs only send updates for those users who were added or removed from the audience since the last sync.\n\nMost destinations require that you configure a column in your schema to receive the audience data, however, some destinations (like Braze and Iterable) allow you to send audiences without doing this. This depends on the individual destination, so consult the destination\u2019s documentation for details.\n\nList Destinations\n\nList destinations can only receive Audiences, and cannot receive computed traits.\n\nUser-Level Audiences: a list of users that belong to an audience\nAccount-Level Audiences: a list of users within an account that satisfy the audience criteria\n\nWhen syncing to a list destination Engage uploads lists of users directly to the destination. When you first create an audience, Segment uploads the entire list of audience users to the destination. Later syncs only upload the users that have been added or removed since the last sync.\n\nUser-list destinations can have individual limits on how often Segment can sync with them. For example, an AdWords audience is updated once every six hours or more, because that\u2019s what AdWords recommends.\n\nJourneys: The destination receives a list of users who qualify for the associated journey step. Unlike lists associated with Engage Audiences, users who are added to a journey list cannot be subsequently removed. See best practices for techniques to suppress targeting with journey lists. For more information, see Using Engage Data.\nWhat do the payloads look like for Engage data?\n\nThe payloads sent from your Engage space to your destinations will be different depending on if you configured the destination to receive identify or track calls, and whether the payload is coming from a computed trait or audience. As a reminder, identify calls usually update a trait on a user profile or table, whereas track calls send a point-in-time event that can be used as a campaign trigger or a detailed record of when a user\u2019s audience membership or computed trait value was calculated.\n\nTo view the events generated by an Engage Space\u2019s audience or computed traits, navigate to Unify settings > Debugger and view the list of sources that are configured to generate events per each destination instance. Each source will only generate events to connected destinations. From the source\u2019s Debugger tab, you\u2019ll find the most recent events generated by that source per the connected destinations\u2019 audiences and computed traits.\n\nIn the full json body of an audience, computed trait, or journey, you\u2019ll find specific details under the context.personas object. These fields can be useful when building out Destination Filters, Actions destination mappings, and Functions.\n\nThe integrations object in the payload displays as {\"All\" : false,} and only lists some destinations. This is due to the fact that each source has multiple destinations connected while each audience/trait may only have a subset of destinations connected to it. See Filtering with the Integrations Object for more information. The integrations object routing specific events to its specified destinations is also why a destination\u2019s Delivery Overview tab will show a large number of events under the Filtered at destination box, as that destination will only receive the events intended to be sent to it by audiences, traits, or journeys that are connected to that specific destination.\n\nComputed Trait generated events\n\nIdentify events generated by a Computed Trait have the trait name set to the Computed Trait value:\n\n{\n  \"context\": {\n    \"personas\": {\n      \"computation_class\": \"trait\", // the type of computation\n      \"computation_id\": \"tra_###\", // the trait's id found in the URL\n      \"computation_key\": \"aud_###\", // the configured trait key that appears on user profile\n      \"namespace\": \"spa_###\", // the Engage Space's ID\n      \"space_id\": \"spa_###\" // the Engage Space's ID\n    }\n  },\n  \"type\": \"identify\",\n  \"userId\": \"u123\",\n  \"traits\": {\n     \"total_revenue_180_days\": 450.00\n  }\n}\n\n\nTrack events generated by a Computed Trait have a key for the trait name, and a key for the Computed Trait value. The default event name is Trait Computed, but you can change it.\n\n{\n  \"context\": {\n    \"personas\": {\n      \"computation_class\": \"trait\", // the type of computation\n      \"computation_id\": \"tra_###\", // the trait's id found in the URL\n      \"computation_key\": \"aud_###\", // the configured trait key that appears on user profile\n      \"namespace\": \"spa_###\", // the Engage Space's ID\n      \"space_id\": \"spa_###\" // the Engage Space's ID\n    }\n  },\n  \"type\": \"track\",\n  \"event\": \"Trait Computed\",\n  \"userId\": \"u123\",\n  \"properties\": {\n     \"trait_key\": \"total_revenue_180_days\",\n     \"total_revenue_180_days\": 450.00\n  }\n}\n\n\nEngage only sends events to the destination if the Computed Trait value has changed for the user. Engage doesn\u2019t send a payload for every user in your trait every time the trait computes.\n\nAudience generated events\n\nIdentify events generated by an Audience have the Audience key set to true or false based on whether the user is entering or exiting the audience:\n\n{\n  \"context\": {\n    \"personas\": {\n      \"computation_class\": \"audience\", // the type of computation\n      \"computation_id\": \"aud_###\", // the audience's id found in the URL\n      \"computation_key\": \"aud_###\", // the configured audience key that appears on user profile\n      \"namespace\": \"spa_###\", // the Engage Space's ID\n      \"space_id\": \"spa_###\" // the Engage Space's ID\n    }\n  },\n  \"type\": \"identify\",\n  \"userId\": \"u123\",\n  \"traits\": {\n     \"first_time_shopper\": true // false when a user exits the audience\n  }\n}\n\n\nTrack events generated by an Audience have a key for the Audience name, and for the Audience value:\n\n{\n  \"context\": {\n    \"personas\": {\n      \"computation_class\": \"audience\", // the type of computation\n      \"computation_id\": \"aud_###\", // the audience's id found in the URL\n      \"computation_key\": \"aud_###\", // the configured audience key that appears on user profile\n      \"namespace\": \"spa_###\", // the Engage Space's ID\n      \"space_id\": \"spa_###\" // the Engage Space's ID\n    }\n  },\n  \"type\": \"track\",\n  \"userId\": \"u123\",\n  \"event\": \"Audience Entered\", // \"Audience Exited\" when a user exits an audience\n  \"properties\": {\n     \"audience_key\": \"first_time_shopper\",\n     \"first_time_shopper\": true // false when a user exits the audience\n  }\n}\n\nJourneys generated events\n\nThe data type you send to a destination depends on whether the destination is an event destination or a list destination. For more information, read the Journeys documentation on how Journeys Identity and Track event payloads get formatted when sending to Event destinations.\n\nSee this doc for more information on Journeys events. Track events generated by a journey have a key for the journey name \u201caudience_key\u201d, and a key for the journey value:\n\n{\n  \"context\": {\n    \"personas\": {\n      \"computation_class\": \"audience\", // the type of computation\n      \"computation_id\": \"aud_###\", // the audience's id found in the URL\n      \"computation_key\": \"j_o_###\", // the configured journey key that appears on user profile\n      \"namespace\": \"spa_###\", // the Engage Space's ID\n      \"space_id\": \"spa_###\" // the Engage Space's ID\n    }\n  },\n  \"type\": \"track\",\n  \"userId\": \"u123\",\n  \"event\": \"Audience Entered\", // \"Audience Exited\" when a user exits an audience\n  \"properties\": {\n     \"audience_key\": \"j_o_###\",\n     \"recent_buyer\": true // false when a user exits the journey\n  }\n}\n\n\nIdentify events generated by a Journey have the Journey key set to true or false based on whether the user is entering or exiting the Journey:\n\n{\n  \"context\": {\n    \"personas\": {\n      \"computation_class\": \"audience\", // the type of computation\n      \"computation_id\": \"aud_###\", // the audience's id found in the URL\n      \"computation_key\": \"j_o_###\", // the configured journey key that appears on user profile\n      \"namespace\": \"spa_###\", // the Engage Space's ID\n      \"space_id\": \"spa_###\" // the Engage Space's ID\n    }\n  },\n  \"type\": \"identify\",\n  \"userId\": \"u123\",\n  \"traits\": {\n     \"recent_buyer\": true // false when a user exits the journey\n  }\n}\n\nAdditional identifiers\n\nEngage has a flexible identity resolution layer that allows you to build user profiles based on multiple identifiers like user_id, email, or mobile advertisingId. However, different destinations may require different keys, so they can do their own matching and identification. For example, Zendesk requires that you include the name property. Engage includes logic to automatically enrich payloads going to these destinations with the required keys.\n\nIf you send events to a destination that requires specific enrichment Segment doesn\u2019t already include, you can use ID Sync or Trait Enrichment to send additional data points to the destination.\n\nProfiles with multiple identifiers (for example, user_id and email) will trigger one API call per identifier when the audience or computed trait is first synced to a destination.\n\nEmail as an identifier is set in traits.email for Identify calls, and context.traits.email for Track calls.\n\nMultiple identifiers of the same type\n\nYou might also see that profiles that have multiple values for the same external_id type, for example a profile might have multiple email addresses. When this happens, Engage sends one event per email for each audience or computed trait event. This ensures that all downstream email-based profiles receive the complete audience or computed trait.\n\nIn some situations, this behavior might cause an unexpected volume of API calls. You can use ID Sync to establish a strategy and control the number of events sent.\n\nNew external identifiers added to a profile\n\nThere are two situations when Engage sends an audience or computed trait to a destination.\n\nThe first is when the value of the trait or audience changes.\n\nThe second, less common case is that Engage re-syncs an audience or computed trait when a new external_id is added to a profile. For example, an ecommerce company has an anonymous visitor with a computed trait called last_viewed_category = 'Shoes'. That visitor then creates an account and an email address is added to that profile, even though the computed trait value hasn\u2019t changed. When that email address is added to the profile, Engage re-syncs the computed trait that includes an email to downstream tools. This allows the ecommerce company to start personalizing the user\u2019s experience from a more complete profile.\n\nFor more granular control that lets you specify which external IDs Segment sends to a destination, see the ID Sync documentation.\n\nRate limits on Engage Event Destinations\n\nMany Destinations have strict rate limits that prevent Segment (and other partners) from sending too much data to a Destination at one time. Engage caps the number of requests per second to certain Destinations to avoid triggering rate limits that would cause data to be dropped. The most common scenario when customers run into rate-limits is when Engage first tries to sync a large set of historical users. Once this initial sync is done, we rarely run into rate-limit issues.\n\nFor additional information on Destination-specific rate limits, check the documentation for that Destination. If you need a higher rate limit, let Segment know which Destination you need it for and why.\n\nDESTINATION\tREQUESTS PER SECOND\nBraze\t100\nCustomer.io\t30\nHubspot\t5 obj/second (2 calls send per object)\nIntercom\t8\nIterable\t500\nMailchimp\t10\nMarketo\t5\nMarketo Static Lists\t5\nPardot\t2\nResci\t200\nResponsys\t3\nResponsys Batch\t3\nSendgrid\t100\nSendgrid Lists\t100\nSalesforce\t5\nSalesforce Marketing Cloud\t20\nSalesforce Marketing Cloud Lists\t20\nZendesk\t50\nSyncing data to a new Destination for the first time\n\nWhen you create a new Computed Trait or Audience in Engage, you can choose to calculate it either using all the available historical data from your Segment implementation, or only using data that arrives after you set up the trait or audience. By default, Segment opts to include historical data. Afterwards, Segment only sends updates to that destination.\n\nWhy would I disable historical data? You might want to disable historical data if you\u2019re sending a triggered campaign. For example, if you want to send an email confirming a purchase, you probably don\u2019t want to email users who bought something months ago, but you do want to target current users as they make purchases (and thus enter the audience).\n\nThe Facebook Custom Audiences Website destination does not accept historical data, and so only uses data from after the moment you configure it.\n\nUse the Engage settings to add a destination to your Engage space.\n\nEngage compatible Destinations: Event type\n\nConnect any Cloud-mode destination that supports Identify or Track calls to Engage as an event type destination.\n\nEngage compatible Destinations: List type\nFacebook Custom Audiences\nGoogle Ads Remarketing Lists\nDisplay and Video 360 (Actions)\nSnapchat Audiences\nPinterest Audiences\nMarketo Static Lists (Actions)\nResponsys\n\nThis page was last modified: 14 Nov 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nEngage Destination types: Event vs. List\nWhat do the payloads look like for Engage data?\nAdditional identifiers\nMultiple identifiers of the same type\nNew external identifiers added to a profile\nRate limits on Engage Event Destinations\nSyncing data to a new Destination for the first time\nEngage compatible Destinations: Event type\nEngage compatible Destinations: List type\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nProtocols\n/\nTracking Plan\n/\nThe Protocols Tracking Plan\nThe Protocols Tracking Plan\n\nA Tracking Plan is a data spec outlining the events and properties you intend to collect across your Segment Sources. Crafting a comprehensive Tracking Plan takes time and effort across a range of teams within your organization, and a deep understanding of your business objectives. Once created, the Tracking Plan becomes a highly valuable resource for both the engineers instrumenting Segment and all consumers of the data flowing through Segment. You can learn more about data quality best practices in the Protocols docs.\n\nWhen building a Tracking Plan, it\u2019s best to start with the key metrics that drive value for your business. Key metrics may include new user signups, top line revenue, product use and more. With key metrics defined, it becomes much easier to define which user actions help track or improve those key metrics. Each user action maps to a distinct event, or Track call, that you will track in Segment. The Tracking Plan can also validate Identify, Page and Group calls.\n\nThe Segment Tracking Plan feature allows you to validate your expected events against the live events that are delivered to Segment. Violations generate when an event doesn\u2019t match the spec\u2019d event in the Tracking Plan.\n\nSegment can infer event data types, but is unable to do so if several data types are sent for a specific category.\n\nTracking Plans are stored in workspaces and can be connected to one or more Sources.\n\nSegment Consent Preference Updated Event\n\nAfter setting up a consent category, users of Consent Management see a Segment Consent Preference Updated Event added to all existing Tracking Plans.\n\nCreate a Tracking Plan\n\nTo create a new Tracking Plan:\n\nContact your Segment account team to enable the Protocols features in your workspace.\nOnce enabled, click Protocols in the left bar navigation.\nClick New Tracking Plan.\nAdd events, properties, traits and filters in the Tracking Plan editor.\nYou\u2019ll see an option to import events and traits to your Tracking Plan that your source received in the last 24 hours, 7 days or 30 days. This option is great if you want to get started with your current events.\n\nConsent Management users see the Segment Consent Preference Updated event on new Tracking Plans\n\nIf you are a Consent Management user and have created at least one consent category, Segment automatically adds the Segment Consent Preference Updated event to all new Tracking Plans.\n\nCopy a Tracking Plan\n\nTo create a copy of an existing Tracking Plan:\n\nClick Protocols in the left navigation bar.\nOn the row of the Tracking Plan you want to copy, open the contextual menu(\u2026), and select Duplicate Tracking Plan.\nEnter a name for the new Tracking Plan instance, and click Duplicate.\nDownload a Tracking Plan\n\nTo download a Tracking Plan:\n\nClick Protocols in the left navigation bar.\nOn the row of the Tracking Plan you want to download, open the contextual menu(\u2026), and select Download Tracking Plan.\nA toast pops up on the top of the page, with the message \u201cYour file is processing. When your file is ready it will be available to download from the Download History page.\u201d\nOpen the Download History page by clicking the link in the toast or clicking the Download History tab in the top navigation bar.\nOnce the file status column indicates that the download was successful, click the link in the File column to download your CSV to your computer. If the file status column shows the download has failed, return to the Tracking Plan Overview page or the Tracking Plan page and try the download again.\nThe Tracking Plan CSV name has the following format:\nworkspaceSlug-trackingPlanName--yyyy-mm-dd--hh-mm-utc\n\nThe columns in the Tracking Plan CSV file corresponds to the Tracking Plan UI options. For example:\n\nAllowed Property Values: In the Tracking Plan UI, when the property type is \u2018String\u2019, you have the option to add a list of permitted values.\nEnum Values: When the property type is \u2018Enum\u2019, you have the option to add a list of permitted values.\n\nOnce you\u2019ve downloaded a Tracking Plan, you can upload it as a template for a new Tracking Plan or use it to make changes to an existing Tracking Plan.\n\nUpload a Tracking Plan\n\nYou can create a Tracking Plan or make changes to an existing Tracking Plan by uploading a CSV that contains the rules and events you\u2019d like to track. Segment provides a Tracking Plan template file that you can download during the import process, or you can download an existing Tracking Plan to use as your template.\n\nTracking Plan CSV requirements\n\nTracking Plan CSV files uploaded to Segment must be smaller than 15 mb and contain one header row and one or more rows of data. Tracking Plans CSVs must also have fewer than 100,000 rows and 2,000 rules.\n\nCreate a new Tracking Plan\n\nTo create a new Tracking Plan by uploading a CSV file:\n\nClick Protocols in the left navigation bar.\nClick New Tracking Plan.\nClick the Import\u2026 button and select From CSV.\nDownload the Tracking Plan template CSV and fill in the template file with your new Tracking Plan rules, or download an existing Tracking Plan.\nOnce you\u2019ve filled in the provided template or made changes to your downloaded Tracking Plan, add your CSV file to the file uploader and click Upload.\n\nAfter uploading your CSV file, you are redirected to the Upload & Download History page while the upload is in progress. If the CSV upload fails, you\u2019ll be able to either view the error directly in the Reports column on the Upload & Download History page or download the error_report.csv file that corresponds to the Tracking Plan you uploaded.\n\nTracking Plans created by an uploaded file are reflected in the Audit Trail and Tracking Plan changelog. If you are a Consent Management user and have created at least one consent category, Segment automatically adds the Segment Consent Preference Updated event to all new Tracking Plans.\n\nUpdate an existing Tracking Plan\n\nTracking Plans with imported libraries cannot be changed using the Upload a Tracking Plan method\n\nIf you have a Tracking Plan with imported libraries, you must make changes to your Tracking Plan in the Segment app.\n\nTo update a Tracking Plan by uploading a CSV file:\n\nClick Protocols in the left navigation bar.\nOn the row of the Tracking Plan you want to edit, open the contextual menu(\u2026) and select View Tracking Plan.\nSelect Edit Tracking Plan.\nClick the Import\u2026 button and select From CSV.\nDownload the Tracking Plan template CSV and fill in the template file with your new Tracking Plan rules, or download an existing Tracking Plan.\nOnce you\u2019ve filled in the provided template or made changes to your downloaded Tracking Plan, add your CSV file to the file uploader and click Upload.\n\nAfter uploading your CSV file, you are redirected to the Upload & Download History page while the upload is in progress. If the CSV upload fails, you\u2019ll be able to either view the error directly in the Reports column on the Upload & Download History page or download the error_report.csv file that corresponds to the Tracking Plan you uploaded.\n\nAny changes made to a Tracking Plan using an uploaded file are reflected in the Audit Trail and Tracking Plan changelog.\n\nDelete a Tracking Plan\n\nDeleting a Tracking Plan requires Workspace Owner or Tracking Plan Admin permissions\n\nYou must have Workspace Owner or Tracking Plan Admin roles to delete a Tracking Plan. For more information about roles in Segment, see the Roles documentation.\n\nTo delete a Tracking Plan:\n\nOpen the Segment app and click Protocols.\nOn the row of the Tracking Plan you want to delete, open the contextual menu(\u2026) and select Delete Tracking Plan\u2026\nOn the \u201cDelete Tracking Plan\u201d popup, click Delete.\nEdit a Tracking Plan\n\nThe Tracking Plan editor is organized as a spreadsheet to help you add new events and properties, and edit the relevant fields for each. Like a spreadsheet, you can navigate across cells in a single event with your arrow keys and press enter to edit a cell.\n\nCOLUMN NAME\tDETAILS\nName\tSpecify the name of your event or property.\nDescription\tEnter a description for your event or property. These descriptions are helpful for both engineers instrumenting Segment and consumers of the data.\nStatus\tSpecify whether a property is required or optional. You can\u2019t require a Track call because Segment is unable to verify when a Track call should be fired.\nData Type\tSpecify the data type of the property. Data type options include any, array, object, boolean, integer, number, string, null, Date time. Note: Date time is required to be in ISO-8601 format\nPermitted Values\tEnter simple regular expressions to validate property values. This works when a property data type is set to string. For example, you can add pipe delimited strings to the regex column to generate violations when a property value does not match fall, winter or spring.\n\nThe Status, Data Type, and Permitted Values columns appear as you add a Track call property.\n\nAdd a new Track call\n\nTo add a new Track call:\n\nClick Add Event to add a new row.\nClick into the row to add an event name and description. The event name strictly validates the name passed in your Track calls. Casing, spacing and spelling matter.\nAdd a Track call property\n\nTo add a Track call property:\n\nClick on the (+) next to the event name to add a new row below the event name.\nClick into the row to add the property name and also specify the description, status, data type and permitted values when applicable.\nYou can use your keyboard arrow and enter keys to navigate across the cells, or use your mouse.\nAdd a Track call object or array property\n\nSegment supports object and array data types in the Tracking Plan editor. These complex data structures have limited use cases and should be used sparingly as some destinations aren\u2019t able to ingest the data structures. To add an object or array:\n\nCreate a new property row and set the Data Type to Object or Array.\nClick the (+) next to the property name to add key value pairs in the object, or objects to an array of objects.\n\nWhen creating array properties in your Tracking Plan, add the items nested property, denoted by the name of the array property with a .$ suffix, to ensure that the nested property is marked as planned in the Source Schema.\n\nAdd Identify or Group traits\n\nYou can define which traits you expect to see passed in Identify or Group calls like how you would add Track calls to the Tracking Plan. Navigate to the Identify or Group tab in your Tracking Plan and click the (+) button to add a new trait.\n\nIt\u2019s best to keep traits optional because Identify and Group are often called and pass only new or changed traits, because Segment\u2019s client-side libraries (analytics.js, Swift, Kotlin) cache traits in local storage. See the Identify Best Practices to learn more.\n\nRemove a source from your Tracking Plan\n\nRemoving a source from a Tracking Plan requires Workspace Owner or Tracking Plan Admin permissions\n\nYou must have Workspace Owner or Tracking Plan Admin roles to remove a source from a Tracking Plan. For more information about roles in Segment, see the Roles documentation.\n\nTo remove a source from your Tracking Plan:\n\nClick Protocols in the left navigation bar.\nFind the Tracking Plan you\u2019d like to remove a source from, then select the source icon in the Connected Sources column.\nOn the list of sources connected to your Tracking Plan, find the source you\u2019d like to remove from your Tracking Plan and click Disconnect.\nReview the \u201cDisconnect source from Tracking Plan\u201d popup and click Disconnect.\nAdd a label\n\nYou can apply key:value labels to each event to help organize your Tracking Plan. These labels are helpful when multiple teams are managing a single Tracking Plan, or if you want to specify a priority, platform, product, or similar meta-data for each event. You can filter by label from the Tracking Plan, Schema, Data Validation and Violations Summary views.\n\nFor consistency purposes, it\u2019s best that you create a standard way of labeling events and share it with all parts of your organization that will use Segment.\n\nNote: Tracking Plan Labels are only available for Track and Page events.\n\nFilter events in the Tracking Plan\n\nYou can filter the Tracking Plan events by keyword or by label. The applied filter generates a permanent link so you can share specific events with teammates. Label filters also persist after you leave the Tracking Plan.\n\nEdit underlying JSON Schema\n\nProtocols Tracking Plans use JSON Schemas to validate Segment event payloads. To support a broader range of validation use-cases, Segment lets you to edit your underlying JSON schema.\n\nEditing a JSON schema requires technical expertise. The JSON schema documentation and JSON schema validator are helpful resources you can use.\n\nYou can edit the JSON schema for each Track event listed in the Tracking Plan, and a common JSON schema definition that applies across all events.\n\nTrack event JSON schema\n\nEach Track event in the Tracking Plan has a separate JSON schema definition to validate the properties in that event. To edit, click on the overflow menu next to each event row in the Tracking Plan.\n\nAdvanced edits to the JSON schema are not visible in the Tracking Plan and make it harder for other users to understand the validation logic. Be sure to communicate to any other Protocols users that you are making changes in the validation logic.\n\nCommon JSON schema\n\nThe Tracking Plan also uses a common JSON schema definition that applies to the entire payload of every event sent from sources connected to the Tracking Plan.\n\nThe common JSON schema definition is unique for each Tracking Plan. An example use of this feature is to validate that all Track, Identify and Page events sent to Segment include a context.device.advertisingId property. This validation ensures that every Segment call has a userId, anonymousId, and context object with a nested \"device\": { \"advertisingId\": \"e23sfsdf\"} object.\n\n{\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n    \"type\":\"object\",\n    \"properties\": {\n      \"context\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"device\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"advertisingId\": { \"type\": \"string\" }\n            },\n            \"required\": [\"advertisingId\"]\n          }\n        },\n        \"required\": [\"device\"]\n      },\n      \"anonymousId\": {\n        \"type\": \"string\"\n      },\n      \"userId\": {\n        \"type\": \"string\"\n      },\n      \"properties\":{\n        \"type\": \"object\",\n        \"properties\": {\n          \"primary_business_unit\": {\n            \"type\": \"string\"\n          }\n        },\n        \"required\": [\"primary_business_unit\"]\n      },\n      \"traits\":{\n        \"type\": \"object\",\n        \"properties\": {\n          \"first_name\": {\n            \"type\": \"string\"\n          },\n          \"last_name\": {\n            \"type\": \"string\"\n          },\n          \"email\": {\n            \"type\": \"string\"\n          }\n        },\n        \"required\": [\"email\"]\n      }\n  },\n  \"required\": [\"context\",\"anonymousId\",\"userId\",\"traits\"]\n}\n\n\nTo edit the common JSON schema:\n\nClick the (\u2026) at the top of the Tracking Plan editor and select Edit Common JSON Schema.\nEnter your new JSON schema and click Update JSON.\nOnce you\u2019ve saved and merged your JSON schema changes, go to the Settings tab for your source.\nClick Schema Configuration in the navigation and go to the Advanced Blocking Controls section to define specific blocking behavior for common JSON schema violations.\n\nTo edit the common JSON schema using the Public API, you\u2019ll need to add your new JSON schema under the \"global\": object.\n\nNegative lookahead regexes (?!) aren\u2019t supported. This means you can\u2019t use regex to prevent matches with a specific following character or expression. But, you can use not in the regex of your JSON schema to generate violations when a property key or value doesn\u2019t match the provided regex pattern.\n\nSpecifying data type\n\nProperty or trait data type should adhere to the data types defined by JSON schema. Data type names must be lower-cased as specified in JSON schema. Date/time properties should be represented as a string type with format keyword (for example: \u201cformat\u201d: \u201cdate-time\u201d).\n\nBlocking data\n\nJSON schema violation event blocking is only supported in cloud-mode Destinations. See the Customize your schema controls docs for more information on blocking data.\n\nExtend the Tracking Plan\n\nSome customers prefer to manage the Tracking Plan with outside tools and resources. See the APIs and extensions section to learn more.\n\nTracking Plan Event Versioning\n\nSegment offers Tracking Plan Event Versioning if you use Protocols to manage mobile sources, or to help you centrally manage a Tracking Plan for multiple teams. With Event Versioning, you can create multiple versions of an event definition, and validate events using a version key included in the Track event payload.\n\nThis can be helpful for mobile developers who might have several released versions of their app sending data at the same time. For example, a new mobile app release might add a new required property to an event like Order Completed. In this scenario, if you updated the Tracking Plan, all Order Completed events from your old mobile app versions would be invalid, because some customers won\u2019t have updated to the latest version yet. Instead, with event versioning, you can allow validation of both the old and new versions of an event at the same time. When you\u2019re ready to deprecate those old event versions, you can delete the version in your Tracking Plan.\n\nFor example, say you want to add subtotal as a required property to your Order Completed event. You would start by adding the required property to the event in the Tracking Plan as shown in the example below.\n\nCreate a new event version\n\nWith event versioning, you can now create multiple versions of the event definition as shown in the example below. To create a new event version, click into the overflow menu for an event and select Add Event Version.\n\nDynamically validate Track events against an event version\n\nTo ensure the Track events you send to a Segment source validate against the correct event version, you need to instrument your events to include a context.protocols.event_version key and version value. The version value must pass as an integer, and should match the number shown in the Tracking Plan version tab. In the example below, the version number would be 2.\n\nNext, add the event version number to the context object. For analytics.js Track calls, you would instrument the event as in the example below. Note how the JSON objects for context, protocols, and event_version are nested.\n\nanalytics.track('Order Completed', {\n  subtotal: 23,\n  products: [{\n    product_name: 'Air Balloon',\n    product_id: '32rd9jfs'\n  }],\n  order_id: '2df90eiwc9wjec',\n  revenue: 33\n}, {\n  context: {\n    protocols: {\n      event_version: 2\n    }\n  }\n});\n\n\nNote: Protocols validate events against the oldest event version in the Tracking Plan for event payloads that are 1) missing the context.protocols.event_version key, or 2) contain an invalid/undefined event version (ex: event_version:3.2).\n\nThis page was last modified: 16 May 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nCreate a Tracking Plan\nCopy a Tracking Plan\nDownload a Tracking Plan\nUpload a Tracking Plan\nDelete a Tracking Plan\nEdit a Tracking Plan\nTracking Plan Event Versioning\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nData Graph\n/\nLinked Events Limits\nLinked Events Limits\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\n\nTo provide consistent performance and reliability at scale, Segment enforces default use limits for Linked Events.\n\nUsage limits\n\nLinked Events provides you with the flexibility to enrich unlimited events in downstream destinations. This means you won\u2019t encounter any limitations or pauses in service related to the number of Linked Events enrichments.\n\nSegment measures Linked Events limits based on entities and entity rows.\n\nEntities: The warehouse tables that are declared in the Data Graph with the enrichment_enabled = true property.\nEntity rows: The total number of rows synced to Segment cache across all enrichment entities at any given time.\n\nTo see how many entities and entity rows you\u2019re using with Linked Events, navigate to Settings > Usage & billing and select the Linked Events tab.\n\nPLAN\tLINKED EVENTS LIMITS\tHOW TO INCREASE YOUR LIMIT\nFree\tNot available\tN/A\nTeams\tNot available\tN/A\nBusiness\tIf you use Unify and Engage, you\u2019ll receive a trial version with:\n* 1 Entity for every Unify space\n* 1 million Entity rows per workspace\tContact your sales rep to upgrade to the full paid version of Linked Events to unlock:\n* Unlimited Entities\n* Additional Entity Rows (10 x the number of MTUs or 0.1 x the number of monthly API calls up to a maximum of 100 million, to be used across your workspaces)\n\nNote: You must already be on a Unify or Engage plan to be eligible for upgrade.\nSpecial cases\nIf you have a non-standard or high volume usage plan, you may have unique Linked Events limits or custom pricing.\nIf you\u2019re on the trial version of Linked Events, you won\u2019t be able to add more than 1 million entity row syncs. Reach out to your Customer Success representative to upgrade to the Linked Events paid tier.\nIf you\u2019re using the paid version of Linked Events, and you reach your entity row limit before the end of your billing period, your syncs won\u2019t automatically pause to avoid disruptions to your business. You may be billed for overages in cases of significant excess usage. If you consistently require a higher limit, contact your sales representative to upgrade your plan with a custom limit.\n\nThere is a hard limit of 100 million entity rows that causes syncs to pause.\n\nThis page was last modified: 15 Oct 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nUsage limits\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nFunctions\n/\nFunctions Copilot Nutrition Facts Label\nFunctions Copilot Nutrition Facts Label\n\nTwilio\u2019s AI Nutrition Facts provide an overview of the AI feature you\u2019re using, so you can better understand how the AI is working with your data. Function Copilot\u2019s AI qualities are outlined in the following Nutrition Facts label. For more information, including the glossary regarding the AI Nutrition Facts label, refer to the AI Nutrition Facts page.\n\nAI Nutrition Facts\n\nCustomer AI Functions Copilot\n\n\n\n\nDescription\n\nFunctions Copilot is an AI-powered coding assistant designed to streamline the development of custom integrations, and enrich and transform Segment Functions.\n\n\n\n\nPrivacy Ladder Level\n1\n\n\n\n\nFeature is Optional\nYes\n\n\n\n\nModel Type\nGenerative\n\n\n\n\nBase Model\nOpenAI - GPT-4\n\n\n\n\nTrust Ingredients\n\n\n\n\nBase Model Trained with Customer Data\nNo\n\n\n\n\n\n\n\nCustomer Data Shared with Model Vendor\nNo\n\n\n\n\n\n\n\nTraining Data Anonymized \u00a0\nN/A\n\n\n\n\nData Deletion\nYes\n\n\n\n\nHuman in the Loop\nYes\n\n\n\n\nData Retention\nN/A\n\n\nCompliance \u00a0 \u00a0\nLogging & Auditing\nN/A\n\nGuardrails\nN/A\n\n\n\nInput/Output Consistency\nYes\n\n\n\n\nOther Resources\n\nLearn more at: https://twilio.com/en-us/customer-ai\n\nThis page was last modified: 28 Jun 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSpec\n/\nSpec: Page\nSpec: Page\n\nThe Page call lets you record whenever a user sees a page of your website, along with any optional properties about the page. Calling Page or Screen in a Segment source is one of the first steps to getting started with Segment.\n\nSegment University: The Page Method\n\nCheck out our high-level overview of the Page method in Segment University. (Must be logged in to access.)\n\nNote: In analytics.js a Page call is included in the snippet by default just after analytics.load. Many destinations require this page event to be fired at least once per page load for proper initialization. You may add an optional name or properties to the default call, or call it multiple times per page load if you have a single-page application.\n\nHere\u2019s the payload of a typical Page call with most common fields removed:\n\n{\n  \"type\": \"page\",\n  \"name\": \"Home\",\n  \"properties\": {\n    \"title\": \"Welcome | Initech\",\n    \"url\": \"http://www.example.com\"\n  }\n}\n\n\nAnd here\u2019s the corresponding JavaScript event that would generate the above payload. If you\u2019re using Segment\u2019s JavaScript library, the page name and URL are automatically gathered and passed as properties into the event payload:\n\nanalytics.page(\"Retail Page\",\"Home\");\n\n\nBased on the library you use, the syntax in the examples might be different. You can find library-specific documentation on the Sources Overview page.\n\nBeyond the common fields, the Page call takes the following fields:\n\nFIELD\t\tTYPE\tDESCRIPTION\ncategory\toptional\tString\tThe category of the page, added to the properties object.\nPassed as the first argument in a page call, for example analytics.page(\"category\", \"name\");\nname\toptional\tString\tName of the page For example, most sites have a \u201cSignup\u201d page that can be useful to tag, so you can see users as they move through your funnel.\nproperties\toptional\tObject\tFree-form dictionary of properties of the page, like url and referrer See the Properties field docs for a list of reserved property names.\nExample\n\nHere\u2019s a complete example of a Page call:\n\n{\n  \"anonymousId\": \"507f191e810c19729de860ea\",\n  \"channel\": \"browser\",\n  \"context\": {\n    \"ip\": \"8.8.8.8\",\n    \"userAgent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36\"\n  },\n  \"integrations\": {\n    \"All\": true,\n    \"Mixpanel\": false,\n    \"Salesforce\": false\n  },\n  \"messageId\": \"022bb90c-bbac-11e4-8dfc-aa07a5b093db\",\n  \"name\": \"Home\",\n  \"properties\": {\n    \"title\": \"Welcome | Initech\",\n    \"url\": \"http://www.example.com\"\n  },\n  \"receivedAt\": \"2015-02-23T22:28:55.387Z\",\n  \"sentAt\": \"2015-02-23T22:28:55.111Z\",\n  \"timestamp\": \"2015-02-23T22:28:55.111Z\",\n  \"type\": \"page\",\n  \"userId\": \"97980cfea0067\",\n  \"version\": \"1.1\"\n}\n\nCreate your own Page call\n\nUse the following interactive code pen to see what your Page calls would look like with user-provided information:\n\nMake a Page Call\nSample Page Call\n\n\nIdentities\n\nThe User ID is a unique identifier for the user performing the actions. Check out the User ID docs for more detail.\n\nThe Anonymous ID can be any pseudo-unique identifier, for cases where you don\u2019t know who the user is, but you still want to tie them to an event. Check out the Anonymous ID docs for more detail.\n\nNote: In our browser and mobile libraries a User ID is automatically added from the state stored by a previous identify call, so you do not need to add it yourself. They will also automatically handle Anonymous IDs under the covers.\n\nProperties\n\nProperties are extra pieces of information that describe the page. They can be anything you want.\n\nSegment handles properties with semantic meanings in unique ways. For example, Segment always expects path to be a page\u2019s URL path, and referrer to be the previous page\u2019s URL.\n\nYou should only use reserved properties for their intended meaning.\n\nReserved properties Segment has standardized:\n\nPROPERTY\tTYPE\tDESCRIPTION\nname\tString\tName of the page. Reserved for future use.\npath\tString\tPath portion of the page\u2019s URL. Equivalent to canonical path which defaults to location.pathname from the DOM API.\nreferrer\tString\tPrevious page\u2019s full URL. Equivalent to document.referrer from the DOM API.\nsearch\tString\tQuery string portion of the page\u2019s URL. Equivalent to location.search from the DOM API.\ntitle\tString\tPage\u2019s title. Equivalent to document.title from the DOM API.\nurl\tString\tPage\u2019s full URL. Segment first looks for the canonical URL. If the canonical URL is not provided, Segment uses location.href from the DOM API.\nkeywords\tArray [String]\tA list/array of keywords describing the page\u2019s content. The keywords would most likely be the same as, or similar to, the keywords you would find in an HTML meta tag for SEO purposes. This property is mainly used by content publishers that rely heavily on pageview tracking. This isn\u2019t automatically collected.\n\nNote: In analytics.js, Segment automatically sends the following properties: title, path, url, referrer, and search.\n\nThis page was last modified: 21 Nov 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nExample\nIdentities\nProperties\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nFunctions\n/\nFunctions usage limits\nFunctions usage limits\n\nFunctions are billed to your account using the total execution time per month.\n\nAn individual function\u2019s execution time is the total time it takes for the function to process events, including mapping, transformations, and requests to external APIs. Generally, requests to external APIs can greatly add to your total execution time.\n\nYour total execution time is the execution time for all of your active functions accumulated over the course of a month. You can see your current execution time on the Functions tab of the Usage page in each workspace. You will receive notifications of your usage when you\u2019ve reached 75%, 90%, and 100% of your allotted execution time.\n\nThe amount of time you are allotted changes depending on your Segment pricing plan.\n\nMeasuring execution time\n\nSegment measures execution time from when the function first receives an event to the time the function either returns successfully or throws an error. If Segment retries your function (for example, if there was a timeout), those retries also count as billable execution time.\n\nStarting on April 8, 2021 Functions usage is measured in millisecond increments. This makes your usage and billing much more precise. Prior to this change, Functions was measured in 100ms increments, and then rounded up. For example, a function that took 80ms to complete was previously billed as 100ms. Using the new usage calculation, it is billed as 80ms.\n\nExecution timeouts\n\nFunctions have a timeout of five seconds. If a function takes longer than five seconds, execution halts and the function is retried periodically for up to four hours.\n\nEstimating execution time\n\nExecution time can vary widely between use cases, so it is extremely difficult to predict. The best way is to look at the function\u2019s actual execution time and multiply it by the event volume.\n\nAnother way to provide a rough estimate is to use an expected source function time of 100ms per invocation, and expected destination function time at 200ms per invocation:\n\nA source function receiving 1M requests and taking an average of 100 milliseconds will use 27.8 hours of execution time: 1,000,000 events * 100ms = 100,000,000ms = 28 hours\nA destination function receiving 1B requests and taking an average of 200 milliseconds will use 55,556 hours: 1,000,000,000 * 200ms = 200,000,000,000ms = 55,556 hours\n\nNote: Test runs are generally slower than the time it takes a function to run once it\u2019s deployed. For more accurate estimates, base your estimates on sending data into a production function, and not on timing the test runs.\n\nYou can (and should!) use Destination Filters to reduce the volume of events reaching your function. Filtering events with a Destination Filter prevents the Function from being invoked for that event entirely.\n\nImproving speed of external requests\n\nIn the most cases, functions are slow due to external requests using the fetch() call. The external API may be under heavy load or it may simply take a long time to process your request.\n\nIf you\u2019re making many requests that could be done in parallel, ensure that you\u2019re not doing them sequentially. If the external API takes 400ms to respond and you issue 10 requests, it would take four seconds to do them sequentially versus 400ms to do them in parallel. For example, if you\u2019re waiting for requests to complete inside of a loop you\u2019re making your requests sequentially:\n\nfor (const objectId of event.properties.objects) {\n   const response = await fetch('https://example.com/?id=' + objectId, {\n       method: 'POST',\n       body: event.properties\n   })\n\n   console.log(response.json())\n}\n\n\nInstead, consider making an array of async requests that are running in parallel and then using Promise.all() to wait for all of them to complete:\n\nconst requests = event.properties.objects.map(objectId => {\n    return fetch('https://example.com/?id=' + objectId, {\n        body: event.properties\n    })\n})\n\nconst responses = await Promise.all(requests)\nfor (const response of responses) {\n    console.log(response.json())\n}\n\n\nIf you\u2019re only issuing a single request in your function and it is slow, you might want to contact the owner of the external API for support.\n\nDefault limit number\n\nEach workspace has a default limit of 25 Functions in total across Source, Insert, and Destination Functions. If you want to create more, please reach out to Segment.\n\nThis page was last modified: 08 Nov 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nMeasuring execution time\nExecution timeouts\nEstimating execution time\nImproving speed of external requests\nDefault limit number\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nAudiences\n/\nAccount-level Audiences\nAccount-level Audiences\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nAccount-level audiences are audiences for businesses that sell to other businesses. They return the set of accounts which match a combination of account-level traits, user-level traits, and user events. You can sync these accounts and associated users with downstream destinations.\n\nYou can use account-level audiences to accomplish the following use cases:\n\nIdentify a set of at-risk accounts based on associated users\u2019 log in patterns, and flag them in your customer service application\nIdentify a set of trial accounts which would benefit from a paid plan based on their use, and flag those in your CRM application\nCreate an email list of all users in accounts showing high adoption of a specific feature to aid in recruiting for user research\nUSE CASE\tREQUIRED FEATURES\tDETAILED DESCRIPTION\nCreate a list of accounts based on account-level traits, and/or the traits and behaviors of individual users associated with accounts\t1. Account-level audiences\tAccount-level audience conditions\nCreate a list of accounts based on the collective behavior of all users associated with accounts\t1. Account-level computed trait or SQL trait\n2. Account-level audience (using computed trait as a condition)\tUsing account-level computed and SQL traits as account-level audience conditions\nCreate a list of users based on a combination of user-level traits and events, and account-level traits\t1. Account-level audience (using account-level trait as a condition)\n2. User-level audience (using account-level audience membership as a condition)\tUsing account-level traits in user-level audiences\nEnable account-level audiences\nContact friends@segment.com and provide your workspace ID to have account-level audiences enabled for your workspace. Navigate to Settings > Workspace Settings > General Settings to view your workspace ID.\nEnsure that group_id is configured as an identifier in Engage Identity Resolution settings. For more information, see Identity Resolution Settings.\nInstrument group calls to send account information to Segment.\nAccount-level audience conditions\n\nA single account-level audience can incorporate any combination of the following condition types:\n\nUser-level events (sent through track, page, and screen calls)\nUser-level computed and SQL traits\nUser-level audience membership\nUser-level custom traits (set through an identify call)\nAccount-level computed traits and SQL traits\nAccount-level audience membership\nAccount-level custom traits (set through a group call)\n\nTo access account-level audience conditions:\n\nNavigate to Engage > Audiences, and click Create.\nSelect Accounts from the Select Type screen.\nFrom the Configure screen, select Accounts in the dropdown.\n\nThe three types of user-level conditions are:\n\nAny User (default): Returns all accounts where at least one user associated with the account satisfies the specified condition\nAll users: Returns all accounts where all users associated with the account satisfy the specified condition\nNone of the users: Returns all accounts where no users associated with the account satisfy the specified condition\n\nYou can create conditions which operate on the set of events collectively triggered by all users associated with an account with account-level computed and SQL traits.\n\nAccount-level computed and SQL traits\n\nWorkspaces with access to account-level audiences can create account-level computed and SQL traits. All user-level computed trait types are supported (see the Types of computed traits docs for a full list). Account-level computed traits operate on the set of events triggered by all users associated with a given account.\n\nUse-cases for account-level computed traits include:\n\nCalculate the number of times users associated with an account logged in during the past month\nCalculate the average NPS survey score across all NPS surveys submitted by users associated with an account\nIdentify the first marketing landing page viewed by any user associated with an account\n\nUse SQL traits for complex calculations not supported by computed traits. For example, you would use SQL traits to calculate the number of unique users associated with an account who have logged in during the past month.\n\nUse account-level SQL traits to associate users to an account\n\nTo associate users to an account with SQL traits, you must return both the group_id and user_id in the account level SQL trait. This fires a Group call which Segment uses to add users to groups in destinations.\n\nWhen a group (account) contains more than one user, the query returns duplicate group_ids (mapped to unique user_ids). However, Segment doesn\u2019t return duplicate group_ids in the account-level SQL trait. As a result, you can\u2019t map users to accounts in a many-to-many situation.\n\nUse account-level computed and SQL traits as account-level audience conditions\n\nOnce created, you can connect account-level computed and SQL traits to downstream destinations. You can also use them as conditions in account-level audiences, enabling you to build audiences based on the set of events triggered by all users associated with a given account.\n\nFor example, you can create an audience which selects all accounts where users associated with the account have collectively logged in fewer than 10 times in the past 30 days. To accomplish this, you would:\n\nCreate an account-level event counter computed trait, selecting the \u201cLogged In\u201d event and specifying a time window of the past 30 days.\n\nThis computed trait will count all \u201cLogged In\u201d events collectively triggered by users associated with each account over the past 30 days, and append the resulting counts to each Account profile.\n\nCreate an account-level audience, containing a single condition using the new computed trait (for example, logins_past_30_days<10).\n\nConnect account-level audiences, computed traits, and SQL traits to destinations\n\nWhen you connect an account-level audience or trait to a destination, you select which events to send to the destination when the audience or trait is calculated. Each destination supports a subset of the following events:\n\nGroup: Segment emits one group call per account that matches the conditions. Audience names are included as a boolean trait (for example audience_name:true), while computed and SQL trait values are included directly (for example trait_name:trait_value). An additional group call is emitted when an account no longer matches audience conditions (audience_name:false).\nIdentify: Segment emits Identify calls for each user who is associated with any accounts matching the conditions. Audience name is included as a boolean trait (for example audience_name:true), while computed and SQL trait values are included directly (for example trait_name:trait_value). An additional identify call is emitted for each user who is associated with an account which no longer matches audiences conditions (audience_name:false).\nTrack: Track calls are an alternative to Identify calls to send events for each user who is associated with any accounts that match the conditions. While identify typically updates a profile, a track event will record an event. The audience name is included as an event property (for example audience_name:true), and the default track name is Audience Entered or Trait Computed for audiences and computed traits, respectively. The event names can be customized. When a user associated with an account which no longer matches audiences conditions, an Audience Exited event is sent with an event property where audience_name:false.\n\nEnable group calls when you want to update a destination\u2019s account records based on audience membership. Enable identify calls when you want to update a destination\u2019s user records based on audience membership.\n\nUse account-level traits in user-level audiences\n\nAccount-level audiences let you target all users associated with accounts that match your audience criteria. However, you may need to target a subset of users based on the traits of their associated accounts.\n\nAccount-level traits are not available in the user-level audience builder. However, account-level audience membership is available in user-level audiences through the \u201cPart of an audience\u201d condition. This enables you to use account-level audiences as a \u201cpassthrough\u201d for account-level traits.\n\nFor example, you may wish to create an audience which selects all admin-level users associated with accounts in the software industry (for example, user.role=Admin AND account.industry=Software). To accomplish this, you would:\n\nCreate an account-level audience containing a single account-level custom trait condition that references the account\u2019s industry(industry=Software).\nCreate a user-level audience with two conditions:\nA custom trait condition referencing the user\u2019s role (role=Admin)\nAn audience membership condition referencing the account-level audience you just created (software_industry_audience=True)\nKnown limitations of account-level audiences\nUnlike user-level audiences, which are computed in real time, account-level audiences are computed on a batched basis. Segment computes account-level audiences roughly every eight hours, but compute times can fluctuate based on system load.\nAccount-level audiences don\u2019t respect the context.groupId property on Track calls. If users are associated with multiple accounts (through multiple group calls), the entire collection of a user\u2019s events is considered when evaluating user-level event conditions (not just those events which are tagged with a matching groupId). This can lead to unexpected results where a user\u2019s events triggered in the context of one account lead to another account incorrectly matching an account-level audience.\nThe identity breakdown report (displayed in the audience builder for user-level audiences) is not available for account-level audiences.\n\nIf you find that these limitations impede your ability to use account-level audiences, contact friends@segment.com with details about your use case.\n\nThis page was last modified: 12 Aug 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nEnable account-level audiences\nAccount-level audience conditions\nAccount-level computed and SQL traits\nConnect account-level audiences, computed traits, and SQL traits to destinations\nUse account-level traits in user-level audiences\nKnown limitations of account-level audiences\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nAn introduction to Segment\nAn introduction to Segment\n\nWelcome! This page is a high-level introduction to the Segment Platform, including what it does and how. (If you\u2019re looking for detailed information about architecture, setup, or maintenance, you can skip ahead.)\n\nWhat is Segment?\n\nSegment is a Customer Data Platform (CDP), which means that it provide a service that simplifies collecting and using data from the users of your digital properties (websites, apps, etc). With Segment, you can collect, transform, send, and archive your first-party customer data. Segment simplifies the process of collecting data and hooking up new tools, allowing you to spend more time using your data, and less time trying to collect it.\n\nYou can also enrich the customer data you collect by connecting data from your other tools, and then aggregate it to monitor performance, inform decision-making processes, and create uniquely customized user experiences. You can also use Unify, Segment\u2019s identity resolution tool, to unify data from individual users to gain a wholistic understanding of their actions.\n\nSegment University: How Segment Works\n\nCheck out how to get started with Segment in Segment University! (Must be logged in to access.)\n\nWhat does it do?\n\nIn its very simplest form, Segment generates messages about what\u2019s happening in your site or app, then translates the content of those messages into different formats for use by other tools (called \u2018Destinations\u2019), and transmits messages to those tools. The Segment servers also archive a copy of the data, and can send data to your storage systems (such as databases, warehouses, or bulk-storage buckets).\n\nHow does Segment work?\n\nSegment\u2019s libraries generate and send messages to the tracking API in JSON format. Segment provides a standard structure for the basic API calls, along with a recommended JSON structure (also known as the \u2018Spec\u2019, a type of schema) that helps keep the most important parts of your data consistent, while allowing great flexibility in what other information you collect and where.\n\nSegment Messages\n\nWhen you implement Segment, you add the Segment code to your website, app, or server, which generates messages based on specific triggers you define. At its very simplest, this code can be a snippet that you copy and paste into the HTML of a website to track page views. It can also be as complex as Segment calls embedded in a React mobile app to send messages when the app is opened or closed, when the user performs different actions, or when time based conditions are met (for example \u201cticket reservation expired\u201d or \u201ccart abandoned after 2 hours\u201d).\n\nSegment has Sources and Destinations. Sources send messages into Segment (and other tools), while Destinations receive messages from Segment.\n\nAnatomy of a Segment message\n\nThe most basic Segment message requires only a userID or anonymousID; all other fields are optional to allow for maximum flexibility. However, a normal Segment message has three main parts: the common fields, the \u201ccontext\u201d object, and the properties (if it\u2019s an event) or traits (if it\u2019s an object).\n\nThe common fields include information specific to how the call was generated, like the timestamp and library name and version. The fields in the context object are usually generated by the library, and include information about the environment in which the call was generated: page path, user agent, OS, locale settings, etc. The properties and traits are optional and are where you customize the information you want to collect for your implementation.\n\nAnother common part of a Segment message is the integrations object, which you can use to explicitly filter which destinations the call is forwarded to. However this object is optional, and is often omitted in favor of non-code based filtering options.\n\nSegment Sources\n\nSegment provides several types of Sources which you can use to collect your data, and which you can choose among based on the needs of your app or site. For websites, you can embed a library which loads on the page to create the Segment messages. If you have a mobile app, you can embed one of Segment\u2019s Mobile libraries, and if you\u2019d like to create messages directly on a server (if you have, for example a dedicated .NET server that processes payments), there are several server-based libraries that you can embed directly into your backend code. (You can also use cloud-sources to import data about your app or site from other tools like Zendesk or Salesforce, to enrich the data sent through Segment.)\n\nDestinations\n\nOnce Segment generates the messages, it can send them directly to the Segment servers for translation and forwarding on to the Destinations you\u2019re using, or it can make calls directly from the app or site to the APIs of your Destination tools. Which of these methods you choose depends on which Destinations you\u2019re using and other factors. You can read more about these considerations in our Connection Modes documentation\n\nWhat happens next?\n\nMessages sent to the Segment servers using the tracking API can then be translated and forwarded on to Destination tools, inspected to make sure that they\u2019re in the correct format or schema, inspected to make sure they don\u2019t contain any Personally Identifying Information (PII), aggregated to illustrate overall performance or metrics, and archived for later analysis and reuse.\n\nWhat are the other parts of the Segment platform?\n\nIn addition to Connections (our core message routing product) Segment offers additional features to help your organization do more with its data, and keep data clean, consistent, and respectful of end-user privacy. The following products are available:\n\nPrivacy Portal - available to all users - Inspect incoming messages to identify PII, classify it by its riskiness, and decide how it\u2019s handled and which tool may use it.\nProtocols - create a unified schema for all the data you collect, coordinate implementation to keep it consistent with that schema, and make sure your data always arrives in the right format and block and alert when it doesn\u2019t.\nEngage - identify groups of users (\u201caudiences\u201d) based on behavior or other metrics calculated from your data, and send these groups to Destinations, identity resolution\nWhere can I learn more?\n\nI\u2019m a Segment Developer\n\nI\u2019m a Segment Data user\n\nI\u2019m a Segment Workspace administrator\n\nWhat\u2019s a Workspace?\n\nA workspace is a group of sources that can be administered and billed together. Workspaces help companies manage access for multiple users and data sources. Workspaces let you collaborate with team members, add permissions, and share sources across your whole team using a shared billing account.\n\nWhen you first log in to your Segment account, you can create a new workspace, or choose to log into an existing workspace if your account is part of an existing organization.\n\nWhat\u2019s a Source?\n\nIn Segment, you create a source (or more than one!) for each website or app you want to track. We highly recommend creating a Source for each unique source of data (each site, app, or server), though this isn\u2019t required.\n\nSources belong to a workspace, and the URL for a source looks something like this: https://segment.com/<my-workspace>/sources/<my-source-name>/\n\nYou can create new sources using the button in the workspace view. Each source you create has a write key, which is used to send data to that source. For example, to load\u00a0analytics.js, the Segment JavaScript library\u00a0on your page, the snippet on the\u00a0Quickstart Guide\u00a0includes:\n\nanalytics.load(\"YOUR_WRITE_KEY\");\n\nWhat\u2019s a Destination?\n\nDestinations are business tools or apps that you can connect to the data flowing through Segment. Some of Segment\u2019s most popular destinations are Google Analytics, Mixpanel, Kissmetrics, Customer.io, Intercom, and KeenIO.\n\nAll of these tools run on the same data: who are your customers and what are they doing? But each tool requires that you send that data in a slightly different format, which means that you\u2019d have to write code to track all of this information, again and again, for each tool, on each page of your app or website.\n\nEnter Segment. Do it once.\n\nSegment eliminates this process by introducing an abstraction layer. You send your data to Segment, and Segment understands how to translate it so we can send it along to any destination. You enable destinations from the catalog in the Segment App, and user data immediately starts flowing into those tools. No extra code required!\n\nSegment supports many categories of destinations, from advertising to marketing, email to customer support, CRM to user testing, and even data warehouses. You can view a complete list of available\u00a0destinations\u00a0or check out the\u00a0destination page\u00a0for a searchable list broken down by category.\n\nWhat\u2019s a Warehouse?\n\nA warehouse is a central repository of data collected from one or more sources. This is what commonly comes to mind when you think about a relational database: structured data that fits neatly into rows and columns.\n\nIn Segment, a Warehouse is a special type of destination. Instead of streaming data to the destination all the time, we load data to them in bulk at regular intervals. When we load data, we insert and update events and objects, and automatically adjust their schema to fit the data you\u2019ve sent to Segment.\n\nThis page was last modified: 28 Mar 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nWhat is Segment?\nWhat does it do?\nHow does Segment work?\nWhat happens next?\nWhat are the other parts of the Segment platform?\nWhere can I learn more?\nWhat\u2019s a Workspace?\nWhat\u2019s a Source?\nWhat\u2019s a Destination?\nWhat\u2019s a Warehouse?\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nTrait Activation\n/\nTrait Enrichment\nTrait Enrichment\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nUse Trait Enrichment to access Segment profile traits when you sync Audiences and Journeys to Destinations and Destination Functions. With Trait Enrichment, you can use custom, SQL, computed, and predictive traits to enrich the data you map to your destinations.\n\nSet up Trait Enrichment\n\nUse the following steps to set up Trait Enrichment with Audiences or Journeys.\n\nThe setup steps you\u2019ll use for Trait Enrichment depend on the type of destination you\u2019ve connected.\n\nFor Facebook Custom Audiences and Google Adwords, use these destination requirements.\nIf you\u2019re using Destination Actions, like Salesforce Marketing Cloud, Braze Actions, or Salesforce Actions, or Destination Functions, use the Destination Actions and Destination Functions setup steps.\nSet up Trait Enrichment with Audiences\n\nTo set up Trait Enrichment with Audiences:\n\nNavigate to Engage > Audiences.\nCreate a new Audience. From the Select Destination tab in the Audience builder, select your destination.\nIf you don\u2019t see any destinations to add, you\u2019ll need to add a destination to your Engage space first.\nFor existing audiences, select the connected Destination from the Audience Overview page.\nIn the Event Settings section, you\u2019ll see two options: Default Setup and Customized Setup. For Trait Enrichment, select Customized Setup.\nSet up Trait Enrichment with Journeys\n\nYou can set up Trait Enrichment with Journeys as you\u2019re creating or editing your journey in the builder.\n\nFrom a journeys step, select the destination you\u2019re going to use with Trait Enrichment.\nOn the Connection Settings tab, select Customized Setup and use the corresponding steps below to customize the way data is sent to your destination by creating identifier and trait mappings.\nDefault setup\n\nDefault setup uses default Segment Destination settings without Trait Enrichment. To use the default settings, select Default Setup, then click Save to resume building your audience or journey.\n\nYou can customize event settings at any time.\n\nCustomized setup\n\nWith Customized setup, you can choose which traits you want to map to your destination or destination function.\n\nClick Customized Setup, then click Add Trait.\nSelect all traits you want to sync and click Save.\nUse the Segment column to select traits from the Segment Spec.\nUse the Destination column to select which traits you want to map to in your destination. By default, Segment attempts to find traits with matching names.\nClick Save and finish building your audience or journey.\n\nSegment sends traits you select for enrichment in the traits object in Identify calls (traits.trait_1, traits.trait_2), and as properties in the properties object in Track calls (properties.trait_1, properties.trait_2).\n\nHere\u2019s an example Identify call payload with traits in the traits object:\n\n{\n  \"messageId\": \"segment-test-message-uozjhr\",\n  \"timestamp\": \"2024-02-22T22:11:15.595Z\",\n  \"type\": \"identify\",\n  \"email\": \"test@example.org\",\n  \"projectId\": \"5kXbpcJxms8WWaEdQUkRWc\",\n  \"traits\": {\n    \"trait1\": 1,\n    \"trait2\": \"test\",\n    \"trait3\": true\n  },\n  \"userId\": \"test-user-cq8idf\"\n}\n\n\nAnd here\u2019s an example Track call payload with properties in the properties object:\n\n{\n  \"messageId\": \"segment-test-message\",\n  \"timestamp\": \"2024-02-22T22:10:13.640Z\",\n  \"type\": \"track\",\n  \"email\": \"test@example.org\",\n  \"projectId\": \"5kXbpcJxms8WWaEdQUkRWc\",\n  \"properties\": {\n    \"property1\": 1,\n    \"property2\": \"test\",\n    \"property3\": true\n  },\n  \"userId\": \"test-user-1tgg9e\",\n  \"event\": \"Segment Test Event Name\"\n}\n\nDestination requirements\n\nThe following are a list of destination-specific requirements for using Trait Enrichment.\n\nFacebook Custom Audiences\n\nYou can only sync the following traits to Facebook:\n\nemail\ncontext.device.advertisingId\nfirstName\nlastName\nphone\ngender\nbirthYear\nbirthMonth\nbirthday\naddress.state\naddress.city\naddress.postalCode\naddress.country\n\nEach trait you select must map to a Facebook key.\n\nGoogle Ads Remarketing Lists\n\nemail is required when syncing to Google, because every payload will send email (as an identifier) downstream in addition to phone number.\n\nAdditionally, you can only map one trait per audience to Google as a phone number.\n\nDestination Actions and Destination Functions setup\n\nIf you\u2019re using Destination Actions or Destination Functions, use the following steps to set up Trait Enrichment.\n\nNavigate to Engage > Engage settings.\nSelect the Destinations tab, then click + Add Destination. Trait Activation supports all Destination Actions and Destination Functions.\nSelect your destination or function.\nNavigate to Engage > Audiences, and click + New audience.\nFrom the Select Destinations screen in the Audience builder, select your destination.\nConfirm that Send Track or Send Identify is toggled on.\nTrait Enrichment supports Track and Identify calls. Follow the corresponding destination instructions to determine which event you\u2019ll need.\nSelect Customized Setup.\nSelect Add Trait. Then, select the traits you want to sync and click Save.\nConfigure mappings in your destination\n\nAfter you add traits, configure how your selected traits will map to your destination.\n\nKeep your Engage Audience open in a separate tab, as you\u2019ll need to return.\n\nNavigate to Connections > Destinations and select your destination.\nFrom the Destination overview screen, select the Mappings tab.\nClick + New Mapping.\nAll actions in Destination Actions can receive traits you configure with Trait Activation.\nLocate the Select mappings section to confirm the default field mappings match the traits in your custom setup.\nTo update a trait field mapping for Identify calls, click on a field, and in the dropdown search bar enter traits. followed by your trait (for example, traits.trait_1). Segment sends traits you select for enrichment as traits in the traits object.\nTo update a trait field mapping for Track calls, click on a field, and in the dropdown search bar enter properties. followed by your trait (for example, properties.trait_1). Segment sends traits you select for enrichment as properties in the properties object.\nClick Use as an event variable to add your trait.\nClick Save and navigate back to Engage to finish building your Audience.\nBest practices\n\nFor best results with Trait Enrichment, Segment recommends:\n\nUsing Trait Enrichment with new audiences.\nUsing smaller audiences for real-time use cases, as data delivery is slower for large audiences.\nFAQs\nWhat\u2019s the difference between Trait Enrichment and ID Sync?\n\nTrait Enrichment lets you map the traits data you\u2019ve collected with Engage to use when syncing audiences and Journeys to destinations.\n\nID Sync lets you map the identities data gathered for a profile for use when syncing audiences and Journeys to destinations.\n\nHow do syncs differ between audiences with Trait Enrichment and audiences without Trait Enrichment?\n\nTrait Enrichment on existing audience destinations doesn\u2019t automatically resync the entire audience. Only new data flowing into Segment will adhere to the new trait criteria.\n\nCan I edit mappings once Segment syncs the audience?\n\nYes, you can edit mappings in the Destination Mappings tab at any time. However, changes will only take place in subsequent audience syncs or in new audiences connected to the destination.\n\nDoes Trait Enrichment guarantee match rate improvements?\n\nNo. Segment doesn\u2019t guarantee match rate improvements with Trait Enrichment. Match rates depend on data quality.\n\nThis page was last modified: 01 Aug 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSet up Trait Enrichment\nDestination requirements\nDestination Actions and Destination Functions setup\nBest practices\nFAQs\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nEngage Audiences Overview\nEngage Audiences Overview\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nAudiences let you group users or accounts based on event behavior and traits that Segment tracks.\n\nYou can build Audiences from core tracking events, traits, and computed traits. You can then sync Audiences to hundreds of Destinations or access them with the Profile API.\n\nBuilding an Audience\n\nYou can build an Audience from existing events, traits, computed traits, or other Audiences.\n\nThe Include Anonymous Users checkbox determines which external IDs need to exist on a profile for Segment to include the user in the audience:\n\nInclude Anonymous Users not selected:\u00a0user_id, email, android.idfa,\u00a0or\u00a0ios.idfa\nInclude Anonymous Users selected:\u00a0user_id, email, android.idfa, ios.idfa, or\u00a0anonymous_id\n\nEditing an audience before the initial backfill is complete can create technical errors.\n\nAudience Keys\n\nAvoid using the same Audience Key twice, even if you\u2019ve deleted the original Audience.\n\nEvents\n\nYou can build an Audience from any events that are connected to Engage, including Track, Page, and Screen calls. You can use the property button to refine the audience on specific event properties, as well.\n\nThe Audience builder doesn\u2019t return every property value in the Constant value or Traits drop-downs. Segment displays a portion of values from the incoming data stream. However, if you don\u2019t see the value you\u2019re looking for, you can manually enter it.\n\nSelect and not who to indicate users that have not performed an event. For example, you might want to look at all users that have viewed a product above a certain price point but not completed the order.\n\nYou can also specify two different types of time-windows, within and in between. The within property lets you specify an event that occurred in the last x number of days, while in between lets you specify events that occurred over a rolling time window in the past. A common use case is to look at all customers that were active 30 to 90 days ago, but have not completed an action in the last 30 days.\n\nBuilding audiences with traits\n\nYou can also build audiences using Custom Traits, Computed Traits, SQL Traits, and audience memberships.\n\nCustom Traits\n\nCustom traits are user or account-specific attributes. You can collect these traits from your apps when a user completes a form or signs up using an Identify call. You can view these traits in the Profile explorer. Custom Traits are mutable and update to the latest value seen by the user\u2019s Identify events.\n\nWhen you delete an audience that previously generated Identify events, the data for the audience key stays attached to profiles that entered the audience. This data then becomes visible in Segment as a custom trait.\n\nComputed Traits\n\nYou can also use computed traits in an audience definition. For example, you can create a total_revenue computed trait and use it to generate an audience of big_spender customers that exceed a certain threshold.\n\nEngage supports nested traits, but the Audience builder doesn\u2019t support accessing objects nested in arrays. When you send arrays of objects, they are flattened into strings. As a result, the same conditions that work on strings will work on the array. Within the builder, you can only use string operations like contains and does not contain to look for individual characters or a set of characters in the flattened array.\n\nSQL Traits\n\nWith SQL Traits, you can use data in your warehouse to build an audience. By running SQL queries on this warehouse data, you can import specific traits back into Segment to enhance both Segment audiences and the data you send to downstream destinations.\n\nAudience memberships\n\nWhen you build an audience based on audience membership, you use existing audiences as criteria for creating new audiences. You can include or exclude profiles based on their membership in other audiences, allowing you to generate more specific audience segments.\n\nTime comparison\n\nYou can use the following time comparison operators in your audience definition:\n\nbefore date\nafter date\nwithin last\nwithin next\nbefore last\nafter next\n\nOnly ISO timestamps can be used with these operators. Additionally, these time comparison operators exclusively apply to custom traits. If the timestamp is not a valid ISO timestamp (for example, a trailing Z is missing), Segment won\u2019t process the audience in real-time. Learn more about real-time compute compared to batch.\n\nNote: Timezones seen in the UI are based on your local timezone, but are converted to UTC on the backend.\n\nFunnel Audiences\n\nFunnel audiences allow you to specify strict ordering between two events. This might be the case if you want an event to happen or not happen within a specific time window, as in the following example:\n\nDynamic property references\n\nDynamic Property references give you more flexibility over funnel audiences. Instead of specifying a constant value in both events, like product_id = 123 for both Product Viewed and Order Completed events, you can specify that a child event references an event property of a parent event. You can also compare an event property to a trait variable.\n\nAccount-level audiences\n\nIf you have a B2B business, you might want to build an Audience of accounts. You can use both account-level traits that you\u2019ve sent through the Group call, or user-level traits and events. For example, you might want to re-engage a list of at-risk accounts defined as companies which are on a business tier plan and where none of the users in that account have logged in recently. When incorporating user-level events or traits, you can specify None of the users, Any users, or All users.\n\nSee Account-level Audiences for more information.\n\nSend audiences to destinations\n\nYou can send audiences and computed traits to third-party services in Segment\u2019s Destinations catalog.\n\nFor step-by-step instructions on how to connect an audience to a destination, see Send Audience Data to Destinations.\n\nUnderstanding compute times\n\nBecause a number of factors (like system load, backfills, or user bases) determine the complexity of an Audience, some compute times take longer than others.\n\nAs a result, Segment recommends waiting at least 24 hours for an Audience to finish computing before you resume working with the Audience.\n\nFrom the Overview page, you can view Audience details including the current compute status and a progress bar for real-time and batch Audiences. Engage updates the progress bar and status for real-time computations approximately every 10 minutes.\n\nViewing compute progress\n\nWhen you create a real-time Audience, you\u2019ll see a progress bar, computed percentage, and status updates. For existing Audiences that you edit, Engage displays the compute status but not the progress bar or percentage.\n\nEngage syncs the Overview page for an individual audience more frequently than the Engage Audiences page (Engage > Audiences). As a result, you might see temporary discrepancies in Audience details, such as user counts, between these two pages.\n\nRefresh real-time Audiences and Traits\n\nFor real-time computations, you can click Refresh Audience or Refresh Trait to update user counts, status, and compute progress.\n\nCompute statuses\n\nEngage displays the following compute statuses for Audiences and Traits.\n\nReal-time computations\nCOMPUTATION STATUS\tDESCRIPTION\nPreparing\tEngage is preparing the computation.\nComputing\tEngage is computing the Audience or Trait.\nLive\tThe Audience or Trait is live. Users will enter in real-time as they meet entry criteria.\nDisabled\tThe Audience or Trait is disabled.\nFailed\tThe computation was cancelled or failed to compute. Please contact Segment support.\nBatch computations\nCOMPUTATION STATUS\tDESCRIPTION\nPreparing\tEngage is preparing the computation.\nComputing\tEngage is computing the Audience or Trait.\nLive\tThe Audience or Trait is up-to-date, based on the most recent sync cadence. When you edit a batch Audience or Trait, Engage displays the compute status as Live and incorporates your edits in the next scheduled sync.\nNot Computing\tEngage displays this status when there are no destinations connected or Compute without connected destinations isn\u2019t selected.\nDisabled\tThe Audience or Trait is disabled.\nFailed\tThe computation was cancelled or failed to compute. Please contact Segment support.\nReal-time compute compared to batch\n\nReal-time Compute allows you to update traits and Audiences as Segment receives new events. Real-time Compute unlocks exciting use cases:\n\nIntra-Session App Personalization: change your app experience with personalized onboarding, product recommendations, and faster funnels based on a user entering and exiting an audience.\nInstant Messaging: Trigger messages in email, live chat, and push notifications instantly, to deliver immediate experiences across channels.\nOperational Workflows: Supercharge your sales and support teams by responding to customer needs faster, based on the latest understanding of a user.\n\nReal-time Compute doesn\u2019t support time window conditions. Segment creates Audiences using time window conditions as batch computations. Additionally, Segment creates Funnel Audiences as batch computations.\n\nTo create a new Audience or Trait:\n\nGo to your Computed Traits or Audiences tab in Engage and select Create.\n\nConfigure and preview your Audience or Trait.\nA lightning bolt next to Realtime Enabled indicates that the computation updates in real-time.\nBy default, Segment queries all historical data to set the current value of the computed trait and Audience. Backfill computes historical data up to the point of audience creation. You can uncheck Include Historical Data to compute values for the Audience or trait without historical data. With backfill disabled, the trait or Audience only uses the data that arrives after you create it.\nSelect destinations to connect, then review and create your Audience or Trait.\n\nWhile Engage is computing, use the Audience Explorer to see users or accounts that enter your Audience. Engage displays the Audience as computing in the Explorer until at least one user or account enters.\n\nFacebook Custom Audiences, Marketo Lists, and Adwords Remarking Lists impose rate limits on how quickly Segment can update an Audience. Segment syncs at the highest frequency allowed by the tool, which is between one and six hours.\n\nReal-time computations connected to List destinations use a separate sync process that can take 12-15 hours to send changes present in the most recent computation.\n\nEditing Realtime Audiences and Traits\n\nEngage supports the editing of realtime Audiences and Traits, which allows you to make nuanced changes to existing Traits and Audiences in situations where cloning or building from scratch may not suit your use case.\n\nTo edit a realtime Trait or Audience, follow these steps:\n\nIn your Engage Space, select the Computed Traits or Audiences tab.\nSelect the realtime Audience or Trait you want to edit.\nSelect the Builder tab and make your edits.\nPreview the results, then select Create Audience to confirm your edits.\n\nEngage then processes your realtime Audience or Trait edits. While the edit task runs, the audience remains locked and you can\u2019t make further changes. Once Engage incorporates your changes, you\u2019ll be able to access your updated Audience or Trait.\n\nIf your audience includes historical data (Historical Backfill is enabled), editing an audience creates a new backfill task. The backfill task, and therefore the edit task, take longer to process if the audience is connected to a destination with rate limits. Rate-limited destinations dictate how fast Engage can backfill. View a list of rate-limited destinations.\n\nIt is not possible to edit an audience to convert it from real-time to batch, or vice-versa. If the computation type needs to be changed, you will need to recreate the audience with the appropriate conditions.\n\nYou can\u2019t edit an audience to include anonymous users. If you need to include anonymous profiles, recreate the audience with the appropriate conditions\n\nAccess your Audiences using the Profiles API\n\nYou can access your Audiences using the Profile API by querying the /traits endpoint. For example, you can query for high_value_user property with the following GET request:\n\nhttps://profiles.segment.com/v1/spaces/<workspace_id>/collections/users/profiles/email:alex@segment.com/traits?limit=100&include=high_value_user\n\n\nThe query would return the following payload:\n\n    {\n        \"traits\": {\n            \"high_value_user\": true\n        },\n        \"cursor\": {\n            \"url\": \"\",\n            \"has_more\": false,\n            \"next\": \"\",\n            \"limit\": 100\n        }\n    }\n\n\nYou can read the full Profile API docs to learn more.\n\nDownload your Audience as a CSV file\n\nYou can download a copy of your Audience by visiting the Audiences overview page.\n\nNavigate to Engage > Audiences.\nSelect the Audience you\u2019d like to download as a CSV, then click Download CSV.\nSelect the data fields that you\u2019d like to include in your CSV as columns.\nYour CSV will contain all users in this audience with the selected fields. You can filter by External ID, SQL trait, Computed Trait, and Custom Trait.\nClick Next.\nBefore you can download the CSV, you\u2019ll need to generate it. There are two different options for formatting:\nFormatted: Displays external IDs and traits as distinct columns.\nUnformatted: Contains the following columns: a user/account key, a JSON object containing the external IDs (optional, if selected), and a JSON object containing the traits (optional, if selected).\nClick Generate CSV.\n\nOnce Segment generates the CSV, you can download the file directly. You\u2019ll receive an email notification of the CSV completion, with a URL to the Audience overview page.\n\nNote the following limits for the CSV downloader:\n\nYou can\u2019t download more than one CSV for the same audience at the same time.\nYou can only generate one CSV every five minutes.\nEach CSV represents a snapshot at a given point in time that references the data from the audience\u2019s most recent computational run. This applies to both real time and batch audiences, as the CSV is not updated in real time. To locate the snapshot\u2019s given point of time, click on the Download CSV button, and the popup modal will contain an information icon \u2139\ufe0f, which when hovered over will reveal the snapshot\u2019s details.\n\nGenerating a CSV can take a substantial amount of time for large audiences. After you generate the CSV file, leave the modal window open while Segment creates the file. (If the audience recalculates between when you click Generate and when you download the file, you might want to regenerate the file. The CSV is a snapshot from when you clicked Generate, and could be outdated.)\n\nYou can\u2019t add account traits and identifiers using the CSV downloader with account level audiences. This is because every row listed in the CSV file is a user, and since account traits and identifiers only exist on accounts, they wouldn\u2019t exist as a user\u2019s custom trait and appear on the CSV.\n\nIdentifier Breakdown\n\nThe audience summary is a breakdown of the percentages of external_ids of users in the audience. These are the default IDs that Segment includes in the Identity resolution configuration. Segment displays the percentage of the audience with each identifier, which you can use to verify the audience size and profiles are correct. The update of identifier breakdowns on profiles doesn\u2019t occur in real time.\n\nThe Identifier Breakdown won\u2019t show custom IDs included in the Identity resolution configuration. Segment only displays external IDs in the breakdown.\n\nFAQ\nWhy do I get a different user count when I use $ on a field?**\n\nSegment recommends using the $ operator when you deal with array properties. However, the $ causes logical conditions to apply independently to each array entry independently. As a result, you\u2019ll get more accurate results by using the equals one of condition:\n\nHow do I populate multiple items off a list for an equals one of condition? **\n\nThe audience builder accepts CSV and TSV lists.\n\nWhy am I receiving the error \u201cThe audience would create a cycle by referencing another audience\u201d?\n\nThis error occurs when creating audiences that reference each other, meaning audience X refers to audience Y in its trigger condition, and later you attempt to modify audience Y\u2019s trigger condition to refer back to audience X. To avoid this error, ensure that the audiences do not reference each other in their conditions.\n\nHow does the historical data flag work?\n\nIncluding historical data lets you take past information into account. You can only exclude historical data for real-time audiences. For batch audiences, Segment includes historical data by default.\n\nThis page was last modified: 22 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nBuilding an Audience\nSend audiences to destinations\nUnderstanding compute times\nReal-time compute compared to batch\nAccess your Audiences using the Profiles API\nDownload your Audience as a CSV file\nIdentifier Breakdown\nFAQ\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nContent\n/\nWhatsApp Template\nWhatsApp Template\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE PREMIER \u2713\n?\n\nEngage Premier entered an End of Sale (EOS) period effective June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:\n\nTwilio Marketing Campaigns\n\nPreferred ISV Partners:\n\nAirship Blog\nBloomreach Blog\nBraze Blog\nInsider Blog\nKlaviyo Blog\nTwilio Engage Foundations Documentation\n\n\nWith Twilio Engage, you can build personalized WhatsApp templates to store and use throughout marketing campaigns.\n\nThis page explains how to create, build, and submit WhatsApp templates for approval.\n\nWhatsApp Template Approval\n\nWhatsApp templates must be approved by Meta before you can use them in campaigns.\n\nWhatsApp template types\n\nYou can choose between three WhatsApp template types:\n\nMedia, which contain media and text content\nText, which contain text content of up to 1600 characters\nCall to action, which contain text content and phone or website buttons\nBuild a WhatsApp message template\n\nBefore you begin\n\nIf you\u2019re new to Engage Premier, you\u2019ll need to sign up for the Twilio Content Editor beta before you can use WhatsApp templates.\n\nFollow these steps to build a WhatsApp template:\n\nNavigate to Engage > Content and click Create template.\nSelect WhatsApp, then click Configure.\nEnter a template name and select your template\u2019s language.\nSelect your template\u2019s content type, then click Next.\nFor text templates, enter your message\u2019s text in the Body field and add any desired merge tags.\nFor media templates, enter your message\u2019s text in the Body field, add the media URL, then add any desired merge tags.\nFor call to action templates, enter your message\u2019s text in the Body field, then add buttons for a phone number or website.\nOnce you\u2019ve finished adding your template\u2019s content, click Save and submit for WhatsApp approval or Save.\nIf you choose to submit your template for approval, confirm by clicking Submit.\nSegment confirms that your template was saved or saved and submitted for approval.\nSubmit a saved template for approval\n\nIf you saved your template without submitting it for approval, it won\u2019t be available for use in campaigns until you submit it for approval.\n\nFollow these steps to submit saved templates for approval:\n\nNavigate to Engage > Content > WhatsApp.\nIn the WhatsApp Templates table, select the template you want to submit for approval.\nReview your template. If you\u2019re ready to submit it for approval, select Save and submit for WhatsApp approval.\nIn the Submit for WhatsApp review overlay, select Submit.\nSegment then confirms that your template was saved and submitted for approval.\nPersonalize with merge tags\n\nYou can personalize your WhatsApp templates with merge tags based on profile traits.\n\nTo include merge tags in your template, click + Add merge tag in the template builder and select the profile trait(s) you want to include in your message.\n\nSegment displays the merge tag in the body as a numerical value surrounded by curly braces, like {{1}}. When a susbcriber triggers your WhatsApp campaign, Segment will replace the merge tag with the specific value associated with that subscriber\u2019s profile.\n\nIf a merge tag doesn\u2019t apply to a subscriber, Engage will use the content you enter into the Default content field.\n\nTo learn more about profile traits, visit Segment\u2019s Computed Traits and SQL Traits documentation.\n\nTemplate approvals\n\nMeta must first review and approve your WhatsApp template before you can use it in a campaign. Meta approves most templates in under an hour, but some approvals can take up to 48 hours. Keep this time frame in mind if you plan to send time-sensitive campaigns.\n\nFor more on the template approval process, view recommendations and best practices for creating WhatsApp Message Templates.\n\nNext steps\n\nOnce your template has been approved, you can create a Journey to send a WhatsApp campaign.\n\nThis page was last modified: 15 Jul 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nWhatsApp template types\nBuild a WhatsApp message template\nSubmit a saved template for approval\nPersonalize with merge tags\nTemplate approvals\nNext steps\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nAudiences, Journeys, and Broadcasts\nAudiences, Journeys, and Broadcasts\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nAudiences, Journeys, and Broadcasts are fundamental to Twilio Engage and let you segment your users, send them personalized content, and show them ads from platforms like Facebook or Google.\n\nIn this guide, you\u2019ll learn how to choose between an Audience, a Journey, and a Broadcast for a number of marketing use cases across the customer lifecycle.\n\nBack to basics\n\nFirst, consider the following definitions for an Audience, Journey, and Broadcast.\n\nAudience\n\nIn Engage, an Audience is a group of users that share certain characteristics. When you create an Audience, you group users who meet certain conditions, like having performed an event or having a Computed Trait.\n\nOnce you\u2019ve created an Audience, you can sync it to marketing automation tools, ads platforms, analytics tools, or data warehouses. Depending on the Audience\u2019s conditions and connected Destination(s), Segment syncs the Audience\u2019s users in batches or in real time, as they meet the Audience\u2019s conditions.\n\nJourney\n\nA Journey is a logic-driven workflow that progresses users through steps based on conditions and time delays. You add users to a Journey with an entry condition, then users progress through the Journey\u2019s steps based on conditions you define during Journey setup.\n\nAs with Audiences, Segment can sync users to Destinations at designated points in the Journey. Unlike an Audience, a Journey can send users to Twilio Engage\u2019s native email and SMS channels.\n\nBroadcast\n\nA Broadcast is a one-time SMS or email campaign sent to a group of users. Whereas Segment continously updates Audience membership, Segment only calculates the users who will receive your Broadcast once. Marketers commonly use Broadcasts for newsletters, promotional campaign, and events.\n\nEngage and the customer lifecycle\n\nThe customer lifecycle provides a helpful framework for thinking about Audiences, Journeys, and Broadcasts.\n\nAudiences and Broadcasts tend to be most effective at the top of the customer lifecycle funnel, where brand awareness and discovery occurs.\n\nA Journey becomes a better option as customers progress down the funnel, where a more complex strategy involving messaging, social ads, and newsletters helps move customers closer to conversion.\n\nChoosing between Audiences, Journeys, and Broadcasts\n\nWith the customer lifecycle in mind, use the following table as a starting point for selecting an Audience or Journey for common marketing use cases:\n\nUSE CASE\tAUDIENCE, JOURNEY, OR BROADCAST\nI want to send email and SMS campaigns.\tJourney or Broadcast\nI want to send a one-time email or SMS campaign.\tBroadcast\nI only have one intended touchpoint.\tAudience or Broadcast\nI need branching logic.\tJourney\nI want to run A/B tests.\tJourney\nI want to re-target customers with the same ad.\tAudience\n\nWhile these suggestions will work for most use cases, you may need to consider other factors before you implement your own campaign. Asking the following questions will help you identify the right approach.\n\nOver the course of a campaign, how many touchpoints do I want to create?\n\nAudiences and Broadcasts work best for single, one-off messages or touchpoints. If you need a campaign with time delays and branching logic, opt for a Journey.\n\nFor example, an Audience works well if you want to show a single ad when a user abandons a cart. If, however, you want to show an ad, wait several days, then send the user an email if they\u2019ve not completed their purchase, go with a Journey.\n\nDo I want to use Engage Premier Channels like SMS and email?\n\nYou can message users with Engage Premier Channels. If you\u2019d like to send an SMS or email campaign to a customer, use a Journey.\n\nDo I need branching logic?\n\nCreate a Journey if you want to incorporate branching logic into your campaign.\n\nDo I want to conduct an A/B test or create a holdout group?\n\nA number of Journeys step types, like randomized splits, let you run experiments and test your campaigns. If you want to experiment with different groups, use a Journey.\n\nDo I want my customers to receive the same campaign more than once?\n\nWith Journeys, you can allow customers to re-enter a Journey they\u2019ve exited or restrict them to a one-time Journey.\n\nAudiences, on the other hand, admit users whenever they meet the Audience\u2019s criteria. For example, you may want to retarget a user with an ad whenever they view a page on your website. In this case, an Audience works well since the user can re-enter the Audience regardless of how many times they\u2019ve already done so.\n\nPutting it together\n\nWith this guidance in mind, take your next steps with Engage by learning how to build a Journey, work with Engage Audiences, and send a Broadcast.\n\nThis page was last modified: 12 Jun 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nBack to basics\nEngage and the customer lifecycle\nChoosing between Audiences, Journeys, and Broadcasts\nPutting it together\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSources Overview\nSources Overview\nWhat is a source?\n\nA source is a website, server library, mobile SDK, or cloud application which can send data into Segment. It\u2019s where your data originates. Add a source to collect data to understand who your customers are and how they\u2019re using your product. Create a source for each website or app you want to track. While it\u2019s not required that you have a single source for each server, site, or app,\u00a0you should create a source for each unique source of data.\n\nEach source you create has a write key, which is used to send data to that source. For example, to load\u00a0analytics.js, the Segment JavaScript library\u00a0on your page, the snippet on the\u00a0Quickstart Guide\u00a0includes:\n\nanalytics.identify('user_123', {\n  email: 'jane.kim@example.com',\n  name: 'Jane Kim'\n  });\n\n\nIf you don't see the source you're looking for in our catalog\n\nIf a tool is not listed as a supported source in Segment\u2019s catalog, then it is not possible to incorporate the integration out-of-the-box within a Segment workspace. However, as an alternative, you can use the HTTP API source to collect data from the tool\u2019s API. You can also use Functions to send or receive data from other tools.\n\nTypes of sources\n\nSegment has three types of sources:\n\nEvent streams\nCloud app sources\nReverse ETL\nEvent streams sources\n\nEvent streams sources collect data from your website or app to monitor user actions. These sources include website libraries, mobile, and server sources.\n\nSource Overview\n\nThe Source Overview page for an event stream source shows you a pipeline view of all events Segment receives from your source, events that failed on ingest, events that are filtered at the source level, and \u201celigible events\u201d, which are the events that will flow into your destinations. If you select one of the steps in the pipeline view, you can see a line chart that reflects the fluctuations in volume alongside a breakdown table that has more details about the events impacted by the selected step.\n\nPipeline view\n\nThe pipeline view shows each of the four steps Segment encounters when processing data from your source:\n\nEvents successfully received: All events that Segment received from your source.\nFailed on ingest: Events that failed at the Tracking API level. For more information about errors that might cause events to fail on ingest, see Delivery Overview\u2019s Troubleshooting documentation.\nFiltered at source: Events that were filtered out by source schema controls, Tracking Plans, or a common JSON schema.\nEligible events: Eligible events are the events that flow downstream to your Segment destinations. This value is read-only, but you can see the events that flow downstream to a particular destination using Delivery Overview.\n\nYou can use the time picker located on the Source Overview page to specify a time period (last 10 minutes, 1 hour, 24 hours, 7 days, 2 weeks, or a custom date range over the last two weeks) for which you\u2019d like to see data. Segment sets the time picker to show data for the last 24 hours by default.\n\nBreakdown table\n\nThe breakdown table displays three tabs, Event type, Event name, and App version.\n\nEvent type: The Segment Spec event type (Track call vs. Identify call, for example). This tab also contains a \u201c% change\u201d metric, which displays how the event counts differ from the last comparable time range, represented as a percentage.\nEvent name: The event name, provided by you or the source.\nApp version: The app/release version, provided by you or the source.\n\nEach of these tabs displays an event count, which is the total number of events that Segment received in a particular step.\n\nThe Unnamed or batched events under the Event Name tab is a collection of all identify and page/screen calls in the source.\n\nWebsite libraries\n\nAnalytics.js, the JavaScript library, is the most powerful way to track customer data from your website. If you\u2019re just starting out, Segment recommends it over server-side libraries as the simplest installation for any website.\n\nThe Analytics Quickstart Guide\n\nAnalytics and data collection is a very broad topic and it can be quite overwhelming. How do you get started?\n\nMobile\n\nSegment\u2019s Mobile SDKs are the best way to simplify your iOS, Android, and Xamarin app tracking. Try them over server-side sources as the default installation for any mobile app.\n\nAMP\nAndroid\nAndroid Wear\niOS\nKotlin\nReact Native\nSwift\nXamarin\n\nAnalytics-Flutter library\n\nThe Analytics-Flutter library is currently only available in pilot phase and is governed by Segment\u2019s First Access and Beta Preview Terms. If you\u2019d like to try out this library, access the Analytics-Flutter GitHub repository.\n\nServer\n\nSegment\u2019s server-side sources let you send analytics data directly from your servers. Segment recommends tracking from your servers when device-mode tracking (tracking on the client) doesn\u2019t work. Check out the guide on server-side tracking if you\u2019re not sure whether it makes sense for your use case.\n\nClojure\nGo\nJava\nNode.js\nPHP\nPython\nRuby\n.NET\n\nCloud-mode tracking\n\nServer-side data management is when tag sends data to the Segment servers, which then passes that data to the destination system.\n\nCloud app sources\n\nCloud app sources empower you to pull together data from all of your different third-party tools into a Segment warehouse or to your other enabled integrated tools. They send data about your users from your connected web apps. There are two types of Cloud Apps: Object cloud sources and Event cloud sources.\n\nComparing Cloud Sources\n\nWondering which cloud-apps send which types of data? Check out the Cloud Sources comparison.\n\nObject Cloud Sources\n\nThese Cloud App Sources can export data from its third party tool and import it directly into your Segment warehouse. Make sure you have a Segment warehouse enabled before you enable any of the following sources:\n\nFacebook Ads\n\nGoogle Ads\n\nHubSpot\n\nIntercom\n\nMailchimp\n\nMandrill\n\nMarketo\n\nSalesforce\n\nSalesforce Marketing Cloud\n\nSendGrid\n\nStripe\n\nTwilio\n\nZendesk\n\nEvent Cloud Sources\n\nThese Cloud App Sources can not only export data into your Segment warehouse, but they can also federate the exported data into your other enabled Segment integrations:\n\nActiveCampaign\n\nAircall\n\nAirship\n\nAlloy Flow\n\nAmazon S3\n\nAmplitude Cohorts\n\nAntavo\n\nAuthvia\n\nAutopilotHQ\n\nBeamer\n\nBlip\n\nBluedot\n\nBlueshift\n\nBraze\n\nCandu\n\nChatlio\n\nCleverTap\n\nCommandBar\n\nConfigCat\n\nCustomer.io\n\nDelighted\n\nDrip\n\nElastic Path\n\nElastic Path CX Studio\n\nFacebook Lead Ads\n\nFactual Engine\n\nFoursquare Movement\n\nFreshchat\n\nFriendbuy\n\nGladly\n\nGWEN Webhooks\n\nHerow\n\nIBM Watson Assistant\n\nInflection\n\nInsider\n\nIterable\n\nJebbit\n\nKlaviyo\n\nKlenty\n\nLaunchDarkly\n\nLeanplum\n\nListrak\n\nLiveLike (Source)\n\nLooker\n\nMailjet\n\nMailmodo\n\nMixpanel Cohorts\n\nMoEngage (Source)\n\nMoesif API Analytics\n\nNavattic\n\nNudgespot\n\nOne Creation\n\nOneSignal\n\nOneTrust\n\nPaytronix\n\nPendo\n\nProveSource\n\nPushwoosh Source\n\nQualtrics\n\nQuin AI\n\nRadar\n\nRefiner\n\nSelligent Marketing Cloud\n\nSendGrid Marketing Campaigns\n\nShopify (by Littledata)\n\nShopify - Powered by Fueled\n\nStatsig\n\nSynap\n\nUpollo\n\nUserGuiding\n\nVero\n\nVoucherify\n\nWhite Label Loyalty\n\nWorkRamp\n\nYotpo\n\nHTTP\n\nIf Segment doesn\u2019t have a library for your environment, you can send your data directly to the HTTP Tracking API. All of Segment\u2019s other sources and platforms use the HTTP API to work their magic behind the scenes.\n\nPixel\n\nSegment\u2019s Pixel Tracking API lets you track events from environments where you can\u2019t execute code, like tracking email opens.\n\nEVENT NAME\tDESCRIPTION\nEmail Delivered\tThe message has been successfully delivered to the receiving server.\nEmail Opened\tThe recipient has opened the HTML message. You need to enable Open Tracking for getting this type of event.\nEmail Link Clicked\tThe recipient clicked on a link within the message. You need to enable Click Tracking for getting this type of event.\nEmail Bounced\tThe receiving server could not or would not accept message.\nEmail Marked as Spam\tThe recipient marked message as spam.\nUnsubscribe\tThe recipient clicked on message\u2019s subscription management link.\nReverse ETL sources\n\nReverse ETL sources are data warehouses that enable you to use Reverse ETL to send data from your warehouse source to your destinations.\n\nReverse ETL supports these sources:\n\nBigQuery\nDatabricks\nPostgres\nRedshift\nSnowflake\n\nSegment is actively working on adding more sources. If you\u2019d like to request Segment to add a particular source, please note it on the feedback form.\n\nCreate a source\n\nTo create a source:\n\nNavigate to Connections and click Add Source.\nClick the Source you\u2019d like to add. Note: More than 80% of workspaces start by adding their JavaScript website.\nClick Add Source.\nEnter a name for your source as well as any information on the setup page.\nClick Add Source.\n\nOnce you\u2019ve created a source, the source is automatically enabled and can immediately receive events. You can review your new events in that source\u2019s Debugger tab.\n\nSources not connected to an enabled destination are disabled after 14 days\n\nIf your source is not connected to any destinations or is only connected to disabled destinations, Segment automatically disables this source after 14 days, even if the source is receiving events. Disabled sources will no longer receive data. You can view when Segment disables your destination in your workspace\u2019s Audit Trail as Event : Source Disabled with Actor : Segment. Workspace members receive an email notification before Segment disables your source so that your team has time to take action. If you would like to prevent this behavior in your workspace, fill out this Airtable form.\n\nOne source or multiple sources?\n\nSegment suggests that you create one source for each type of data you want to collect. For example, you might have one source for all of your website tracking and a different source for any mobile tracking. Creating one source per data type provides the following benefits:\n\nDebugger ease of use - mixing libraries/sources on a single API key means you\u2019re heavily reliant on filtering to actually test events\nFlexibility sending data to different projects - if you want to have different warehouse schemas, analytics projects, etc, having multiple sources would create this separation\nMore control - as your account grows with the number of destinations you enable, having separate sources allows you to have more control\nA source type cannot be changed after it is created. You must create a new source if you would like to use a different source type.\nLibrary tiers\n\nSegment has defined three tiers for libraries: Flagship, Maintenance, and Community. These tiers indicate the level of support, enhancements, and maintenance each library receives from Segment.\n\nThe criteria for assigning a library to a tier include its overall usage by customers and the availability of newer versions. Here\u2019s how Segment defines each tier:\n\nFlagship libraries offer the most up-to-date functionality on Segment\u2019s most popular platforms. Segment actively maintains Flagship libraries, which benefit from new feature releases and ongoing development and support.\nMaintenance libraries send data as intended but receive no new feature support and only critical maintenance updates from Segment. When possible, Segment recommends using a Flagship version of these libraries.\nCommunity libraries are neither managed nor updated by Segment. These libraries are available on GitHub under the MIT License for the open-source community to fork or contribute.\n\nIf a library falls into one of these tiers, you\u2019ll see the tier label at the beginning of the library\u2019s page.\n\nThis page was last modified: 18 Nov 2024\n\nFurther reading\nSources Catalog\n\nA list of the available sources on the Segment platform.\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nWhat is a source?\nTypes of sources\nEvent streams sources\nCloud app sources\nReverse ETL sources\nCreate a source\nLibrary tiers\n\nRelated content\n\nSources Catalog\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nStorage\n/\nWarehouses\n/\nWarehouse FAQs\nWarehouse FAQs\nCan I control what data is sent to my warehouse?\n\nYes. Customers on Segment\u2019s Business plan can choose which sources, collections, and properties sync to your data warehouse using Warehouse Selective Sync.\n\nSelective Sync helps manage the data Segment sends to each warehouse, allowing you to sync different sets of data from the same source to different warehouses.\n\nWhen you disable a source, collection or property, Segment no longer syncs data from that source. Segment won\u2019t delete any historical data from your warehouse. When you re-enable a source, Segment syncs all events since the last sync. This doesn\u2019t apply when a collection or property is re-enabled. Only new data generated after re-enabling a collection or property will sync to your warehouse.\n\nYou can also use the Integration Object to control whether or not data is sent to a specific warehouse.\n\nDon\u2019t send data to any Warehouse\nintegrations: {\n All: true,\n Warehouses: {\n    all: false\n }\n}\n\nSend data to all Warehouses\nintegrations: {\n All: false,\n Warehouses: {\n    all: true,\n }\n}\n\nSend data to specific Warehouses\nintegrations: {\n All: false,\n Warehouses: {\n    warehouseIds: [\"<id1>\", \"<id2>\"]\n }\n}\n\nCan we add, tweak, or delete some of the tables?\n\nYou have full admin access to your Segment Warehouse. However, don\u2019t tweak or delete Segment generated tables, as this may cause problems for the systems that upload new data.\n\nIf you want to join across additional datasets, feel free to create and upload additional tables.\n\nCan we transform or clean up old data to new formats or specs?\n\nThis is a common question if the data you\u2019re collecting has evolved over time. For example, if you used to track the event\u00a0Signup\u00a0but now track\u00a0Signed Up, you\u2019d probably like to merge those two tables to make querying simple and understandable.\n\nSegment does not have a way to update the event data in the context of your warehouse to retroactively merge the tables created from changed events. Instead, you can create a \u201cmaterialized\u201d view of the unioned events. This is supported in Redshift, Postgres, Snowflake, and others, but may not be available in all warehouses.\n\nProtocols customers can also use Transformations to change events at the source, which applies to all cloud-mode destinations (destinations that receive data from the Segment servers) including your data warehouse. Protocols Transformations offer an excellent way to quickly resolve implementation mistakes and help transition events to a Segment spec.\n\nNote: Transformations are currently limited to event, property and trait name changes, and do not apply to historical data.\n\nCan I change the data type of a column in the warehouse?\n\nYes. Data types are initially set up in your warehouse based on the first value that comes in from a source, but you can request data type changes by reaching out to Segment support for assistance.\n\nKeep in mind that Segment only uses general data types when loading data in your warehouse. Therefore, some of the common scenarios are:\n\nChanging data type from timestamp to varchar\nChanging data type from integer to float\nChanging data type from boolean to varchar\n\nMore granular changes (such as the examples below) wouldn\u2019t normally be handled by the Support team, thus they often need to be made within the warehouse itself:\n\nExpanding data type varchar(256) to varchar(2048)\nUpdating data type integer to bigint\nUpdating data type float to float8\nCan the data type definitions in Protocols be enforced in a warehouse schema?\n\nThe data type definitions in Protocols have no impact on the warehouse schema.\n\nHow do I find my source slug?\n\nYour source slug can be found in the URL when you\u2019re looking at the source destinations page or live debugger. The URL structure will look like this:\n\nhttps://segment.com/[my-workspace]/sources/[my-source-slug]/overview\n\nHow do I find my warehouse id?\n\nYour warehouse id appears in the URL when you look at the warehouse destinations page. The URL structure looks like this:\n\napp.segment.com/[my-workspace]/warehouses/[my-warehouse-id]/overview\n\nHow fresh is the data in Segment Warehouses?\n\nData is available in Warehouses within 24-48 hours, depending on your tier\u2019s sync frequency. For more information about sync frequency by tier, see Sync Frequency.\n\nReal-time loading of the data into Segment Warehouses would cause significant performance degradation at query time. To optimize for your query speed, reliability, and robustness, Segment guarantees that your data will be available in your warehouse within 24 hours. The underlying datastore has a subtle tradeoff between data freshness, robustness, and query speed. For the best experience, Segment needs to balance all three of these.\n\nWhat if I want to add custom data to my warehouse?\n\nYou can freely load data into your Segment Warehouse to join against your source data tables.\n\nThe only restriction when loading your own data into your connected warehouse is that you should not add or remove tables within schemas generated by Segment for your sources. Those tables have a naming scheme of\u00a0<source-slug>.<table>\u00a0and should only be modified by Segment. Arbitrarily deleting columns from these tables may result in mismatches upon load.\n\nIf you want to insert custom data into your warehouse, create new schemas that are not associated with an existing source, since these may be deleted upon a reload of the Segment data in the cluster.\n\nSegment recommends scripting any sort of additions of data you might have to warehouse, so that you aren\u2019t doing one-off tasks that can be hard to recover from in the future in the case of hardware failure.\n\nWhich IPs should I allowlist?\n\nSegment recommends enabling IP allowlists for added security. All Segment users with workspaces hosted in the US who use allowlists in their warehouses must update those allowlists to include the following ranges:\n\n52.25.130.38/32\n34.223.203.0/28\n\nUsers with workspaces in the EU must allowlist 3.251.148.96/29.\n\nWill Segment sync my historical data?\n\nSegment loads up to two months of your historical data when you connect a warehouse.\n\nFor full historical backfills you\u2019ll need to be a Segment Business plan customer. If you\u2019d like to learn more about our Business plan and all the features that come with it,\u00a0check out our pricing page.\n\nWhat do you recommend for Postgres: Amazon or Heroku?\n\nHeroku\u2019s simple set up and administration process make it a great option to get up and running quickly.\n\nAmazon\u2019s service has some more powerful features and will be more cost-effective for most cases. However, first time users of Amazon Web Services (AWS) will likely need to spend some time with the documentation to get set up properly.\n\nHow do I prevent a source from syncing to some or all warehouses?\n\nWhen you create a new source, the source syncs to all warehouse(s) in the workspace by default. You can prevent the source from syncing to some or all warehouses in the workspace in two ways:\n\nSegment app: When you add a source from the Workspace Overview page, deselect the warehouse(s) you don\u2019t want the source to sync to as part of the \u201cAdd Source\u201d process. All warehouses are automatically selected by default.\nPublic API: Send a request to the Update Warehouse endpoint to update the settings for the warehouse(s) you want to prevent from syncing.\n\nAfter a source is created, you can enable or disable a warehouse sync within the Warehouse Settings page.\n\nCan I be notified when warehouse syncs fail?\n\nIf you enabled activity notifications for your storage destination, you\u2019ll receive notifications in the Segment app for the fifth and 20th consecutive warehouse failures for all incoming data. Segment does not track failures on a per connection (\u2018source<>warehouse\u2019) basis. Segment\u2019s notification structure also identifies global issues encountered when connecting to your warehouse, like bad credentials or being completely inaccessible to Segment.\n\nTo sign up for warehouse sync notifications:\n\nOpen the Segment app.\nGo to Settings > User Preferences.\nIn the Activity Notifications section, select Storage Destinations.\nEnable Storage Destination Sync Failed.\nHow is the data formatted in my warehouse?\n\nData in your warehouse is formatted into schemas, which involve a detailed description of database elements (tables, views, indexes, synonyms, etc.) and the relationships that exist between elements. Segment\u2019s schemas use the following template:\n<source>.<collection>.<property>, for example, segment_engineering.tracks.user_id, where source refers to the source or project name (segment_engineering), collection refers to the event (tracks), and the property refers to the data being collected (user_id). Note: It is not possible to have different sources feed data into the same schema in your warehouse. While setting up a new schema, you cannot use a duplicate schema name.\n\nSchema data for Segment warehouses is represented in snake case.\n\nFor more information about Warehouse Schemas, see the Warehouse Schemas page.\n\nIf my syncs fail and get fixed, do I need to ask for a backfill?\n\nIf your syncs fail, you do not need to reach out to Segment Support to request a backfill. Once a successful sync takes place, Segment automatically loads all of the data generated since the last successful sync occurred.\n\nCan I change my schema names once they\u2019ve been created?\n\nSegment stores the name of your schema in the SQL Settings page. Changing the name of your schema in the app without updating the name in your data warehouse causes a new schema to form, one that doesn\u2019t contain historical data.\n\nTo change the name of your schema without disruptions:\n\nOpen the Segment app, select Connections and click Destinations.\nSelect the warehouse you\u2019d like to rename the schema for from the list of destinations.\nOn the overview page for your source, select Settings.\nDisable the Sync Data toggle and click Save Settings.\nSelect Connections and click Sources.\nSelect a source that syncs data with your warehouse from your list of sources, and select Settings.\nSelect SQL Settings and update the \u201cSchema Name\u201d field with the new name for your schema and click Save Changes.\n\nNote: This will set the schema name for all existing and future destinations. The new name must be lowercase and may include underscores.\n\nRepeat steps six and seven until you rename all sources that sync data to your warehouse.\nOpen the third-party host of your database, and rename the schema.\nOpen the Segment app, select Connections and click Destinations.\nSelect the warehouse you disabled syncs for from the list of destinations.\nOn the overview page for your source, select Settings.\nEnable the Sync Data toggle and click Save Settings.\nCan I selectively filter data/events sent to my warehouse based on a property?\n\nAt the moment, there isn\u2019t a way to selectively filter events that are sent to the warehouse. The warehouse connector works quite differently from our streaming destinations and only has the selective sync functionality that allows you to enable/disable specific properties or events.\n\nCan data from multiple sources be synced to the same database schema?\n\nIt\u2019s not possible for different sources to sync data directly to the same schema in your warehouse. When setting up a new schema within the Segment UI, you can\u2019t use a schema name that\u2019s already in use by another source. Segment recommends syncing the data separately and then joining it downstream in your warehouse.\n\nFor more information about Warehouse Schemas, see the Warehouse Schemas page.\n\nThis page was last modified: 21 Oct 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nCan I control what data is sent to my warehouse?\nCan we add, tweak, or delete some of the tables?\nCan we transform or clean up old data to new formats or specs?\nCan I change the data type of a column in the warehouse?\nCan the data type definitions in Protocols be enforced in a warehouse schema?\nHow do I find my source slug?\nHow do I find my warehouse id?\nHow fresh is the data in Segment Warehouses?\nWhat if I want to add custom data to my warehouse?\nWhich IPs should I allowlist?\nWill Segment sync my historical data?\nWhat do you recommend for Postgres: Amazon or Heroku?\nHow do I prevent a source from syncing to some or all warehouses?\nCan I be notified when warehouse syncs fail?\nHow is the data formatted in my warehouse?\nIf my syncs fail and get fixed, do I need to ask for a backfill?\nCan I change my schema names once they\u2019ve been created?\nCan I selectively filter data/events sent to my warehouse based on a property?\nCan data from multiple sources be synced to the same database schema?\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nJourneys\n/\nJourneys Edits & Versioning\nJourneys Edits & Versioning\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nWith Journeys edits and versioning, you can make changes to live journeys.\n\nBefore you begin\n\nKeep the following in mind when you edit a journey:\n\nTo create a new journey version, you can edit the current, live version or restore a previous version.\nYou can only have one live journey version at any time. When you publish a draft journey version, Segment prompts you to pause or archive the previous version of the journey (if it isn\u2019t archived already). If you pause the previous version, it begins to drain any users left in the version while you paused it.\nIf the previous version is in a paused, draining state, users can continue flowing through it until you archive it, or if users successfully exit that version through its exit settings.\nIf you enable version exclusion when you edit a live journey, users in previous versions of the journey won\u2019t enter the latest version.\nSegment preserves destination sync keys in Segment for destination steps that remain the same between versions. You don\u2019t need to update these keys in your downstream destinations when creating a new journey version that has the same destination step as a previous version.\nSegment displays a destination key on destination steps where a downstream destination is using a previous version key. Segment also preserves keys between versions even if you create a new draft version from a previously archived version.\nYou can create up to 25 versions of a journey. Segment keeps previous journey versions in a journey container, which you can access from the Journeys list page.\nEdit a journey\n\nFollow these steps to edit a journey:\n\nFrom your Engage space, click the Journeys tab.\nFrom the Journeys list page, select the journey version you want to edit.\nOn the Journey overview page, select Edit. Segment creates a new version in draft mode.\nEdit your Journey, then select Publish version.\nIf the previous version is live, Segment asks whether you want to pause or archive the previous version.\nIf you pause the previous version, users can continue through the journey version, but no new users can enter. You will not be able to resume this journey version if it is paused this way.\nIf you archive the previous version, users won\u2019t continue through the journey version.\n(Optional:) Enable version exclusion.\nClick Publish.\n\nAfter you\u2019ve published, users who meet the entry criteria can enter the new journey version.\n\nYou can return to the Journeys list page to view the new live journey and its previous versions, which are nested under the journey container.\n\nJourney settings\n\nA Journey\u2019s settings can\u2019t be edited once the Journey has been published, including entry and exit settings. The only settings you can change after publishing a Journey are the Journey\u2019s name and description.\n\nWorking with Journeys versioning\nExit settings and user flow between journeys\n\nExit settings determine how users flow between journey versions.\n\nSuppose you have a journey with exit settings enabled. The following table lists the actions you can take with the journey, as well as the results:\n\nACTION\tRESULT\nPause Version 1 and publish Version 2\tUsers will flow through Version 1 until they meet its exit settings.\n\nOnce users exit Version 1, they can enter Version 2 upon meeting its entry criteria.\nArchive Version 1 and publish Version 2\tUsers can enter Version 2 when they meet its entry criteria.\nPause Version 1, publish Version 2, then archive Version 1 before users have exited Version 1\tUsers won\u2019t be able to enter Version 2. Users must successfully exit the paused Version 1 before entering Version 2.\n\nIn this situation, Segment recommends that you wait until users have exited the journey through exit settings before archiving the version.\n\nAlternatively, instead of pausing and archiving a version, Segment recommends that you archive the previous version when you publish the subsequent version.\n\nSuppose you have a journey without enabled exit settings. If you pause or archive Version 1 when publishing Version 2 of that journey, then users can immediately enter Version 2 when they meet its entry criteria, even if they\u2019re still in Version 1.\n\nVersion exclusion\n\nTo prevent users from a previous journey from ever entering a new journey version, enable version exclusion when you create the new journey version.\n\nList destinations\n\nAdding a list destination to a journey version creates a new record in Segment\u2019s systems. This process can take up to ten hours. During this time, you\u2019ll be unable to publish new versions of a journey.\n\nFor example, if you add a list destination to Version 1 of a journey, and users begin flowing into the version, then Segment will begin creating the new record. If you create a Version 2 draft from Version 1 of the journey while Segment is still creating the new record, you won\u2019t be able to publish Version 2 until this process is completed.\n\nIf the version has a list destination but no users have flowed into the version, though, Segment won\u2019t create a new record for that list destination, and you won\u2019t have to wait to publish a new journey version.\n\nThis page was last modified: 03 Oct 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nBefore you begin\nEdit a journey\nWorking with Journeys versioning\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSources\n/\nSource Schema\nSource Schema\n\nSegment Business Tier customers can use Schema Controls to manage which events are allowed to pass through Segment and on to Destinations. These filters are a first-line defense to help you protect the integrity of your data, and the decisions made with it.\n\nBlocking events within the source schema will exclude them from API and MTU calculations. These events are discarded before they reach the pipeline that Segment uses for MTU calculations.\n\nSchema view\n\nThe Schema tab shows the schema of events, properties, and traits for each source that Segment receives over a specific timeframe. It also shows when the events were last seen, how many events were allowed vs. blocked, and the downstream destinations those events are connected to.\n\nYou can view events by Segment call type in the Source Schema with the Track, Identify, and Group tabs. The Schema tracks:\n\nTrack event details by event name\nIdentify and Group event details by trait name\n\nClick the arrow to the left of the event name to view additional event properties for Page or Track events. Since the Schema tracks Identify traits, you will need to make sure you are passing traits into your Identify call in order to view event data in your schema.\n\nThe Schema shows \u201cPage Viewed\u201d for all Page calls under the Track tab.\n\nThe Source Schema UI changes slightly depending on whether you have a Protocols Tracking Plan connected to the source. If you have a Tracking Plan connected to your source, the UI displays a Planned column that will indicate if the event is planned or unplanned. This allows you to quickly identify unplanned events and take action to align your schema with your Tracking Plan. If there is no Tracking Plan connected to the source, the UI will display a toggle next to each event where, if you\u2019re a Business Tier customer, you can simply block or allow that event at the source level.\n\nArray properties are represented with an additional nested property representing the array\u2019s items. The nested property is the property\u2019s name with a .$ suffix. If an array property in the connected Tracking Plan does not include the items nested property, nested properties might be marked as unplanned in the Source Schema.\n\nEvent filters\n\nIf you no longer want to track a specific event, you can either remove it from your code or, if you\u2019re on the Business plan and don\u2019t have a Tracking Plan connected, you can block track calls from the Segment UI. To do so, click on the Schema tab in a Source and toggle the event to enable or block an event.\n\nFor sources with a connected Tracking Plan, use Protocols to block unplanned events.\n\nOnce you block an event, Segment stops forwarding it to all of your Cloud and Device-mode Destinations, including your warehouses. You can remove the events from your code at your leisure. In addition to blocking track calls, Business plan customers can block all Page and Screen calls, as well as Identify traits and Group properties.\n\nWhen an event is blocked, the name of the event or property is added to your Schema page with a counter to show how many events have been blocked. By default, data from blocked events and properties is not recoverable. You can always re-enable the event to continue sending it to downstream Destinations.\n\nIn most cases, blocking an event immediately stops that event from sending to Destinations. In rare cases, it can take up to six hours to fully block an event from delivering to all Destinations.\n\nIdentify and Group Trait Filters\n\nIf you no longer want to capture specific traits within .identify() and .group() calls, you can either remove those traits from your code, or if you\u2019re on the Business plan, you can block specific traits right from the Segment UI. To do so, click on the Schema tab in a Source and navigate to the Identify or Group events where you can block specific traits.\n\nBlocked traits are not omitted from calls to device-mode Destinations.\n\nSchema Integration Filters\n\nAll customers can filter specific events from being sent to specific Destinations (except for warehouses) by updating their tracking code. Here is an example showing how to send a single message only to Intercom and Google Analytics:\n\nanalytics.identify('user_123', {\n  email: 'jane.kim@example.com',\n  name: 'Jane Kim'\n}, {\n  integrations: {\n    'All': false,\n    'Intercom': true,\n    'Google Analytics': true\n  }\n});\n\n\nDestination flags are case sensitive and match the Destination\u2019s name in the docs (for example, \u201cAdLearn Open Platform\u201d, \u201cawe.sm\u201d, \u201cMailChimp\u201d, and so on).\n\nSegment Business tier customers can block track calls from delivering to specific Destinations in the Segment UI. Visit a Source Schema page and click on the Integrations column to view specific schema integration filters. Toggle the filter to block or enable an event to a Destination.\n\nThis page was last modified: 19 Mar 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSchema view\nEvent filters\nIdentify and Group Trait Filters\nSchema Integration Filters\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nAudiences\n/\nLinked Audiences\nLinked Audiences\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nLinked Audiences empowers marketers to effortlessly create targeted audiences by combining behavioral data from the Segment Profile and warehouse entity data within a self-serve, no-code interface.\n\nThis tool accelerates audience creation, enabling precise targeting, enhanced customer personalization, and optimized marketing spend without the need for constant data team support.\n\nWith Linked Audiences, you can:\n\nPreserve rich relationships between all the data in your warehouse by creating connections with any entity data back to your audience profile.\nBuild advanced audience segments that include the rich context needed for personalization downstream.\nUse a low code builder, enabling marketers to activate warehouse data without having to wait for data pull requests before launching campaigns to targeted audiences.\n\nTo learn more about specific use cases you can set up with Linked Audiences, see Linked Audiences Use Cases.\n\nPrerequisites\n\nBefore you begin setting up your Linked Audience, ensure you have:\n\nSet up Profiles Sync.\nSet up your warehouse permissions using Snowflake.\nEnsure someone has set up your data graph.\nWorkspace Owner or Unify Read-only, Engage User, Entities Read-only, and Source Admin roles in Segment.\nSetting up Linked Audiences\n\nTo set up your Linked Audience, complete the following steps:\n\nStep 1: Build a Linked Audience\nStep 2: Activate your Linked Audiences\nStep 3: Send a test event to your destination\nStep 4: Enable your Linked Audience\nStep 5: Monitor your Activation\nStep 1: Build a Linked Audience\n\nLinked Audiences allows you to filter based on properties like profile traits, relational data mapped to the Data Graph, events, and existing audiences.\n\nTo build a Linked Audience:\n\nNavigate to Engage > Audiences.\nSelect + New audience > Audience.\nOn the Select Audience Type screen, select Linked audience, then click Next. Note: If you cannot select Linked audience, ensure you\u2019ve set up your Data Graph in Unify.\nSelect the conditions on which to build your audience.\nClick Preview to view your audience selection and see a count and list of audience members who meet the criteria.\nWhen your audience is complete and accurate, click Next.\nEnter an audience name and description to identify this configuration. Optionally, select a folder to add this audience.\nClick Create Audience.\n\nAfter creating your Linked Audience, you will be brought to the Overview page with the Linked Audience in a disabled state.\n\nLinked Audience conditions\n\nThe linked audiences builder sources profile trait and event keys from the data warehouse. This data must be synced to the data warehouse through Profiles Sync before you can reference it in the linked audience builder. If there is a profile trait that exists in the Segment Profile that hasn\u2019t successfully synced to the data warehouse yet, it will be grayed out so that it can\u2019t be selected.\n\nThe linked audience builder also returns a subset of available entity property key values, event property and context key values, and profile trait key values that you can select in the input field drop-down so that you don\u2019t need to type in the exact value that you want to filter on. If you don\u2019t see the value you\u2019re looking for, you can manually enter it into the input field. Segment displays:\n\nthe first 100 unique string entity property values from the data warehouse.\nthe top 65 event property and context key values.\nthe top 65 profile trait key values.\n\nYou can duplicate your conditions in the audience builder into the same condition group.You can only create nested entity conditions up to six levels in depth. For example, an entity condition that queries for relationships between Profiles, Accounts, Credit Cards, and Transactions has four levels of depth.\n\nAs you\u2019re building your Linked Audience, you can choose from the following conditions:\n\nCONDITIONS\tDESCRIPTION\nwith entity\tCreates a condition that filters profiles associated with entity relationships defined in the Data Graph. With this condition, you can navigate the full, nested entity relationships, and filter your audience on entity column values. Each subsequent entity you select in an entity branch acts as a filter over the profiles that are available at the next depth of that specific branch.\nwithout entity\tCreates a condition that filters profiles that are not associated with entity relationships defined in the Data Graph. With this condition, you can navigate the full, nested entity relationships, and filter your audience on entity column values. Each subsequent entity you select in an entity branch acts as a filter over the profiles that are available at the next depth of that specific branch.\nwith trait\tCreates a condition that filters profiles with a specific trait.\nwithout trait\tCreates a condition that filters profiles without a specific trait.\npart of audience\tCreates a condition that filters profiles that are part of an existing audience.\nnot part of audience\tCreates a condition that filters profiles that are not part of an existing audience.\nwith event\tCreates a condition that filters profiles that have a specific event in their event history. You can also filter on event property values.\nwithout event\tCreates a condition that filters profiles that do not have a specific event in their event history. You can also filter on event property values.\n\nThe entity and event condition type supports these configurations: at least: supports 1 or greater,\nexactly: supports 0 or greater, at most: supports 0 or greater.\n\n*When filtering by 0, you can\u2019t filter on by entity properties or on additional nested entities.\n\nOperator selection\n\nYou can create audience definitions using either AND or OR operators across all condition levels. You can switch between these operators when filtering on multiple entity or event properties, between conditions within a condition group, and between condition groups.\n\nExample:\n\nEntity Explorer\n\nIf you have defined entity conditions in your audience definition, you will see a \u201cMatched Entities\u201d tab in the audience preview to help you understand what entities qualified a user to be a part of an audience.\n\nThis information appears when you click the user profile generated from the audience preview. The contextual information encompasses entity relationships as well as entity column values that were used as filtering criteria in the audience definition. By default, Segment includes the entity ID. The data being returned is truncated - 10 entities at each level, 6 levels of depth. If you want to opt out of this functionality, contact Segment Support.\n\nDynamic references\n\nEvent conditions\n\nWhen filtering on event properties, you can dynamically reference the value of another profile trait, or enter a constant value. These operators support dynamic references: equals, not equals, less than, greater than, less than or equal, greater than or equal, contains, does not contain, starts with, ends with.\n\nEntity conditions\n\nWhen filtering on entity properties, you can dynamically reference the value of another entity column (from the same entity branch at the same level or above it), profile trait, or enter a constant value. You can only dynamically reference properties of the same data type. Dynamic references are supported for specific operators depending on the data type, as in the following table:\n\nDATA TYPE\tSUPPORTED OPERATORS\nNUMBER\tequals, not equals, less than, greater than, less than or equal, greater than or equal\nSTRING\tequals, not equals, contains, does not contain, starts with, ends with\nDATE\tequals, not equals, less than, greater than, less than or equal, greater than or equal\nTIME\tequals, not equals, less than, greater than, less than or equal, greater than or equal\nTIMESTAMP\tequals, not equals, less than, greater than, less than or equal, greater than or equal\nStep 2: Activate your Linked Audience\n\nAfter you build your Linked Audience, you can send events to your chosen destinations and use them for personalizing your customer communications.\n\nTo activate your Linked Audience:\n\nStep 2a: Connecting to a Destination\nStep 2b: Selecting your Destination Actions\nStep 2c: Defining how and when to trigger an event to your Destination\nStep 2d: Configuring the event payload\nStep 2a: Connecting to a destination\n\nDestinations are the business tools or apps that Segment forwards your data to. Adding a destination allows you to act on your data and learn more about your customers in real time. To fully take advantage of Linked Audiences, you must connect and configure at least one destination.\n\nLinked Audiences destinations\n\nLinked Audiences only supports Actions Destinations. List destinations aren\u2019t supported.\n\nNote: Ensure your destination has been enabled in Segment before you begin the steps below.\n\nNavigate to Engage > Audiences.\nSelect the Linked Audience you set up in the previous step.\nSelect Add destination.\nSelect a destination from the catalog.\nClick Configure data to send to destination.\nStep 2b: Select your Destination Actions\n\nThe Destination Actions framework allows you to see and control how Segment sends the event data it receives from your sources to actions-based destinations. Each Action in a destination lists the event data it requires and the event data that is optional. Segment displays available Actions based on the destination you\u2019ve connected to your Linked Audience. You can see details of each option and how to use it in the Actions Destinations Catalog documentation.\n\nSelect the Destination Action to call when the event happens, then click Next.\n\nStep 2c: Define how and when to trigger an event to your destination\n\nConfigure how and when events are produced with each audience run. Select the entities referenced in the audience builder to act as a trigger for your events.\n\nTRIGGER\tEVENT TYPE\tDEFINITION\tEXAMPLES\nProfile enters audience\tTrack\tSend an event when a profile matches the audience condition.\tSend a congratulatory email when a traveler qualifies for premium status with a mileage program. Send a discount to all customers with a particular product on their wishlist.\nProfile exits audience\tTrack\tSend an event when a profile no longer matches the audience condition.\tSend an email to credit card owners to confirm that their credit cards have been paid in full. Send a confirmation to a patient when they have completed all their pre-screening forms.\nEntity enters audience\tTrack\tSend an event when an entity condition associated with a profile matches the audience condition.\tSend a reminder to a customer when a credit card associated with their profile has an outstanding balance. Notify a traveler when a flight associated with their profile is delayed. Notify a customer when a product associated with their profile\u2019s wishlist is back in stock.\nEntity exits audience\tTrack\tSend an event when an entity condition associated with a profile no longer matches the audience condition.\tSend a confirmation to a customer when a credit card associated with their profile has been paid off. Send a confirmation to the primary doctor when each of their associated patients completes their annual check up.\nProfile enters or exits audience\tIdentify\tSend an event when a profile\u2019s audience membership changes.\tUpdate a user profile in a destination with the most recent audience membership.\nStep 2d: Configure the event\n\nAfter you select an action, Segment attempts to automatically configure the data fields that will be sent to the destination. You can review and adjust these settings before enabling this event.\n\nEnrich event\n\nSelect additional traits and properties to include when the event is sent.\n\nCopy personalization syntax\n\nClick Copy to use in Braze Cloud Mode (Actions) to copy the personalization syntax for the selected traits and properties to use in your destination messaging templates.\n\nThis feature is in beta for customers using Braze. Some functionality may change before it becomes generally available. This feature is governed by Segment\u2019s First Access and Beta Preview Terms.\n\nShow/hide preview\n\nAs you\u2019re enriching your events in Linked Audiences, you should view a preview of the event payload schema based on the properties you select. It might look like the following:\n\nMap event\n\nOnly required fields are displayed. All optional & pre-filled fields are hidden, though you can view hidden fields by clicking Show hidden fields.\n\nThese fields are pre-filled with properties configured by default.\n\nStep 3: Send a test event to your destination\n\nSend a test event to ensure that everything is connected properly and your destination receives the event.\n\nEnter the destination User id for the profile you want to use to test the event, then click Send test event to destination.\n\nThe Event content drop-down shows you a preview of what the data sent to your destination might look like.\n\nStep 4: Enable your Linked Audience\n\nAfter building your Linked Audience, choose Save and Enable. You\u2019ll be redirected to the Audience Overview page, where you can view the audience you created. Segment automatically disables your audience so that it doesn\u2019t start computing until you\u2019re ready. A run is when Segment runs the audience conditions on your data warehouse and sends events downstream.\n\nTo enable your audience, select the Enabled toggle, then select Enable audience.\n\nRun Now\n\nYou can trigger a run for your audience if you want to send events to your destination without waiting for the next scheduled run. To do so, select Run Now. This triggers a run for the audience and sends events downstream.\n\nSet a run schedule\n\nUse the Audience Overview page to view the audience profile count, current run schedule, run status, and upcoming run time.\n\nDetermine when an audience should run and send data to enabled destinations with a run schedule:\n\nManual: Trigger audience runs manually by clicking Run Now on the Audience Overview page.\nInterval: Trigger audience runs based on a predefined set of time intervals. Supported intervals are: 15 minutes, 30 minutes, 1 hour, 2 hours, 4 hours, 6 hours, 8 hours, 12 hours, 1 day. If you select this option, Segment will run your audience after you enable the audience.\nDay and time: Trigger audience runs at specific times on selected days of the week. If you select this option, Segment will run your audience at the first selected date and time.\n\nYou can maintain your run schedule at any time from the audience\u2019s Settings tab.\n\nYou can also click Run Now on the Audience Overview page at any time (even if the run schedule is Interval Overview Day and time) to manually trigger a run on your warehouse and send data to enabled destinations.\n\nThere may be up to a 5 minute delay from the configured start time for audiences that are configured with the Interval and Day and time run schedules. For example, if you configured an audience with the Day and time compute schedule to run on Mondays at 8am, it can compute as late as Monday at 8:05am. This is to help us better manage our system load.\n\nStep 5: Monitor your activation\n\nWith your Linked Audience activated, follow these steps to monitor your activation:\n\nFrom the Audience Overview page, selected one of your connected destinations.\nUnder the Settings tab, click Destination delivery, which then opens the Linked Audiences Delivery Overview.\nDelivery Overview for Linked Audiences\n\nDelivery Overview shows you four steps in your data activation pipeline:\n\nEvents from Audience: Events that Segment created for your activation. The number of events for each compute depends on the changes detected in your audience membership.\nFiltered at Destination: The activation pipeline is rich with features that let you control which events make it to the destination. If any events aren\u2019t eligible to be sent (for example, due to destination filters, insert function logic, and so on), Segment will show them in Filtered at Destination.\nFailed Delivery: Events that Segment attempted but failed to deliver to your destination. Failed Delivery indicates an issue with the destination, like invalid credentials, rate limits, or other error statuses received during delivery.\nSuccessful Delivery: Events that Segment successfully delivered to your destination. You\u2019ll see these events in your downstream integration.\nMaintaining Linked Audiences\n\nYou can maintain your Linked Audience by accessing these tabs on the main page of your Linked Audience:\n\nTAB NAME\tINFORMATION\nOverview\tOn this tab you can:\n* View relevant audience information, such as Profiles in audience count, run schedule, latest run, and next run.\n* Enable or disable, manually run, clone and delete audiences.\n\u00a0\u00a0- Note: Cloning a linked audience creates a new linked audience in the builder create flow with the same conditions as the linked audience that it was cloned from.\n* View the list of profiles in the audience with the Audience Explorer.\n* View connected destinations and configured activation events.\nBuilder\tOn this tab you can:\n* View or edit your linked audience conditions.\n\u00a0\u00a0 - Note: If you edit an audience with configured activation events, you should disable or delete impacted events for your audience to successfully compute. Events are impacted if they reference entities that are edited or removed from the audience definition.\nRuns\tOn this tab you can:\n* View information about the last 50 audience runs, such as start time, run duration, run result, and change summary. You can also view granular run stats to help you understand the duration of each step in the run such as:\n\u00a0\u00a0 - Queueing run: The time spent in the queue waiting for other runs to finish before this one begins.\n\u00a0\u00a0 - Extracting from warehouse: The duration of the audience query and data transfer from the source warehouse.\n\u00a0\u00a0 - Preparing to deliver events: The time taken to process and ready events for delivery to connected destinations.\n* If there are no changes associated with a run, there will be no values shown for the granular run stats.\nSettings\tOn this tab you can view or edit the linked audience name, description, and run schedule.\n\nThis page was last modified: 21 Nov 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nPrerequisites\nSetting up Linked Audiences\nStep 1: Build a Linked Audience\nStep 2: Activate your Linked Audience\nStep 3: Send a test event to your destination\nStep 4: Enable your Linked Audience\nStep 5: Monitor your activation\nMaintaining Linked Audiences\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nReverse Etl\n/\nReverse Etl Source Setup Guides\n/\nSnowflake Reverse ETL Setup\nSnowflake Reverse ETL Setup\n\nSet up Snowflake as your Reverse ETL source.\n\nAt a high level, when you set up Snowflake for Reverse ETL, the configured user/role needs read permissions for any resources (databases, schemas, tables) the query needs to access. Segment keeps track of changes to your query results with a managed schema\n(__SEGMENT_REVERSE_ETL), which requires the configured user to allow write permissions for that schema.\n\nSegment now supports key-pair authentication for Snowflake Reverse ETL sources. Key-pair authentication is available for Business Tier users only.\n\nSnowflake Reverse ETL sources support Segment's dbt extension\n\nIf you have an existing dbt account with a Git repository, you can use Segment\u2019s dbt extension to centralize model management and versioning, reduce redundancies, and run CI checks to prevent breaking changes.\n\nSet up guide\n\nFollow the instructions below to set up the Segment Snowflake connector. Segment recommends you use the ACCOUNTADMIN role to execute all the commands below, and that you create a user that authenticates with an encrypted key pair.\n\nSegment has a Terraform provider, powered by the Public API, that you can use to create a Snowflake Reverse ETL source. See the segment_source (Resource) documentation for more information.\n\nLog in to your Snowflake account.\nNavigate to Worksheets.\n\nEnter and run the code below to create a database. Segment uses the database specified in your connection settings to create a schema called __segment_reverse_etl to avoid collision with your data. The schema is used for tracking changes to your model query results between syncs. An existing database can be reused, if desired. Segment recommends you to use the same database across all your models attached to this source to keep all the state tracking tables in 1 place.\n\n-- not required if another database is being reused\nCREATE DATABASE segment_reverse_etl;\n\n\nEnter and run the code below to create a virtual warehouse. Segment Reverse ETL needs to execute queries on your Snowflake account, which requires a Virtual Warehouse to handle the compute. You can also reuse an existing warehouse.\n\n-- not required if reusing another warehouse\nCREATE WAREHOUSE segment_reverse_etl\n WITH WAREHOUSE_SIZE = 'XSMALL'\n   WAREHOUSE_TYPE = 'STANDARD'\n   AUTO_SUSPEND = 600 -- 5 minutes\n   AUTO_RESUME = TRUE;\n\n\nEnter and run the code below to create specific roles for Reverse ETL. All Snowflake access is specified through roles, which are then assigned to the user you\u2019ll create later.\n\n-- create role\nCREATE ROLE segment_reverse_etl;\n\n-- warehouse access\nGRANT USAGE ON WAREHOUSE segment_reverse_etl TO ROLE segment_reverse_etl;\n\n-- database access\nGRANT USAGE ON DATABASE segment_reverse_etl TO ROLE segment_reverse_etl;\nGRANT CREATE SCHEMA ON DATABASE segment_reverse_etl TO ROLE segment_reverse_etl;\n\n\nEnter and run one of the following code snippets below to create the user Segment uses to run queries. For added security, Segment recommends creating a user that authenticates using a key pair.\n\nTo create a user that authenticates with a key pair, create a key pair and then execute the following SQL commands:\n\n-- create user (key-pair authentication)\nCREATE USER segment_reverse_etl_user\nDEFAULT_ROLE = segment_reverse_etl\nRSA_PUBLIC_KEY = 'enter your public key';\n\n-- role access\nGRANT ROLE segment_reverse_etl TO USER segment_reverse_etl_user;\n\n\nTo create a user that authenticates with a password, execute the following SQL commands:\n\n-- create user (password authentication)\nCREATE USER segment_reverse_etl_user\n MUST_CHANGE_PASSWORD = FALSE\n DEFAULT_ROLE = segment_reverse_etl\n PASSWORD = 'my_strong_password'; -- Do not use this password\n\n-- role access\nGRANT ROLE segment_reverse_etl TO USER segment_reverse_etl_user;\n\nAdd the account information for your source.\nClick Test Connection to test to see if the connection works.\nClick Add source if the test connection is successful.\n\nLearn more about the Snowflake Account ID in Snowflake\u2019s Account identifiers documentation.\n\nAfter you\u2019ve successfully added your Snowflake source, add a model and follow the rest of the steps in the Reverse ETL setup guide.\n\nSecurity\nAllowlisting IPs\n\nIf you create a network policy with Snowflake and are located in the US, add 52.25.130.38/32 and 34.223.203.0/28 to the \u201cAllowed IP Addresses\u201d list.\n\nIf you create a network policy with Snowflake and are located in the EU, add 3.251.148.96/29 to your \u201cAllowed IP Addresses\u201d list.\n\nThis page was last modified: 04 Nov 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSet up guide\nSecurity\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nDestinations Overview\nDestinations Overview\n\nDestinations are the business tools or apps that Segment forwards your data to. Adding Destinations allow you to act on your data and learn more about your customers in real time.\n\nDestinations Catalog\n\nIf you want to explore the destinations compatible with Segment, check out the Destinations catalog. Select an item from the catalog to learn more about it. The documentation for each destination explains how the Segment Tracking API methods are implemented for that destination.\n\nSources vs Destinations\n\nSegment has Sources and Destinations. Sources send data into Segment, while Destinations receive data from Segment.\n\nDestination connection types\n\nSegment has three destination connection types:\n\nEvent streams\nStorage\nReverse ETL\nEvent streams destinations\n\nEvent streams destinations are all destinations that aren\u2019t storage or Reverse ETL destinations. Adding these destinations allow you to act on your data and learn more about your customers in real time. These include Destination Actions.\n\nStorage destinations\n\nStorage destinations enable you to store your raw Segment data. This enables data analysts and data scientists to work with the raw data to derive deeper and more customized insights to support your organization. Learn more from the storage overview page.\n\nReverse ETL destinations\n\nReverse ETL destinations are the business tools or apps you use that Segment syncs the data from your warehouse to.\n\nIf your destination is not listed in the Reverse ETL catalog, use the Segment Connections destination to send data from your Reverse ETL warehouse to other destinations listed in the catalog. The Segment Connections destination enables you to mold data extracted from your warehouse in Segment Spec API calls that are then processed by Segment\u2019s HTTP Tracking API. The Segment HTTP Tracking API lets you record analytics data. The requests hit Segment\u2019s servers, and then Segment routes your data to any destination you want. Get started with the Segment Connections destination.\n\nMethod compatibility\n\nNot all destinations can accept data from specific method types. To know if a destination can accept data from specific method types, look for the Quick Info box at the top of the destination\u2019s documentation page, or check out the Destinations Methods comparison chart.\n\nSource compatibility\n\nMany destinations can accept data from all types of sources, but some are only compatible with specific source types (for example, web only, or server only). To find out which source types a specific destination can accept data from, check the documentation for that destination in the Quick info box, or in the Supported Sources and Connection Modes section.\n\nDestinations Compatibility Matrix\n\nWondering which destinations take which data? Check out the Destination connection modes list by category.\n\nDestination Actions\n\nIn June 2021, Segment released a new form of destinations called Destinations Actions. These destinations allow users to create subscriptions: sets of conditions in which data is sent to the destinations and data mappings, to format that data for the destination tool. Segment watches for data that matches the conditions you create (triggers) for the subscription, and when the conditions are met, uses an explicit mapping to transform the incoming data to an output format that your destination can use.\n\nConnection modes\n\nSegment\u2019s web source (Analytics.js), and native client-side libraries (iOS, Android, React-native) allow you to choose how you send data to Segment from your website or app. There are two ways to send data:\n\nCloud-mode: The sources send data directly to the Segment servers, which then translate it for each connected downstream destination, and send it on. Translation is done on the Segment servers, keeping your page size, method count, and load time small.\n\nHealthcare and Life Sciences (HLS) customers can encrypt data flowing into their destinations\n\nHLS customers with a HIPAA eligible workspace can encrypt data in fields marked as Yellow in the Privacy Portal before they flow into an event stream, cloud-mode destination.\nTo learn more about data encryption, see the HIPAA Eligible Segment documentation\n\nDevice-mode: You include additional code on your website or mobile app which allows Segment to use the data you collect on the device to make calls directly to the destination tool\u2019s API, without sending it to the Segment servers first. (You still send your data to the Segment servers, but this occurs asynchronously.) This is also called wrapping or bundling, and it might be required when the source has to be loaded on the page to work, or loaded directly on the device to function correctly. When you use Analytics.js, you can change the device-mode destinations that a specific source sends from within the Segment web app, without touching any code.\n\nIf you use Server source libraries, they only send data directly to Segment in Cloud-mode. Server library implementations operate in the server backend, and can't load additional destination SDKs.\n\nChoosing a connection mode\n\nCloud-mode destinations send data through Segment. Device-mode destinations send data in parallel to Segment. There are tradeoffs between using cloud-mode and device-mode destinations. In general, Cloud-mode is preferred because you then benefit from Segment\u2019s system features, like retries, Replay, Warehouses, Privacy blocking, filtering, and more.\n\nYou should consider using device-mode if you use destinations which record information directly on the user\u2019s device. These types of tools might lose functionality if they aren\u2019t loaded directly on the device.\n\nTake a look at the pros and cons chart of device-mode and cloud-mode destinations to determine which connection mode is best for you:\n\nCONNECTION MODE\tPROS\tCONS\nCloud-mode\t* Increased site or app performance\n* Unaffected by ad blockers\t* May limit Destination features\nDevice-mode\t* Access to all features of the Destination\t* Decreased site or app performance\nWebsite source connection modes\n\nSegment\u2019s website sources use device-mode by default, because so many website-based destinations require that they be loaded on the page, and because size and page performance are less of a concern than on mobile. If your website source only collects information that you can instrument yourself, then you can use cloud-mode.\n\nFor example, a web-chat destination must be loaded to connect to the service and collect metrics efficiently - you don\u2019t expect it to route chat messages through Segment! This does mean that Segment might not receive a small amount of the destination-specific information from your users. In the chat example, if the destination is calculating idle time between messages, that data would appear in the destination\u2019s tooling, but not necessarily in the Segment data.\n\nMobile source connection modes\n\nBy default, destinations configured on a mobile source send their data directly to the Segment servers, then translate it and use Cloud-mode to forward it to destinations. Cloud-mode means that Segment sends the data directly from the Segment servers, to their servers. This means you don\u2019t need to package third-party SDKs for destinations that can accept cloud-mode data. Some primarily web-based destinations also allow cloud-mode, which can help reduce app size, and improve load time and performance. You can read more about the effects of mobile app size on downloads in Segment\u2019s blog.\n\nBefore you turn on or opt-in for cloud-mode for a mobile source, consider if your destinations have features that require interactions on the device or require device-specific data (see the examples above). For example, if you use cloud-mode for Mixpanel, you\u2019ll get your data on reporting and people, but won\u2019t be able to use their features for in-app surveys or auto-tracking. These can be really valuable, but might not be a priority for your team.\n\nHow Segment determines Device-mode and Cloud-mode destinations\n\nThere are two main things Segment considers when deciding to use Device-mode or Cloud-mode, or both, for a destination partner:\n\nAnonymous Attribution Methodology\nClient-native Destination Features\nAnonymous attribution methodology\nMobile attribution\n\nThe anonymous identifiers used on mobile devices are usually static, which means Segment doesn\u2019t need to do additional resolution, and can build Cloud-mode destinations by default. Because Segment uses native advertising identifiers on mobile devices, you don\u2019t need a full SDK on the device to reconcile or identify a user. For example, you might track users who viewed an advertisement in one app and installed another app as a result.\n\nHowever, some mobile attribution tools do more advanced reconciliation based on more than the native identifier, which requires the SDK on the device to work properly. For those destinations, Segment offers device-mode, which packages the tool\u2019s SDK with the client-side library so that you can get the entire range of tool functionality.\n\nWeb Attribution\n\nCross-domain identity resolution for websites requires that the attribution tool use a third-party cookie so it can track a user anonymously across domains. This is a critical component of attribution modeling. As a matter of principle, Segment only uses first-party cookies and doesn\u2019t share cookies with partners, so Analytics.js and the data it collects aren\u2019t enough to generate view-through attribution in ad networks.\n\nCustomers can load their libraries and pixels in the context of the browser and trigger requests to attribution providers from their device in response to Segment API calls to take advantage of advertising and attribution tools.\n\nClient-native destination features\n\nMany of Segment\u2019s destinations offer client-side features beyond data collection in their SDKs and libraries, for both mobile and web. In these cases, Segment offers Device-mode SDKs so that you can collect information on the device using Segment, but still get the destination\u2019s complete native functionality.\n\nSome features that usually require Device-mode include: automatic A/B testing, displaying user surveys, live chat or in-app notifications, touch and hover heatmapping, and accessing rich device data such as CPU usage, network data, or raised exceptions.\n\nHow can I tell which connection modes and platforms are supported for a destination?\n\nThe first place to look is the individual destination documentation. Each one includes a matrix of supported Sources and Connection Modes. Segment provides a list of all destinations and their connection modes.\n\nIn order to override the default, check the destination settings pane in the Segment web App either for a Connection Mode toggle or instructions on bundling any additional mobile components required.\n\nSync modes\n\nSync modes allow users to define how changes in the source should send downstream to your destination. Depending on which destinations you set up in Segment, you may need to choose a sync mode for your data. This configuration determines how Segment updates your destination based on the source data.\n\nThe available sync modes can vary based on the destination, integration type, and actions within the destination. For example, if you sync customer data, you might have the option to Insert, Update, or Upsert records.\n\nAvailable sync modes include:\n\nUpdate: Modify existing records in the destination without adding new ones.\nUpsert: Update existing records and add new ones, if necessary.\nAdd: Add records to a list, segment, or journey.\nRemove: Remove records from a list, audience, or journey.\nAdd a destination\n\nTo add a Destination:\n\nNavigate to Connections.\nClick Add Destination.\nChoose the Destination you want to add and click Configure. Most users eventually add destinations for: Analytics, Advertising, Email Marketing and/or Live Chat.\nSelect the Source you want to connect to your Destination.\nClick Next.\nGive you Destination a name.\nClick Save.\nConfigure the settings and enable your destination on the destination settings page.\n\nLearn more about what adding a destination entails.\n\nDisabled destinations do not receive data\n\nIf you haven\u2019t enabled your destination for the first time after you created it or if you actively disable a destination, Segment prevents any data from reaching the destination. Business Tier customers can request a Replay, which resends data from the time the destination was disabled to the time it was re-enabled. Replays can also send data to currently disabled destinations.\n\nSome destinations are not compatible with Replays after a certain period of time, for example, 14 days. Check with Segment\u2019s support team friends@segment.com to confirm that your intended destination allows historical timestamps.\n\nData deliverability\n\nSegment increases deliverability to destinations using retries and replays. Retries happen automatically for all customers, while replays are available on request for Business Tier customers.\n\nSegment\u2019s data flow is primarily unidirectional, from Segment to integrated destinations. Segment does not inherently support a bidirectional flow where events, once delivered and processed by a destination, are sent back to Segment.\n\nSegment also uses batching to increase deliverability to your destinations. Some destinations have batching enabled by default, and some, like Segment\u2019s Webhook (Actions) Destination, let you opt in to batching.\n\nSome cases of event batching might lead to observability loss\n\nWhile batching does increase event deliverability, you might experience error amplification, as if the entire batch fails, all events will be marked with the same status. For example, if a batch fails due to one 429 (Rate Limit) error, it might appear in the UI that there was one 429s request failure for each item in the batch.\n\nRetries\nRetries in Segment\u2019s client libraries\n\nSegment\u2019s client libraries ensure delivery of your data to the API reliably in the face of spotty connections, device failure, or network partitions in your data centers.\n\nWhen you use Segment\u2019s mobile SDK, Segment dispatches each event to a background thread where the event is then written to a queue. Later, Segment\u2019s SDK batches together many requests in to one compressed request and sends it to Segment\u2019s servers. Segment\u2019s SDKs minimize battery use and bandwidth use by powering up the radio less frequently and for shorter time periods.\n\nIf the delivery of the payload is not successfully sent due to connection issues, all of your SDKs will automatically retry the request until successful receipt of the payload according to the following policies. Note that retry policies are subject to change / tuning in the future.\n\nPLATFORM\tINITIAL WAIT - SLEEP DURATION BEFORE THE FIRST RETRY\tWAIT GROWTH - RATE OF GROWTH OF THE SLEEP DURATION BETWEEN EACH RETRY\tMAX WAIT - MAXIMUM SLEEP DURATION BETWEEN RETRIES\tMAX ATTEMPTS - MAXIMUM NUMBER OF INDIVIDUAL RETRIES\nC++\t1s\tNone\t1s\t5\nClojure\t15s\tExponential\t1h\t50\nGo\t100ms\tExponential\t10s\t10\nJava\t15s\tExponential\t1h\t50\nJavaScript\t1s\tExponential\t1h\t10\n.Net\t100ms\tExponential\t6.4s\t7\nNode.js\t100ms\tExponential\t400ms\t3\nPHP\t100ms\tExponential\t6.4s\t7\nPython\t1s\tExponential\t34m\t10\nRuby\t100ms\tExponential\t10s\t10\nMobile library retries\n\nAll mobile libraries handle retries by periodically attempting to flush their internal queue of events to Segment. If the flush is unsuccessful, the library waits until the next regularly-scheduled flush time to try again. The background queue of requests to Segment is bounded in size so if events are being queued faster than we can successfully flush them to Segment, some events may be dropped.\n\nRetries between Segment and destinations\n\nThe destination endpoint APIs have fluctuations in availability due to a number of issues ranging from network failures to bugs to overload. Segment\u2019s internal systems retry failed destination API calls for four hours with a randomized exponential backoff after each attempt. This substantially improves delivery rates.\n\nHere\u2019s an example destination that was only successfully accepting 93.36% of all API requests but was achieving a 99.28% final deliverability rate due to Segment\u2019s retry functionality.\n\nYou can see the current destination endpoint API success rates and final delivery rates for Segment\u2019s server-side destinations on Segment\u2019s status page.\n\nReplays\n\nReplay is available to Business tier customers. Contact Segment to learn more.\n\nReplays allow customers to load historical data from Segment\u2019s S3 logs into downstream destinations which accept cloud-mode data. So, for example, if you wanted to try out a new email or analytics tool, Segment can replay your historical data into that tool. This gives you a great testing environment and prevents data lock-in when vendors try to hold data hostage.\n\nIf you submitted suppress_only requests, Segment still retains historical events for those users, which can be replayed. If you do not want historical events replayed for suppressed users, submit suppress_and_delete requests instead.\n\nBatching\n\nSegment uses stream batching for all destinations that require near-realtime data and bulk batching for some data flows in our pipeline.\n\nStream batching\n\nFor all destinations, except for non-realtime Engage syncs and Reverse ETL syncs, Segment processes events from your source as they arrive and then flows the data downstream to your destinations in small batches, in a process called stream batching. These batches might contain different events between retry attempts, as events in previous batches may have succeeded, failed with a permanent error, or expired. This variability reduces the workload the system processes during partial successes, allows for better per-event handling, and reduces the chance of load-related failures by using variable batch formations.\n\nBulk batching\n\nSome data flows may be able to use a process called bulk batching, which supports batching for destinations that produce between several thousand and a million events at a time. Real-time workloads or using a Destination Insert Function may prevent bulk batches from being formed. Batches contain the same events between retries.\n\nThe following destinations support bulk batching:\n\nDV360\nGoogle Adwords Remarketing Lists\nKlaviyo (Actions)\nPinterest Audiences\nSnapchat Audiences\nLiveRamp\nThe Trade Desk CRM\n\nYou must manually configure bulk batches for Actions destinations\n\nTo support bulk batching for the Actions Webhook destination, you must set enable-batching: true and batch_size: >= 1000.\n\nIP Allowlisting\n\nIP Allowlisting uses a NAT gateway to route traffic from Segment\u2019s servers to your destination through a limited range of IP addresses, which can prevent malicious actors from establishing TCP and UDP connections with your integrations.\n\nIP Allowlisting is available for customers on Business Tier plans.\n\nSupported destinations\n\nSegment supports IP Allowlisting in all destinations except for the following:\n\nLiveRamp\nTradeDesk\nAmazon Kinesis\n\nDestinations that are not supported receive traffic from randomly assigned IP addresses.\n\nConfigure IP Allowlisting\n\nTo enable IP Allowlisting for your workspace:\n\nFrom your Segment workspace, navigate to Settings > Workspace settings > Destination IP settings.\nOn the Destination IP settings page, click Enable IP allowlisting.\nThe page displays the IP address ranges that Segment uses to route data from Segment\u2019s internal systems to your destination. Note these ranges, as you\u2019ll need this information to enforce IP restriction in your downstream destinations.\nOpen each of your downstream tools and configure IP restriction for each destination. For more information, refer to the documentation for your downstream tool.\n\nIP restriction might not be supported in all destinations.\n\nThis page was last modified: 04 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSources vs Destinations\nDestination connection types\nMethod compatibility\nSource compatibility\nDestination Actions\nConnection modes\nSync modes\nAdd a destination\nData deliverability\nIP Allowlisting\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage Introduction\nEngage Introduction\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nPowered by real-time data, Twilio Engage is a customizable personalization platform with which you can build, enrich, and activate Audiences.\n\nEngage Channels builds on top of these Audiences, helping you connect with and market to your customers through email, SMS, and WhatsApp campaigns.\n\nWhat can you do with Engage?\nCreate unified customer profiles\n\nEngage uses Segment Identity Resolution to take event data from across devices and channels and intelligently merge it into complete user- or account-level profiles. This gives your organization a single view of your customer base. To learn more, read the Identity Resolution documentation.\n\nPersonalizing customer interactions\n\nSupport teams rely on Segment's unified profiles to make real-time and informed decisions about customers when answering tickets or taking support calls. Read about how the support team at Frame.io reduced ticket response time by 80%.\n\nEnrich profiles with new traits\n\nAdd detail to user profiles with new traits and use them to power personalized marketing campaigns. You can add new traits to your user or account profiles in Engage using:\n\nComputed Traits: Use the Engage drag-and-drop interface to build per-user (B2C) or per-account (B2B) metrics on user profiles (for example, \u201clifetime value\u201d or \u201clead score\u201d).\nSQL Traits: Run custom queries on your data warehouse using the Engage SQL editor, and import the results into Segment. With SQL Traits, you can pull rich, uncaptured user data back into Segment.\nPredictions: Predict the likelihood that users will perform custom events tracked in Segment, like LTV, churn, and purchase.\nBuild Audiences\n\nCreate lists of users or accounts that match specific criteria. For example, after creating an inactive accounts audience that lists paid accounts with no logins in 60 days, you can push the audience to your analytics tools or send an SMS, email, or WhatsApp campaign with Engage Channels. Learn more about Engage audiences.\n\nSync audiences to downstream tools\n\nOnce you create your Computed Traits and Audiences, Engage sends them to your Segment Destinations in just a few clicks. You can use these Traits and Audiences to personalize messages across channels, optimize ad spend, and improve targeting. You can also use the Profile API to build in-app and onsite personalization. Learn more about using Engage data and the Profile API.\n\nPersonalizing marketing campaigns\n\nMarketing teams use Engage to run real-time multi-channel marketing campaigns based off specific user attributes they've computed in Engage. Read about how Drift used Engage to increase prospect engagement by 150% in two months.\n\nMarket to customers with Engage Premier and Channels\n\nTo send email, SMS, and WhatsApp campaigns with Engage Channels, you\u2019ll connect a Twilio messaging service, SendGrid subuser account, and WhatsApp messaging service to your Segment Engage space. Use existing accounts, or create new ones.\n\nView the onboarding steps for more on how to connect Twilio and SendGrid accounts.\n\nSend email, SMS, and WhatsApp messages in Journeys\n\nUse Engage to build email, SMS, and WhatsApp campaigns within Journeys. Send campaigns to subscribed users based on event behavior and profile traits. With message analytics, you can track the performance of your campaigns.\n\nSend Email: Build email campaigns with existing templates, or create a new email template within Journeys. Before you send the email, test the template and set conversion goals.\n\nSend SMS messages: Build SMS campaigns to message users in real-time as a step in a Journey. For example, create an abandoned cart campaign that texts users a reminder to complete their purchase, along with a promo code. Add merge tags and set conversion goals.\n\nSend WhatsApp messages: Build WhatsApp campaigns that deliver messages to your customers on the world\u2019s most used messaging app.\n\nTo learn more, visit the CSV Uploader documentation.\n\nBuild Email, SMS, and WhatsApp message templates\n\nBuild personalized email, SMS, and WhatsApp templates in Twilio Engage for use in your campaigns. Design email templates with a WYSIWYG Drag and Drop Editor or the HTML Editor. Engage saves the templates for you to preview, edit, and reuse throughout Journeys.\n\nPersonalize with merge tags\n\nInsert real-time user profile traits from merge tags to personalize each message. For example, address recipients by name or highlight new products from a user\u2019s favorite brand.\n\nCSV Uploader\n\nUse the CSV uploader to add or update user profiles and subscription states. To learn more, visit the CSV Uploader documentation.\n\nUser subscriptions\n\nSet user subscription states in two ways:\n\nUpload a CSV file with lists of users along with their phone, email, and WhatsApp subscription states.\nProgrammatically with Segment\u2019s Public API\n\nUse Engage to add subscription states to user email addresses and phone numbers. Subscription states help determine which users you can send campaigns to in Engage. You can set user subscription states with a CSV file upload, or programmatically with Segment\u2019s Public API.\n\nMessage Analytics\n\nWith analytics in Engage, you can monitor real-time conversion data. Track message performance and customer interaction beyond clicks and opens. Use campaign dashboards to view events such as Email Delivered, Unsubscribed, Spam Reported, and more.\n\nConversion Goals\n\nFor each message step in a Journey, you can set conversion conditions with events and properties in your Segment space. Then, define a duration after message delivery to track goals.\n\nFor example, track users who perform the event Order Completed with a promo code that you send them.\n\nVisit Message Analytics to learn more.\n\nThis page was last modified: 23 Aug 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nWhat can you do with Engage?\nMarket to customers with Engage Premier and Channels\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGetting Started\n/\nTesting and Debugging\nTesting and Debugging\n\nOne of the most important questions you\u2019ll ask early on is \u201cHow do I know if Segment is working?\u201d\n\nThere are several ways to check if your data is flowing. One is the Debugger tab in each Source in the Segment web app, where you can see data coming from a source into Segment. Another is the Event Delivery tool which shows which data is arriving at specific destinations.\n\nFor monitoring purposes, you\u2019ll also see alerts in the Workspace Health tool if your sources or destinations produce repeated errors.\n\nSegment University: Debugging and Troubleshooting\n\nWant more? Check out our course on debugging and troubleshooting. (Must be logged in to access.)\n\nThe Source Debugger\n\nThe Source Debugger is a real-time tool that helps you confirm that API calls made from your website, mobile app, or servers arrive to your Segment Source, so you can troubleshoot your Segment set up even quicker. With the Debugger, you can check that you\u2019re sending calls in the expected format, without having to wait for any data processing.\n\nThe Debugger is separate from your workspace\u2019s data pipeline and is not an exhaustive view of all the events ever sent to your Segment workspace. The Debugger only shows a sample of the events that the Source receives in real time, with a cap of 500 events. The Debugger is a great way to test specific parts of your implementation to validate that events are being fired successfully and arriving to your Source.\n\nTo see a more complete view of all your events, we recommend that you set up either a warehouse or an S3 destination.\n\nThe Debugger shows a live stream of sampled events arriving into the Source, but you can also pause the stream from displaying new events by toggling \u201cLive\u201d to \u201cPause\u201d. Events continue to arrive to your Source while you Pause the stream.\n\nYou can search in the Debugger to find a specific payload using any information you know is available in the event\u2019s raw payload. You can also use advanced search options to limit the results to a specific event.\n\nTwo views are available when viewing a payload:\n\nThe Pretty view is a recreation of the API call you made that was sent to Segment.\nThe Raw view is the complete JSON object Segment received from the calls you sent. These calls include all the details about what is being tracked: timestamps, properties, traits, ids, and contextual information Segment automatically collects the moment the data is sent.\nEvent Delivery\n\nThe Event Delivery tool helps you see if Segment is encountering issues delivering your data from your sources to their connected destinations.\u00a0\n\nSegment sends billions of events to destinations every week. If our systems encounter errors when trying to deliver your data, we report them in the Event Delivery tool.\n\nHere is an example of what the Event Delivery tool looks like:\n\nEvent Delivery is most useful when:\u00a0\n\nWhen data seems to be missing in your destination. For example, you have Google Analytics set up as a destination and your recent data looks incomplete\nWhen setting up a destination for the first time.\u00a0 For example, you are connecting Google Analytics to your Node Source. Once you\u2019ve entered your credentials and turned the destination on, you can use this feature to see whether events are successfully making it to GA in near realtime.\u00a0\n\nYou can access the Event Delivery tool from the destination Settings tab in any supported destination.\n\nEvent Delivery is only available for cloud-mode destinations, which receive data through the Segment servers. Device-mode destinations receive data through an API endpoint outside the Segment servers, where we cannot monitor or report on it. Event delivery is not available for Warehouses or Amazon S3 destinations.\n\nUsing Event Delivery\n\nThe UI shows three parts that report on Segment\u2019s ability to deliver your source data: Key Metrics, Error Details, and Delivery Trends.\n\nBefore you begin, select a time period from the drop down menu at the right. The Event Delivery display updates to show only information about your selected time period.\n\nKey metrics\n\nThis panel displays quantitative information about the destination\u2019s data flow:\n\nDelivered: The number of messages Segment successfully delivered to the destination in the selected time period.\n\nNot Delivered: The number of messages Segment was unable to deliver. If this number is greater than zero, the reasons for these failures appear in the errors table below.\u00a0\n\nP95 Latency: The time it takes for Segment to deliver the slowest 5% of your data (known as P95 latency). The latency reported is end-to-end: from the event being received through the Segment API, to the event being delivered to partner API.\u00a0This helps tell you if there is a delay in your data pipeline, and how severe it is.\n\nError details\n\nThe Error details table displays a summary of the errors in a given period, and the most important information about them. You can click any row in the table to expand it to show more information.\u00a0\n\nThe Error Details view gives you as much information as possible to help you resolve the issue. The example below shows an example Error Details panel.\u00a0\n\nThis view includes:\u00a0\n\nDescription The event delivery UI provides a human-friendly summary of the error, based on the payload Segment received back from the partner.\nActions These are actions you can take, based on what Segment knows about the issue.\u00a0\nMore Info Links to any documentation that might be helpful to you.\u00a0\nSample payloads To help you debug, Segment provides sample payloads from every step of the data\u2019s journey:\n\nYou Sent - the data you sent to Segment\u2019s API.\n\nRequest to Destination - the request Segment made to the Partner API. This payload will likely be different from what you sent it because Segment is mapping your event to the partner\u2019s spec to ensure the message is successfully delivered.\u00a0\n\nResponse from Destination - the response Segment received from the Partner API. This will have the raw partner error. If you need to troubleshoot an issue with a Partner\u2019s Success team, this is usually something they\u2019ll want to see.\u00a0\n\nView Segment\u2019s list of Integration Error Codes for more information about what might cause an error.\n\nTrends\n\nWhen debugging, it\u2019s helpful to see when issues start, stop and how they trend over time.\u00a0\n\nThe Event Delivery view shows a graph with the following information:\n\nDelivered: The number of events that were successfully delivered in the time period you selected.\u00a0\n\nNot delivered: The number of events that were not successfully delivered in the time period you selected.\u00a0\n\nThe Latency view shows the end-to-end P95 latency during the time period you selected.\n\nBACK\nSending data to Destinations\n\nUnlock the power of Segment with Destinations\n\nNEXT\nWhat's next?\n\nLearn about what you can do next with Segment\n\nThis page was last modified: 06 Jul 2022\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nThe Source Debugger\nEvent Delivery\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nRegional Segment\nRegional Segment\nFREE X\nTEAM X\nBUSINESS \u2713\nADD-ON X\n?\n\nOn July 10, 2023, the European Commission adopted the Adequacy Decision for the EU-US Data Privacy Framework (DPF). This concludes that EU personal data transferred to the United States under the DPF is adequately protected when compared to the protection in the EU. With this adequacy decision in place, personal data can safely flow from the EU to US companies participating in the DPF without additional safeguards in place.\n\nTwilio is certified under the DPF and relies on the DPF as its primary personal data transfer mechanism for EU-US personal data transfer. Twilio will rely on the DPF for any Swiss-US personal data transfers as soon as a corresponding Swiss adequacy decision is made. Twilio understands that interpretations of data residency are multi-faceted and some customers might still want their data to reside in the EU. Twilio Segment therefore offers a data residency solution outside of the DPF.\n\nSegment offers customers the option to lead on data residency by providing regional infrastructure in both Europe and the United States. The default region for all users is in Oregon, United States. You can configure workspaces to use the EU West Data Processing Region to ingest (for supported sources), process, filter, deduplicate, and archive data through Segment-managed archives hosted in AWS S3 buckets located in Dublin, Ireland. The regional infrastructure has the same rate limits and SLA as the default region.\n\nExisting Workspaces\n\nTo ensure a smooth transition from a US-based Segment workspace to an EU workspace, Segment will provide additional support and tooling to help with the transition later this year. Use the form link below to provide more information about your current setup and goals for transitioning.\n\nThe Segment UI doesn\u2019t support moving workspaces between regions. To request help with this move, complete the Data Residency Workspace Provisioning Flow form.\n\nClick to access the form\n\nRegional Data Ingestion\n\nRegional Data Ingestion enables you to send data to Segment from both Device-mode and Cloud-mode sources through regionally hosted API ingest points. The regional infrastructure can fail-over across locations within a region, but never across regions.\n\nCloud-event sources\n\nThe following cloud sources are supported in EU workspaces:\n\nAmazon S3\nFactual Engine\nIterable\nClient-side sources\n\nYou can configure Segment\u2019s client-side SDKs for JavaScript, iOS, Android, and React Native sources to send data to a regional host after you\u2019ve updated the Data Ingestion Region in that source\u2019s settings. Segment\u2019s EU instance only supports data ingestion from Dublin, Ireland with the events.eu1.segmentapis.com/ endpoint. If you are using the Segment EU endpoint with an Analytics-C# source, you must manually append v1 to the URL. For instance, events.eu1.segmentapis.com/v1.\n\nFor workspaces that use the EU West Data Processing region, the Dublin Ingestion region is preselected for all sources.\n\nTo set your Data Ingestion Region:\n\nGo to your source.\nSelect the Settings tab.\nClick Regional Settings.\nChoose your Data Ingestion Region.\nIf you\u2019re in the US West data processing region, you can select from: Dublin, Singapore, Oregon, and Sydney.\nIf you\u2019re in the EU West data processing region, Segment\u2019s EU instance only supports data ingestion from Dublin with the events.eu1.segmentapis.com/ endpoint.\n\nAll regions are configured on a per-source basis. You\u2019ll need to configure the region for each source separately if you don\u2019t want to use the default region.\n\nAll Segment client-side SDKs read this setting and update themselves automatically to send data to new endpoints when the app reloads. You don\u2019t need to change code when you switch regions.\n\nServer-side and project sources\n\nWhen you send data from a server-side or project source, you can use the host configuration parameter to send data to the desired region:\n\nOregon (Default) \u2014 https://events.segmentapis.com/v1\nDublin \u2014 https://events.eu1.segmentapis.com/\n\nIf you are using the Segment EU endpoint with an Analytics-C# source, you must manually append v1 to the URL. For instance, events.eu1.segmentapis.com/v1.\n\nHere is an example of how to set the host:\n\nAnalytics.Initialize(\"<YOUR WRITEKEY HERE>\", new Config().SetHost(\"https://events.eu1.segmentapis.com (https://events.eu1.segmentapis.com/)\"));\n\nCreate a new workspace with a different region\n\nUse this form if you need to transition from your existing US-based workspace to an EU workspace.\n\nTo create a workspace with a different data processing region, reach out your Segment account executive, and they will assist you with enabling the feature. Once the feature has been enabled, you\u2019ll be able to self-serve and create a new workspace in a different data processing region by following these steps:\n\nLog in to your Segment account.\nClick New Workspace.\nSelect your Data processing region. This determines the location in which Segment collects, processes, and stores data that\u2019s sent to and from your workspace. You can choose from US West or EU West.\nClick Create workspace.\n\nOnce you create a workspace with a specified data processing region, you can\u2019t change the region. You must create a new workspace to change the region.\n\nEU Storage Updates\nSegment Data Lakes (AWS)\n\nRegional Segment in the EU changes the way you configure the Segment Data Lakes (AWS) environment\n\nWarehouse Public IP Range\n\nUse Segment\u2019s custom CIDR 3.251.148.96/29 while authorizing Segment to write in to your Redshift or Postgres port. BigQuery doesn\u2019t require you to allow a custom IP address.\n\nKnown Limitations\n\nRegional Segment is currently limited to the EU. Future expansion of Regional Segment beyond the EU is under evaluation by Segment Product and R&D.\n\nEdge proxies are deprecated. Customers using Regional Endpoints may see US-based IP addresses in event payloads, Segment recommends using the US-based endpoint (api.segment.io) to preserve client IP addresses. For EU customers, Segment recommends using a Regionalized EU workspace.\n\nDestination support and Regional endpoint availability\n\nDon't see a regional endpoint for a tool you're using?\n\nAs more of the partner tools you use (Sources, Destinations, and Warehouses) start to support a regional endpoint, Segment will update this list. Your contact for that tool should have a timeline for when they\u2019re hoping to support regional data ingestion. You can also visit Segment\u2019s support page for any Segment-related questions.\n\nThe following integrations marked with a  (checkmark) support EU Regional endpoints.\n\nIntegrations available in EU workspaces do not guarantee data residency\n\nBefore you configure an integration, you should check directly with the integration partner to determine if they offer EU endpoints.\n\nAll\nDestinations\nWarehouses\nINTEGRATION\tUS WORKSPACE\tEU WORKSPACE W/ US ENDPOINT\tEU WORKSPACE W/ EU ENDPOINT\nDESTINATIONS\n1Flow\t\t\t\n1Flow Mobile Plugin\t\t\t\n1Flow Web (Actions)\t\t\t\n2mee\t\t\t\nAampe\t\t\t\nAB Smartly\t\t\t\nAB Tasty client side\t\t\t\nABsmartly (Actions)\t\t\t\nAccoil Analytics\t\t\t\nAcoustic (Actions)\t\t\t\nActable Predictive\t\t\t\nActions Pipedrive\t\t\t\nActiveCampaign\t\t\t\nAdikteev\t\t\t\nAdjust\t\t\t\nAdLearn Open Platform\t\t\t\nAdobe Analytics\t\t\t\nAdobe Target Cloud Mode\t\t\t\nAdobe Target Web\t\t\t\nAdQuick\t\t\t\nAdRoll\t\t\t\nAdtriba\t\t\t\nAggregations.io (Actions)\t\t\t\nAirship\t\t\t\nAirship (Actions)\t\t\t\nAkita Customer Success\t\t\t\nAlexa\t\t\t\nAlgolia Insights (Actions)\t\t\t\nAmazon Ads DSP and AMC\t\t\t\nAmazon EventBridge\t\t\t\nAmazon Kinesis\t\t\t\nAmazon Kinesis Firehose\t\t\t\nAmazon Lambda\t\t\t\nAmazon Personalize\t\t\t\nAmbassador\t\t\t\nAmberflo\t\t\t\nAmplitude\t\t\t\nAmplitude (Actions)\t\t\t\nAngler AI\t\t\t\nAnodot\t\t\t\nAppcues\t\t\t\nAppcues Mobile\t\t\t\nAppFit\t\t\t\nAppNexus\t\t\t\nAppsFlyer\t\t\t\nApptimize\t\t\t\nAsayer\t\t\t\nAstrolabe\t\t\t\nAtatus\t\t\t\nAttentive Mobile\t\t\t\nAttio (Actions)\t\t\t\nAttribution\t\t\t\nAuryc\t\t\t\nAutopilotHQ\t\t\t\nAvo\t\t\t\nAWS S3\t\t\t\nAzure Function\t\t\t\nBatch\t\t\t\nBeamer\t\t\t\nBing Ads\t\t\t\nBlackbaud Raiser's Edge NXT\t\t\t\nBlend Ai\t\t\t\nBlendo\t\t\t\nBlitzllama\t\t\t\nBloomreach Engagement\t\t\t\nBlueshift\t\t\t\nBranch Metrics\t\t\t\nBraze\t\t\t\nBraze Cloud Mode (Actions)\t\t\t\nBraze Cohorts\t\t\t\nBraze Web Device Mode (Actions)\t\t\t\nBreyta CRM\t\t\t\nBronto\t\t\t\nBucket\t\t\t\nBugHerd\t\t\t\nBugsnag\t\t\t\nButton\t\t\t\nBuzzBoard\t\t\t\nByteGain\t\t\t\nBytePlus\t\t\t\nCalixa\t\t\t\nCallingly\t\t\t\nCandu\t\t\t\nCanny (Actions)\t\t\t\nCastle\t\t\t\nChameleon\t\t\t\nChartbeat\t\t\t\nChartMogul\t\t\t\nChurned\t\t\t\nChurnZero\t\t\t\nClearbit Enrichment\t\t\t\nClearbit Reveal\t\t\t\nClearBrain\t\t\t\nCleverTap\t\t\t\nCleverTap (Actions)\t\t\t\nClicky\t\t\t\nClientSuccess\t\t\t\nCliff\t\t\t\nClose\t\t\t\nCommandBar\t\t\t\ncomScore\t\t\t\nContentstack Cloud\t\t\t\nContentstack Web\t\t\t\nConvertFlow\t\t\t\nConvertly\t\t\t\nConvertro\t\t\t\nCordial (Actions)\t\t\t\nCorrelated\t\t\t\nCountly\t\t\t\nCourier\t\t\t\nCrazy Egg\t\t\t\nCrisp\t\t\t\nCriteo App & Web Events\t\t\t\nCriteo Audiences\t\t\t\nCrittercism\t\t\t\nCrossing Minds\t\t\t\nCrowdPower\t\t\t\nCruncher\t\t\t\nCustify\t\t\t\nCustomer.io\t\t\t\nCustomer.io (Actions)\t\t\t\nData Lakes\t\t\t\nDelighted\t\t\t\nDelivr.ai Resolve\t\t\t\nDisplay and Video 360 (Actions)\t\t\t\nDoubleClick Floodlight\t\t\t\nDreamdata\t\t\t\nDrip\t\t\t\nDrip (Actions)\t\t\t\nDynamic Yield by Mastercard Audiences\t\t\t\nElevio\t\t\t\nEloqua\t\t\t\nEmail Aptitude\t\t\t\nEmarsys\t\t\t\nEmarsys (Actions)\t\t\t\nEMMA\t\t\t\nEncharge (Actions)\t\t\t\nEngage Messaging\t\t\t\nEnjoyHQ\t\t\t\nEPICA\t\t\t\nEquals\t\t\t\nErrorception\t\t\t\nevents.win\t\t\t\nEverflow\t\t\t\nExperiments by GrowthHackers\t\t\t\nExtole Platform\t\t\t\nFacebook App Events\t\t\t\nFacebook Conversions API (Actions)\t\t\t\nFacebook Custom Audiences (Actions)\t\t\t\nFacebook Offline Conversions\t\t\t\nFacebook Pixel\t\t\t\nFactorsAI\t\t\t\nFirebase\t\t\t\nFL0\t\t\t\nFlagship.io\t\t\t\nFlurry\t\t\t\nFoxMetrics\t\t\t\nFreshmarketer\t\t\t\nFreshsales\t\t\t\nFreshsales Suite - CRM\t\t\t\nFriendbuy (Cloud Destination)\t\t\t\nFriendbuy (Legacy)\t\t\t\nFriendbuy (Web Destination)\t\t\t\nFullStory\t\t\t\nFullstory (Actions)\t\t\t\nFullstory Cloud Mode (Actions)\t\t\t\nFunnelEnvy\t\t\t\nFunnelFox\t\t\t\nGainsight\t\t\t\nGainsight PX\t\t\t\nGainsight Px Cloud (Actions)\t\t\t\nGameball (Actions)\t\t\t\nGauges\t\t\t\nGist\t\t\t\nGleap (Action)\t\t\t\nGoogle Ads (Classic)\t\t\t\nGoogle Ads (Gtag)\t\t\t\nGoogle Ads Conversions\t\t\t\nGoogle Ads Remarketing Lists\t\t\t\nGoogle Analytics 4 Cloud\t\t\t\nGoogle Analytics 4 Web\t\t\t\nGoogle Cloud Function\t\t\t\nGoogle Cloud PubSub\t\t\t\nGoogle Cloud Storage\t\t\t\nGoogle Sheets\t\t\t\nGoogle Tag Manager\t\t\t\nGoSquared\t\t\t\nGraphJSON\t\t\t\nGroundswell\t\t\t\nGWEN (Actions)\t\t\t\nHasOffers\t\t\t\nHawkei\t\t\t\nHeap\t\t\t\nHello Bar\t\t\t\nHelp Scout\t\t\t\nHitTail\t\t\t\nHotjar\t\t\t\nHouseware\t\t\t\nHubble (Actions)\t\t\t\nHubSpot\t\t\t\nHubSpot Cloud Mode (Actions)\t\t\t\nHubSpot Web (Actions)\t\t\t\nHumanic AI\t\t\t\nhydra\t\t\t\nIBM UBX\t\t\t\nImpact Partnership Cloud\t\t\t\nImprovely\t\t\t\nIndicative\t\t\t\nInflection\t\t\t\nInkit\t\t\t\nInleads AI\t\t\t\nInMoment (formerly Wootric)\t\t\t\nInnovid\t\t\t\nInsider Audiences\t\t\t\nInsider Cloud Mode (Actions)\t\t\t\nInspectlet\t\t\t\nIntercom\t\t\t\nIntercom Cloud Mode (Actions)\t\t\t\nIntercom Web (Actions)\t\t\t\nIron.io\t\t\t\nIterable\t\t\t\nIterable (Actions)\t\t\t\nIterate Web (Actions)\t\t\t\nJimo\t\t\t\nJimo (Actions)\t\t\t\nJivox\t\t\t\njourny io\t\t\t\nJune\t\t\t\nJune (Actions)\t\t\t\nKable\t\t\t\nKafka\t\t\t\nKahuna\t\t\t\nKameleoon (Actions)\t\t\t\nKana\t\t\t\nKeen\t\t\t\nKevel\t\t\t\nKissmetrics\t\t\t\nKitemetrics\t\t\t\nKlaviyo\t\t\t\nKlaviyo (Actions)\t\t\t\nKoala\t\t\t\nKoala (Cloud)\t\t\t\nKochava\t\t\t\nKubit\t\t\t\nKustomer\t\t\t\nLaunchDarkly (Actions)\t\t\t\nLaunchDarkly Audiences\t\t\t\nLeanplum\t\t\t\nLearndot\t\t\t\nLibrato\t\t\t\nLinkedIn Audiences\t\t\t\nLinkedIn Conversions API\t\t\t\nLinkedIn Insight Tag\t\t\t\nListrak (Actions)\t\t\t\nLiveChat\t\t\t\nLiveIntent Audiences\t\t\t\nLiveLike\t\t\t\nLiveRamp Audiences\t\t\t\nLocalytics\t\t\t\nLogRocket\t\t\t\nLoops (Actions)\t\t\t\nLou\t\t\t\nLucky Orange\t\t\t\nLumen\t\t\t\nLytics\t\t\t\nmabl\t\t\t\nMadkudu\t\t\t\nMailChimp\t\t\t\nMailjet\t\t\t\nMailmodo\t\t\t\nMammoth\t\t\t\nMarketo Static Lists (Actions)\t\t\t\nMarketo V2\t\t\t\nMarkettailor\t\t\t\nMatcha\t\t\t\nMatomo\t\t\t\nMediaMath\t\t\t\nMetronome (Actions)\t\t\t\nMillennial Media\t\t\t\nMixpanel (Actions)\t\t\t\nMixpanel (Legacy)\t\t\t\nMoEngage\t\t\t\nMoengage (Actions)\t\t\t\nMoesif API Analytics\t\t\t\nMoloco MCM\t\t\t\nMonetate\t\t\t\nMoosend\t\t\t\nMouseflow\t\t\t\nMouseStats\t\t\t\nMovable Ink (Actions)\t\t\t\nMutiny\t\t\t\nNanigans\t\t\t\nNat\t\t\t\nNatero\t\t\t\nNavilytics\t\t\t\nNew Relic\t\t\t\nNielsen DCR\t\t\t\nNinetailed by Contentful\t\t\t\nNoora\t\t\t\nNudgespot\t\t\t\nOlark\t\t\t\nOneSignal (New)\t\t\t\nOptimizely Advanced Audience Targeting\t\t\t\nOptimizely Data Platform\t\t\t\nOptimizely Feature Experimentation (Actions)\t\t\t\nOptimizely Full Stack\t\t\t\nOptimizely Web\t\t\t\nOrb\t\t\t\nOrtto\t\t\t\nPardot (Actions)\t\t\t\nParsely\t\t\t\nPeaka\t\t\t\nPendo\t\t\t\nPendo Web (Actions)\t\t\t\nPerfect Audience\t\t\t\nPerkville\t\t\t\nPersistIQ\t\t\t\nPersonas Facebook Custom Audiences\t\t\t\nPersonyze\t\t\t\nPingdom\t\t\t\nPinterest Audiences\t\t\t\nPinterest Conversions API\t\t\t\nPinterest Tag\t\t\t\nPlanhat\t\t\t\nPlayerZero Web\t\t\t\nPlotline\t\t\t\nPodscribe (Actions)\t\t\t\nPodsights\t\t\t\nPointillist\t\t\t\nPostHog\t\t\t\nPostscript\t\t\t\nProductBird\t\t\t\nProfitWell\t\t\t\nProof Experiences\t\t\t\nProsperStack\t\t\t\nPushwoosh\t\t\t\nQualaroo\t\t\t\nQualtrics\t\t\t\nQuantcast\t\t\t\nQuanticMind\t\t\t\nQuora Conversion Pixel\t\t\t\nRabble AI\t\t\t\nRadiumOne Connect\t\t\t\nRamen\t\t\t\nRecombee AI\t\t\t\nReddit Conversions API\t\t\t\nRefersion\t\t\t\nRefiner\t\t\t\nRegal.io\t\t\t\nRehook\t\t\t\nRepeater\t\t\t\nResponsys\t\t\t\nRetina\t\t\t\nRevX Cloud (Actions)\t\t\t\nRichpanel\t\t\t\nRipe Cloud Mode (Actions)\t\t\t\nRipe Device Mode (Actions)\t\t\t\nRockerbox\t\t\t\nRokt\t\t\t\nRokt Audiences (Actions)\t\t\t\nRollbar\t\t\t\nRupt\t\t\t\nSaaSquatch v2\t\t\t\nSailthru v2\t\t\t\nSalescamp CRM\t\t\t\nSalesforce (Actions)\t\t\t\nSalesforce Marketing Cloud (Actions)\t\t\t\nSalesmachine\t\t\t\nSaleswings (Actions)\t\t\t\nSatisMeter\t\t\t\nSavio\t\t\t\nSchematic\t\t\t\nScopeAI\t\t\t\nScreeb\t\t\t\nScreeb Web (Actions)\t\t\t\nScuba Analytics\t\t\t\nSegment Connections\t\t\t\nSegment Data Lakes (Azure)\t\t\t\nSegment Profiles\t\t\t\nSegMetrics\t\t\t\nSelligent Marketing Cloud\t\t\t\nSendGrid\t\t\t\nSentry\t\t\t\nSerenytics\t\t\t\nShareASale\t\t\t\nSherlock\t\t\t\nSIGNL4 Alerting\t\t\t\nSimpleReach\t\t\t\nSingleStore\t\t\t\nSingular\t\t\t\nSkalin\t\t\t\nSlack\t\t\t\nSlack (Actions)\t\t\t\nSlicingDice\t\t\t\nSmartlook\t\t\t\nSnapboard\t\t\t\nSnapchat Audiences\t\t\t\nSnapchat Conversions API\t\t\t\nSnapEngage\t\t\t\nSpideo\t\t\t\nSpinnakr\t\t\t\nSplit\t\t\t\nSprig (Actions)\t\t\t\nSprig Cloud\t\t\t\nStackAdapt\t\t\t\nStartdeliver\t\t\t\nStartdeliver-v2\t\t\t\nStatsig\t\t\t\nStitch Data\t\t\t\nStonly\t\t\t\nStories\t\t\t\nStormly\t\t\t\nStrikedeck\t\t\t\nSurvicate\t\t\t\nSurvicate (Actions)\t\t\t\nSwrve\t\t\t\nTaboola (Actions)\t\t\t\nTalkable\t\t\t\nTalon.One\t\t\t\nTalon.One (Actions)\t\t\t\nTamber\t\t\t\nTaplytics\t\t\t\nTapstream\t\t\t\nThe Trade Desk Crm\t\t\t\nTikTok Audiences\t\t\t\nTikTok Conversions\t\t\t\nTiktok Offline Conversions\t\t\t\nTikTok Pixel\t\t\t\nToplyne Cloud Mode (Actions)\t\t\t\nTopsort\t\t\t\nTotango\t\t\t\nTrack JS\t\t\t\nTrackier\t\t\t\nTractionboard\t\t\t\nTrafficGuard\t\t\t\ntray.io\t\t\t\nTreasure Data\t\t\t\nTrustpilot\t\t\t\nTUNE\t\t\t\nTwitter Ads\t\t\t\nUnwaffle\t\t\t\nUpcall\t\t\t\nUpollo\t\t\t\nUpollo Web (Actions)\t\t\t\nUser.com\t\t\t\nUserGuiding\t\t\t\nUserIQ\t\t\t\nUserlist\t\t\t\nUsermaven (Actions)\t\t\t\nUserMotion (Actions)\t\t\t\nUserpilot Cloud (Actions)\t\t\t\nUserpilot Web (Actions)\t\t\t\nUserpilot Web Plugin\t\t\t\nUserVoice\t\t\t\nVariance\t\t\t\nVero\t\t\t\nVespucci\t\t\t\nVidora\t\t\t\nVisual Website Optimizer\t\t\t\nVitally\t\t\t\nVoucherify\t\t\t\nVoucherify (Actions)\t\t\t\nVWO Cloud Mode (Actions)\t\t\t\nVWO Web Mode (Actions)\t\t\t\nWalkMe\t\t\t\nWebEngage\t\t\t\nWebhooks (Actions)\t\t\t\nWhale Alerts\t\t\t\nWhale Watch\t\t\t\nWigzo\t\t\t\nWindsor\t\t\t\nWisepops\t\t\t\nWishpond\t\t\t\nWoopra\t\t\t\nXplenty\t\t\t\nXtremepush\t\t\t\nXtremepush (Actions)\t\t\t\nYahoo Audiences\t\t\t\nYandex Metrica\t\t\t\nYellowhammer\t\t\t\nYoubora\t\t\t\nZaius\t\t\t\nZapier\t\t\t\nZendesk\t\t\t\nZopim\t\t\t\nWAREHOUSES\nAmazon S3\t\t\t\nAWS S3\t\t\t\nAzure Synapse Analytics Warehouse\t\t\t\nBigQuery\t\t\t\nDatabricks\t\t\t\nIBM Db2 Warehouse\t\t\t\nPostgres\t\t\t\nRedshift\t\t\t\nSegment Data Lakes\t\t\t\nSnowflake\t\t\t\nSource Regional support\n\nDon't see regional support for a source you're using?\n\nAs more of the partner Sources start to support posting data to our regional endpoint, Segment will update this list. Your contact for that tool should have a timeline for when they\u2019re hoping to support regional data ingestion. You can also visit Segment\u2019s support page for any Segment-related questions.\n\nThe following Sources marked with a  (checkmark) are supported in EU workspaces.\n\nINTEGRATION\tUS WORKSPACE\tEU WORKSPACE\nSOURCES\n.NET\t\t\nActiveCampaign\t\t\nAircall\t\t\nAirship\t\t\nAlloy Flow\t\t\nAmazon S3\t\t\nAmplitude Cohorts\t\t\nAntavo\t\t\nApple\t\t\nAuthvia\t\t\nAutopilotHQ\t\t\nBeamer\t\t\nBlip\t\t\nBluedot\t\t\nBlueshift\t\t\nBraze\t\t\nCandu\t\t\nChatlio\t\t\nCleverTap\t\t\nClojure\t\t\nCommandBar\t\t\nConfigCat\t\t\nCustomer.io\t\t\nDelighted\t\t\nDrip\t\t\nElastic Path\t\t\nElastic Path CX Studio\t\t\nFacebook Ads\t\t\nFacebook Lead Ads\t\t\nFactual Engine\t\t\nFlutter\t\t\nFoursquare Movement\t\t\nFreshchat\t\t\nFriendbuy\t\t\nGladly\t\t\nGo\t\t\nGoogle Ads\t\t\nGWEN Webhooks\t\t\nHerow\t\t\nHTTP API\t\t\nHubSpot\t\t\nIBM Watson Assistant\t\t\nInflection\t\t\nInsider\t\t\nIntercom\t\t\nIterable\t\t\nJava\t\t\nJavascript\t\t\nJebbit\t\t\nKlaviyo\t\t\nKlenty\t\t\nKotlin\t\t\nKotlin (Android)\t\t\nLaunchDarkly\t\t\nLeanplum\t\t\nListrak\t\t\nLiveLike (Source)\t\t\nLooker\t\t\nMailchimp\t\t\nMailjet\t\t\nMailmodo\t\t\nMandrill\t\t\nMarketo\t\t\nMixpanel Cohorts\t\t\nMoEngage (Source)\t\t\nMoesif API Analytics\t\t\nNavattic\t\t\nNudgespot\t\t\nOne Creation\t\t\nOneSignal\t\t\nOneTrust\t\t\nPaytronix\t\t\nPendo\t\t\nPHP\t\t\nPixel Tracking API\t\t\nProveSource\t\t\nPushwoosh Source\t\t\nPython\t\t\nQualtrics\t\t\nQuin AI\t\t\nRadar\t\t\nReact Native\t\t\nRefiner\t\t\nRoku (alpha)\t\t\nRuby\t\t\nSalesforce\t\t\nSalesforce Marketing Cloud\t\t\nSelligent Marketing Cloud\t\t\nSendGrid\t\t\nSendGrid Marketing Campaigns\t\t\nShopify (by Littledata)\t\t\nShopify - Powered by Fueled\t\t\nStatsig\t\t\nStripe\t\t\nSynap\t\t\nTwilio\t\t\nUnity\t\t\nUpollo\t\t\nUserGuiding\t\t\nVero\t\t\nVoucherify\t\t\nWhite Label Loyalty\t\t\nWorkRamp\t\t\nXamarin\t\t\nYotpo\t\t\nZendesk\t\t\n\nThis page was last modified: 29 May 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nExisting Workspaces\nRegional Data Ingestion\nCreate a new workspace with a different region\nEU Storage Updates\nKnown Limitations\nDestination support and Regional endpoint availability\nSource Regional support\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nProtocols\n/\nApis And Extensions\n/\nAnomaly Detection\nAnomaly Detection\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nPROTOCOLS \u2713\n?\n\nIf you\u2019re using Protocols, you might want to get notifications when an anomaly in event volumes or Protocols violation counts occurs. This document clarifies what Segment means by anomaly detection, gives examples of anomalies that might be relevant to your business, and provides some example solutions of how to monitor and alert on anomalies using some standard tools available today.\n\nWhat is anomaly detection?\n\nAnomaly detection means finding out when your data collection is broken, missing, or incorrect.\n\nWhen you first start using Protocols, you might focus on fixing data quality issues for a limited set of business critical events. After those first issues are resolved, you might get notifications if new issues occur or if old issues reoccur, so you can avoid manually monitoring data quality. New issues often occur when a new app version is released, and for many companies, that\u2019s weekly.\n\nThe issues you care about for anomaly detection are different for each business. An anomaly for one company could be completely normal for another company. For example, an B2B company would expect a steep drop-off of traffic and event volume on weekends, while a media or entertainment company would expect to see a rise in activity in the evenings and on weekends for their different locales.\n\nOther types of issues you can monitor for include:\n\nAn increase or decrease of event volumes by more than X standard deviations (per source or event)\nAn increase of unique violations generated by Protocols (per source, event or property)\nAn increase of total violations generated by Protocols (per source, event or property)\nAn increase of unplanned events not defined in Tracking Plan\nAn increase of unplanned properties properties not defined in your Tracking Plan\nAnomaly Detection solutions\n\nThere are several easy ways to set up anomaly detection using the destination partner tools you probably already use. Many of these solutions come from Segment customers using Protocols. They use these solutions to help manage data quality and get notified when relevant anomalies are detected.\n\nYou can send anomalous events directly from your source to a Slack channel using the Slack (Actions) destination. To get started:\n\nCreate a Slack (Actions) destination.\nOnce you\u2019ve created your destination, select the Mappings tab and click + New Mapping.\nOn the Add Mapping popup, select Post Message.\nUnder \u201cSelect events to map and send\u201d, create an event with the following format:\n\nConfigure the rest of the Post Message settings and click Save.\n\nIf you\u2019re not using the Slack (Actions) destination to forward violations, Segment recommends that you create a new Segment source to collect all violations and Segment workspace activity. To do this, create a new HTTP source in your workspace, and assign a name that you can easily understand (for example, Protocols Audit Source).\n\nNext, set up Violation forwarding for each Tracking Plan connected to the Source. Once connected, your sources will look like the following diagram:\n\nNote: When you enable violation forwarding, it counts as 1 MTU toward your monthly MTU limit. If you are on an API plan, all forwarded violations count against your API limit. Violations might also generate costs in downstream destinations and data warehouses.\n\nOnce violation forwarding is enabled, you can build a custom anomaly detection solution that works for your business. The examples Segment covers here include:\n\nForward violations to a Slack channel\nCreate violation and event count Anomaly Detection dashboards in a BI tool\nForward violations to a Slack Channel\n\nAfter you\u2019ve enabled Violation Forwarding, enable the Slack destination for your Protocols Audit Source. In the destination\u2019s settings, add an Incoming Webhook URL for the Slack channel you want to push notifications to. Next, add the Violation Generated event to the Event Templates settings.\n\nYou can copy and paste the example snippet below into the Event Template field to format the Slack message with the event name, violation description and source name. You can customize this message however you want, including adding @ mentions, and any of the properties included in the Violation Generated event.\n\nSource: `{{properties.sourceName}}` \\nEvent: `{{properties.eventName}}` \\nViolation: `{{properties.violationDescription}}`\n\n\nWhen you\u2019re done, it\u2019ll look like the screenshot below.\n\nCreate customized Anomaly Detection dashboards in a BI tool\n\nCustom dashboards are a great way to focus your teams around the metrics and events that matter most to your business. With a few simple queries you can build a dashboard to share with teams, so everyone can understand how well they\u2019re doing against your data quality objectives. Here\u2019s an example dashboard that combines forwarded Violations with production event data to track data quality. See below for detailed SQL queries.\n\nNote: For all queries below, replace protocols_audit_source with whatever schema name you set for your forwarded violations source.\n\nSource-level event to violation count comparison: This query produces a table showing the total event and violation counts, along with a ratio of the two, broken out by day. A bar chart from this data can show when violations increase or decrease disproportionately to event volume in a source. This is the first place you would want to check to see if anomalies are occurring.\n\nwith\ntotal_track_event_volume as (\n      select  sent_at::date as date,\n              count(*) as total_event_count\n        from  <<YOUR_PROD_SOURCE_SCHEMA_NAME>>.tracks\n    group by  sent_at::date\n),\n\ntotal_violations as (\n      select  event_sent_at::date as date,\n              count(id) as violation_count\n        from  protocols_audit_source.violation_generated\n       where  source_slug = '<<YOUR SOURCE SLUG>>'\n         and  event_type = 'track'\n    group by  event_sent_at::date\n)\n\n      select  v.date,\n              t.total_event_count \"Total Violation Count\",\n              nvl(v.violation_count, 0) \"Total Event Count\",\n              nvl(v.violation_count, 0)::float/t.total_event_count::float as \"Violations Per Event\"\n        from  total_track_event_volume t\n   left join  total_violations v\n          on  t.date = v.date\n    order by  v.date desc\n\n\nRatio of High priority events to violation counts: This query produces a table showing all violations and event counts by day for a single event sent to Segment. A bar chart from this data can show when violations increase or decrease disproportionately to event volume for the single event. Segment recommends selecting a few events that are important for your business (for example, Order Completed, Video Viewed, User Signed Up) and creating a separate query and chart for each event.\n\nwith\ndistinct_track_event_volume as (\n      select  sent_at::date as date,\n              count(*) as event_count\n        from  <<YOUR_PROD_SOURCE_SCHEMA_NAME>>.<<IMPORTANT_EVENT_TABLE_NAME>>\n    group by  sent_at::date\n),\n\ndistinct_track_event_violations as (\n      select  event_sent_at::date as date,\n              count(id) as violation_count\n        from  protocols_audit_source.violation_generated\n       where  source_slug = '<<YOUR SOURCE SLUG>>'\n         and  event_name = '<<IMPORTANT EVENT NAME>>'\n    group by  event_sent_at::date\n)\n\n      select  v.date,\n              t.event_count as \"Distinct Event Count\",\n              nvl(v.violation_count, 0) as \"Violation Count\",\n              nvl(v.violation_count, 0)::float/t.event_count::float as \"Violations Per Distinct Event\"\n        from  distinct_track_event_volume t\n   left join  distinct_track_event_violations v\n          on  t.date = v.date\n    order by  v.date desc\n\n\nSource-level distinct and total violation count (Last 7 days):\n\nThis query produces a table that lists all sources connected to a Tracking Plan. For each source, the table shows distinct violations and total violations seen in the source. This table is similar to the violations summary view in the Segment app.\n\n      select  source_name,\n              count(distinct(violation_description)) as distinct_violations,\n              count(*) as total_violations\n        from  protocols_audit_source.violation_generated\n       where  event_sent_at >= current_date - 7\n    group by  source_name\n\n\nEvent violation count (Top 10):\n\nThis query produces a table listing the top 10 events with the most violations. A bar chart showing the worst offending events is a great way to focus your efforts on fixing the least reliable events.\n\n      select  event_name,\n              count(*) as total_violations\n        from  protocols_audit_source.violation_generated\n    group by  event_name\n    order by  total_violations desc\n       limit  10\n\n\nThis page was last modified: 02 Nov 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nWhat is anomaly detection?\nAnomaly Detection solutions\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nProtocols\n/\nProtocols: APIs and Extensions\nProtocols: APIs and Extensions\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nPROTOCOLS \u2713\n?\n\nBuilt from the ground up, Protocols addresses a wide range of customer needs.\n\nWith Protocols, you can help engineers reduce tracking errors, create issue notifications, and get the most out of your Tracking Plan. Below, learn about several Protocols resources that can help you address these and other common use cases.\n\nAnomaly detection\n\nIf you\u2019re using Protocols, you might want to get notifications when event volume anomalies or Protocols violation counts occur. Read Segment\u2019s anomaly detection documentation to learn about common anomalies, as well as monitoring and alerting solutions you can implement using standard tools.\n\nPublic API\n\nProtocols customers can access Segment\u2019s Public API, which enables programmatic creation, configuration, and fetching of core Segment platform resources like Sources, Destinations, and Tracking Plans.\n\nThe Public API represents Segment\u2019s commitment to developers, helping you extend your workflow around customer data collection and activation.\n\nSupported Operations\nList Tracking Plans\nGet Tracking Plan\nCreate Tracking Plan\nUpdate Tracking Plan\nDelete Tracking Plan\nTypewriter\n\nTypewriter is a tool for generating strongly-typed Segment analytics libraries based on your pre-defined Tracking Plan spec. View Segment\u2019s Typewriter documentation to get started.\n\nThis page was last modified: 03 Aug 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nAnomaly detection\nPublic API\nTypewriter\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nPrivacy\n/\nConsent Management\n/\nConsent in Unify\nConsent in Unify\nFREE X\nTEAM X\nBUSINESS \u2713\nADDON X\n?\n\nConsent in Unify and Twilio Engage is currently unavailable.\n\nHowever, Segment\u2019s OneTrust consent wrappers automatically generate the Segment Consent Preference Updated Track event, which will be required for future integrations with Unify and Twilio Engage.\n\nSegment uses Profiles in Unify as the source of truth of an end user\u2019s consent preference when enforcing consent in Twilio Engage. To get consent preference on the Profile, Segment requires the use of the Segment Consent Preference Updated event and Identify events to route events to Unify. The Segment Consent Preference Updated and Identify events should include the consent object.\n\nSegment Consent Preference Updated event\n\nEvery time an end user provides or updates their consent preferences, Segment requires you to generate a Segment Consent Preference Updated event. If you are using Segment\u2019s OneTrust consent wrappers, Segment automatically generates a Segment Consent Preference Updated event. This event is required to add the end user\u2019s consent preference on their Profile in Unify.\n\nFor example, if an end user agreed to share their information for functional and advertising purposes but not for analytics or data sharing, the Segment Consent Preference Updated Track call demonstrating their new consent preferences would have the following format:\n\n{\n  \"anonymousId\": \"23adfd82-aa0f-45a7-a756-24f2a7a4c895\",\n  \"type\": \"track\",\n  \"event\": \"Segment Consent Preference Updated\",\n  \"userId\": \"u123\",\n  \"traits\": {\n     \"email\": \"peter@example.com\",\n     \"phone\": \"555-555-5555\",\n  }\n  \"timestamp\": \"2023-01-01T00:00:00.000Z\",\n  \"context\": {\n    \"consent\": {\n      \"categoryPreferences\" : {\n        \"Advertising\": true,\n        \"Analytics\": false,\n        \"Functional\": true,\n        \"DataSharing\": false\n      }\n    }\n  }\n}\n\n\nIf you use Protocols, the Segment app automatically adds the Segment Consent Preference Updated event to all your existing Tracking Plans and for every new Tracking Plan. Segment recommends you don\u2019t edit or delete the default fields in the Segment Consent Preference Updated events, but you can add new fields as needed.\n\nSegment Consent Preference Updated is a reserved event name\n\nSegment has standardized a series of reserved event names that have special semantic meaning and maps these events to tools that support them.\n\nSee the Semantic Events docs for more details.\n\nSharing consent with Actions destinations\n\nIn addition to enforcing consent in Connections, you may want these preferences to flow to each destination so your destinations can be aware when an end-user revokes their consent. You can use the Destination Actions framework to edit the destination\u2019s mapping and copy the consent preferences from the Segment Consent Preference Updated event to a destination-specified consent field.\n\nIf you use Destination Actions to send consent information to your destinations, the Segment Consent Preference Updated event should only include information about a user\u2019s consent preferences because this event is sent regardless of an end-user\u2019s consent preferences.\n\nSharing consent with Classic Destinations is not available\n\nSegment only supports sharing consent with Actions Destinations.\n\nThis page was last modified: 07 May 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSegment Consent Preference Updated event\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSources\n/\nSource Schema\nSource Schema\n\nSegment Business Tier customers can use Schema Controls to manage which events are allowed to pass through Segment and on to Destinations. These filters are a first-line defense to help you protect the integrity of your data, and the decisions made with it.\n\nBlocking events within the source schema will exclude them from API and MTU calculations. These events are discarded before they reach the pipeline that Segment uses for MTU calculations.\n\nSchema view\n\nThe Schema tab shows the schema of events, properties, and traits for each source that Segment receives over a specific timeframe. It also shows when the events were last seen, how many events were allowed vs. blocked, and the downstream destinations those events are connected to.\n\nYou can view events by Segment call type in the Source Schema with the Track, Identify, and Group tabs. The Schema tracks:\n\nTrack event details by event name\nIdentify and Group event details by trait name\n\nClick the arrow to the left of the event name to view additional event properties for Page or Track events. Since the Schema tracks Identify traits, you will need to make sure you are passing traits into your Identify call in order to view event data in your schema.\n\nThe Schema shows \u201cPage Viewed\u201d for all Page calls under the Track tab.\n\nThe Source Schema UI changes slightly depending on whether you have a Protocols Tracking Plan connected to the source. If you have a Tracking Plan connected to your source, the UI displays a Planned column that will indicate if the event is planned or unplanned. This allows you to quickly identify unplanned events and take action to align your schema with your Tracking Plan. If there is no Tracking Plan connected to the source, the UI will display a toggle next to each event where, if you\u2019re a Business Tier customer, you can simply block or allow that event at the source level.\n\nArray properties are represented with an additional nested property representing the array\u2019s items. The nested property is the property\u2019s name with a .$ suffix. If an array property in the connected Tracking Plan does not include the items nested property, nested properties might be marked as unplanned in the Source Schema.\n\nEvent filters\n\nIf you no longer want to track a specific event, you can either remove it from your code or, if you\u2019re on the Business plan and don\u2019t have a Tracking Plan connected, you can block track calls from the Segment UI. To do so, click on the Schema tab in a Source and toggle the event to enable or block an event.\n\nFor sources with a connected Tracking Plan, use Protocols to block unplanned events.\n\nOnce you block an event, Segment stops forwarding it to all of your Cloud and Device-mode Destinations, including your warehouses. You can remove the events from your code at your leisure. In addition to blocking track calls, Business plan customers can block all Page and Screen calls, as well as Identify traits and Group properties.\n\nWhen an event is blocked, the name of the event or property is added to your Schema page with a counter to show how many events have been blocked. By default, data from blocked events and properties is not recoverable. You can always re-enable the event to continue sending it to downstream Destinations.\n\nIn most cases, blocking an event immediately stops that event from sending to Destinations. In rare cases, it can take up to six hours to fully block an event from delivering to all Destinations.\n\nIdentify and Group Trait Filters\n\nIf you no longer want to capture specific traits within .identify() and .group() calls, you can either remove those traits from your code, or if you\u2019re on the Business plan, you can block specific traits right from the Segment UI. To do so, click on the Schema tab in a Source and navigate to the Identify or Group events where you can block specific traits.\n\nBlocked traits are not omitted from calls to device-mode Destinations.\n\nSchema Integration Filters\n\nAll customers can filter specific events from being sent to specific Destinations (except for warehouses) by updating their tracking code. Here is an example showing how to send a single message only to Intercom and Google Analytics:\n\nanalytics.identify('user_123', {\n  email: 'jane.kim@example.com',\n  name: 'Jane Kim'\n}, {\n  integrations: {\n    'All': false,\n    'Intercom': true,\n    'Google Analytics': true\n  }\n});\n\n\nDestination flags are case sensitive and match the Destination\u2019s name in the docs (for example, \u201cAdLearn Open Platform\u201d, \u201cawe.sm\u201d, \u201cMailChimp\u201d, and so on).\n\nSegment Business tier customers can block track calls from delivering to specific Destinations in the Segment UI. Visit a Source Schema page and click on the Integrations column to view specific schema integration filters. Toggle the filter to block or enable an event to a Destination.\n\nThis page was last modified: 19 Mar 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSchema view\nEvent filters\nIdentify and Group Trait Filters\nSchema Integration Filters\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nHow To Guides\n/\nAutomating Multi-Channel Re-Engagement Campaigns\nAutomating Multi-Channel Re-Engagement Campaigns\n\nCompelling and engaging brands delight their customers at every interaction. As customers move seamlessly across channels\u2014such as email, push notifications, display ads\u2014brands must similarly meet them with tailored and consistent messages.\n\nWith Segment, you can craft a tailored message while using a combination of AdRoll, Customer.IO, and other tools to dynamically switch between channels.\n\nTalk to a product specialist today about using data to tailor your brand experience.\n\nTools used\n\nRetargeting with AdRoll: AdRoll is a retargeting and prospecting tool that allows you to show display ads to a behaviorally-defined cohort\n\nPush notifications with Braze: Braze is a multi-channel marketing campaign focused on the mobile experience\n\nEmails with Customer.io: Customer.io is a flexible email provider that allows you to create cohorts based on customer actions. You can build complex onboarding emails, nurture email campaigns, as well as marketing automation workflows.\n\nThere are other email tools on Segment\u2019s platform, such as Bronto, SendGrid, and Mailchimp. Check out the full list of email tools.\n\nIt\u2019s important to register for these tools and enable them on your Segment source project. When Segment collects tracking data, it\u2019ll also route it to all of your enabled tools. Then your tools, especially ones like Customer.io, Braze, and AdRoll, where you can define cohorts of your users, will be working off a dynamic, yet consistent data set. This is paramount in getting the dynamic messaging to update accordingly.\n\nSet it up\n\nWhen you send tracking data from your app or website to Segment, Segment will send the same data to all of your tools. Segment also collects key messaging events like Push Notification Opened and Email Opened from Braze and Customer.io, respectively, and sends that to other tools. By defining cohorts based on these events, you can create dynamic campaign audiences, to which customers can add and remove themselves.\n\nIn each of your destinations\u2014Braze, Facebook, Customer.io, AdRoll\u2014you can create custom campaigns to show display ads or send emails to a specific segment of users who have performed (or not performed) a given action, or \u201cevent.\u201d In this cross-channel re-engagement example, we\u2019ll start with push notifications.\n\n1st line of defense: the push notification\n\nIn Braze, create a segment of customers who added a product to their cart, but did not check out. The segment definition, in this case, should be people who have performed Product Added, but have not performed Order Completed . Send a push notification to these customers with a message that the cart was abandoned and that they can complete the transaction with, for example, a 10% coupon.\n\n2nd line of defense: the email reminder\n\nBecause Segment automatically collects second-party data from Braze, you now also have push notification event data, like Push Notification Opened and Push Notification Received in Segment. You can use the properties on each of these events to define a property called campaign_name so you can tie these activities to a given campaign.\n\nThis is helpful because now, you can define segments in Customer.io for customers who have triggered Push Notification Received, but not Push Notification Opened . You\u2019ve now automated the process of targeting customers who don\u2019t open your push notifications. In Customer.io, you can create a campaign that sends an email to those people asking them to check their push notifications and offering them a coupon to complete their order.\n\n3rd line of defense: paid advertising\n\nSince Segment collects email event data, like Email Opened, from Customer.io, you can similarly create segments in Facebook Ads and AdRoll for when customers don\u2019t open your email. Create a segment where users have an Email Delivered event, but no Email Opened event. When users meet these criteria, they\u2019ll get automatically added to your retargeting campaigns. You can then serve them custom creatives about them neglecting to open your emails and, again, perhaps offer them a coupon to complete the transaction.\n\nWhen users do not open an activation email, add them to a specific retargeting campaign that contains messaging to remind them to activate.\n\nWith Segment, automate not just switching across channels, but also the messaging in each channel so that the entire experience is cohesive. The added benefit is that we can create specifically targeted retargeting campaigns for people who no longer open our emails or push notifications. Automating these processes with Segment makes channel-switching more seamless for your customers.\n\nCreate an engaging and consistent brand experience\n\nThis is just a simple cart abandonment example that dynamically follows customers as they switch between channels. Because Segment collects and routes the second party data of emails and push notifications being opened, you can create specific campaigns with messaging that targets your customers as they interact with your brand.\n\nWith over 200+ different tools on Segment\u2019s platform, you can take this idea and create other tailored shopping experiences to re-engage your customers.\n\nTalk to a product specialist today about using data to tailor your brand experience.\n\nThis page was last modified: 06 Oct 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nTools used\nSet it up\n1st line of defense: the push notification\n2nd line of defense: the email reminder\n3rd line of defense: paid advertising\nCreate an engaging and consistent brand experience\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nThe page you were looking for doesn't exist.\n\nYou may have mistyped the address or the page may have moved. Double-check the URL and try again or search the term.\n\n404",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nData Graph\n/\nLinked Events Overview\nLinked Events Overview\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\n\nUse Linked Events to enrich real-time event streams with entities from your data warehouse to your destinations. Insert additional event context for downstream applications for richer data about each event.\n\nConsent enforcement for Linked Events\n\nYou can use Consent Management to enforce consent in your downstream destinations for Linked Events stamped with the consent object. You must enable Consent Management and have consent stamped on events from event streaming sources to use Consent Management. You cannot use Linked Events to enrich events with consent preferences that are stored in your warehouse.\n\nOn this page, you\u2019ll learn how to get started with Linked Events.\n\nLinked Events warehouse support\n\nLinked Events supports Snowflake, BigQuery, Redshift, and Databricks.\n\nUse cases\n\nWith Linked Events, you can:\n\nAdd details to events for precise targeting. Enable targeting by appending product events that only have product_id with full product SKU details from your warehouse.\nSync enriched data. Add a loyalty ID to event payloads before sending it downstream to destinations such as Amplitude, Mixpanel, and more.\nReduce load times. Enrich page view events with products and subscriptions connected to that view, and send that to Google Analytics 4 to lighten the front end and reduce page loading time.\nPrerequisites\n\nTo use Linked Events, you\u2019ll need the following:\n\nA supported data warehouse.\nAccess to Unify in your workspace.\nAccess to the actions-based destination you\u2019ll be using with Linked Events so that you can validate your data.\n\nSegment stores and processes all data in the United States.\n\nProfiles Sync isn\u2019t required for Linked Events.\n\nLinked Events roles\n\nThe following Segment access roles apply to Linked Events:\n\nEntities Admin Access: Entities Admins have the ability to view and edit entity models and connection details.\n\nEntities Read-only Access: Entities Read-only users have the ability to view entity models.\n\nTo create models and enrich events in destinations, you need to be a Workspace Owner or have the following roles:\n\nUnify Admin\nEntities Admin\nSource Admin\nStep 1: Set up your data warehouse and permissions\n\nLinked Events uses Segment\u2019s Reverse ETL infrastructure for pulling in data from your warehouse.\n\nTo get started, you\u2019ll need to set up your data warehouse and provide the correct access detailed in the set up steps below. Linked Events supports BigQuery, Databricks, Snowflake, and Redshift.\n\nStep 2: Connect your warehouse to the Data Graph\n\nBefore getting started with the Data Graph, be sure to set up your warehouse permissions.\n\nNavigate to Unify > Data graph and click Add warehouse.\nSelect a warehouse to connect from the supported data warehouses.\nConnect your warehouse.\nClick Test Connection to be sure your warehouse is connected.\nAfter a successful test, click Save.\nSchema\n\nLinked Events uses Reverse ETL to compute the incremental changes to your data directly within your data warehouse. The Unique Identifier column detects data changes, such as new, updated, and deleted records.\n\nFor Segment to compute data changes in your warehouse, Segment requires both read and write permissions to the warehouse schema table. At a high level, the extract process requires read permissions for the query being executed. Segment tracks changes to the query results through tables that Segment manages in a dedicated schema (for example, _segment_reverse_etl), which requires some write permissions.\n\nOnly sync what you need for enrichment. There may be cost implications to having Segment query your warehouse tables.\n\nLinked Events syncs data from your warehouse approximately once every hour.\n\nSupported data warehouses\n\nThe table below shows the data warehouses Linked Events supports. View the Segment docs for your warehouse, then carry out the corresponding steps.\n\nDATA WAREHOUSE\tSTEPS\nSnowflake\t1. Configure your snowflake database.\n2. Enter your credentials.\n3. Test the Connection.\n4. Click Save.\nBigQuery\t1. Add your credentials to the database that has tables with the entities you want to enrich your event with.\n2. Test your connection.\nRedshift\t1. Select the Redshift cluster you want to connect.\n2. Configure the correct network and security settings.\nDatabricks\t1. Configure your Databricks catalog.\n2. Enter your credentials.\n3. Test the Connection.\n4. Click Save.\nStep 3: Build your Data Graph\n\nThe Data Graph is a semantic layer that represents a subset of relevant business data that you\u2019ll use to enrich events in downstream tools. Use the configuration language spec below to add models to build out your Data Graph.\n\nEach Unify space has one Data Graph. The current version is v0.0.6 but this may change in the future as Segment accepts feedback about the process.\n\nDeleting entities and relationships are not yet supported.\n\nDefining entities\n\nSnowflake schemas are case sensitive, so you\u2019ll need to reflect the schema, table, and column names based on how you case them in Snowflake.\n\nAn entity is a stateful representation of a business object. The entity corresponds to a table in the warehouse that represents that entity.\n\nPARAMETERS\tDEFINITION\nentity\tA unique slug for the entity, which is immutable and treated as a delete if you make changes. The slug must be in all lowercase, and supports dashes or underscores (for example, account-entity or account_entity).\nname\tA unique label which will display across Segment.\ntable_ref\tDefines the table reference. In order to specify a connection to your table in Snowflake, a fully qualified table reference is required: [database name].[schema name].[table name].\nprimary_key\tThe unique identifier for the given table. Should be a column with unique values per row.\n(Optional) enrichment_enabled = true\tIndicates if you plan to also reference the entity table for Linked Events.\n# Define an entity and indicate if the entity will be referenced for Linked Events (enrichment_enabled=true)\n\nentity \"account-entity\" {\n     name = \"account\"\n     table_ref = \"CUST.ACCOUNT\"\n     primary_key = \"id\"\n     enrichment_enabled = true\n}\n\nStep 4: Add an actions-based destination\n\nTo use Linked Events, you\u2019ll need to add an action destination to send enriched events to. Navigate to Connections > Destinations. Select an existing action destination, or click + Add destination to add a new action destination.\n\nFor Linked Events, Segment supports Destination Actions in cloud-mode only.\n\nStep 5: Enrich events with entities\n\nWith Linked Events, you can select entities and properties from your data warehouse, then add enrichments to map properties to your connected destination.\n\nTo enrich events with entities:\n\nNavigate to Connections > Destinations > Event streams\nSelect the destination you\u2019d like to create an enrichment on.\nFrom the Destination overview page, click Mappings.\nClick New Mapping, and select the type of mapping you\u2019d like to add.\nClick the \u2026 icon to edit an existing mapping.\nIn the \u201cSelect Events to Map and Send\u201d, define the conditions under which the action should run.\nClick Load Sample Event, then add your entities.\nAdd entities\n\nAfter you load a sample event, you can add entities from the Enrich events with entities section. You\u2019ll select an entity, then an entity match property.\n\nThe entity match property is the property in the event that you want to match to the primary key.\n\nAfter you\u2019ve added an entity and match property, add your event enrichments.\n\nAdd enrichments\n\nUse enrichments to select the entity you wish to send to your downstream destination.\n\nIn the Mappings tab, locate the Select Mappings section where you can enrich source properties from the entities you\u2019ve selected in the previous step.\n\nSelect the property field that you\u2019d like to enrich, then select the Enrichments tab.\nSelect the entity you want to send to your destination.\nYou\u2019ll have access to all rows/columns in your data warehouse associated with the property you\u2019ve selected in the previous step.\nAdd the key name on the right side, which is what Segment sends to your destination.\n\nAt this time, Linked Events doesn\u2019t support a preview of enriched payloads.\n\nSave your enrichments\n\nWhen you\u2019re satisfied with the mappings, click Save. Segment returns you to the Mappings table.\n\nAt this time, when you select mappings or test events, you won\u2019t see enrichment data. Enrichment data is only available with real events.\n\nEnrichment observability\n\nTo verify which of your events matched one or more enrichments:\n\nNavigate to Delivery Overview for your connected destination.\nSelect the Successfully received step in the pipeline view.\nSelect the Events enriched tab. This table breaks down events into the following categories:\nSuccessfully enriched: Events that were enriched by all entities\nPartially enriched: Events that were only enriched by only some of your entities\nUnenriched events: Events that did not match any entities\nFAQs\nWhat data warehouse permissions does Segment require?\n\nTo use Linked Events, be sure that you have proper permissions for the Data Warehouse you\u2019re using. Visit the BigQuery, Databricks, Snowflake, and Redshift setup guides to learn more about updating permissions.\n\nHow often do syncs occur?\n\nSegment currently syncs once every hour.\n\nWhich Destinations does Linked Events support?\n\nFor Linked Events, Segment supports all actions-based destinations in cloud-mode. Device-mode destinations are not supported.\n\nWhy aren\u2019t test events working?\n\nTest events don\u2019t send Linked Events. You\u2019ll only see test events that come from the source debugger, which is ahead of the event enrichment.\n\nCan I view my Linked Events Audit Trail?\n\nLinked Events uses the existing Audit Trail in your Segment workspace. To view your Audit Trail, navigate to Settings > Admin > Audit Trail.\n\nHow can I refresh linked data from my warehouse?\n\nYou can define a schedule for refreshing the linked data from your data warehouse.\n\nHow do I use entities in my data graph with Linked Events?\n\nTo use entities with Linked Events, you\u2019ll need to set the enrichment_enabled flag to true. Here\u2019s the sample code:\n\n# Define an entity and indicate if the entity will be referenced for Linked Events (enrichment_enabled=true)\n\nentity \"account-entity\" {\n     name = \"account\"\n     table_ref = \"CUST.ACCOUNT\"\n     primary_key = \"id\"\n     enrichment_enabled = true\n}\n\n\nThis page was last modified: 15 Oct 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nUse cases\nPrerequisites\nStep 1: Set up your data warehouse and permissions\nStep 2: Connect your warehouse to the Data Graph\nStep 3: Build your Data Graph\nStep 4: Add an actions-based destination\nStep 5: Enrich events with entities\nEnrichment observability\nFAQs\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nTraits\n/\nPredictions\nPredictions\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY PLUS \u2713\n?\n\nPredictions, Segment\u2019s artificial intelligence and machine learning feature, lets you predict the likelihood that users will perform any event tracked in Segment.\n\nWith Predictions, you can identify users with, for example, a high propensity to purchase, refer a friend, or use a promo code. Predictions also lets you predict a user\u2019s lifetime value (LTV).\n\nSegment saves predictions to user profiles, letting you build Audiences, trigger Journeys, and send data to downstream destinations.\n\nFor more details on AI usage and data, see Predictions Nutrition Facts Label.\n\nOn this page, you\u2019ll learn how to build a prediction.\n\nBuild a prediction\n\nFollow these steps to build a prediction:\n\nIn the Trait Builder, click Predictions, select the prediction you want to create, then click Next..\n(For custom Predictive Goals) Add a condition(s) and event(s) to predict.\nSelect the event and (optional) property that you want to use to make a prediction.\nSelect a time period for the prediction.\n(Optional) In Include all events, uncheck any events you don\u2019t want Segment to factor into the prediction.\nClick Calculate. If you\u2019re satisfied with the available data, click Next.\n(Optional) Connect a Destination, then click Next.\nAdd a name and description for the Trait, then click Create Trait.\n\nKeep the following in mind when you build a prediction:\n\nSegment lets you predict the likelihood of a customer performing multiple events.\nYou can choose a time period of 15, 30, 60, 90, or 120 days.\nYou have granular control over the events Segment factors into the predictive model. By default, Segment\u2019s model makes predictions on all events sent to Engage. Segment lets you exclude events you don\u2019t want included by unselecting Include all events, then filtering out any events you want excluded from the model.\n\nIn the next section, you\u2019ll learn more about the four available predictions.\n\nChoosing a prediction\n\nSegment offers four predictions: Custom Predictive Goals, Likelihood to Purchase, Predicted LTV, and Likelihood to Churn.\n\nCustom Predictive Goals\n\nCustom Predictive Goals require a starting cohort, target event, and quality data.\n\nStarting cohort\n\nWhen you build a Custom Predictive Goal, you\u2019ll first need to select a cohort, or a group of users, for which you want to make a prediction. Traits with small cohorts compute faster and tend to be more accurate. If you want to predict for an entire audience, though, skip cohort selection and move to selecting a target event.\n\nTarget event\n\nThe target event is the Segment event that you want to predict. In creating a prediction, Segment determines the likelihood of the user performing the target event. Segment lets you include up to two target events and an event property in your prediction.\n\nAccess and data requirements\n\nIn machine learning, better data leads to better predictions. Because Segment prioritizes trust and performance, Segment has a number of data checks to ensure that each prediction is reliable and of high quality. Segment provides guidance in the UI before you create a trait, but some checks only occur during model training. If a trait fails, you\u2019ll see an error message and description in the UI.\n\nThis sections lists Segment\u2019s access and data requirements, service limits, and best practices for Predictions.\n\nDefinitions\nFeature Window: The past time period that contains the data used for model training.\nTarget Window: The time horizon for which you want to make the prediction. You can select this in the UI for each trait.\nTarget Event: The event predicting the likelihood of customer action.\n\nFor example, to predict a customer\u2019s propensity to purchase over the next 30 days, set the Target Window to 30 days and the Target Event to Order Completed (or the relevant purchase event that you track).\n\nPredictions access requirements\n\nTo access Predictions, you must:\n\nTrack more than 1 event type, but fewer than 2,000 event types. An event type refers to the total number of distinct events seen across all users in an Engage Space within the past 15 days.\nIf you currently track more than 2,000 distinct events, reduce the number of tracked events below this limit and wait around 15 days before creating your first prediction.\nEvents become inactive if they\u2019ve not been sent to an Engage Space within the past 15 days.\nTo prevent events from reaching your Engage Space, modify your event payloads to set integrations.Personas to false.\nFor more information on using the integrations object, see Spec: Common Fields, Integrations, and Filtering with the Integrations object.\nAnalytics.js example: analytics.track(\"Button Clicked\", {button:\"submit form\"}, {\"integrations\":{\"Personas\":false}})\nSuccessful trait computation\n\nThis table lists the requirements for a trait to compute successfully:\n\nREQUIREMENT\tDETAILS\nEvent Types\tTrack at least 5 different event types in the Feature Window.\nHistorical Data\tEnsure these 5 events have data spanning 1.5 times the length of the Target Window. For example, to predict a purchase propensity over the next 60 days, at least 90 days of historical data is required.\nSubset Audience (if applicable)\tEnsure the audience contains more than 1 non-anonymous user.\nUser Limit\tEnsure that you are making a prediction for fewer than 100 million users. If you track more than 100 million users in your space, define a smaller audience in the Make a Prediction For section of the custom predictions builder.\nUser Activity\tAt least 100 users performing the Target Event and at least 100 users not performing the Target Event.\nSelecting events (optional)\n\nSome customers want to specifically include or exclude events that get fed into the model. For example, if you track different events from an EU storefront compared to a US storefront and you only want to make predictions using data from the US, you could unselect the events from the EU space. This step is optional, Segment only recommends using it if you have a clear reason in mind for removing events from becoming a factor in the model.\n\nPredictive Traits and anonymous events\n\nPredictive Traits are limited to non-anonymous events, which means you\u2019ll need to include an additional external_id other than anonymousId in the targeted events. If want to create Predictive Traits based on anonymous events, reach out to your CSM with your use case for creating an anonymous Predictive Trait and the conditions for trait.\n\nLikelihood to Purchase\n\nLikelihood to Purchase is identical to Custom Predictive Goals, but Segment prefills the Order Completed event, assuming it\u2019s tracked in your Segment instance.\n\nIf you don\u2019t track Order Completed, choose a target event that represents a customer making a purchase.\n\nPredicted Lifetime Value\n\nPredicted Lifetime Value predicts a customer\u2019s future spend over the next 120 days. To create this prediction, select a purchase event, revenue property, and the currency (which defaults to USD). LTV is only calculated for customers that have performed the selected purchase events 2 or more times. The following table contains details for each property:\n\nPROPERTY\tDESCRIPTION\nPurchase event\tChoose a target event that represents a customer making a purchase. For most companies, this is usually Order Completed.\nPurchase amount\tSelect the purchase event property that represents the total amount. For most companies, this is the Revenue property.\nCurrency\tSegment defaults all currencies to USD.\nLikelihood to Churn\n\nLikelihood to Churn proactively identifies customers likely to stop using your product. Segment builds this prediction by determining whether or not a customer will perform a certain action.\n\nTo use Likelihood to Churn, you\u2019ll need to specify a customer event, a future time frame for which you want the prediction to occur, and if you want to know whether the customer will or won\u2019t perform the event.\n\nFor example, suppose you wanted to predict whether or not a customer would view a page on your site over the next three months. You would select not perform, Page Viewed, and at least 1 time within 90 days.\n\nChurn predictions are only made for eligible customers. In the previous example, only customers that have performed Page Viewed in the last 90 days would be eligible to recieve this prediction. The Segment app shows you which customers are eligibile to recieve this prediction.\n\nSegment then uses this criteria to build the prediction and create specific percentile cohorts. You can then use these cohorts to target customers with retention flows, promo codes, or one-off email and SMS campaigns.\n\nUse cases\n\nFor use cases and information on how Segment builds prediction, read Using Predictions.\n\nThis page was last modified: 05 Aug 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nBuild a prediction\nChoosing a prediction\nUse cases\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nFunctions\n/\nSet up functions for calling AWS APIs\nSet up functions for calling AWS APIs\n\nThe aws-sdk module is built-in, which allows you to make calls to AWS services in your own AWS accounts. The AWS SDK requires additional setup to ensure access to your AWS resources is secure. This page describes the process for allowing your functions to securely call AWS APIs in your AWS account.\n\nTo set up your functions to call AWS APIs:\n\nCreate an IAM role in your AWS account that your function will assume before making AWS API calls.\nMake sure you have these two values:\nPrincipal account ID: This is the ID number for the AWS account that your function runs in. For destination functions, this is 458175278816 and for source functions this is 300240842537.\nExternal ID: This is the value your IAM role uses to ensure that only your functions have the ability to assume the role. Segment recommends you to choose a long string of at least 32 random characters and treat it as if it were an API key or a password.\nCreate an IAM role in your AWS account with the minimum set of necessary permissions.\nAdd a trust relationship to your role with the following policy, filling in the principal account ID and external ID from step 1.1:\n {\n   \"Version\": \"2012-10-17\",\n   \"Statement\": [\n     {\n       \"Effect\": \"Allow\",\n       \"Principal\": {\n         \"AWS\": \"<PRINCIPAL_ACCOUNT_ID>\"\n       },\n       \"Action\": \"sts:AssumeRole\",\n       \"Condition\": {\n         \"StringEquals\": {\n           \"sts:ExternalId\": \"<EXTERNAL_ID>\"\n         }\n       }\n     }\n   ]\n }\n\nCreate your function.\nNow that you have an IAM role in your AWS account, you can create your source or destination function. Segment recommends you to use function settings to make the IAM role configurable. This allows you to use different roles for different instances of your function and to securely store your external ID value by making it a \u201csensitive\u201d setting. Here are the required settings:\nIAM Role ARN: A string setting that is the ARN for the IAM role above. For example, arn:aws:iam::1234567890:role/my-secure-role.\nIAM Role External ID: A sensitive string setting that is the external ID for your IAM role.\n\nBelow is an example destination function that uploads each event received to an S3 bucket (configured using additional \u201cS3 Bucket\u201d and \u201cS3 Bucket Region\u201d settings). It uses the built-in local cache to retain S3 clients between requests to minimize processing time and to allow different instances of the function to use different IAM roles.\n\n async function getS3(settings) {\n   const ttl = 30 * 60 * 1000; // 30 minutes\n   const key = [settings.iamRoleArn, settings.s3Bucket].join();\n\n   return cache.load(key, ttl, async () => {\n     const sts = new AWS.STS();\n\n     const opts = await sts\n       .assumeRole({\n         RoleArn: settings.iamRoleArn,\n         ExternalId: settings.iamRoleExternalId,\n         RoleSessionName: 'segment-function'\n       })\n       .promise()\n       .then(data => {\n         return {\n           region: settings.s3BucketRegion,\n           accessKeyId: data.Credentials.AccessKeyId,\n           secretAccessKey: data.Credentials.SecretAccessKey,\n           sessionToken: data.Credentials.SessionToken\n         };\n       });\n\n     return new AWS.S3();\n   });\n }\n\n async function onTrack(event, settings) {\n   const s3 = await getS3(settings);\n\n   return s3\n     .putObject({\n       Bucket: settings.s3Bucket,\n       Key: `${event.type}/${Date.now()}.json`,\n       Body: JSON.stringify(event)\n     })\n     .promise()\n     .then(data => {\n       console.log(data);\n     });\n }\n\n\nThis page was last modified: 11 May 2022\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nStorage\n/\nData Lakes\n/\nLake Formation\nLake Formation\nFREE X\nTEAM X\nBUSINESS \u2713\nADD-ON X\n?\n\nLake Formation is a fully managed service built on top of the AWS Glue Data Catalog that provides one central set of tools to build and manage a Data Lake. These tools help import, catalog, transform, and deduplicate data, as well as provide strategies to optimize data storage and security. To learn more about Lake Formation features, see Amazon Web Services documentation.\n\nThe security policies in Lake Formation use two layers of permissions: each resource is protected by Lake Formation permissions (which control access to Data Catalog resources and S3 locations) and IAM permissions (which control access to Lake Formation and AWS Glue API resources). When any user or role reads or writes to a resource, that action must pass a both a Lake Formation and an IAM resource check: for example, a user trying to create a new table in the Data Catalog may have Lake Formation access to the Data Catalog, but if they don\u2019t have the correct Glue API permissions, they will be unable to create the table.\n\nFor more information about security practices in Lake Formation, see Amazon\u2019s Lake Formation Permissions Reference documentation.\n\nConfigure Lake Formation\n\nYou can configure Lake Formation using the IAMAllowedPrincipals group or by using IAM policies for access control. Configuring Lake Formation using the IAMAllowedPrincipals group is an easier method, recommended for those exploring Lake Formation. Setting up Lake Formation using IAM policies for access control is a more advanced setup option, recommended for those who want additional customization options.\n\nPermissions required to configure Data Lakes\n\nTo configure Lake Formation, you must be logged in to AWS with data lake administrator or database creator permissions.\n\nConfigure Lake Formation using the IAMAllowedPrincipals group\nExisting databases\nOpen the AWS Lake Formation service.\nUnder Data catalog, select Settings. Ensure the checkboxes under the Default permissions for newly created databases and tables are not checked.\nUnder Permissions, select the Data lake permissions section. Click Grant.\nOn the Grant data permissions page, select the IAMAllowedPrincipals group in the Principals section.\nIn the Database permissions section, select the checkboxes for Super database permissions and Super grantable permissions.\nClick Grant.\nOn the Permissions page, verify the IAMAllowedPrincipals group has \u201cAll\u201d permissions.\nNew databases\nOpen the AWS Lake Formation service.\nUnder Data catalog, select Settings. Ensure the checkboxes under Default permissions for newly created databases and tables are not checked.\nSelect the Databases tab and click Create database. On the Create database page:\nSelect the Database button.\nName your database.\nSet the location to s3://$datalake_bucket/segment-data/.\nOptional: Add a description to your database.\nSelect the Use only IAM access control for new tables in this database.\nClick Create database.\nOn the Databases page, select your database. From the Actions menu, select Grant.\nOn the Grant data permissions page, select the IAMAllowedPrincipals group in the Principals section.\nIn the Database permissions section, select the checkboxes for Super database permissions and Super grantable permissions.\nClick Grant.\nOn the Permissions page, verify the IAMAllowedPrincipals group has \u201cAll\u201d permissions.\nVerify your configuration\n\nTo verify that you\u2019ve configured Lake Formation, open the AWS Lake Formation service, select Data lake permissions, and verify the IAMAllowedPrincipals group is listed with \u201cAll\u201d permissions.\n\nConfigure Lake Formation using IAM policies\n\nGranting Super permission to IAM roles\n\nIf you manually configured your database, assign the EMR_EC2_DefaultRole Super permissions in step 8. If you configured your database using Terraform, assign the segment_emr_instance_profile Super permissions in step 8.\n\nExisting databases\nOpen the AWS Lake Formation service.\nUnder Data catalog, select Settings. Ensure the checkboxes under the Default permissions for newly created databases and tables are not checked.\nOn the Databases page, select your database. From the Actions menu, select Grant.\nOn the Grant data permissions page, select the EMR_EC2_DefaultRole (or segment_emr_instance_profile, if you configured your data lake using Terraform) and segment-data-lake-iam-role roles in the Principals section.\nIn the Database permissions section, select the checkboxes for Super database permissions and Super grantable permissions.\nClick Grant.\nOn the Permissions page, verify the EMR_EC2_DefaultRole (or segment_emr_instance_profile) and segment-data-lake-iam-role roles have \u201cAll\u201d permissions.\nNew databases\nOpen the AWS Lake Formation service.\nUnder Data catalog, select Settings. Ensure the checkboxes under the Default permissions for newly created databases and tables are not checked.\nSelect the Databases tab and click Create database. On the Create database page:\nSelect the Database button.\nName your database.\nSet the location to s3://$datalake_bucket/segment-data/.\nOptional: Add a description to your database.\nClick Create database.\nOn the Databases page, select your database. From the Actions menu, select Grant.\nOn the Grant data permissions page, select the EMR_EC2_DefaultRole (or segment_emr_instance_profile, if you configured your data lake using Terraform) and segment-data-lake-iam-role roles in the Principals section.\nIn the Database permissions section, select the checkboxes for Super database permissions and Super grantable permissions.\nClick Grant.\nOn the Permissions page, verify the EMR_EC2_DefaultRole (or segment_emr_instance_profile) and segment-data-lake-iam-role roles have \u201cAll\u201d permissions.\n\nThis page was last modified: 03 Aug 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nConfigure Lake Formation\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nStorage\n/\nWarehouses\n/\nUseful SQL Queries for Redshift\nUseful SQL Queries for Redshift\n\nBelow you\u2019ll find a library of some of the most useful SQL queries customers use in their Redshift warehouses. You can run these in your Redshift instance with little to no modification.\n\nIf you\u2019re looking to improve the speed of your queries, check out Segment\u2019s Speeding Up Redshift Queries page.\n\nYou can use SQL queries for the following tasks:\n\nTracking events\nGrouping events by day\nDefine sessions\nHow to define user sessions using SQL\nIdentify users\nHistorical traits\nConvert the identifies table into a users table\nCounts of user traits\nGroups to accounts\nHistorical Traits\nConverting the Groups Table into an Organizations Table\n\nIf you\u2019re looking for SQL queries for warehouses other than Redshift, check out some of Segment\u2019s Analyzing with SQL guides.\n\nTracking events\n\nThe Track call allows you to record any actions your users perform. A Track call takes three parameters: the userId, the event, and any optional properties.\n\nHere\u2019s a basic Track call:\n\nanalytics.track('Completed Order',\n  item: 'pants',\n  color: 'blue'\n  size: '32x32'\n  payment: 'credit card'\n});\n\n\nA completed order Track call might look like this:\n\nanalytics.track('Completed Order', {\n item: 'shirt',\n color: 'green'\n size: 'Large'\n payment: 'paypal'\n});\n\n\nEach Track call is stored as a distinct row in a single Redshift table called tracks. To get a table of your completed orders, you can run the following query:\n\nselect *\nfrom initech.tracks\nwhere event = 'completed_order'\n\n\nThat SQL query returns a table that looks like this:\n\nBut why are there columns in the table that weren\u2019t a part of the Track call, like event_id? This is because the Track method (for client-side libraries) includes additional properties of the event, like event_id, sent_at, and user_id!\n\nGrouping events by day\n\nIf you want to know how many orders were completed over a span of time, you can use the date() and count function with the sent_at timestamp:\n\nselect date(sent_at) as date, count(event)\nfrom initech.tracks\nwhere event = 'completed_order'\ngroup by date\n\n\nThat query returns a table like this:\n\nDATE\tCOUNT\n2021-12-09\t5\n2021-12-08\t3\n2021-12-07\t2\n\nTo see the number of pants and shirts that were sold on each of those dates, you can query that using case statements:\n\nselect date(sent_at) as date,\nsum(case when item = 'shirt' then 1 else 0 end) as shirts,\nsum(case when item = 'pants' then 1 else 0 end) as pants\nfrom initech.tracks\nwhere event = 'completed_order'\ngroup by date\n\n\nThat query returns a table like this:\n\nDATE\tSHIRTS\tPANTS\n2021-12-09\t3\t2\n2021-12-08\t1\t2\n2021-12-07\t2\t0\nDefine sessions\n\nSegment\u2019s API does not impose any restrictions on your data with regard to user sessions.\n\nSessions aren\u2019t fundamental facts about the user experience. They\u2019re stories Segment builds around the data to understand how customers actually use the product in their day-to-day lives. And since Segment\u2019s API is about collecting raw, factual data, there\u2019s no API for collecting sessions. Segment leaves session interpretation to SQL partners, which let you design how you measure sessions based on how customers use your product.\n\nFor more on why Segment doesn\u2019t collect session data at the API level, check out a blog post here.\n\nHow to define user sessions using SQL\n\nEach of Segment\u2019s SQL partners allow you to define sessions based on your specific business needs. With Looker, for example, you can take advantage of their persistent derived tables and LookML modeling language to layer sessionization on top of your Segment SQL data. Segment recommends checking out Looker\u2019s approach here.\n\nTo define sessions with raw SQL, a great query and explanation comes from Mode Analytics.\n\nHere\u2019s the query to make it happen, but read Mode Analytics\u2019 blog post for more information. Mode walks you through the reasoning behind the query, what each portion accomplishes, how you can tweak it to suit your needs, and the kinds of further analysis you can add on top of it.\n\n-- Finding the start of every session\nSELECT *\n  FROM (\n       SELECT *\n              LAG(sent_at,1) OVER (PARTITION BY user_id ORDER BY sent_at) AS last_event\n        FROM \"your_source\".tracks\n      ) last\nWHERE EXTRACT('EPOCH' FROM sent_at) - EXTRACT('EPOCH' FROM last_event) >= (60 * 10)\n   OR last_event IS NULL\n\n-- Mapping every event to its session\nSELECT *,\n       SUM(is_new_session) OVER (ORDER BY user_id, sent_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS global_session_id,\n       SUM(is_new_session) OVER (PARTITION BY user_id ORDER BY sent_at ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS user_session_id\n  FROM (\n       SELECT *,\n              CASE WHEN EXTRACT('EPOCH' FROM sent_at) - EXTRACT('EPOCH' FROM last_event) >= (60 * 10) \n                     OR last_event IS NULL \n                   THEN 1 ELSE 0 END AS is_new_session\n         FROM (\n              SELECT *,\n                     LAG(sent_at,1) OVER (PARTITION BY user_id ORDER BY sent_at) AS last_event\n                FROM \"your_source\".tracks\n              ) last\n       ) final\n\nIdentify users\nHistorical traits\n\nThe Identify method ties user attributes to a userId.\n\nanalytics.identify('bob123',{\n  email: 'bob@initech.com',\n  plan: 'Free'\n});\n\n\nAs these user traits change over time, you can continue calling the Identify method to update their changes. With this query, you can update Bob\u2019s account plan to \u201cPremium\u201d.\n\nanalytics.identify('bob123', {\n  email: 'bob@initech.com',\n  plan: 'Premium'\n});\n\n\nEach Identify call is stored in a single Redshift table called identifies. To see how a user\u2019s plan changes over time, you can run the following query:\n\nselect email, plan, sent_at\nfrom initech.identifies\nwhere email = 'bob@initech.com'\n\n\nThis SQL query returns a table of Bob\u2019s account information, with each entry representing the state of his account at different time periods:\n\nUSER_ID\tEMAIL\tPLAN\tSENT_AT\nbob123\tbob@intech.com\tPremium\t2021-12-20 19:44:03\nbob123\tbob@intech.com\tBasic\t2021-12-18 17:48:10\n\nIf you want to see what your users looked like at a previous point in time, you can find that data in the identifies table. To get this table for your users, replace \u2018initech\u2019 in the SQL query with your source slug.\n\nIf you only want the current state of the users, convert the identifies table into a distinct users table by returning the most recent Identify call for each account.\n\nConvert the identifies table into a users table\n\nThe following query returns the identifies table:\n\nselect *\nfrom initech.identifies\n\n\nThat query returns a table like this:\n\nUSER_ID\tEMAIL\tPLAN\tSENT_AT\nbob123\tbob@intech.com\tPremium\t2021-12-20 19:44:03\nbob123\tbob@intech.com\tBasic\t2021-12-18 17:48:10\njeff123\tjeff@intech.com\tPremium\t2021-12-20 19:44:03\njeff123\tjeff@intech.com\tBasic\t2021-12-18 17:48:10\n\nIf all you want is a table of distinct user with their current traits and without duplicates, you can do so with the following query:\n\nwith identifies as (\n  select user_id,\n         email,\n         plan,\n         sent_at,\n         row_number() over (partition by user_id order by sent_at desc) as rn\n  from initech.identifies\n),\nusers as (\n  select user_id, \n         email,\n         plan\n  from identifies\n  where rn = 1\n)\n\nselect *\nfrom users\n\nCounts of user traits\n\nLet\u2019s say you have an identifies table that looks like this:\n\nUSER_ID\tEMAIL\tPLAN\tSENT_AT\nbob123\tbob@intech.com\tPremium\t2021-12-20 19:44:03\nbob123\tbob@intech.com\tBasic\t2021-12-18 17:48:10\njeff123\tjeff@intech.com\tPremium\t2021-12-20 19:44:03\njeff123\tjeff@intech.com\tBasic\t2021-12-18 17:48:10\n\nIf you want to query the traits of these users, you first need to convert the identifies table into a users table. From there, run a query like this to get a count of users with each type of plan:\n\nwith identifies as (\n  select user_id,\n         email,\n         plan,\n         sent_at,\n         row_number() over (partition by user_id order by sent_at desc) as rn\n  from initech.identifies\n),\nusers as (\n  select plan\n  from identifies\n  where rn = 1\n)\n\nselect sum(case when plan = 'Premium' then 1 else 0 end) as premium,\n       sum(case when plan = 'Free' then 1 else 0 end) as free\nfrom users\n\n\nAnd there you go: a count of users with each type of plan!\n\nPREMIUM\tFREE\n2\t0\nGroups to accounts\nHistorical Traits\n\nThe group method ties a user to a group. It also lets you record custom traits about the group, like the industry or number of employees.\n\nHere\u2019s what a basic group call looks like:\n\nanalytics.group('0e8c78ea9d97a7b8185e8632', {\n  name: 'Initech',\n  industry: 'Technology',\n  employees: 329,\n  plan: 'Premium'\n});\n\n\nAs these group traits change over time, you can continue calling the group method to update their changes.\n\nanalytics.group('0e8c78ea9d97a7b8185e8632', {\n  name: 'Initech',\n  industry: 'Technology',\n  employees: 600,\n  plan: 'Enterprise'\n});\n\n\nEach group call is stored as a distinct row in a single Redshift table called groups. To see how a group changes over time, you can run the following query:\n\nselect name, industry, plan, employees, sent_at\nfrom initech.groups\nwhere name = 'Initech'\n\n\nThe previous query will return a table of Initech\u2019s group information, with each entry representing the state of the account at different times.\n\nNAME\tINDUSTRY\tEMPLOYEES\tPLAN\tSENT_AT\nInitech\tTechnology\t600\tPremium\t2021-12-20 19:44:03\nInitech\tTechnology\t349\tFree\t2021-12-18 17:18:15\n\nIf you want to see a group\u2019s traits at a previous point in time, this query is useful (To get this table for your groups, replace \u2018initech\u2019 with your source slug).\n\nIf you only want to see the most recent state of the group, you can convert the groups table into a distinct groups table by viewing the most recent groups call for each account.\n\nConverting the Groups Table into an Organizations Table\n\nThe following query will return your groups table:\n\nselect *\nfrom initech.groups\n\n\nThe previous query returns the following table:\n\nNAME\tINDUSTRY\tEMPLOYEES\tPLAN\tSENT_AT\nInitech\tTechnology\t600\tPremium\t2021-12-20 19:44:03\nInitech\tTechnology\t349\tFree\t2021-12-18 17:18:15\nAcme Corp\tEntertainment\t15\tPremium\t2021-12-20 19:44:03\nAcme Corp\tEntertainment\t10\tFree\t2021-12-18 17:18:15\n\nHowever, if all you want is a table of distinct groups and current traits, you can do so with the following query:\n\nwith groups as (\n  select name,\n         industry,\n         employees,\n         plan,\n         sent_at,\n         row_number() over (partition by name order by sent_at desc) as rn\n  from initech.groups\n),\norganizations as (\n  select name,\n         industry,\n         employees,\n         plan\n  from groups\n  where rn = 1\n)\n\nselect *\nfrom organizations\n\n\nThis query will return a table with your distinct groups, without duplicates.\n\nNAME\tINDUSTRY\tEMPLOYEES\tPLAN\tSENT_AT\nInitech\tTechnology\t600\tPremium\t2021-12-20 19:44:03\nAcme Corp\tEntertainment\t15\tPremium\t2021-12-20 19:44:03\n\nThis page was last modified: 21 Apr 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nTracking events\nDefine sessions\nIdentify users\nGroups to accounts\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGetting Started\n/\nUse Cases\n/\nChoosing a Use Case\nChoosing a Use Case\n\nSegment built Use Cases to streamline the process of implementing Segment for specific business objectives.\n\nThis guide will help you navigate through the available use cases and select the one that best aligns with your business goals.\n\nYou can onboard to Segment with a Use Case if you\u2019re a new Business Tier customer or haven\u2019t yet connected a source and destination.\n\nUnderstanding business goals and use cases\n\nSegment supports 25 use cases, organized into 4 main business goals:\n\nOptimize advertising\nPersonalize first conversion\nBoost retention, upsell, and cross-sell\nPersonalize communications and product experiences\n\nThese goals represent key ways businesses often use customer data for improved performance and growth.\n\nLooking for a technical breakdown of each use case? View the Use Cases Reference.\n\nSelecting your use case\n\nFollow these steps to identify which use case to implement:\n\nIdentify your primary business challenge or goal from the four business goals.\nReview the use cases associated with that goal, considering how each aligns with your specific needs.\nEvaluate your current data collection capabilities and the resources you have available.\nConsider your long-term business strategy and how different use cases might support your future goals.\nIf you\u2019re unsure, start with a use case that addresses your most pressing current need.\n\nThe use case you select will guide your Segment setup, including the events you\u2019ll track and the integrations you\u2019ll implement. However, Segment\u2019s flexibility allows you to adapt and expand your strategy over time as your business needs evolve.\n\nThe following sections explore each business goal and associated use cases in detail.\n\nOptimize advertising\n\nThe Optimize advertising business goal focuses on improving the efficiency and effectiveness of your advertising efforts. By using your customer data effectively, you can create more targeted campaigns, reduce wasted ad spend, and increase your return on investment (ROI).\n\nKey considerations for this goal:\n\nAre you looking to expand your customer base with similar high-value customers?\nDo you need to drive app installations?\nAre you trying to increase signups or prevent cart abandonment?\nDo you want to retain high-value customers or optimize your ad spend?\n\nUse cases in this category include:\n\nUSE CASE\tDESCRIPTION\nBuild high-value lookalikes\tIdentify and target potential customers who share characteristics with your most valuable existing customers.\nBuild lookalikes for app install\tFind potential users who are likely to install your app. This is particularly useful for mobile app businesses looking to efficiently grow their user base.\nIncrease signups with lookalikes\tTarget potential users who are likely to sign up for your service, based on the characteristics of your existing registered users.\nMitigate cart abandonment\tIdentify users who have abandoned their carts and create targeted campaigns to encourage these users to complete their purchases.\nMitigate high value churn\tFocus on identifying high-value customers who are at risk of churning and create targeted campaigns to retain them.\nSuppress based on time\tOptimize your ad spend by suppressing ads to users who have recently converted or interacted with your brand, preventing unnecessary ad exposure.\nSuppress with purchase\tFocus on suppressing ads to users who have recently made a purchase.\nPersonalize first conversion\n\nThe Personalize first conversion goal focuses on optimizing the initial interactions a potential customer has with your brand. By personalizing these early touchpoints, you can increase the likelihood of converting prospects into customers.\n\nKey considerations for this goal:\n\nAre you looking to increase app installations or user sign-ups?\nDo you want to improve your onboarding process?\nAre you trying to convert free users to paid subscribers?\nDo you need to reduce cart abandonment rates?\n\nUse cases in this category include:\n\nUSE CASE\tDESCRIPTION\nAccelerate app install\tFocus on optimizing the user journey to encourage app installation.\nAccelerate onboarding\tCreate a personalized onboarding experience once a user has signed up or installed your app.\nAccelerate signup\tOptimize the signup process, reducing friction and personalizing the experience to encourage more users to complete registration.\nAcquire paid subscriptions\tFocus on identifying the most effective strategies to convert free users to paid subscribers.\nConvert trials to paid subscriptions\tTailored for businesses offering free trials. This use case helps you identify the best times and methods to encourage trial users to convert to paid subscriptions.\nMitigate cart abandonment\tIdentify users who have abandoned their carts and create targeted campaigns to encourage these users to complete their purchases.\nBoost retention, upsell, and cross-sell\n\nThe Boost retention, upsell, and cross-sell business goal focuses on maximizing the value of your existing customer base. By analyzing customer behavior and preferences, you can create targeted strategies to encourage repeat purchases, introduce customers to higher-value products or services, and increase overall customer lifetime value.\n\nKey considerations for this goal:\n\nDo you want to find more customers who share traits with your most valuable existing customers?\nAre you looking to increase the frequency of purchases from existing customers?\nDo you need to prevent churn among your high-value customers?\nDo you need to personalize your upsell or cross-sell efforts?\n\nUse cases in this category include:\n\nUSE CASE\tDESCRIPTION\nBuild high value lookalikes\tIdentify characteristics of your most valuable customers to inform retention and upsell strategies.\nIncrease repeat purchases\tAnalyze customer purchase history and behavior to create personalized recommendations and incentives that encourage repeat purchases.\nMitigate high value churn\tFocus on identifying high-value customers who are at risk of churning and create targeted campaigns to retain them.\nNurture with content\tFocus on creating and delivering personalized content to keep customers engaged with your brand between purchases, ultimately driving long-term loyalty.\nPersonalize upsell content\tAnalyze customer behavior and purchase history to create targeted upsell recommendations, increasing the average order value and customer lifetime value.\nPersonalize winback\tFocus on re-engaging inactive customers, using personalized messaging and offers based on their past behavior and preferences.\nPersonalize communications and product experiences\n\nThe Personalize communications and product experiences business goal focuses on creating tailored experiences for your customers across all touchpoints. With this business goal, you can create more relevant and engaging communications and product experiences, leading to increased satisfaction and loyalty.\n\nKey considerations for this goal:\n\nDo you want to personalize your onboarding process?\nDo youw want to increase customer engagement and repeat purchases?\nDo you need to create targeted content for different user segments?\nAre you trying to re-engage inactive customers?\n\nUse cases in this category include:\n\nUSE CASE\tDESCRIPTION\nAccelerate onboarding\tCreate a personalized onboarding experience that extends beyond initial signup, helping to drive long-term engagement.\nIncrease repeat purchases\tFocus on personalizing the overall customer experience to drive repeat purchases.\nMitigate high value churn\tCreate personalized experiences and communications to retain high-value customers at risk of churning.\nNurture with content\tDeliver personalized content experiences based on individual user interests and behaviors, keeping customers engaged with your brand.\nPersonalize upsell content\tFocus on personalizing the entire product experience to facilitate upsells.\nPersonalize winback\tCreate personalized re-engagement campaigns for inactive users, tailoring the messaging and offers based on their past interactions with your brand.\nNext steps\n\nOnce you\u2019ve selected a use case, follow the Use Cases Setup Guide, which explains how to set up a use case.\n\nThis page was last modified: 08 Oct 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nUnderstanding business goals and use cases\nSelecting your use case\nOptimize advertising\nPersonalize first conversion\nBoost retention, upsell, and cross-sell\nPersonalize communications and product experiences\nNext steps\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nEvent Delivery\nEvent Delivery\n\nThe Event Delivery tool helps you understand if data is reaching your destinations, and also helps you to see if Segment encountered any issues delivering your source data.\u00a0\n\nSegment sends billions of events to destinations every week. If Segment encounters any errors when attempting to deliver your data, Segment reports them in the Event Delivery tool.\n\nAvailable for server side event streaming destinations only\n\nThis feature is only available for server side integrations (also known as cloud-mode destinations). You will not be able to use this for client side integrations (also known as device-mode destinations) because device-mode data is sent directly to the destination tool\u2019s API. In order to report on deliverability, the data must be sent to destinations using a server side connection.\n\nNot available for Warehouses or Amazon S3. These destinations work differently from other destinations in Segment, and aren\u2019t supported at this time.\n\nHere\u2019s an example of what the Event Delivery tool looks like:\n\nWhen to use Event Delivery\n\nScenarios when this tool will be useful:\u00a0\n\nWhen data seems to be missing in your destination. For example, you have Google Analytics set up as a destination and your recent data looks incomplete.\nWhen setting up a destination for the first time.\u00a0 For example, you are connecting Google Analytics to your Node Source. Once you\u2019ve entered your credentials and turned the destination on, you can use this feature to see whether events are successfully making it to GA in near realtime.\u00a0\nWhere do I find it?\u00a0\n\nEvent Delivery can be accessed within any supported destination in the App. It\u2019s located on the tab under \u201cSettings\u201d for each destination.\u00a0\n\nHow do I use Event Delivery?\n\nThe UI consists of three key parts that report on Segment\u2019s ability to deliver your source data - Key Metrics, Error Details, and Delivery Trends. Reporting on core functionality from top to bottom:\u00a0\n\n1. Time period\n\nThere\u2019s a drop down menu to select your time period. All of the UI is updated based on the time period you select.\u00a0\n\n2. Key Metrics\n\nFrom left to right in the above graphic:\n\nDelivered: This tells you the number of messages Segment successfully delivered to a destination in the time period you selected.\n\nNot Delivered: This count tells you the number of messages Segment was unable to deliver. If this number is higher than zero, you will see the reasons for this failure in the errors table below.\u00a0\n\nP95 Latency: This is the time it takes for Segment to deliver the slowest 5% of your data (known as P95 latency). The latency reported is end-to-end: the event being received through the Segment API to the event being delivered to partner API.\u00a0This helps tell you if there is a delay in your data and how bad it is.\n\n3. Error details\n\nThe purpose of the table is to provide you a summary of the different errors we\u2019ve seen in a given period and the most important information on them. All of the rows in the table are clickable and expand to give you more information.\u00a0\n\nView Segment\u2019s list of Integration Error Codes for more information about what might cause an error.\n\nError detail view\n\nWhen there\u2019s an error, Segment wants to give you as much information as possible to help you resolve the issue. See below for an example of what this view looks like.\u00a0\n\nThis view includes:\u00a0\n\nDescription\n\nThe event delivery UI provides a human-friendly summary of the error, based on the payload Segment received back from the partner.\n\nActions\n\nThese are actions you can take, based on what Segment knows about the issue.\u00a0\n\nMore Info\n\nThis section displays links to any documentation that might be helpful to you.\u00a0\n\nSample payloads\n\nTo help you debug, Segment provides sample payloads from every step of the data\u2019s journey:\n\nYou Sent - the data you sent to Segment\u2019s API.\n\nRequest to Destination - the request Segment made to the Partner API. This payload will likely be different from what you sent it because Segment is mapping your event to the partner\u2019s spec to ensure the message is successfully delivered.\u00a0\n\nResponse from Destination - the response Segment received from the Partner API. This will have the raw partner error. If you need to troubleshoot an issue with a Partner\u2019s Success team, this is usually something they\u2019ll want to see.\n\nEmail Alerts\n\nYou can opt in to receive email alerts regarding failed events/syncs by going to your workspaace Settings > User Preferences > Activity Notifications > Destiantions. Then you can select the option to be alerted by Email and/or In-app.\n\n4. Trends\n\nWhen debugging, it\u2019s helpful to see when issues start, stop, and trend over time.\u00a0\n\nEvent Delivery\n\nDelivered: The number of events that were successfully delivered in the time period you selected.\u00a0\n\nNot delivered: The number of events that were not successfully delivered in the time period you selected.\u00a0\n\nLatency\n\nHow P95 latency has trended over the time period you selected.\n\nThis page was last modified: 05 Mar 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nWhen to use Event Delivery\nWhere do I find it?\u00a0\nHow do I use Event Delivery?\n1. Time period\n2. Key Metrics\n3. Error details\n4. Trends\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nLocate your Write Key\nLocate your Write Key\n\nThe write key is a unique identifier for each source. It lets Segment know which source is sending the data and which destinations should receive that data.\n\nFind the write key for a source\n\nTo find a write key, you first need to create an event streams source like a website, server, or mobile source. (Cloud-sources do not have write keys, as they use a token or key from your account with that service.)\n\nThen, in the Source, go to Settings and select API Keys.\n\nNow you can add the source\u2019s write key to your app and begin sending data to Segment.\n\nLocate a source using your write key\n\nTo find the source given a write key within your workspace, open your workspace and select the search icon. Enter your write key into the search bar. If the write key exists in the workspace and is connected to a source, the source shows up in the list of results.\n\nThis method is only available to locate event streams sources\n\nThis method cannot be used to find a destination or cloud event source.\n\nThis page was last modified: 20 Nov 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nFind the write key for a source\nLocate a source using your write key\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nProfiles Sync\n/\nProfiles Sync Tables and Materialized Views\nProfiles Sync Tables and Materialized Views\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\n\nThrough Profiles Sync, Segment provides data sets and models that can help you enrich customer profiles using any warehouse data available to you.\n\nUsing a practical example of how Segment connects and then merges anonymous profiles, this page explains the tables that Segment lands, as well as the tables you materialize as part of Profiles Sync.\n\nCase study: anonymous site visits lead to profile merge\n\nTo help illustrate the possible entries and values populated into Profiles Sync tables, view the event tabs below and consider the following scenario.\n\nSuppose the following four events lead to the creation of two separate profiles:\n\nEvent 1\nEvent 2\nEvent 3\nEvent 4\n// An anonymous visit to twilio.com triggers a Page call:\n\nanonymous_id: 5285bc35-05ef-4d21\ncontext.url: twilio.com\ntimestamp: May 2, 14:01:00\n\n// Segment generates Profile 1, with a single known ID: 5285bc35-05ef-4d21\n\n\n\nInitially, Segment generates two profiles for the first three calls. In the final event, though, Segment understands that Profile 2 should be merged into Profile 1. Segment then merges Profile 2 into Profile 1, merging away Profile 2 in the process.\n\nProfiles Sync tracks and provides information about these events through a set of tables, which you\u2019ll learn about in the next section.\n\nProfile raw tables\n\nProfile raw tables contain records of changes to your Segment profiles and Identity Graph over time.\n\nWith raw tables, you have full control over the materialization of Profiles in your warehouse, as well as increased observibility.\n\nRaw tables contain complete historical data when using historical backfill.\n\nThe id_graph_updates table\n\nThe id_graph_updates table maps between the following:\n\nsegment_id: the profile ID that Segment appends to an event or an identifier at the time it was first observed\ncanonical_segment_id: the fully-merged segment ID (that is, the profile Segment now understands any events or identifiers to map to)\n\nAs a result, this table contains information about the creation and merging of profiles, as well as the specific events that triggered those changes.\n\nUsing the events from the profile merge case study, Segment would generate three new entries to this table:\n\nSEGMENT_ID (VARCHAR)\tCANONICAL_SEGMENT_ID (VARCHAR)\tTRIGGERING_EVENT_TYPE (VARCHAR)\tTRIGGERING_EVENT_ID (VARCHAR)\tTIMESTAMP (DATETIME)\nprofile_1\tprofile_1\tpage\tevent_1\t2022-05-02 14:01:00\nprofile_2\tprofile_2\tpage\tevent_3\t2022-06-22 10:47:15\nprofile_2\tprofile_1\tidentify\tevent_4\t2022-06-22 10:48:00\n\nIn this example, the table shows profile_2 mapping to two places: first to itself, then, later, to profile_1 after the merge occurs.\n\nRecursive entries\n\nSegment shows the complete history of every profile. If, later, profile_1 merges into a different profile_0, Segment adds recursive entries to show that profile_1 and profile_2 both map to profile_0. These entries give you a comprehensive history of all profiles that ever existed.\n\nIf you\u2019ll use Profiles Sync to build models, refer to the id_graph model, which can help you put together a complete view of a customer.\n\nThe external_id_mapping_updates table\n\nThis table maps Segment-generated identifiers, like segment_id, to external identifiers that your users provide. It has the following columns:\n\nFIELD\tDESCRIPTION\nEXTERNAL_ID_HASH\tThe hash of the identifier sent in the incoming event.\nEXTERNAL_ID_TYPE\tThe type of external identifier sent in the incoming event, such as user_id or anonymous_id. External identifiers become the identities attached to a user profile.\nEXTERNAL_ID_VALUE\tThe value of the identifier sent in the incoming event.\nID\tA unique identifier for the table row.\nRECEIVED_AT\tThe timestamp when the Segment API receives the payload from the client or server.\nSEGMENT_ID\tThe Profile ID that Segment appends to an event or an identifier at the time it was first observed.\nSEQ\tA sequential value derived from the timestamp.\nTIMESTAMP\tThe UTC-converted timestamp set by the Segment library.\nTRIGGERING_EVENT_ID\tThe specific ID of the incoming event.\nTRIGGERING_EVENT_NAME\tThe specific name of the incoming event.\nTRIGGERING_EVENT_SOURCE_ID\tThe specific source ID of the incoming event.\nTRIGGERING_EVENT_SOURCE_NAME\tThe name of the source that triggered the event.\nTRIGGERING_EVENT_SOURCE_SLUG\tThe slug of the source that triggered the event.\nTRIGGERING_EVENT_TYPE\tThe type of tracking method used for triggering the incoming event.\nUUID_TS\tA unique identifier of the timestamp.\n\nThe anonymous site visits sample used earlier would generate the following events:\n\nSEGMENT_ID (VARCHAR)\tEXTERNAL_ID_TYPE (VARCHAR)\tEXTERNAL_ID_VALUE (VARCHAR)\tTRIGGERING_EVENT_TYPE (VARCHAR)\tTRIGGERING_EVENT_ID (VARCHAR)\tTIMESTAMP (DATETIME)\nprofile_1\tanonymous_id\t5285bc35-05ef-4d21\tpage\tevent_1\t2022-05-02 14:01:00\nprofile_1\temail\tjane.kim@segment.com\tidentify\tevent_2\t2022-05-02 14:01:47\nprofile_2\tanonymous_id\tb50e18a5-1b8d-451c\tpage\tevent_3\t2022-06-22 10:48:00\n\nIn this table, Segment shows three observed identifiers. For each of the three identifiers, Segment outputs the Segment ID initially associated with the identifier.\n\nThe profile_traits_updates table\n\nThe profile_traits_updates table maps each segment_id with all associated profile traits.\n\nSegment updates this table:\n\nfor each identify call that updates one or more traits for a segment_id.\nfor any merge where traits from two previously separated profiles are now combined.\n\nIn the event that two profiles merge, Segment only updates the profile_traits_updates table for the canonical_segment_id, or the fully merged id.\n\nFrom the profile_traits_updates table, use Segment\u2019s open-source dbt models, or your own tools to materialize the profile_traits table with all profiles and associated profile traits in your data warehouse.\n\nEvent type tables\n\nEvent type tables provide a complete history for each type of event. Segment syncs events based on the event sources you\u2019ve connected to Unify.\n\nIdentity Resolution processes these events, and includes a segment_id, enabling the data to be joined into a single Profile record.\n\nEvent type tables will have 2 months of historical data on backfill.\n\nEvent type tables includes the following tables:\n\nIdentify\nPage\nGroup\nScreen\nAlias\nTrack\n\nThese event tables are similar to the tables landed by Segment warehouse integrations, with the following exceptions:\n\nEvents are combined in a single schema. For example, if you have three sources going into a single space, Segment produces one schema, not three.\nThese tables have two extra columns:\nsegment_id: the profile ID at the time the event came through. That profile may have since merged.\nevent_source_id: the specific source ID of the incoming event\n\nThe previous result would generate two entries in the pages table:\n\nSEGMENT_ID (VARCHAR)\tCONTEXT_URL (ARRAY)\tANONYMOUS_ID (VARCHAR)\tEVENT_SOURCE_ID (VARCHAR)\tEVENT_ID (VARCHAR)\tTIMESTAMP (DATETIME)\nprofile_1\ttwilio.com\t5285bc35-05ef-4d21\tsource_1\tevent_1\t2022-05-02 14:01:00\nprofile_2\ttwilio.com/education\tb50e18a5-1b8d-451c\tsource_1\tevent_3\t2022-06-22 10:47:15\n\nAnd two entries in the identifies table:\n\nSEGMENT_ID (VARCHAR)\tCONTEXT_URL (ARRAY)\tANONYMOUS_ID (VARCHAR)\tEMAIL (VARCHAR)\tEVENT_SOURCE_ID (VARCHAR)\tEVENT_ID (VARCHAR)\tTIMESTAMP (DATETIME)\nprofile_1\ttwilio.com/try_twilio\t5285bc35-05ef-4d21\tjane.kim@segment.com\tsource_1\tevent_2\t2022-05-02 14:01:47\nprofile_2\ttwilio.com/events/webinars\tb50e18a5-1b8d-451c\tjane.kim@segment.com\tsource_2\tevent_4\t2022-06-22 10:48:00\n\nAll these events were performed by the same person. If you use these tables to assemble your data models, though, always join them against id_graph to resolve each event\u2019s canonical_segment_id.\n\nYou might see columns appended with hidden_entry or hidden_entry_joined_at in profile data of users in Journeys. Segment uses these for internal purposes, and they do not require any attention or action.\n\nProfiles Sync schema\n\nProfiles Sync uses the following schema: <profiles_space_name>.<tableName>.\n\nNote that the Profiles Sync schema is different from the Connections Warehouse schema: <source_name>.<tableName>.\n\nIf your space has the same name as a source connected to your Segment Warehouse destination, Segment overwrites data to the Event tables.\n\nFor more on Profiles Sync logic, table mappings, and data types, download this Profiles Sync ERD or visit schema evolution and compatibility.\n\nTrack event tables\n\nTrack event tables provide a complete event history, with one table for each unique named Track event. Segment syncs events based on the event sources you\u2019ve connected to Unify.\n\nThese tables include a full set of Track event properties, with one column for each property.\n\nSegment\u2019s Identity Resolution has processed these events, which contain a segment_id, enabling the data to be joined into a single profile record.\n\nThese tables will have two months of historical data on backfill.\n\nTo view and select individual track tables, edit your sync settings after you enable Profiles Sync, and wait for the initial sync to complete.\n\nTables Segment materializes\n\nWith Profiles Sync, you can access the following three tables that Segment materializes for a more complete view of your profile:\n\nuser_traits\nuser_identifiers\nprofile_merges\n\nThese materialized tables provide a snapshot of your Segment profiles, batch updated according to your sync schedule.\n\nSwitching to materialized Profile Sync\n\nIf you\u2019re not using materialized views for Profile Sync and would like to switch, follow these steps:\n\nEnable Materialized Views through Selective Sync:\nNavigate to Unify on the sidebar and select Profiles Sync.\nEnsure you are viewing the Engage space you would like to enable materialized views for.\nGo to Settings \u2192 Selective Sync and enable the following tables:\nuser_traits\nuser_identifiers\nprofile_merges\nRequest a Full Profiles and Events Backfill\nAfter enabling the materialized views, you\u2019ll need to ensure historical data is populated in the materialized tables.\nWrite to friends@segment.com and request:\nA full Profiles Backfill to populate historical profiles data.\nAn Events Backfill to include any relevant historical events, including a date range for Segment to pull data in for the events backfill.\nVerify Your Data\nOnce the backfill is complete, review the data in your warehouse to confirm all necessary historical information has been included.\n\nFor materialized view tables, you must have delete permissions for your data warehouse.\n\nWhy materialized views?\n\nMaterialized views offer several advantages:\n\nFaster queries: Pre-aggregated data reduces query complexity.\nImproved performance: Access enriched profiles and historical events directly without manual joins.\nData consistency: Automatically updated views ensure your data stays in sync with real-time changes.\nThe user_traits table\n\nWith the user_traits table, you\u2019ll see all traits that belong to a profile, represented by the canonical_segment_id. Use this table for a complete picture of your Profiles Sync data with external data sources such as customer purchase history, product usage, and more.\n\nThis view is a fixed schema, and contains a row for each trait associated with the profile.\nAs new traits are added to the profile, new rows are added to the table.\n\nWhen a merge occurs, two things happen:\n\nSegment deletes the merge from profile in the table, along with with all the traits that belong to it.\nSegment updates the merge to profile with the traits from the profile deleted in step 1.\nFor any conflicting traits, Segment appends the most recent trait to the profile.\n\nThis table has the following columns:\n\nFIELD\tDESCRIPTION\ncanonical_segment_id\tThe fully-merged Segment ID (the profile Segment now understands any events or identifiers to map to).\nname\tThe name of the trait provided by a customer\u2019s Identify payload.\nvalue\tThe value of the trait provided by the customer\u2019s Identify payload.\nseq\tA sequential value derived from the timestamp. Enables ordering/sorting within a given unique trait.\nreceived_at\tThe timestamp when the Segment API receives the payload from the client or server.\nuuid_ts\tA unique identifier of the timestamp.\ntimestamp\tThe UTC-converted timestamp set by the Segment library.\nThe user_identifiers table\n\nThe user_identifiers table contains all external ID values that map to a profile, which is represented by the canonical_segment_id.\n\nWith the user_identifiers table:\n\nThere\u2019s one row per identifier associated with the profile. This view has a fixed schema.\nAs new identifiers are added to a profile, new rows are added to the table.\n\nWhen a merge occurs:\n\nSegment deletes the merge from profile in the view, along with all associated identifiers.\nSegment updates the merge to profile with the identifiers that belonged to the profile deleted in step 1.\n\nThis table has the following columns:\n\nFIELD\tDESCRIPTION\ncanonical_segment_id\tThe fully-merged Segment ID (the profile Segment now understands any events or identifiers to map to).\ntype\tThe type of external identifier sent in the incoming event, such as user_id or anonymous_id. External identifiers become the identities attached to a user profile.\nvalue\tThe value of the trait provided by the customer\u2019s Identify payload.\nseq\tA sequential value derived from the timestamp. Enables ordering/sorting within a given unique trait.\nreceived_at\tThe timestamp when the Segment API receives the payload from the client or server.\nuuid_ts\tA unique identifier of the timestamp.\ntimestamp\tThe UTC-converted timestamp set by the Segment library.\nThe profile_merges table\n\nThe profile_merges table contains all mappings from a segment_id to a profile, represented by the canonical_segment_id. This mapping indicates that a profile has been created within Segment.\n\nWith the profile_merges table:\n\nThere\u2019s one row per profile associated with the canonical_segment_id that represents the profile. This view is a fixed schema.\nWhen a profile is created, a new row is created with the segment_id and canonical_segment_id having the same value.\n\nWhen a merge occurs:\n\nSegment deletes the merge from profile, along with all Segment IDs that belong to it.\nSegment updates the merge to profile with Segment IDs that previously belonged to the profile deleted in step 1.\n\nThis table has the following columns:\n\nFIELD\tDESCRIPTION\ncanonical_segment_id\tThe fully-merged Segment ID (the profile Segment now understands any events or identifiers to map to).\nsegment_id\tThe profile ID that Segment appends to an event or an identifier at the time it was first observed.\nseq\tA sequential value derived from the timestamp. Enables ordering/sorting within a given unique trait.\nreceived_at\tThe timestamp when the Segment API receives the payload from the client or server.\nuuid_ts\tA unique identifier of the timestamp.\ntimestamp\tThe UTC-converted timestamp set by the Segment library.\nTables you materialize\n\nYou can materialize the following tables with your own tools, or using Segment\u2019s open-source dbt models:\n\nid_graph\nexternal_id_mapping\nprofile_traits\n\nYou might want to materialize your own tables if, for example, you want to transform additional data or join Segment profile data with external data before materialization.\n\nYou can alternatively use tables that Segment materializes and syncs to your data warehouse. Learn more about the tables Segment materializes.\n\nPlease note that dbt models are in beta and need modifications to run efficiently on BigQuery, Synapse, and Postgres warehouses. Segment is actively working on this feature.\n\nEvery customer profile (or canonical_segment_id) will be represented in each of the following tables.\n\nThe id_graph table\n\nThis table represents the current state of your identity graph, showing only where a segment_id is now understood to point.\n\nThe most recent entry for each segment_id from id_graph_updates reflects this. After the four example events, id_graph would show the following:\n\nSEGMENT_ID (VARCHAR)\tCANONICAL_SEGMENT_ID (VARCHAR)\tTIMESTAMP (DATETIME)\nprofile_1\tprofile_1\t2022-05-02 14:01:00\nprofile_2\tprofile_1\t2022-06-22 10:48:00\n\nSegment drops most diagnostic information from this table, since it\u2019s designed for reference use. In this case, you\u2019d learn that any data references to profile_2 or profile_1 now map to the same customer, profile_1.\n\nThe external_id_mapping table\n\nUse this table to view the full, current-state mapping between each external identifier you\u2019ve observed and its corresponding, fully-merged canonical_segment_id.\n\nIn the case study example, you\u2019d see the following:\n\nCANONICAL_SEGMENT_ID (VARCHAR)\tEXTERNAL_ID_TYPE (VARCHAR)\tEXTERNAL_ID_VALUE (VARCHAR)\tTIMESTAMP (DATETIME)\nprofile_1\tanonymous_id\t5285bc35-05ef-4d21\t2022-05-02 14:01:00\nprofile_1\temail\tjane.kim@segment.com\t2022-05-02 14:01:47\nprofile_1\tanonymous_id\tb50e18a5-1b8d-451c\t2022-06-22 10:48:00\nThe profile_traits table\n\nUse the profile_traits table for a singular view of your customer. With this table, you can view all custom traits, computed traits, SQL traits, audiences, and journeys associated with a profile in a single row.\n\nThe profile_traits table contains the last seen value for any of your customer profile traits that Segment processes as an Identify call.\n\nIf Segment later merges away a profile, it populates the segment_id it merged in the merged_to column.\n\nIn the case study example, Segment only collected email. As a result, Segment would generate the following profile_traits table:\n\nCANONICAL_SEGMENT_ID (VARCHAR)\tEMAIL (VARCHAR)\tMERGED_TO (VARCHAR)\nprofile_1\tjane.kim@segment.com\t\u00a0\nprofile_2\t\u00a0\tprofile_1\n\nMerged profiles\n\nProfiles that Segment merges away are no longer canonical.\n\nThis page was last modified: 12 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nCase study: anonymous site visits lead to profile merge\nProfile raw tables\nEvent type tables\nTrack event tables\nTables Segment materializes\nTables you materialize\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nData Graph\n/\nSetup Guides\n/\nSnowflake Data Graph Setup\nSnowflake Data Graph Setup\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\n\nData Graph, Reverse ETL, Profiles Sync require different warehouse permissions.\n\nOn this page, you\u2019ll learn how to connect your Snowflake data warehouse to Segment for the Data Graph.\n\nSnowflake credentials\n\nSegment assumes that you already have a warehouse that includes the datasets you\u2019d like to use for the Data Graph. Log in to Snowflake with admin privileges to provide the Data Graph with the necessary permissions below.\n\nStep 1: Create a user and internal database for Segment to store checkpoint tables\n\nSegment recommends setting up a new Snowflake user and only giving this user permissions to access the required databases and schemas. Run the SQL code block below in your SQL worksheet in Snowflake to execute the following tasks:\n\nCreate a new role and user for the Segment Data Graph. This new role will only have access to the datasets you provide access to for the Data Graph.\nGrant the Segment user access to the warehouse of your choice. If you\u2019d like to create a new warehouse, uncomment the SQL below.\nSegment requires write access to this database in order to create a schema for internal bookkeeping and to store checkpoint tables for the queries that are executed. Therefore, Segment recommends creating a new database for this purpose. This is also the database you\u2019ll be required to specify for the \u201cDatabase Name\u201d when connecting Snowflake with the Segment app.\n\nSegment recommends creating a new database for the Data Graph. If you choose to use an existing database that has also been used for Segment Reverse ETL, you must follow the additional instructions to update user access for the Segment Reverse ETL schema.\n\n-- ********** SET UP THE FOLLOWING WAREHOUSE PERMISSIONS **********\n\n-- Update the following variables \nSET segment_connection_username = 'SEGMENT_LINKED_USER';\nSET segment_connection_password = 'my-safe-password';\nSET segment_connection_warehouse = 'SEGMENT_LINKED_WH';\nSET segment_connection_role = 'SEGMENT_LINKED_ROLE';\n\n-- The DB  used for Segment's internal bookkeeping.\n-- Note: Use this DB in the connection settings on the Segment app. This is the only DB that Segment requires write access to.\nSET segment_connection_db = 'SEGMENT_LINKED_PROFILES_DB';\n\n-- ********** [OPTIONAL] UNCOMMENT THE CODE BELOW IF YOU NEED TO CREATE A NEW WAREHOUSE **********\n\n-- CREATE WAREHOUSE IF NOT EXISTS identifier($segment_connection_warehouse)\n-- WITH WAREHOUSE_SIZE = 'XSMALL'\n--   WAREHOUSE_TYPE = 'STANDARD'\n--   AUTO_SUSPEND = 600 -- 5 minutes\n--   AUTO_RESUME = TRUE;\n\n-- ********** RUN THE COMMANDS BELOW TO FINISH SETTING UP THE WAREHOUSE PERMISSIONS **********\n\n-- Use admin role for setting grants\nUSE ROLE ACCOUNTADMIN;\n\n-- Create a role for the Data Graph\nCREATE ROLE IF NOT EXISTS identifier($segment_connection_role)\nCOMMENT = 'Used for Segment Data Graph';\n\n-- Create a user for the Data Graph\nCREATE USER IF NOT EXISTS identifier($segment_connection_username)\nMUST_CHANGE_PASSWORD = FALSE\nDEFAULT_ROLE = $segment_connection_role\nPASSWORD = $segment_connection_password\nCOMMENT = 'Segment Data Graph User'\nTIMEZONE = 'UTC';\n\n-- Grant permission to the role to use the warehouse\nGRANT USAGE ON WAREHOUSE identifier($segment_connection_warehouse) TO ROLE identifier($segment_connection_role);\n\n-- Grant role to the user\nGRANT ROLE identifier($segment_connection_role) TO USER identifier($segment_connection_username);\n\n-- Create and Grant access to a Segment internal DB used for bookkeeping. This is the only DB that Segment requires write access to. This is also the DB you will use in the \"Database Name\" config while setting up the connection in the Segment app. \nCREATE DATABASE IF NOT EXISTS identifier($segment_connection_db);\nGRANT USAGE ON DATABASE identifier($segment_connection_db) TO ROLE identifier($segment_connection_role);\nGRANT USAGE ON ALL SCHEMAS IN DATABASE identifier($segment_connection_db) TO ROLE identifier($segment_connection_role);\nGRANT CREATE SCHEMA ON DATABASE  identifier($segment_connection_db) TO ROLE identifier($segment_connection_role);\n\n\nStep 2: Grant read-only access to additional databases for the Data Graph\n\nNext, give the Segment role read-only access to additional databases you want to use for Data Graph including the Profiles Sync database. Repeat the following SQL query for each database you want to use for the Data Graph.\n\n\nSET segment_connection_role = 'SEGMENT_LINKED_ROLE';\n\n-- ********** REPEAT THE SQL QUERY BELOW FOR EACH DATABASE YOU WANT TO USE FOR THE DATA GRAPH **********\n-- Change this for each DB you want to grant the Data Graph read-only access to\nSET linked_read_only_database = 'MARKETING_DB';\n\nGRANT USAGE ON DATABASE identifier($linked_read_only_database) TO ROLE identifier($segment_connection_role);\nGRANT USAGE ON ALL SCHEMAS IN DATABASE identifier($linked_read_only_database) TO ROLE identifier($segment_connection_role);\nGRANT SELECT ON ALL TABLES IN DATABASE identifier($linked_read_only_database) TO ROLE identifier($segment_connection_role);\nGRANT SELECT ON FUTURE TABLES IN DATABASE identifier($linked_read_only_database) TO ROLE identifier($segment_connection_role);\nGRANT SELECT ON ALL VIEWS IN DATABASE identifier($linked_read_only_database) TO ROLE identifier($segment_connection_role);\nGRANT SELECT ON FUTURE VIEWS IN DATABASE identifier($linked_read_only_database) TO ROLE identifier($segment_connection_role);\nGRANT SELECT ON ALL EXTERNAL TABLES IN DATABASE identifier($linked_read_only_database) TO ROLE identifier($segment_connection_role);\nGRANT SELECT ON FUTURE EXTERNAL TABLES IN DATABASE identifier($linked_read_only_database) TO ROLE identifier($segment_connection_role);\nGRANT SELECT ON ALL MATERIALIZED VIEWS IN DATABASE identifier($linked_read_only_database) TO ROLE identifier($segment_connection_role);\nGRANT SELECT ON FUTURE MATERIALIZED VIEWS IN DATABASE identifier($linked_read_only_database) TO ROLE identifier($segment_connection_role);\n\n\n(Optional) Step 3: Restrict read-only access to schemas\n\nIf you want to restrict access to specific Snowflake schemas and tables, then run the following commands:\n\n-- [Optional] Further restrict access to only specific schemas and tables \nSET db = 'MY_DB';\nSET schema = 'MY_DB.MY_SCHEMA_NAME';\nSET segment_connection_role = 'SEGMENT_LINKED_ROLE';\n\n-- View specific schemas in database\nGRANT USAGE ON DATABASE identifier($db) TO ROLE identifier($segment_connection_role);\nGRANT USAGE ON SCHEMA identifier($schema) TO ROLE identifier($segment_connection_role);\nGRANT SELECT ON ALL TABLES IN SCHEMA identifier($schema) TO ROLE identifier($segment_connection_role);\nGRANT SELECT ON FUTURE TABLES IN SCHEMA identifier($schema) TO ROLE identifier($segment_connection_role);\nGRANT SELECT ON ALL VIEWS IN SCHEMA identifier($linked_read_only_database) TO ROLE identifier($segment_connection_role);\nGRANT SELECT ON FUTURE VIEWS IN SCHEMA identifier($linked_read_only_database) TO ROLE identifier($segment_connection_role);\nGRANT SELECT ON ALL EXTERNAL TABLES IN SCHEMA identifier($linked_read_only_database) TO ROLE identifier($segment_connection_role);\nGRANT SELECT ON FUTURE EXTERNAL TABLES IN SCHEMA identifier($linked_read_only_database) TO ROLE identifier($segment_connection_role);\nGRANT SELECT ON ALL MATERIALIZED VIEWS IN SCHEMA identifier($linked_read_only_database) TO ROLE identifier($segment_connection_role);\nGRANT SELECT ON FUTURE MATERIALIZED VIEWS IN SCHEMA identifier($linked_read_only_database) TO ROLE identifier($segment_connection_role);\n\n\nStep 4: Confirm permissions\n\nTo verify you have set up the right permissions for a specific table, log in with the username and password you created for SEGMENT_CONNECTION_USERNAME and run the following command to verify the role you created has the correct permissions. If this command succeeds, you should be able to view the respective table.\n\nset segment_connection_role = 'SEGMENT_LINKED_ROLE';\nset linked_read_only_database = 'YOUR_DB';\nset table_name = 'YOUR_DB.SCHEMA.TABLE';\n\nUSE ROLE identifier($segment_connection_role);\nUSE DATABASE identifier($linked_read_only_database) ;\nSHOW SCHEMAS;\nSELECT * FROM identifier($table_name) LIMIT 10;\n\n\nStep 5: Connect your warehouse to the Data Graph\n\nTo connect your warehouse to the Data Graph:\n\nNavigate to Unify > Data Graph. This should be a Unify space with Profiles Sync already set up.\nClick Connect warehouse.\nSelect Snowflake as your warehouse type.\nEnter your warehouse credentials. Segment requires the following settings to connect to your Snowflake warehouse:\nAccount ID: The Snowflake account ID that uniquely identifies your organization account\nDatabase: The only database that Segment requires write access to in order to create tables for internal bookkeeping. This database is referred to as segment_connection_db in the script below\nWarehouse: The warehouse in your Snowflake account that you want to use for Segment to run the SQL queries. This warehouse is referred to as segment_connection_warehouse in the script below\nUsername: The Snowflake user that Segment uses to run SQL in your warehouse. This user is referred to as segment_connection_username in the script below\nAuthentication: There are 2 supported authentication methods:\nKey Pair: This is the recommended method of authentication. You would need to first create the user and assign it a key pair following the instructions in the Snowflake docs. Then, follow the Segment docs above to set up Snowflake permissions and set the segment_connections_username variable in the SQL script to the user you just created\nPassword: The password of the user above. This password is referred to as segment_connection_password in the script below.\nTest your connection, then click Save.\nUpdate user access for Segment Reverse ETL schema\n\nIf Segment Reverse ETL has ever run in the database you are configuring as the Segment connection database, a Segment-managed schema is already created and you need to provide the new Segment user access to the existing schema. Run the following SQL if you run into an error on the Segment app indicating that the user doesn\u2019t have sufficient privileges on an existing _segment_reverse_etl schema.\n\n-- If you want to use an existing database that already has Segment Reverse ETL schemas, you\u2019ll need to run some additional steps below to grant the role access to the existing schemas.\n\nSET retl_schema = concat($segment_connection_db,'.__segment_reverse_etl');\nGRANT USAGE ON SCHEMA identifier($retl_schema) TO ROLE identifier($segment_connection_role);\nGRANT CREATE TABLE ON SCHEMA identifier($retl_schema) TO ROLE identifier($segment_connection_role);\nGRANT SELECT,INSERT,UPDATE,DELETE ON ALL TABLES IN SCHEMA identifier($retl_schema) TO ROLE identifier($segment_connection_role);\n\n\nThis page was last modified: 28 Oct 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSnowflake credentials\nStep 1: Create a user and internal database for Segment to store checkpoint tables\nStep 2: Grant read-only access to additional databases for the Data Graph\n(Optional) Step 3: Restrict read-only access to schemas\nStep 4: Confirm permissions\nStep 5: Connect your warehouse to the Data Graph\nUpdate user access for Segment Reverse ETL schema\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSources\n/\nSchema\n/\nSegment Schema Limits\nSegment Schema Limits\nHow many unique events can be logged in my Segment Schema table?\n\nWhile you can technically track unlimited events with Segment, only the first 4,000 events will be visible on the Schema page for a given Source. After you hit the 4,000 event limit, all future events will still be tracked and sent to your Destinations. They will not, however, be logged in the Segment Schema table.\n\nHow many unique event properties and traits can be logged on the event details page?\n\nWhile you can track unlimited event properties and traits with Segment, the Schema page has the following default limits:\n\nProperties: The event details page for a specific event can only show the first 300 properties by default.\nTraits: The Identify page can only show the first 300 traits by default.\n\nAfter you hit the limit for both properties or traits, future properties and traits are still tracked and sent to your Destinations, but they won\u2019t appear on the event details page. This limit includes nested properties in an event\u2019s properties object.\n\nThese limits can also affect the traits and properties that you can see in the Computed Trait and Audience builder tools in Engage. If expected traits or properties do not appear in these tools, contact the Segment Support team.\n\nHow can I clear the Schema if I have hit the limits?\n\nIf you hit any of the limits or would like to clear out old events or properties, you can clear the Schema data from your Source Settings. In your Source, navigate to Settings, then Schema Configuration. Scroll down to the Clear Schema History setting.\n\nClearing events from the Source Schema only clears them from the Segment interface. It does not impact the data sent to your destinations or warehouses. Once you clear the events, the Schema page starts to repopulate new events.\n\nHow can I remove specific events from my Source Schema?\n\nYou can archive events in order to declutter the Source Schema. If your Source Schema is connected to a Tracking Plan, events need to be blocked or unplanned for you to archive them. If your Source Schema not connected to a Tracking Plan, you must disable the event to see the archive button.\n\nArchiving an event triggers an \u201cSchema Event Archived\u201d activity to the Audit Trail.\n\nTo view archived events, you can filter your view by \u201cArchived\u201d.\n\nWhile this is particularly useful for Protocols customers that want to keep events \u201cUnplanned yet acknowledged\u201d and build a process to monitor for unplanned events, Protocols is not required to use this feature.\n\nHow can I remove properties from my Source Schema?\n\nAt this time, you cannot clear or archive old event properties individually. An alternative to this is to archive the event itself and then clear the archive. After you clear the archive, the event will re-populate in the schema with only the current properties.\n\nThis page was last modified: 15 Nov 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nData Export Options\nData Export Options\n\nThere are a few ways to export your Segment data. Segment\u00a0Business customers\u00a0have the most data flexibility, but our self-service customers also have options.\n\nBusiness plan customers\n\nCustomers on our\u00a0business plan\u00a0can take advantage of Replay when they change vendors or add a vendor to their marketing and analytics stack.\n\nReplay\n\nWhen you want to trial or start using a new vendor, Segment can replay your timestamped, historical data so it\u2019s like you\u2019ve been using that app all along.\n\nEliminate vendor lock-in\nTake your data to new tools\nIncrease advantage in vendor negotiations\n\nReplay works for all server-side destinations that have or accept timestamps, including our Amazon S3 destination, so you can get all your data history since the first event you sent to Segment.\n\nFree, Team, and Business plan customers\n\nIf you are on any of our plans, there are multiple options available to you to gain access to your raw data.\n\nWarehouses\n\nAll customers can connect a data warehouse to Segment \u2013 Free and Team customers can connect one, while Business customers can connect as many as they need. We translate and load your raw data logs into your warehouse for more powerful analysis in SQL.\n\nS3 Logs\n\nWe store all your API calls as line-separated JSON objects in Amazon S3. If you enable Amazon S3 in your destinations catalog, we will copy the same data to your own S3 bucket. The data copied will only include data sent to Segment after you turn on the destination. Read our\u00a0Amazon S3 docs\u00a0to learn more about how we structure that data.\n\nWebhooks\n\nYou can use our webhooks destination to fire off requests in realtime to an endpoint that you would need to spin up and manage on your side. This is basically re-creating how our business system works but takes a bit of work on your side. If your event volume is high it can be difficult to keep a server up to receive those messages in realtime.\n\nIron.io\n\nAnother one of our destinations is Iron.io. They function similar to webhooks, but they will manage the message queue and allow you to run scripts on your data before routing it to another end point. Again this is similar to what Segment does for our business customers, but will require a decent amount of work from your team, however it will be much more reliable if your event volume gets high.\n\n3rd Party Reporting APIs\n\nThis option is the most restrictive but might be the easiest if you need only basic data to be exported. A few examples would be to use the reporting APIs\u00a0Clicky\u00a0or\u00a0Google Analytics\u00a0provide (after turning those tools on in Segment and sending them data). Those APIs aren\u2019t super flexible and you won\u2019t see all the data from Segment, but for basic metrics they should work. One tool that\u2019s a bit more flexible when it comes to a reporting API is\u00a0Keen.io, which is also available on the Segment platform.\n\nThis page was last modified: 01 Dec 2022\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nTwilio Engage Foundations Onboarding Guide\nTwilio Engage Foundations Onboarding Guide\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nThis guide walks you through the set up process for a simplified Engage space. If your implementation is complex, use this to demonstrate and test Engage before working on a more complex configuration.\n\nThe first four steps in this guide also apply to Unify set up. To learn more, visit the Unify Onboarding Guide.\n\nRegional Segment\n\nEngage Foundations is available on Segment\u2019s regional infrastructure. For more information, see the article Regional Segment.\n\nEngage configuration requirements\n\nEngage requires both Connections and Unify.\n\nThe following are prerequisites to configuring and using Engage:\n\nA Segment account and Workspace.\nEvents flowing into Connections from your digital properties where most of your valuable user behavior occurs.\nProfiles identity admin access. You must have edit access to identity resolution rules.\nEngage Administrator access. You must be either be a workspace admin, or a workspace user with Engage admin access to set up audiences and computed traits. You can check your permissions by navigating to Access Management in your workspace settings. See Segment Access Management for more details.\nStep 1: Create a new Developer space\n\nWhen you first start working with Engage, start by creating a \u201cDeveloper\u201d Engage space. This is your experimental and test environment while you learn more about how Engage works.\n\nStep 2: Invite teammates to your Engage space\n\nInvite teammates to your Engage dev space and grant them access to the space. Navigate to Access Management in your workspace settings to add them.\n\nStep 3: Connect production sources\nOn the left sidebar navigate to Unify > Unify Settings > Profile Sources > + Connect Source.\nChoose one or two production sources from your Connections workspace. Segment recommends connecting your production website or App source as a great starting point.\nToggle the Replay data flag when connecting a new source : enabled vs. disabled.\nEnabled : The Replay data flag is enabled by default when connecting a source to an Engage/Unify Space. Enable this flag to replay the last month of data into the Engage/Unify Space.\nDisabled : You can disable this option by toggling it, which prevents the replaying of historical data from the source to the Space. This means that only data that the source has received after the point when the source was connected to the Space will be available within the Engage/Unify Space.\nIf you need more historical data available from this source, please fill out the form below for each replay and contact Segment Support at friends@segment.com or create a ticket:\n\n```Segment Source Details:\n\nName: source-name\nSourceId: XXXXX or Link to Source\n\nDetails for replay:\n\nDestination: Name of destination you want to replay to or link to Profiles space\nStart time: (Use the following UTC format) 2020-11-21T05:10:00Z UTC\nEnd time: (Use the following UTC format) 2023-01-21T10:10:00Z UTC\n\nAll the events or only a subset of event names? Provide event names and/or method calls (page/identify/track/group) if only a subset of events is needed. ```\n\nHow much data can I replay from my source into Engage? Your workspace\u2019s \u201ccomputations history\u201d limit is defined in your workspace\u2019s contract and can be found on the page Usage & billing under the section Additional packages : Engage : \u201cUp to __ days computation history\u201d.\n\nTo learn more, visit Connect production sources.\n\nStep 4: Check your profile data\n\nAfter the replay finishes, you can see the data replayed into Engage using the Profile Explorer. Visit the Profiles Onboarding Guide for more info.\n\nStep 5: Create an Audience\n\nYou can build an audience using any source data that flows into your Engage space.\n\nIn this step, use the Audience Builder UI to create an Audience using properties you\u2019re familiar with. For example, you might know the number of new website user signups in the last seven days, if you\u2019ve connected your production website source to Engage.\n\nTo build your own audience:\n\nNavigate to your Engage space.\nSelect the Audiences tab, then click Create.\nClick Add Condition, and choose among the options that appear:\nPerformed an Event\nPart of an Audience\nHave a Computed Trait\nHave a SQL Trait\nHave a Custom Trait\nConfigure the settings for your condition. These vary by type, so explore the different options.\nOptionally, add more conditions until you\u2019re satisfied that the audience will only contain the users you want to target.\n\nAfter you build your audience, click Preview Results to see the total number of users who meet the audience criteria, for example all users who signed up within the last seven days.\n\nStep 6: Connect the Audience to a Destination\n\nAfter you create your test audience, click Select Destinations. Engage guides you through configuration steps to set up a destination for your audience. If you don\u2019t already have destinations configured for the Engage space, you are prompted to select one or more. Finally, enter a name for the audience.\n\nThe larger the audience you\u2019re creating, the longer it takes Engage to successfully compute the Audience. The Audience page shows a status that indicates if the audience is still being calculated. When the total number of users appears in the Audience overview, as in the example screenshot below, the audience has successfully finished computing, and Engage then sends the audience to the destination you selected.\n\nStep 7: Validate that your audience is appearing in your destination\n\nAudiences are either sent to destinations as a boolean user-property (for example New_Users_7days=true or a user-list, depending on what the destination supports. Read more about which destinations support which types of data.\n\nThe UIs for the destination tools you send the audience data to are different, so the process of validating the audience varies per tool. However, the guiding principle is the same. You should be able to identify the full group of users who are members of your audience in your destination.\n\nStep 8: Create your production space\n\nAfter you validate that your full audience is arriving in your destination, you\u2019re ready to create a Production space. Segment recommends that you repeat the same steps outlined above, focusing on your production use cases and data sources.\n\nThis page was last modified: 21 Jun 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nEngage configuration requirements\nStep 1: Create a new Developer space\nStep 2: Invite teammates to your Engage space\nStep 3: Connect production sources\nStep 4: Check your profile data\nStep 5: Create an Audience\nStep 6: Connect the Audience to a Destination\nStep 7: Validate that your audience is appearing in your destination\nStep 8: Create your production space\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nTwilio Engage Premier Onboarding Guide\nTwilio Engage Premier Onboarding Guide\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE PREMIER \u2713\n?\n\nEngage Premier entered an End of Sale (EOS) period effective June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:\n\nTwilio Marketing Campaigns\n\nPreferred ISV Partners:\n\nAirship Blog\nBloomreach Blog\nBraze Blog\nInsider Blog\nKlaviyo Blog\nTwilio Engage Foundations Documentation\n\n\nTwilio Engage brings Segment, Twilio, SendGrid, and WhatsApp together to help you create and send email, SMS, and WhatsApp campaigns to your customers.\n\nBefore sending your first Engage campaign, though, you\u2019ll need to configure and connect accounts with all four platforms.\n\nThis guide lists all required onboarding steps and walks you through Engage setup. By the end of the onboarding process, you\u2019ll be ready to send your first campaign.\n\nThe steps in this guide are only required if you plan to send email, SMS, and WhatsApp messages with Engage. Visit the Engage Foundations Onboarding Guide for general onboarding steps to set up your Engage space, connect sources, create audiences, and more.\n\nRegional Segment\n\nYou can use Engage Premier on Segment\u2019s regional infrastructure in the EU. Twilio Engage ensures data residency in the EU, but the channels you connect to, may not guarantee the same level of data residency. Check directly with the providers of the channels you use for information about data residency in their applications. Native channels like email and SMS, which use Twilio, are not data resident.\n\nTwilio is GDPR compliant, and has Binding Corporate Rules to ensure that data is protected when it\u2019s transferred between countries.\n\nBefore you begin: overview and task checklist\n\nYou\u2019ll set up Twilio Engage in four stages:\n\nConfigure Engage identifiers in Unify.\nCreate and configure a SendGrid account.\nCreate and configure Twilio SMS services.\nCreate and configure Twilio WhatsApp services.\n\nThe following table shows a high-level checklist of tasks you\u2019ll need to complete in each platform:\n\nPLATFORM\tTASKS\nSegment\t1. Verify Engage identifiers in your Segment workspace.\n2. Add any missing identifiers.\nSendGrid\t1. Create a SendGrid account.\n2. Upgrade your account to a Pro plan.\n3. Configure an IP.\n4. Create a SendGrid subuser.\n5. Authenticate your domain.\n6. Enable subscription tracking.\n7. Enable an event webhook.\n8. Generate API credentials, then copy them into Engage settings.\n9. Warm up your IP.\n10. Contact SendGrid support.\nTwilio\t1. Create a Twilio account.\n2. Purchase phone number(s).\n3. If necessary, register phone number(s).\n4. Create a messaging service.\n5. Generate an API key, then copy it into the Engage settings.\nWhatsApp\t1. Register a Twilio number with WhatsApp.\n2. Connect your Facebook account.\n2. Create the WhatsApp messaging service.\n\nSeveral onboarding steps require copying and pasting information between Segment and SendGrid or Twilio. To streamline setup, open your Segment workspace in one browser tab and open two others for tasks you\u2019ll carry out in SendGrid and Twilio.\n\nContinue reading for a detailed, step-by-step breakdown of each onboarding stage.\n\nStage 1: Configure Engage Identifiers in Unify\n\nThrough identity resolution, Segment uses the phone and email traits to identify users who can receive your Engage campaigns. To begin using Engage, you\u2019ll need to verify that these identifiers exist in your workspace and add them if they don\u2019t.\n\nFollow these steps to configure the traits:\n\nIn your Segment workspace, navigate to Unify > Unify settings > Identity resolution.\nUnder the Identity resolution configuration table, verify that phone and email appear under the Identifier column. If so, the Engage identifiers are configured correctly; skip to create and configure a SendGrid account.\nIf either identifier is missing, click the Add identifier button, then click Custom identifiers.\nIn the New Custom Identifier modal, add the first missing trait (phone or email) in the Trait/Property key field, then click Add Identifier.\nIf both traits were missing, repeat Step 4 and add the other missing trait (phone or email). Finish by clicking Add New Identifier.\n\nYour Segment workspace is now configured for Engage. Next, you\u2019ll create a SendGrid account and connect it to Segment.\n\nStage 2: Create and configure a SendGrid account\n\nSendGrid powers delivery of your Engage email campaigns. During this stage of onboarding, you\u2019ll create and set up a SendGrid Pro account. You\u2019ll then configure SendGrid and Engage to enable both subscription tracking and an event webhook.\n\nCreate your SendGrid Pro account\n\nStart by creating a SendGrid account and then upgrading to the SendGrid Pro Plan:\n\nVisit the SendGrid website and sign up for an account.\nWithin your SendGrid space, navigate to Settings > Account Details > Your Products.\nUnder the Email API section, select Change Plan.\nOn the Email API Plans page, select a Pro option that fits your anticipated sending needs.\nAdd the Pro option to your cart, and complete checkout.\n\nUpgrading to SendGrid Pro\n\nUpgrading to a SendGrid Pro account may require additional action on your part. Follow the instructions in SendGrid\u2019s account upgrade guide to complete your upgrade.\n\nCreate a subuser and check the dedicated IP address\n\nNext, you\u2019ll create a SendGrid subuser and ensure that a dedicated IP has been assigned:\n\nIn your SendGrid space, navigate to Settings > Subuser Management, then click Create New Subuser.\n\nIn the Create New Subuser window, create a username for the subuser, then add an email address and password. Your SendGrid subuser username must begin with the prefix twilio_engage_app_. Add a unique identifier to the end of the prefix, for example, twilio_engage_app_someusername.\n\nIn the same window, click the checkbox next to the dedicated IP address you\u2019ll user for the subuser. Segment recommends that you choose an IP address that you\u2019ll only use for Engage.\nFill out the remaining fields in the window, then click Create Subuser.\nUsing SendGrid\u2019s documentation, warm up the IP address.\nAuthenticate your domain\n\nSendGrid parent account\n\nIn this section, you\u2019ll authenticate your domain and set up reverse DNS with your SendGrid parent account.\n\nNow, you\u2019ll authenticate your domain with SendGrid and your DNS provider and enable link branding. Domain authentication protects your sending reputation by showing email providers that you\u2019ve given SendGrid permission to send email campaigns for you.\n\nTo authenticate your domain, you\u2019ll copy CNAME records given to you by SendGrid and paste them into your DNS provider. Before you begin, verify that you have the necessary permissions to add CNAME records to your DNS. If you\u2019re not sure if you have the right permissions, reach out to your organization\u2019s IT department.\n\nYou\u2019ll authenticate your domain using the SendGrid platform and your DNS provider:\n\nFrom your SendGrid parent account, follow SendGrid\u2019s domain authentication guide.\nDuring the authentication process, SendGrid asks if you would like to brand links for your domain. Select Yes.\nSendGrid provides you with five CNAME records. Add them to your DNS host.\nReturn to SendGrid and verify your DNS.\n\nComplete authentication by setting up reverse DNS:\n\nFrom your SendGrid parent account, follow SendGrid\u2019s reverse DNS (rDNS) documentation.\nSendGrid provides you with one A record. Add it to your DNS host, along with the five CNAME records from the previous steps.\nReturn to SendGrid and verify your DNS.\nEnable subscription tracking\n\nYou\u2019ll also need to enable subscription tracking, which keeps records of users who unsubscribe from your email campaigns:\n\nWithin your SendGrid space, navigate to Settings > Tracking.\nOn the Tracking Settings page, click Subscription Tracking in the Settings column.\nAt the end of the Subscription Tracking window, toggle Setting State to enabled. Click Save.\nEnable event webhook\n\nSubuser Step\n\nThis step takes place in the subuser space.\n\nYou\u2019ll now need to enable event webhooks, which trigger webhook notifications for campaign-related events like clicks and opens:\n\nWithin your SendGrid subuser space, navigate to Settings > Mail Settings.\nClick the pencil edit icon next to Event Webhook.\nOn the Event Webhook page, set authorization method to none.\nCopy and paste the following URL, depending on your region, into the HTTP Post URL field:\nUS: https://engage-ma-webhook-api.engage.segment.com/sendgrid\nEU:https://engage-ma-webhook-api.euw1.engage.segment.com/sendgrid\n\nCheck Select All under both Deliverability Data and Engagement Data.\nToggle the Event Webhook Status to Enabled. Click Save.\nGenerate an API key\n\nCopying SendGrid Credentials\n\nThis step creates an API key and API Key ID that you\u2019ll immediately add to Segment. Make sure you\u2019re ready to copy and save the API key before proceeding; SendGrid only displays the API key once. You must follow these steps from within the SendGrid subuser account you created for use with Twilio Engage.\n\nRe-using API Keys\n\nIt is not possible to re-use API Keys in different Engage spaces. For each space, a new API Key is required.\n\nNow, you\u2019ll generate an API key and API Key ID within SendGrid. With your SendGrid account open in one tab, open your Segment workspace open in another. You\u2019ll need both open to copy and paste the API credentials into your Engage settings.\n\nSendGrid Subuser Step\n\nCarry out the following steps in your SendGrid subuser space.\n\nWithin your SendGrid subuser space, navigate to Settings > API Keys.\nClick the Create API Key button.\nIn the Create API Key window, name your API key using the prefix twilio_engage_app_, with a suffix of your choice added to the end, like twilio_engage_app_somekey.\nUnder API Key Permissions, select the Full Access radio button, then click Create & View.\nSendGrid displays your API Key. Click the API key to copy it to your computer\u2019s clipboard, then select Done.\nSendGrid returns you to the API Keys page; leave this tab open.\n\nTo finish linking the API credentials to your Segment account, follow these steps:\n\nSwitch to the browser tab with your Segment workspace open.\nNavigate to Engage > Engage settings > Channels. Under Email Service with SendGrid, select the Get Started button.\nIn the Set up and validate your SendGrid account window (shown below), enter the subuser username you previously created into the Subuser Name field.\nPaste the Subuser API Key you just copied from SendGrid into the Subuser API Key field. Leave this tab open.\nReturn to the tab showing your SendGrid API Keys. Copy the key displayed after API Key ID.\nReturn to the tab with your Segment workspace. Paste the copied API Key ID into the Subuser API Key ID field.\nClick Verify.\n\nWarm up your IP\n\nIf you already send emails regularly on your chosen IP, proceed to Stage 3.\n\nTo finish configuring your SendGrid account for usage with Twilio Engage, you\u2019ll warm up your IP. IP warmup protects your sender reputation and ensures that you avoid email deliverability issues.\n\nAs a best practice, only warm up your IP when you\u2019re ready to begin sending campaigns.\n\nAutomated IP warmup\n\nYou can enable automated IP warmup by following these steps:\n\nWithin your SendGrid space, navigate to Settings > IP Addresses.\nClick the action menu for the IP you want to warmup, which brings up the Edit Your Dedicated IP Address screen.\nSelect Use Automated IP warmup, then click Save.\n\nOnce you\u2019ve enabled IP warmup, you\u2019re ready to send as many campaigns as you\u2019d like. SendGrid will begin sending your campaigns through a shared pool of IP addresses, including your own. Over the next 30 days, SendGrid will gradually increase the number of campaigns sent through your chosen IP. After 30 days, SendGrid will send all your emails from your dedicated IP.\n\nIP warmup best practices\n\nKeep the following in mind once you\u2019ve enabled automated IP warmup:\n\nUse the IP you warmed up for marketing campaigns; avoid using the IP for transactional emails.\nSegment recommends that you start by sending low-volume campaigns before high-volume campaigns. This will help establish your domain reputation with SendGrid.\n\nOnce you\u2019ve completed IP warmup, your SendGrid account will be fully configured and ready to use with Engage. You\u2019re ready to move to Stage 3 and set up Twilio SMS.\n\nStage 3: Create and configure Twilio SMS services\n\nTo add the ability to send SMS campaigns in Engage, you\u2019ll now create a Twilio account, set up a phone number and messaging service, and generate an API key.\n\nSet up a Twilio Messaging Service\n\nPhone Number Registration\n\nYou\u2019ll need to purchase a phone number to set up Twilio Messaging. Depending on the phone number type you purchase, you may have to register the number. Before completing this section, read Twilio\u2019s documentation on short code, long code, and toll free numbers.\n\nOnce you\u2019ve identified the type of phone number you\u2019ll use with Twilio Engage, follow these steps to create a Twilio Messaging Service:\n\nVisit the Twilio website and sign up for a paid account. Trial accounts generate sending errors.\nPurchase a phone number within your Twilio Console. If necessary, register the number.\nIn the Twilio Console side menu, navigate to Messaging > Services.\nOn the Messaging Services page, click Create Messaging Service.\nEnter a name for your Messaging Service.\nUnder the Messaging use dropdown, select Market my services.\nFrom the Sender Pool tab, click Add Senders, then select the phone number you purchased in Step 1. Click Step 3: Set up Integration. Leave this tab open.\nVerify that the dropdown next to the Request URL field is set to HTTP Post.\n(If applicable:) Click Step 4: Add compliance info. Finish compliance setup, then click Complete Messaging Service Setup.\nGenerate an API key, and select your messaging service(s)\n\nCopying Twilio Credentials\n\nThis step generates an Account SID, API key SID, and API key secret that you\u2019ll later add to Segment. Make sure you\u2019re ready to copy and save both before proceeding.\n\nStart by creating your Twilio account and getting an API key for Engage:\n\nIn your Twilio console, select the Account dropdown menu, then API keys & tokens.\nOn the Auth tokens & API keys page, click Create API key.\nEnter a name for the API key in the Friendly name field.\nSet the region to United States (US1) - Default and key type to Main.\nClick Create API Key.\n\nCopy and save both the SID and Secret field contents.\n\nReturn to the API keys & tokens page. In the Live credentials section, copy the Account SID credentials.\n\nSwitch to the browser tab with your Segment workspace.\nNavigate to Engage > Engage settings > Channels. Under SMS Service with Twilio, click the Get Started button. The Set up and validate your Twilio account page appears.\n\nUnder Enter your Twilio API Key information (shown below), paste the Account SID, API Key SID, and API Key Secret you copied above into their corresponding fields.\n\nClick Verify, then select the messaging services you want to use in your space.\n\nClick Save Twilio Account.\n\nIf you\u2019re unable to verify your Account SID, SID, or API Key secret, you may have copied an extra space at the end of one or the other. Verify that you\u2019ve not added any extra characters or spaces, then try to verify again.\n\nStage 4: Create and configure Twilio WhatsApp services\n\nTo send WhatsApp messages in Twilio Engage, you\u2019ll register a Twilio number with WhatsApp, connect your Facebook account, and create a WhatsApp messaging service.\n\nRegister a Twilio number with WhatsApp\nPurchase an SMS-capable phone number within your Twilio Console.\nFor international numbers, view Twilio\u2019s Phone Number Regulations guidelines.\nFrom the Twilio side menu, navigate to Messaging > Senders > WhatsApp Senders.\nSelect Create new sender.\nFrom the New Sender builder, find Twilio phone number, then choose the phone number you purchased in Step 1. Select Continue.\nSelect Continue with Facebook. A Facebook popup window appears; leave it and the Twilio console open.\nConnect your Facebook account\n\nIn the Facebook popup from the previous section, carry out these steps:\n\nFollow Facebook\u2019s instructions to log in to your Facebook account.\nWhen you reach the Fill in your business information page, choose your WhatsApp Business Account or create a new account. Select Next.\nCreate a new WhatsApp Business Profile that follows Meta\u2019s display name guidelines. Fill out all fields, then select Next.\nIn your Twilio console, copy the number shown in the Number to register with WhatsApp field. Paste it into the Phone number field on the Facebook Add a phone number for WhatsApp page, then select Next.\nFacebook prompts you to verify your phone number. Select the Text message radio button, then select Next.\nIn your Twilio console, copy the code in the Verify via text messages section, then enter it into the Facebook Verification code field. Select Next.\nFacebook displays You're now ready to chat with people on WhatsApp. Click Finish to close the window.\nCreate the WhatsApp messaging service\n\nYou\u2019ll now create a messaging service to connect your number to Engage:\n\nIn the Twilio Console side menu, navigate to Messaging > Services.\nOn the Messaging Services page, click Create Messaging Service.\nEnter a name for your Messaging Service. You must include the word WhatsApp in the messaging service name, for example, My New Service WhatsApp.\nUnder the Messaging use dropdown, select Market my services, then select Create messaging service.\nFrom the Sender Pool tab, click Add Senders > Add WhatsApp Numbers > Confirm.\nTwilio confirms that the WhatsApp number has been assigned to the service.\n\nYour WhatsApp messaging service is now created.\n\nRegional Segment\n\nYou can use Engage Premier on Segment\u2019s regional infrastructure in the EU. Twilio Engage ensures data residency in the EU, but the channels you connect to, may not guarantee the same level of data residency. Check directly with the providers of the channels you use for information about data residency in their applications. Native channels like email and SMS, which use Twilio, are not data resident.\n\nTwilio is GDPR compliant, and has Binding Corporate Rules to ensure that data is protected when it\u2019s transferred between countries.\n\nNext steps\n\nWith configured accounts and services for all platforms, you\u2019ve completed Engage onboarding and are ready to create and send campaigns to your users.\n\nNot sure where to start? Read the Engage documentation on sending email campaigns, SMS campaigns, and WhatsApp campaigns. To save time when generating Engage campaigns, check out the Engage guides on creating SMS templates, email templates, and WhatsApp templates.\n\nIf you\u2019re planning to import contacts to Engage, learn how to update your audiences with a CSV file.\n\nThis page was last modified: 15 Jul 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nBefore you begin: overview and task checklist\nStage 1: Configure Engage Identifiers in Unify\nStage 2: Create and configure a SendGrid account\nStage 3: Create and configure Twilio SMS services\nStage 4: Create and configure Twilio WhatsApp services\nRegional Segment\nNext steps\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSpec\n/\nBest Practices for Event Calls\nBest Practices for Event Calls\n\nHow do you to determine which type of call you should use? When should you make a Page call instead of a just using a Track call? In theory, you could put together a full implementation using only track events, but this is a bad idea - this page explains some of the things you should consider when deciding which call to use.\n\nSegment strongly encourages you to follow the Spec for clarity and simplicity\u2019s sake, however we also give you the flexibility to make only the calls that fit your needs. In the end, it is up to you.\n\nWhat is the Spec?\n\nSegment recommends that you follow the Spec, which gives general guidance about which methods to use when. You might read about \u201csemantic spec\u201d, which simply means Page calls should be about the page you\u2019re viewing, and Track calls should be about events or activities you want to track.\n\nThe Spec outlines the specific data you should collect with each type of call. Each call type represents and is intended to collect specific information about a user or their activities. This means that your choice of method can imply things about the data you intend to collect.\n\nFor example, the properties for page() and screen() calls are intended to describe the page, not the user or their actions. Similarly, the data automatically included in a page() in particularly is important for UTM campaign capture.\n\nSimplifying implementation\n\nAs we mentioned above, you could build a full Segment implementation using only Track events, and this is probably a bad idea. To do this, you would need to include page-related data in every Track call, which means adding all of the information that Page calls automatically include, except now manually as event properties. As you might imagine, this gets unwieldy fast!\n\nIt\u2019s better to pair a Page and a Track call together (making one of each call), especially if you have a complex tracking implementation. When you use the semantic methods you reduce the amount of information and other properties required in a single call.\n\nEnsuring destination compatibility\n\nThe track() call, and page() or screen() calls are handled very differently by your downstream tools, and how you can use that data is different. When you use the Spec, Segment uses the call type to help translate the data into destination\u2019s tracking format. This ensures the highest level of compatibility with the end tools.\n\nSome destinations were built around a specific call type and Segment maps to those specific methods. Some downstream tools do not accept page() and screen() calls at all. Many of the destinations that do accept these calls, also expect a limited range of data in a page() call, and may not properly receive or handle data that would be expected in track() calls.\n\nTo help you with this, the Segment documentation includes a list of all of the supported destinations and the calls they accept.\n\nFiltering data by purpose\n\nFinally, when you use the different methods correctly, it can help you separate out \u201ctypes\u201d of information in your downstream tools and warehouses, so you can use them for different purposes.\n\nThis page was last modified: 21 Nov 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nWhat is the Spec?\nSimplifying implementation\nEnsuring destination compatibility\nFiltering data by purpose\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGetting Started\n/\nPlanning a Full Installation\nPlanning a Full Installation\n\nNow that you\u2019ve seen Segment in action, step back and think through what a full implementation of Segment for your organization would look like. Figuring out what events to track in Segment can feel overwhelming. You should expect this planning process to have the following steps:\n\nDefine Business Objectives\nDecide what to collect\nShortcut: Check if a Business Spec meets your needs\nB2B Spec\nEcommerce Spec\nMobile Spec\nVideo Spec\nCreate naming conventions\nDevelop a Tracking Plan\nPlan your Identify and Group calls\nPlan your Track events\nDefine your Track event properties\nPlan for destination tools\n\nBe prepared to invest time deciding with stakeholders how to track your data, and planning how you\u2019ll analyze it. The time you spend here will save you lots of time in the future, as following Segment\u2019s best practices allows you to easily change your tracking later.\n\nDefine Business Objectives\n\nTracking is about learning and taking action. Think about what you want to know about your product or customers. Think about what assumptions need to be tested and what theories need to be proven true or false. Think about the unknowns. Here are some helpful questions to get started:\n\nWhat kind of events or data best illustrate or explain how your customers use your product?\nHow do people discover, start using, and paying for your product?\nWhat are the most important steps in a customer\u2019s journey?\n\nWhile it may seem obvious, we highly recommend documenting your high-level business objectives. More specifically, ask yourself: what are the measurable business outcomes you want to achieve this year? Do you want to acquire new customers? Generate more new sign-ups, drive more incremental revenue among your current customer base?\n\nThe best way to answer this question is to interview stakeholders in your organization who will consume the data.\n\nWith your business goals documented, the next step is to map user actions to those business goals. For example, if one of your goals is to activate new signups, you want to think about which activities are related to a signup. Ask yourself, what actions do people take before signing up? Do specific actions predict a user signing up?\n\nAs an example, you might end up with a list like this:\n\nAd Campaign Clicked\nLink Clicked\nArticle Completed\nCampaign Opened\nForm Initiated\nForm Submitted\nUser Signed Up\n\nWhile this list represents a tiny fraction of the user actions you could track, it gives a list focused on your top business objectives. This helps break up the huge project of data collection into smaller chunks.\n\nDecide what to collect\n\nWith your business objectives documented and mapped to user actions, it\u2019s time to build standards that you can use when deciding what to track. With your stakeholders, make a list of the actual events (page or screen views, and user actions) that you want to track. Think about all of the ways your users can interact with your site or app\n\nWhen you\u2019re first starting out, we recommend that you limit your tracking plan to a few core events, but add lots of properties to provide context about them. We generally see more success with the \u201cless is more\u201d philosophy of tracking data, but you might also decide to take a more liberal \u201ctrack more and analyze later\u201d approach. Like everything, each alternative has pros and cons that are important to consider especially as it relates to your company\u2019s needs.\n\nShortcut: Check if a Business Spec meets your needs\n\nSegment maintains several \u201cBusiness Specs\u201d, which are recommendations based on your type of business that give recommendations on what to track, what additional traits or properties to collect, and how to format them. The two most common are the B2B (business-to-business) Spec, Ecommerce Spec, and Mobile and Video specs.\n\nIf these specs meet your business needs, you\u2019re in luck. These specs are built into Segment tracking plan templates, so you don\u2019t need to start from a blank slate.\n\nB2B Spec\n\nIf your organization sells a product or services to other businesses, you might have different analytics and marketing needs than most companies. You need to understand your customer behaviors both at the user-level, and also at the company or team-level. You can read more about how Segment thinks about B2B tracking, and read more about the B2B Spec.\n\nEcommerce Spec\n\nIf your organization sells products online, the E-commerce Spec covers the customer\u2019s journey as they browse your store, click on promotions, view products, add those products to a cart, and complete a purchase. It also provides recommendations about off-page interactions, including interactions with email promotions, coupons, and other systems. You can read more about why companies need an Ecommerce Spec, read more about Ecommerce tracking plans, and dive directly into our Ecommerce Spec.\n\nMobile Spec\n\nThe\u00a0native Mobile Spec\u00a0is a common blueprint for the mobile user lifecycle. The Spec outlines the most important events for mobile apps to track, and automatically collects many of these events when you use the Segment Android and iOS SDKs. Read more about the benefits of the native mobile spec, or read through the Native Mobile Spec directly.\n\nVideo Spec\n\nSegment\u2019s video spec helps you understand how customers engage with your video and ad content, including playback events, types of media displayed, and performance metrics. You can read more about our Video Spec.\n\nCreate naming conventions\n\nRegardless of approach, here are some important best practices to keep in mind:\n\nPick a casing convention: We recommend Title Case for event names and snake_case for property names. Make sure you pick a casing standard and enforce it across your events and properties.\n\nPick an event name structure: As you may have noticed from our specs, we\u2019re big fans of the Object (Blog Post) + Action (Read) framework for event names. Pick a convention and stick to it.\n\nDon\u2019t create event names dynamically: Avoid creating events that pull a dynamic value into the event name (for example, User Signed Up (11-01-2019)). If and when you send these to a warehouse for analysis, you end up with huge numbers of tables and schema bloat.\n\nDon\u2019t create events to track properties: Avoid adding values to event names when they could be a property. Instead, add values as a property. For example, rather than having an event called \u201cRead Blog Post - Best Tracking Plans Ever\u201d, create a \u201cBlog Post Read\u201d event and with a property like \"blog_post_title\":\"Best Tracking Plans Ever\".\n\nDon\u2019t create property keys dynamically: Avoid creating property names like \"feature_1\":\"true\",\"feature_2\":\"false\" as these are ambiguous and very difficult to analyze\n\nGot all that? Great. You\u2019re now ready to develop a Tracking Plan.\n\nDevelop a tracking plan\n\nA tracking plan clarifies what events to track, where those events live in the code base, and why you\u2019re tracking those events (from a business perspective). A good tracking plan represents the single source of truth about what data you collect, and why.\n\nYour tracking plan is probably maintained in a spreadsheet (unless you use Segment\u2019s tracking-plan tool, Protocols), and serves as a project management tool to get your organization in agreement about what data to use to make decisions. A tracking plan helps build a shared understanding of the data among marketers, product managers, engineers, analysts, and any other data users.\n\nPlan your Identify and Group calls\n\nThe Identify call updates all records of the user with a set of traits, and so is extremely important for building your understanding of your users. But how do you choose which traits to include? The example below shows an Identify call using analytics.js) for Segment:\n\nanalytics.identify({\n  name: 'Jane Kim',\n  email: 'janekim@example.com',\n  login: 'janekay',\n  type: 'user',\n  created: '2016-11-07T16:40:52.238Z',\n});\n\n\nThe traits represent dimensions in your data that you can group or pivot on. For example, in the above, you can easily create cohorts of all types that are users or accounts created within a time window of your choosing.\n\nWhen you plan your deployment, think about what information you can collect as traits that would be useful to you when grouping users together, and plan how you will collect that information.\n\nThe Group call is similar to the Identify call, but it adds traits associated with a parent account to the user\u2019s profile. If your organization is a B2B company, you should also plan the group traits to collect, and how you\u2019ll use them once they\u2019re applied to a user account.\n\nPlan your Track events\n\nWe recommend starting with fewer events that are directly tied to one of your business objectives, to help avoid becoming overwhelmed by endless number of possible actions to track. As you get more comfortable, you can add more events to your tracking plan that can answer more specialized questions.\n\nAt Segment, we started out tracking these events:\n\nUser Signed Up\nSource Data Sent\nSubscription Started\n\nThen we added some peripheral events to to better understand how we\u2019re performing, for the following reasons:\n\nUser Invited When users invite more people to their organization, it\u2019s a good indicator that they\u2019re engaged and serious about using the product. This helps us measure growth in organizations.\nDestination Enabled Turning on a destination is a key value driver for our customers.\nDebugger Call Expanded When we see that a certain customer has used the live event stream feature a number of times, we can contact see if we can help them debug.\n\nFor an Ecommerce company, however, the main events might be something like:\n\nAccount Created\nProduct Added\nOrder Completed\n\nTip: As mentioned above, Segment has a set of \u201creserved\u201d event names specifically for ecommerce, called the Ecommerce Spec. Check it out to see which events Segment covers and how they are used in downstream destinations.\n\nAn online community, on the other hand, has an entirely different set of actions that indicate engagement, as listed below. For example, a community might want to track actions like:\n\nContent Viewed\nContent Shared\nComment Submitted\nContent Produced\nContent Curated\n\nWith these actions tracked, the community can develop metrics around engagement, and understand how users move towards their ultimate conversion events. You can read more in this article from the online community GrowthHackers about the events they track and why.\n\nDefine your Track event properties\n\nEach Track call can accept an optional dictionary of properties, which can contain any key-value pair. These properties act as dimensions that allow destination tools to group, filter, and analyze the events. They give you additional detail on broader events.\n\nEvents should be generic and high-level, but properties should be specific and detailed. For example, at Segment, Business Tier Workspace Created is a horrible event name. Instead, we used Workspace Created with a property of account_tier and value of business :\n\nanalytics.track('Workspace Created', {\n  account_tier: 'business'\n})\n\n\nSimilar to the traits in the Identify call, the properties provide a column that you can pivot against or filter on in your analytics tools or allow you to create a cohort of users in email tools.\n\nDon\u2019t create dynamically generated property names in the properties dictionary. Each key creates a new column in your downstream tools, and dynamically generated keys clutter your tools with fragmented data that makes it difficult and confusing to use later.\n\nHere is Segment\u2019s Lead Captured Track call:\n\nanalytics.track(userId, 'Lead Captured', {\n  email: 'email',\n  location: 'header navbar'\n  url: 'https://segment.com/'\n});\n\n\nThe high-level event is Lead Captured, and all of the details appear in the properties dictionary. Because of this, we can easily see in our downstream tools how many leads were captured, and from which parts of the site.\n\nIf you want to learn more about how properties are used by downstream tools, check out The Anatomy of a Track Call.\n\nPlan for destination tools\n\nOnce you\u2019ve completed your tracking plan, there\u2019s one more step you might want to do before you move on to actually implementing Segment. The Segment destination catalog contains hundreds of tools, many of which you\u2019ll be familiar with already.\n\nIf your organization has an established set of analytics tools, look for those tools in the catalog and bookmark their documentation pages. These docs pages contain important information about how Segment transforms data for the destination tool, and they also contain useful details about troubleshooting, set-up, and implementation considerations.\n\nOnce you have an initial list of the destination tools your organization uses, you can also check which Segment methods those tools accept. This helps you at implementation time to ensure that the calls you use can be consumed by the tools they\u2019re intended for.\n\nAdditionally, you should check which connection modes each tool supports, so you know ahead of time which destinations may need to be bundled.\n\nTip: If you know you\u2019re looking for a tool for a specific purpose, but haven\u2019t chosen one yet, you can also check the Connection Modes by category page to see which tools might be compatible with the least implementation changes.\n\nBACK\nA simple Segment installation\n\nWalk through a disposable, demo implementation.\n\nNEXT\nA full Segment implementation\n\nTake your plans, and make them real.\n\nThis page was last modified: 30 Mar 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nDefine Business Objectives\nDecide what to collect\nCreate naming conventions\nDevelop a tracking plan\nPlan for destination tools\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nReverse Etl\n/\nReverse ETL System\nReverse ETL System\n\nView reference information about how Segment detects data changes in your warehouse and the rate and usage limits associated with Reverse ETL.\n\nRecord diffing\n\nReverse ETL computes the incremental changes to your data directly within your data warehouse. The Unique Identifier column is used to detect the data changes, such as new, updated, and deleted records.\n\nDelete Records Payload\n\nThe only value passed for deleted records is their unique ID, which can be accessed as __segment_id. As of September 24, 2024, deleted records also contain all columns selected by your model, with null values in place of data.\n\nFor Segment to compute the data changes within your warehouse, Segment needs to have both read and write permissions to the warehouse schema table. At a high level, the extract process requires read permissions for the query being executed. Segment keeps track of changes to the query results through tables that Segment manages in a dedicated schema (for example, _segment_reverse_etl), which requires some write permissions.\n\nThere may be cost implications to having Segment query your warehouse tables.\n\nLimits\n\nTo provide consistent performance and reliability at scale, Segment enforces default use and rate limits for Reverse ETL.\n\nUsage limits\n\nReverse ETL usage limits are measured based on the number of records processed to each destination \u2013 this includes both successful and failed records. For example, if you processed 50K records to Braze and 50K records to Mixpanel, then your total Reverse ETL usage is 100K records.\n\nProcessed records represents the number of records Segment attempts to send to each destination. Keep in mind that not all processed records are successfully delivered, for example, such as when the destination experiences an issue.\n\nYour plan determines how many Reverse ETL records you can process in one monthly billing cycle. When your limit is reached before the end of your billing period, your syncs will pause and then resume on your next billing cycle. To see how many records you\u2019ve processed using Reverse ETL, navigate to Settings > Usage & billing and select the Reverse ETL tab.\n\nPLAN\tNUMBER OF REVERSE ETL RECORDS YOU CAN PROCESS TO DESTINATIONS PER MONTH\tHOW TO INCREASE YOUR NUMBER OF REVERSE ETL RECORDS\nFree\t500K\tUpgrade to the Teams plan in the Segment app by navigating to Settings > Usage & billing.\nTeams\t1 million\tContact your sales representative to upgrade your plan to Business.\nBusiness\t50 x the number of MTUs\nor .25 x the number of monthly API calls\tContact your sales rep to upgrade your plan.\n\nIf you have a non-standard or high volume usage plan, you may have unique Reverse ETL limits or custom pricing. To see your Reverse ETL limits in the Segment app, select Settings > Usage & Billing.\n\nConfiguration limits\nNAME\tDETAILS\tLIMIT\nModel query length\tThe maximum length for the model SQL query.\t65,535 characters\nModel identifier column name length\tThe maximum length for the ID column name.\t191 characters\nModel timestamp column name length\tThe maximum length for the timestamp column name.\t191 characters\nSync frequency\tThe shortest possible duration Segment allows between syncs.\t15 minutes\nExtract limits\n\nThe extract phase is the time spent connecting to your database, executing the model query, updating internal state tables and staging the extracted records for loading.\n\nNAME\tDETAILS\tLIMIT\nRecord count\tThe maximum number of records a single sync will process. If a sync would contain more than 150 million records, Segment separates the data into multiple syncs, each containing no more than 150 million records\n\nNote: This is the number of records extracted from the warehouse, not the limit for the number of records loaded to the destination (for example, new/update/deleted).\t*150 million records\nColumn count\tThe maximum number of columns a single sync will process.\t512 columns\nColumn name length\tThe maximum length of a record column.\t128 characters\nRecord JSON size\tThe maximum size for a record when converted to JSON (some of this limit is used by Segment).\t512 KiB\nColumn JSON size\tThe maximum size of any single column value.\t128 KiB\n\n*: If Segment identifies a sync would be larger than 150 million records, Segment extracts 150 million of the records in the initial sync and syncs any additional records during the next scheduled or manual sync.\n\nFor example, if a sync would contain 700 million records, Segment would run an initial 150 million record sync, and during the next three scheduled or manual syncs, would sync 150 million records. The fifth scheduled or manual sync would contain the remaining 100 million records.\n\nThis page was last modified: 25 Sep 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nRecord diffing\nLimits\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nStorage\n/\nData Warehouses\nData Warehouses\nFREE \u2713\nTEAM \u2713\nBUSINESS \u2713\nADD-ON X\n?\nWhat\u2019s a Warehouse?\n\nA warehouse is a central repository of data collected from one or more sources. This is what commonly comes to mind when you think about a relational database: structured data that fits neatly into rows and columns.\n\nIn Segment, a Warehouse is a special type of destination. Instead of streaming data to the destination all the time, we load data to them in bulk at regular intervals. When we load data, we insert and update events and objects, and automatically adjust their schema to fit the data you\u2019ve sent to Segment.\n\nWhen selecting and building a data warehouse, consider three questions:\n\nWhat type of data will be collected?\nHow many data sources will there be?\nHow will the data be used?\n\nRelational databases are great when you know and predefine the information collected and how it will be linked. This is usually the type of database used in the world of user analytics. For instance, a users table might be populated with the columns name, email_address, or plan_name.\n\nExamples of data warehouses include Amazon Redshift, Google BigQuery, and Postgres.\n\nWhen Segment loads data into your warehouse, each sync goes through two steps:\n\nPing: Segment servers connect to your warehouse. For Redshift warehouses, Segment also runs a query to determine how many slices a cluster has. Common reasons a sync might fail at this step include a blocked VPN or IP, a warehouse that isn\u2019t set to be publicly accessible, or an issue with user permissions or credentials.\nLoad: Segment de-duplicates the transformed data and loads it into your warehouse. If you have queries set up in your warehouse, they run after the data is loaded into your warehouse.\n\nLooking for the Warehouse Schemas docs?\n\nThey\u2019ve moved: Warehouse Schemas.\n\nAnalytics Academy: When to use SQL for analysis\n\nWhen your existing analytics tools can't answer your questions, it's time to level-up and use SQL for analysis.\n\nMore Help\n\nHow do I send custom data to my warehouse?\n\nHow do I give users permissions to my warehouse?\n\nCheck out the Frequently Asked Questions about Warehouses page and a list of helpful SQL queries to get you started with Redshift .\n\nFAQs\n\nHow do I decide between Redshift, Postgres, and BigQuery?\n\nWhat do you recommend for Postgres: Amazon or Heroku?\n\nHow do I give users permissions?\n\nWhat are the limitations of Redshift clusters and warehouses connectors?\n\nWhere do I find my source slug?\n\nSetting up a warehouse\n\nHow do I create a user, grant usage on a schema and then grant the privileges that the user will need to interact with that schema?\n\nWhich IPs should I allowlist?\n\nWill Segment sync my historical data?\n\nCan I load in my own data into my warehouse?\n\nCan I control what data is sent to my warehouse?\n\nManaging a warehouse\n\nHow fresh is the data in my warehouse?\n\nCan I add, tweak, or delete some of the tables?\n\nCan I transform or clean up old data to new formats or specs?\n\nWhat are common errors and how do I debug them?\n\nHow do I speed up my Redshift queries?\n\nAnalyzing with SQL\n\nHow do I forecast LTV with SQL and Excel for e-commerce businesses?\n\nHow do I measure the ROI of my Marketing Campaigns?\n\nThis page was last modified: 12 Aug 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nWhat\u2019s a Warehouse?\nFAQs\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nFunctions Overview\nFunctions Overview\n\nFunctions let you create your own sources and destinations directly within your workspace to bring new types of data into Segment and send data to new tools with just a few lines of JavaScript - no additional infrastructure required.\n\nFunctions is available to all customer plan types with a free allotment of usage hours. Read more about Functions usage limits, or see your workspace\u2019s Functions usage stats.\n\nWhat can you do with Functions?\n\nFunctions can help you bring external data into Segment (Source functions) and send data in Segment out to external destinations (Destination functions). Use Insert functions to transform data before it reaches your downstream destinations. Functions are scoped to your specific workspace. If you\u2019re a technology partner and want to build a new integration and publish it in Segment\u2019s catalog, see the Developer Center documentation.\n\nSource functions\n\nSource functions receive external data from a webhook and can create Segment events, objects, or both. Source functions have access to the full power of JavaScript so you can validate and transform the incoming data and even make external API requests to annotate your data.\n\nUse cases:\n\nIngest data into Segment from a source that\u2019s unavailable in the catalog\nTransform or reject data before it\u2019s received by Segment\nEnrich incoming data using external APIs\n\nLearn more about source functions.\n\nDestination functions\n\nDestination functions can take events from a Segment source, transform the events, and deliver them to external APIs. Destination functions can make arbitrary requests to annotate data, as well.\n\nUse cases:\n\nSend data from Segment to a service that\u2019s unavailable in the catalog\nTransform data before sending it downstream\nEnrich outgoing data using external APIs\n\nLearn more about destination functions.\n\nDestination insert functions\n\nDestination insert functions help you enrich your data with code before you send it to downstream destinations.\n\nUse cases:\n\nImplement custom logic and enrich data with third party sources\nTransform outgoing data with advanced filtration and computation\nEnsure data compliance by performing tokenisation, encryption, or decryption before sending data downstream\n\nTo learn more, visit destination insert functions.\n\nFunctions Copilot\n\nWith Functions Copilot, you can instrument custom integrations, enrich and transform data, and even secure sensitive data nearly instantaneously without writing a line of code.\n\nTo learn more, visit the Functions Copilot documentation.\n\nIP Allowlisting\n\nIP Allowlisting uses a NAT gateway to route outbound Functions traffic from Segment\u2019s servers to your destinations through a limited range of IP addresses, which can prevent malicious actors from establishing TCP and UDP connections with your integrations.\n\nIP Allowlisting is available for customers on Business Tier plans.\n\nTo learn more, visit Segment\u2019s IP Allowlisting documentation.\n\nThis page was last modified: 04 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nWhat can you do with Functions?\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nProfiles Sync\n/\nProfiles Sync Overview\nProfiles Sync Overview\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\n\nProfiles Sync connects identity-resolved customer profiles to a data warehouse of your choice.\n\nWith a continual flow of synced profiles, teams can enrich and use these data sets as the basis for new audiences and models. Profiles Sync addresses a number of use cases, with applications for identity graph monitoring, attribution analysis, machine learning, and more. View Profiles Sync Sample Queries for an in-depth guide to Profiles Sync applications.\n\nProfiles Sync use cases\n\nTo help you get started, here are a few example use cases:\n\nUnderstand how Segment creates Profiles\n\nUse Profiles Sync for more insight into profiles generated by Segment\u2019s Identity Resolution. Query the Profile Sync data set to answer questions such as:\n\nWhat\u2019s causing profile merges in a given period?\nHow many merges occur for each profile?\nHow many emails are associated with a profile?\n\nUnderstanding how Segment creates profiles helps you detect potential instrumentation errors.\n\nCreate golden profiles\n\nJoin Segment\u2019s profile data with existing object data from your warehouse to create a single view of the customer. You can then use this data set to create personalized experiences on any channel. For example, B2B companies can build a report that maps sales executives (object data synced with a source like Salesforce) with customers who are most likely to buy a certain product (Profile Traits data synced with Profiles Sync).\n\nUnderstand a customer\u2019s journey\n\nWith Profiles Sync, your data teams can better understand profile merge events. Connect anonymous IDs, User IDs, and emails to understand your customer\u2019s journey with details such as:\n\nHow often a user has paid.\nIf someone is a bad actor.\nIf a user has subscribed or not.\nWhat products users are viewing, even when they aren\u2019t logged in.\nBuild attribution models\n\nUse Profiles Sync to build data models that marketing partners can trust. Trace prospective customer journeys before buying products, and build models that help you understand which channels provide the most value.\n\nUse machine learning\n\nAccess profile traits and see how they change over time. This will help you to better understand how your customer\u2019s behavior evolves, and build models that predict LTV, churn, and propensity scores.\n\nNext steps\n\nTo learn more about Profiles Sync, visit the following docs:\n\nProfiles Sync Setup: Learn how to set up Profiles Sync, enable historical backfill, and adjust settings for warehouses you\u2019ve connected.\nSample Queries: View sample queries you can run to help you familiarize yourself with Profiles Sync.\nTables and materialized views: Learn how to use data sets and models that Segment provides to enrich customer profiles.\n\nFor more on Profiles Sync logic, table mappings, and data types, download this Profiles Sync ERD.\n\nThis page was last modified: 31 Oct 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nProfiles Sync use cases\nNext steps\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nHow To Guides\n/\nMeasuring Your Advertising Funnel\nMeasuring Your Advertising Funnel\n\nIt\u2019s surprisingly hard to answer questions about the ROI of your ad campaigns. What does a click actually result in? How much should I pay for it? We built our Sources for Facebook Ads and Google Adwords to help you understand the true performance and cost of your campaigns.\n\nIn this article, we dig into the nuances of data collection and potential gotchas around measuring clicks, pageviews, and ultimately, conversions.\n\nMeasuring Campaign Performance\n\nToday, most marketing teams think about their paid acquisition funnel as three major steps\u2026\n\nThis makes sense when looking at overall campaign performance, but hides several crucial funnel steps that can make the difference between increasing a campaign\u2019s spend and shutting it off due to poor results.\n\nBecause page optimization and ad blockers can impact measurement of your funnel, it\u2019s important to look at the four additional steps happening between the ad click and conversions.\n\nLet\u2019s go through each true funnel step in a little more detail.\n\nImpressions & Clicks: When a user views an ad, the ad platform increments the count of impressions for that ad. When an ad is clicked, the ad platform logs a click. This is all handled by the ad platform\u2019s servers. Facebook and Google work hard to filter invalid and fraudulent traffic, whether that\u2019s a mistaken click, a bot, or a competitor looking to drain your advertising budget. Any bad traffic is removed from both your reporting and your monthly bill.\n\nPage Request Initiated: After an ad is clicked, a user\u2019s browser attempts to load your landing page. This request is the first contact your application has with the user, and the server responds with the content to render the landing page.\n\nFirst JavaScript Loaded: The user\u2019s browser starts to download the landing page content, which includes the HTML, JavaScript, and CSS. The browser parses and renders this content, loading the JavaScript sequentially as it parses the page. By default, analytics.js uses the async tag, which means that the browser won\u2019t block the page and will load analytics.js once everything else is ready. Analytics.js wants to get out of the way where possible so you can create the best experience for your customers.\n\nPage Fully Rendered: The page is fully rendered once all the html, css and scripts have been loaded on the page. This time can vary a lot based on the speed of the internet connection (how fast all the assets download) and the device itself (how fast the local computer can run all of the scripts).\n\nThird-Party Scripts Loaded: Finally, third-party scripts are asynchronously loaded onto the page. The speed at which these scripts are loaded depends on a variety of factors, like the page size, network speed, and the size and number of the third-party scripts. Once these scripts are loaded, analytics.js triggers a page call to our API.\n\nConversion Event: From there, a user might fill out a form, signup, or buy your product!\n\nHow does this impact my ad reporting?\n\nThere are three less-obvious contributors to fall-off across the paid acquisition funnel: slow loads, ad blockers, and bounces.\n\nFor the sake of illustration, this means that if you have 100 ad clicks, you will be able to count most but not all corresponding page views because some visitors may bounce (exit or hit the back button) before analytics.js is executed. Similarly, you may miss some attributable conversions due to slow load times (your page calls can\u2019t fire in time) and ad blockers (which often block analytics not just ads).\n\nHere\u2019s how it works.\n\nSlow Loads\n\nSlow loads can impact your attribution modeling, making campaigns appear to have worse performance than reality. In the general case, when a user hits your landing page, your tracking code loads and triggers a pageview event that you can use to attribute that user to a campaign.\n\nBut if third-party scripts take on the order of seconds to load (for example, on 1x or 3G networks), users may click off the page before your tracking code executes. In this case, the pageview never gets recorded and your ability to attribute that click to a conversion is lost.\n\nThis is generally not an issue for most companies because they are focused more on people who spend a good deal of time on their pages. However, it is a potential source of opaqueness, particularly for users with slow or bad network connection.\n\nBounces\n\nBounces can occur at any stage of the funnel between an ad being clicked and third-party scripts loading on the page.\n\nSome bounces are not tracked because the user doesn\u2019t even last the few seconds to request your HTML, render it, and execute tracking. If they quickly hit back or close the browser window, your ad platform will report clicks that don\u2019t show up in your analytics tracking.\n\nAd blockers\n\nIt is likely the case that some percentage of your users are using ad blockers. It\u2019s estimated that 22% of mobile smartphones worldwide and 16% of US web traffic use ad blockers. Segment customers have reported ad blockers for as little as a few percentage points of their visitors, to upwards of 70% of traffic for companies with very tech-forward audiences.\n\nBut just because a user is using ad blockers doesn\u2019t mean that they aren\u2019t seeing and clicking on ads. Facebook recently announced that they would be suppressing ad blockers, and Adblock Plus, the most popular ad blocking and anti-tracking software, categorizes Google Search ads as acceptable ads.\n\nThat said, many ad blockers do block analytics tools like Google Analytics, Mixpanel and Segment. This means that there exists some percentage of your conversions that actually came through your paid acquisition channels, but are unattributable due to ad blockers.\n\nWhat if I need more precise tracking?\n\nSegment offers two ways of joining your user clickstream data to your paid acquisition channels: standard client-side tracking or advanced server-side page calls.\n\nBoth options come with their own tradeoffs that are important to consider for your use case.\n\nClient-side Tracking (Standard)\n\nAnalytics.js is loaded with the async tag by default, which means that the library and all it\u2019s destinations are loaded near the end of the page rendering. The benefit is that analytics.js doesn\u2019t slow down page loads, but it does mean that tracking is not executed immediately on page load.\n\nWhen you use standard client-side tracking, you\u2019ll lose pageview data for visitors who bounce or click off the page before analytics.js executes, and for visitors with ad blockers enabled.\n\nServer-side Page Calls (Advanced)\n\nIf you want to capture adblock, bounce, and slow load traffic, we recommend adding an additional page() call to the server-side. This allows you avoid the browser altogether and see the total number of requests emanating from your paid acquisition channels. You\u2019ll get visibility on an extra step in that funnel.\n\nThe general approach is to use an arbitrary anonymousId (e.g. a UUID) in the server-side page() call and then also set the anonymousId as the ajs_anonymous_id cookie in the browser. You can read more about how to implement that here. This approach is tricky to implement, so we recommend that this is undertaken only for use cases in which bounce and/or adblock data is critical.\n\nEstimating the Impact of Moving Server-side\n\nIf you want to get a quick estimate for the number of additional clicks you\u2019d track using server-side tracking, you can use \u201credirect tracking\u201d with a URL shortener to estimate the number of clicks coming from Google Adwords or Facebook Ads. This will give you an estimate for the number of times an ad is clicked (minus some bounce in the few hundred milliseconds of the redirect), which will closely match server-side page() tracking should you choose to implement it.\n\nHere\u2019s how it works\u2026\n\nUse a URL shortener like bit.ly to link to a landing page, with a custom parameter like ?ttg=2 .\n\nAdd the shortened link to your ad.\n\nMeasure total clicks from the bit.ly stats page.\n\nIn your warehouse, count the number of pages with that unique url parameter from step 1 (make sure you\u2019re looking at the same timeframe).\n\nselect received_at, url\nfrom <site>.pages\nwhere url like '%/warehouses%'\nand search like '%ttg=2'\norder by received_at\n\n\nWe hope this overview helps explain the technical nuances of measuring what happens when a customer finds you using an ad! If you have any other questions, feel free to share them in the Segment Community for discussion.\n\nThis page was last modified: 21 Apr 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nMeasuring Campaign Performance\nHow does this impact my ad reporting?\nWhat if I need more precise tracking?\nEstimating the Impact of Moving Server-side\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nUser Subscriptions\n/\nSet User Subscriptions\nSet User Subscriptions\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE PREMIER \u2713\n?\n\nEngage Premier entered an End of Sale (EOS) period effective June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:\n\nTwilio Marketing Campaigns\n\nPreferred ISV Partners:\n\nAirship Blog\nBloomreach Blog\nBraze Blog\nInsider Blog\nKlaviyo Blog\nTwilio Engage Foundations Documentation\n\n\nSegment associates a user subscription state with each email address and phone number in your Engage audiences. Subscription states give you insight into the level of consent a user has given you to receive your Engage campaigns.\n\nYou can set a user\u2019s subscription state using a CSV file or, programmatically, using Reverse ETL or Segment\u2019s APIs. On this page, you\u2019ll learn how and when to use these processes.\n\nSetting user subscriptions with a CSV file upload\n\nSetting user subscriptions by uploading a CSV file proves useful when you\u2019re importing batch contacts to Segment for the first time or when you need to change a specific user\u2019s subscription status.\n\nFor example, you may want to add contacts to Segment using an audience list sourced from a third-party tool, or you may have gathered a large number of contacts from an in-person event.\n\nSubscription state CSV fields\n\nTo learn how to upload a CSV file to Segment, view the Engage CSV Uploader page.\n\nTo change the subscription status of an email or phone number with a CSV file, include at least one of the following user subscription columns next to the contact column:\n\nemail_subscription_status\nsms_subscription_status\n\nThese columns take the following values:\n\nsubscribed; for users who opted in to your marketing campaigns\nunsubscribed; for users who have unsubscribed from your marketing campaigns\ndid-not-subscribe; for users who have neither subscribed nor unsubscribed from your marketing campaigns\nBlank; for email addresses or phone numbers that Segment collected without the user explicitly providing them\n\nEngage accepts both uppercase and lowercase subscription status values.\n\nRefer to the User Subscription States documentation for detailed explanations of each subscription state.\n\nManage user subscriptions with Segment APIs\n\nWith Segment\u2019s APIs, you can manage user subscriptions programmatically on a real-time basis. Use the APIs for ongoing subscription status updates, like when users subscribe to your marketing campaigns on a website form or modify their subscription from within a notification center.\n\nChoosing between the Identify call and the Public API\n\nTo update Engage user subscriptions with Segment\u2019s APIs, first choose between the Identify call, for non-critical subscription updates, or the Public API, for critical updates that require immediate confirmation, like unsubscribes.\n\nWhen you use the Identify call, Segment replies with a standard HTTP 200 OK status response code if it successfully received the request. Because the Identify call updates user traits asynchronously, though, the 200 OK code indicates that Segment has received, but not yet processed, the request. As a result, use the Identify call for non-critical subscription updates, like form signups on your website or adding a subscription from within the user\u2019s notification center.\n\nWhen you update user subscriptions with Segment\u2019s Public API, however, you\u2019ll get an immediate response that confirms that Segment both received and processed the request. Use the Public API, then, for unsubscribes, so users immediately find out if their subscription updated.\n\nFormat the Identify call payload\n\nFor Segment to process the subscription status request, your Identify call payload must include at least one object that contains an email address or phone number, its subscription type, and its subscription status. Engage accepts both uppercase and lowercase subscription statuses in Identify calls.\n\nThe following example payload shows an Identify call with a context object, which you\u2019ll add to the Identify call to update user subscriptions. The context object contains a messaging_subscriptions array with three objects that update SMS, WhatsApp, and email subscription statuses:\n\n{\n  \"userId\": \"12aBCDeFg4hIjKlM5OPq67RS8Tu\",\n  \"context\": {\n    \"messaging_subscriptions\": [\n      {\n        \"key\": \"(123) 555-5555\",\n        \"type\": \"SMS\",\n        \"status\": \"SUBSCRIBED\" | \"UNSUBSCRIBED\" | \"DID_NOT_SUBSCRIBE\"\n      },\n      {\n        \"key\": \"(123) 555-5555\",\n        \"type\": \"WhatsApp\",\n        \"status\": \"SUBSCRIBED\" | \"UNSUBSCRIBED\" | \"DID_NOT_SUBSCRIBE\"\n      },\n      {\n        \"key\": \"test@example.com\",\n        \"type\": \"EMAIL\",\n        \"status\": \"SUBSCRIBED\" | \"UNSUBSCRIBED\" | \"DID_NOT_SUBSCRIBE\"\n      }\n    ],\n    \"externalIds\": [\n      {\n        \"id\": \"(123) 555-5555\",\n        \"type\": \"phone\",\n        \"collection\": \"users\",\n        \"encoding\": \"none\"\n      }\n    ],\n    \"traits\": {\n       \"email\": \"test@example.com\"\n    }\n  },\n  \"integrations\": {},\n  \"traits\": {}\n}\n\n\nFor successful requests, Segment instantly updates subscription states in your workspace. You can then display successful updates or error messages with users in your notification center.\n\nWhile SMS and WhatsApp share the same number, you must add a separate subscription state for both of them.\n\nSetting user subscriptions with Reverse ETL\n\nEngage Premier Subscriptions users can use Reverse ETL to sync subscription data from warehouses to destinations. To get started with using Reverse ETL for managing subscriptions, see Reverse ETL for Engage Premier Subscriptions.\n\nThis page was last modified: 15 Jul 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSetting user subscriptions with a CSV file upload\nManage user subscriptions with Segment APIs\nSetting user subscriptions with Reverse ETL\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nHow To Guides\n/\nImporting Historical Data\nImporting Historical Data\n\nWhen transitioning over to Segment, customers commonly want to import historical data into tools they are migrating to or evaluating.\n\nNote:\u00a0Historical imports can only be done into destinations that can accept historical timestamped data. Most analytics tools like Mixpanel, Amplitude, or Kissmetrics can handle that type of data just fine. One common destination that doesn\u2019t accept historical data is Google Analytics, since their API cannot accept historical data.\n\nMethod 1: Using a Custom Solution\nGeneral Instructions\n\nUse any\u00a0server-side library, which sends requests in batches to improve performance. Once you have data to import, follow the steps below:\n\nExport or collect the data to be imported.\n\nInclude timestamp data in your export if the data needs to appear in end tools in a historical reference. For instance, if you\u2019re importing emails and it\u2019s relevant to know when someone joined your email list, you may need to export the timestamp.\u00a0If no timestamp is specified when importing, the data will show a timestamp from the time the data was received.\n\nDecide which destinations need to receive the data.\n\nBy default, data coming into Segment will be forwarded to all destinations connected to a given source.\u00a0To limit data to specific destinations, the\u00a0integrations\u00a0object must be modified. With historical data, you often only want to send the data to a specific destination or into your data warehouse. For example, in\u00a0Node.js\u00a0set the\u00a0integrations\u00a0object as follows.\n\nanalytics.track({\n    event: 'Upgraded Membership',\n    userId: '97234974',\n    integrations: { 'All': false, 'Vero': true, 'Google Analytics': false }\n })\n\n\nOnce you\u2019ve done that, you\u2019ll need to write an application or worker to send the data to Segment.\n\nYou will need to cycle through each set of data and map it to a Segment server-side library method or build an array matching the\u00a0HTTP Import API format.\n\nTip: Segment recommends using a Segment library for this process, as they set contextual message fields like\u00a0message_id\u00a0(used for deduping) and\u00a0sent_at\u00a0(used for correctly client clock skew) that Segment\u2019s API uses to correct behavior upon ingestion.\n\nTip: The server-side libraries will automatically batch requests to optimize for performance and prevent linear request volume. This batching behavior is modifiable, and some of the underlying libraries implement a configurable max queue size that may discard messages if you enqueue requests much faster than the client can flush them. We recommend overriding the max queue size parameter for the library to a high value you\u2019re comfortable you can remain under in your batch job.\n\nDemo projects\n\nThe following projects are open-source and do not have official Segment support. If you encounter issues, the best way to get help is by opening an issue on the project\u2019s GitHub page. Feel free to clone the repository and adjust the code to suit your unique needs.\n\nOne of Segment\u2019s Success Engineers wrote an alpha prototype Node.js app for importing data utilizing the HTTP API, which we\u2019ve included below:\n\nExample Node.js import application\n\nAdditionally, one of Segment\u2019s Software Engineers developed a React App with more out of the box functionality for importing events. The features include a modern UI, transformations, and event format checking prior to import:\n\nDesktop React CSV uploader\n\nMarketLytics\u00a0has documented their experience using the alpha prototype importer and offer some\u00a0helpful visuals and tips.\n\nAlternative solution\n\nIf a server-side library doesn\u2019t meet your needs, you can use the Segment\u00a0bulk import HTTP API directly.\n\nNote: When you use the HTTP API to export historical data to upload to Segment, remove all the original\u00a0sent_at,\u00a0message_id, and\u00a0project_id\u00a0fields from the archived message before forwarding them back to Segment.\n\nMethod 2: Using Reverse ETL\n\nPlease refer to the Reverse ETL guide for more details.\n\nThis page was last modified: 14 May 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nMethod 1: Using a Custom Solution\nMethod 2: Using Reverse ETL\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nUser Subscriptions\n/\nUpdate Subscriptions with a CSV\nUpdate Subscriptions with a CSV\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE PREMIER \u2713\n?\n\nEngage Premier entered an End of Sale (EOS) period effective June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:\n\nTwilio Marketing Campaigns\n\nPreferred ISV Partners:\n\nAirship Blog\nBloomreach Blog\nBraze Blog\nInsider Blog\nKlaviyo Blog\nTwilio Engage Foundations Documentation\n\n\nUse the CSV Uploader to add or update user subscription states.\n\nWhen you upload a CSV file, Engage adds new profiles and updates existing user profiles. Each CSV row corresponds to a user profile and columns to an identifier in your identity resolution configuration.\n\nYou can also set subscription states for each email and phone number that you upload in the CSV. Subscription states help you track which email addresses and numbers you have permission to market to.\n\nUploading a CSV creates new profiles and updates existing profiles. These profile updates may lead to users entering existing audiences or message campaigns.\n\nUsing the CSV Uploader to upload user profiles to Engage will not increase your MTUs count. Learn more about MTUs and Engage.\n\nUpload a CSV file\n\nFollow these steps to add subscribers with a CSV file upload\n\nNavigate to Engage > Engage settings > Subscriptions.\nClick Add global subscribers.\nOn the Update subscription statuses page, click Upload a CSV.\n1. Download your CSV template\n\nClick Download Template to download a CSV template with identifier columns from your identity resolution configuration. Engage adds subscription columns next to email and SMS identifiers, where you can update subscription states for email addresses and phone numbers.\n\nCSV files can only have a single email and phone identifier column. Include any additional email addresses or phone numbers for a user profile as a separate row.\n\nNavigate to Unify > Unify settings and select the Identity resolution tab to view or add identifiers in your Segment workspace.\n\n2. Fill out your CSV file\n\nEnter values for the identifiers in your CSV file. You can also set email, phone, and WhatsApp subscriptions using the email_subscription_status, sms_subscription_status, and whatsapp_subscription_status columns.\n\nA few best practices to keep in mind as you fill out your CSV:\n\nLeave any unknown values blank to avoid bad data. Engage can create a user profile from a single identifier in your CSV.\nEnter phone numbers in your CSV in a format that\u2019s consistent with your Segment space. For example, if existing profiles in your workspace are in E.164 format +15555550123, enter numbers in your CSV using the same format +##########.\n3. Upload your CSV file\n\nUpload a CSV file to Twilio Engage in two ways:\n\nDrag and drop the CSV file in the dropzone.\nClick Browse to locate the CSV file.\n\nEngage processes CSV rows sequentially. Column values, except for a blank subscription status, override previous values for a user profile.\n\nA blank subscription status in the CSV doesn\u2019t overwrite current email or phone subscription states in your Segment space.\n\n4. Name your custom trait\n\nEvery time you upload a file, you have the option to add a custom trait to user profiles in the CSV. Use custom traits to help you create audiences or send messages to a specific group of users. You can also add an existing custom trait name from your Segment workspace to the list of users in the CSV file.\n\nCustom traits display in the Custom Traits tab of a user profile in the Profile explorer.\n\nView Update History\n\nUse the Update History page to view CSV file uploads in your workspace over the last 30 days.\n\nTo view the Update History page:\n\nNavigate to Unify > Profile explorer or Engage > Engage settings > Subscriptions.\nFrom the Subscriptions section, click View update history.\nFrom the Upload history table, click the file name link to download the error reports.\n\nView the status of the file upload and the custom trait name added to user profiles in the CSV upload. The error report only shows rows that Segment couldn\u2019t successfully process.\n\nError reports\n\nUse error reports to fix invalid rows and quickly re-upload data.\n\nFrom the Update History page:\n\nSelect the link in the Report column to download an error report CSV. All rows not present in the error report were processed successfully.\nCorrect data in the invalid rows.\nRemove any extra columns such as row_number, error_message, and error_code.\nClick Update subscription statuses, and select Upload a CSV to re-upload the file.\n\nEngage uses the following error codes on the report:\n\nERROR CODE\tDESCRIPTION\nINVALID_EMAIL\tThe email address isn\u2019t formatted correctly.\nINVALID_PHONE\tThe phone number is invalid.\nINVALID_SUBSCRIPTION_STATUS\tThe subscription status is invalid. Check the status or leave it blank.\nCONFIGURATION_ERROR\tYour SendGrid settings are not configured correctly. Contact Segment support for help.\nSYSTEM_ERROR\tSomething went wrong. Please try again.\nUNABLE_TO_SUBSCRIBE\tYou can\u2019t update the subscription status for this phone number because the user unsubscribed by replying STOP. The user must reply START to resubscribe.\nValidation errors\n\nThe following table lists validation errors you may run into with your CSV upload:\n\nERROR\tERROR MESSAGE\nInvalid file types\tYou can upload only .csv files. Change your file format, then try again.\nEmpty files\tThis file contains no data. Add data to your CSV, then try again.\nCSV parsing error\tWe encountered an issue while parsing your CSV file. Validate the CSV file and try again.\nUnexpected/fallback\tSomething went wrong. Try again later.\nEmpty header row\tThis file contains empty header(s). Remove the empty header(s), then try again.\nFile exceeds one million rows\tToo many rows. You can upload up to 1000000 rows.\nFile exceeds 100 MB\tFiles can be up to 100 MB.\nExtraneous columns/column name typos\tThis file has columns that do not match the identifiers in your identity resolution configuration.\nSet user subscriptions\n\nUse the CSV Uploader to set subscription states for user email addresses and phone numbers.\n\nEach user profile in a Segment workspace can have multiple email addresses and phone numbers, all with different subscription states.\n\nFor each CSV file, Engage adds:\n\nAn email_subscription_status column next to the Email column.\nwhatsapp_subscription_status and sms_subscription_status columns next to the Phone column.\n\nIn the email_subscription_status, sms_subscription_status, and whatsapp_subscription_status columns, set subscription states for email and phone numbers with the following values:\n\nsubscribed: The user has actively subscribed.\nunsubscribed: The user has actively unsubscribed.\ndid-not-subscribe: The user has provided their contact information but didn\u2019t actively subscribe or unsubscribe.\nNo subscription status (blank value): The user\u2019s profile exists in Segment, but they haven\u2019t explicitly provided their contact information, and no subscription information is available.\n\nEngage accepts both uppercase and lowercase subscription status values.\n\nOnly contact users that subscribe to your communications. View User Subscription States to learn more.\n\nMessage consent\n\nSegment recommends sending to subscribed users. If a recipient deletes or flags an unwanted message as spam, inbox providers might start to filter your messages straight to spam folders. View more SendGrid delivery Best Practices to prevent email from going to spam.\n\nThis page was last modified: 15 Jul 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nUpload a CSV file\nView Update History\nSet user subscriptions\nMessage consent\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nApi\n/\nPublic API\nPublic API\nFREE X\nTEAM \u2713\nBUSINESS \u2713\nADD-ON X\n?\n\nThe Segment Public API helps you manage your Segment workspaces and its resources. You can use the API to perform CRUD (create, read, update, delete) operations at no extra charge. This includes working with resources such as Sources, Destinations, Warehouses, Tracking Plans, and the Segment Destinations and Sources Catalogs. The Public API is available to Team and Business Tier customers.\n\nAll CRUD endpoints in the API follow REST conventions and use standard HTTP methods. Different URL endpoints represent different resources in a workspace.\n\nSegment Public API Documentation\n\nResearch and test the Public API's available endpoints.\n\nIf your application is built in Javascript / Typescript, Go, Java, or Swift, check out Segment\u2019s Public API SDKs.\n\nConfig API vs Public API\n\nThe Public API includes the following benefits over the Config API:\n\nBENEFIT\tDETAILS\t\u00a0\nFuture Enhancements\tFuture improvements will be added to the Public API only.\t\u00a0\nImproved error handling\tThe Public API offers more specific error messages, for faster issue resolution.\t\u00a0\nVersioning\tEach endpoint on the Public API can have multiple versions. Stable versions can coexist with beta or alpha versions.\t\u00a0\nHigher rate limits\tThe Public API can offer higher rate limits when needed or different rate limits per endpoint or token.\t\u00a0\nImproved architecture\tThe Public API is built with improved security, checks for authentication, authorization, input validation, HTTPS exposed services, auto-scaling, and more in mind.\t\u00a0\nCleaner mapping\tThe Public API uses unique IDs for reference, in place of slugs in the Config API. Unique IDs are, by design, unique.\t\u00a0\nAvailable in Europe\tThe Public API is accessible to both US and EU-based workspaces.\t\u00a0\nIncreased reliability\tThe Public API features more stable endpoints, and a 99.8% success rate\t\u00a0\nCreate a Public API token\n\nOnly Workspace Owners can create a Public API token\n\nOnly users with the Workspace Owner role can create a Public API token. For more information about roles, see Segment\u2019s Roles documentation.\n\nTo create a Public API token in your Segment workspace:\n\nNavigate to Settings > Workspace settings > Access Management > Tokens.\nClick the + Create Token button.\nCreate a description for the token and assign it either Workspace Owner or Workspace Member access.\nClick Create.\nCopy your workspace token somewhere secure and click Done.\n\nTo begin sending requests to the Public API, make sure to include the Public API Token into your HTTP requests with the Authorization Header and configured with Bearer Token and the value of the newly generated Public API token.\n\nAPI Token Security\n\nTo enhance API token security, Segment partners with GitHub to prevent fraudulent use of exposed API tokens found in public git repositories. This helps to prevent malicious actors from using exposed tokens to perform unauthorized actions in your Segment workspace.\n\nWithin seconds, GitHub scans each commit in public repositories for Public API tokens, and sends detected tokens to Segment. Valid tokens are automatically revoked and workspace owners are notified.\n\nLearn more about GitHub\u2019s secret scanning program.\n\nFAQs\nWhat should I do if I see a notification that my token was exposed?\n\nIn most cases, identifying and revoking an exposed token takes seconds. Segment recommends you check the audit trail to ensure no unauthorized actions were taken with the token.\n\nHow did my token get exposed?\n\nDevelopers can accidentally commit tokens to public repositories, exposing them to the public. This can happen when developers use a token in a local development environment and forget to remove it before committing their code.\n\nWhy are exposed tokens automatically revoked?\n\nBy automatically revoking the exposed token, Segment helps keep your workspace secure and prevents potential abuse of the token.\n\nHow do I enable this feature?\n\nThis feature is automatically enabled for all workspaces on Team or Business tier plans.\n\nWhat should I do when I see a CORS error?\n\nIf you see a CORS error, this means you\u2019re attempting to make a request to the Public API on the front-end. The Public API is used for server-side only. To get rid of the error, move all Public API requests to a server.\n\nWhat User Role / Workspace permissions are required to generate Public API tokens?\n\nOnly users that have a Workspace Owner role can create Public API Tokens.\n\nTroubleshooting\nThe Update Schema Settings in Source endpoint returns error for field forwardingViolationsTo and forwardingBlockedEventsTo\n\nWhen you don\u2019t have a source to forward violations or blocked events to, then exclude the fields forwardingViolationsTo or forwardingBlockedEventsTo entirely from the request and the setting will be disabled.\n\nPATCH endpoint : https://api.segmentapis.com/sources/{sourceId}/settings\n\n{\n    \"group\": {\n      \"allowTraitsOnViolations\": false,\n      \"allowUnplannedTraits\": false,\n      \"commonEventOnViolations\": \"ALLOW\"\n    },\n    \"identify\": {\n      \"allowTraitsOnViolations\": true,\n      \"allowUnplannedTraits\": true,\n      \"commonEventOnViolations\": \"Block\"\n    },\n    \"track\": {\n      \"allowEventOnViolations\": false,\n      \"allowPropertiesOnViolations\": false,\n      \"allowUnplannedEventProperties\": false,\n      \"allowUnplannedEvents\": false,\n      \"commonEventOnViolations\": \"OMIT_PROPERTIES\"\n    }\n  }\n\nWhat is the difference between a destination\u2019s Instance ID and Meta ID?\n\nThe destination\u2019s Instance ID is specific to a single destination within your workspace. The destination\u2019s Meta ID, which is returned by the delivery metrics endpoint, identifies which integration you\u2019ve set up. For example, if you had a dev Mixpanel (Actions) destination and a prod Mixpanel (Actions) destination, they would have the same Meta ID but two different Instance IDs.\n\nThis page was last modified: 24 Jun 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nConfig API vs Public API\nCreate a Public API token\nAPI Token Security\nFAQs\nTroubleshooting\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nDestinations\n/\nSending Segment Data to Destinations\nSending Segment Data to Destinations\n\nYou\u2019ve decided how to format your data, and collected it using Segment Sources. Now what do you do with it? You send the data to Destinations.\n\nDestinations are tools or services which can use the data sent from Segment to power analytics, marketing, customer outreach, and more.\n\nEach Segment Workspace has its own set of destinations, which are connected to the workspace\u2019s sources. When you add or modify a destination, make sure you\u2019re working with the correct workspace.\n\nHealthcare and Life Sciences (HLS) customers can encrypt data flowing into their destinations\n\nHLS customers with a HIPAA eligible workspace can encrypt data in fields marked as Yellow in the Privacy Portal before they flow into an event stream, cloud-mode destination.\n\nTo learn more about data encryption, see the HIPAA Eligible Segment documentation.\n\nAdding a destination\n\nThere are two ways to add a destination to your deployment: using the Segment web app, or using the Public API.\n\nAdding a destination from the Segment web app\n\nSome third-party tools (such as Customer.io, Leanplum, and Airship) can both consume Segment data (as destinations), and send their data to Segment Warehouses as Cloud-Sources. When you add a destination, make sure you\u2019re viewing the Destinations tab in the catalog so you add the correct one.\n\nBefore you start, log in to your account on the destination tool, and get the token or other credentials needed to access the tool. You\u2019ll use these to give Segment permission to send data to the tool.\nLog in to the Segment web app, and select the workspace you want to add the destination to.\nClick Catalog in the left navigation, and click the Destinations tab.\nSearch for the destination you want to add, and click it in the results. If there are multiple results for your search, you can click each result to read more about them until you find the one you\u2019re looking for.\nIn the panel that appears, click Configure.\nOn the next page, select a source to connect to the destination, and click Confirm Source.\nOn the next page, find the Connection Settings section, and enter the token or credentials for that destination tool.\nClick the toggle at the top of the Settings page to enable the destination.\n\nIf you have more than one instance of the same destination, you can click Copy Settings From Other Destination to save yourself time entering the settings values manually.\n\nAdding a destination to a specific Segment Source\n\nYou can also add a destination directly from the source\u2019s settings page in the Segment web app.\n\nBefore you start, log in to your account on the destination tool, and get the token or other credentials needed to access the tool. You\u2019ll use these to give Segment permission to send data to the tool.\nLog in to the Segment web app, and select the workspace you want to work in, and navigate to the source you want to add the destination to.\nFrom the source information page, click Add Destination.\nSearch for the destination you want to add, and click it in the results. If there are multiple results for your search, you can click each result to read more about them until you find the one you\u2019re looking for.\nIn the panel that appears, click Configure.\nOn the next page, find the Connection Settings section, and enter the token or credentials for that destination tool.\nClick the toggle at the top of the Settings page to enable the destination.\nAdding a destination using the Public API\n\nYou can use the Segment Public API to add destinations to your workspace using the Create Destination endpoint. The API requires an authorization token, and uses the name field as a namespace that defines which source the destination is connected to. You send the rest of the destination\u2019s configuration as a JSON blob. View the documentation page in the Segment Catalog, or query the Segment Catalog API, for a destination to see the available settings.\n\nYou must use an authorization token to access the Public API, and these tokens are tied to specific workspaces. If you use the Public API to access multiple workspaces, make sure you\u2019re using the token for the workspace you want to access before proceeding.\n\nWhat happens when you add a destination\n\nAdding a destination can have a few different effects, depending on which sources you set up to collect your data, and how you configured them.\n\nAnalytics.js\n\nIf you are using Segment\u2019s JavaScript library, Analytics.js, then Segment handles any configuration changes you need for you. If you\u2019re using Analytics.js in cloud-mode, the library sends its tracking data to the Segment servers, which route it to your destinations. When you change which destinations you send data to, the Segment servers automatically add that destination to the distribution list.\n\nIf you\u2019re using Analytics.js in device-mode, then Analytics.js serves as a wrapper around additional code used by the individual destinations to run on the user\u2019s device. When you add a destination, the Segment servers update a list of destinations that the library queries. When a user next loads your site, Analytics.js checks the list of destinations to load code for, and adds the new destination\u2019s code to what it loads. It can take up to 30 minutes for the list to update, due to CDN caching.\n\nYou can enable device-mode for some destinations from the destination\u2019s Settings page in the Segment web app. You don\u2019t need to use the same mode for all destinations in a workspace; some can use device-mode, and some can use cloud-mode.\n\nMobile sources\n\nBy default, Segment\u2019s mobile sources send data to Segment in cloud-mode to help minimize the size of your apps. In cloud-mode the mobile source libraries forward the tracking data to the Segment servers, which route the data to the destinations. Since the Segment servers know which destinations you\u2019re using, you don\u2019t need to take any action to add destinations to mobile apps using cloud-mode.\n\nHowever, if the destination you\u2019re adding has features that run on the user\u2019s device, you might need to update the app to package that destination\u2019s SDK with the library. Some destinations require that you package the SDK, and some only offer it\n\nServer sources\n\nSegment\u2019s server sources run on your internal app code, and never have access to the user\u2019s device. They run in cloud-mode only, and forward their tracking calls to the Segment servers, which forward the data to any destinations you enabled.\n\nDestination authentication\n\nWhen you add a destination in Segment, you must tell Segment how to connect with that destination\u2019s app or endpoints. Most destinations offer an API token or authentication code which you can get from their web app. The documentation for each Segment destination includes information about what you need, and how to find it. Copy this information, and paste it into the Settings for the destination, or include it in the create API call.\n\nDestination settings\n\nEach destination can also have destination settings. These control how Segment transforms the data you send to the destination, and can be used to adapt it to your configuration or enable or disable certain destination features.\n\nConnecting one source to multiple instances of a destination\n\nMultiple-destination support is available for all Segment customers on all plan tiers.\n\nSegment allows you to connect a source to multiple instances of a destination. You can use this to set up a single Segment source that sends data into different instances of your analytics and other tools.\n\nFor example, you might set up a single Segment source to send data both to separate instances of Google Analytics for each business unit in your organization, and to another instance for executive-level reporting. You could also use this to make sure that tooling instances for different geographic teams are populated with the same data from the same source.\n\nYou can also connect multiple instances of a destination to help you smoothly migrate from one configuration to another. By sending each version the same data, you can check and validate the new configuration without interrupting use of the old one.\n\nHowever, there are a few considerations:\n\nDevice-mode destinations do not support connecting multiple instances of the destination to the same source. If you try to a connect an additional instance of a device-mode destination to your source, the option to add a second instance does not appear.\n\nMobile sources, and the legacy Project source, can connect to multiple instances of destinations that operate only in cloud-mode. Mobile and Project sources cannot connect to multiple instances of destinations that operate in both cloud-mode and device-mode. Non-mobile sources can only connect to one device-mode instance of a destination.\n\nMulti-instance support is not available for most hybrid Actions destinations or Web mode Actions destinations.\n\nSegment does not support connecting a single source to multiple instances of a Data Lakes destination.\n\nNon-mobile sources can only connect to _one_ device-mode instance of a destination\n\nYou cannot connect a source to more than one instance of a destination that operates only in device-mode. For more information about device-mode restrictions, see the Sending Segment data to Destinations documentation.\n\nIf your organization is on a Segment Business tier plan, you can use Replay to send historical data to new instances of a destination.\n\nConnect a source to more than one instance of a destination\n\nTo connect a source to more than one instance of a destination in the Segment web app, start by adding the first instance of the destination and giving it a unique name, as described above. To add another instance of the destination, follow either of those two methods and choose another unique name.\n\nYou must give each instance of the destination connected to the same source a unique name. Segment recommends that you use descriptive names rather than numbers, so other Segment users can understand which Segment destinations are linked to which tool instances. For example, you might use \u201cAmplitude North America\u201d and \u201cAmplitude South America\u201d, instead of \u201cAmplitude 1\u201d and \u201cAmplitude 2\u201d. You can edit the destination instance name at any time.\n\nIf you added the first instance of your destination before multi-instance destinations became available, that instance is automatically named for the destination with no other identifiers, for example \u201cAmplitude\u201d.\n\nSome destinations do not support having multiple instances connected to the same source. In that case, the option to add a second instance of that destination does not appear.\n\nYou can create unique destination filters for each destination instance connected to the same source.\n\nSome destinations don\u2019t support multiple instances connected to the same source. If this is the case, you won\u2019t see the option to add a second instance of that destination.\n\nConnect multiple sources to one instance of a destination\n\nIt is not possible to connect multiple instances of one source (for example, two website sources) to the same destination. However, you can create another instance of the destination for the other sources, and click Copy Settings From Other Destination to save yourself time entering the settings values again.\n\nConnect to more than one instance of a destination using the Public API\n\nYou can add multiple instances of a destination using the Segment Public API. See the Segment Config API documentation. If a destination does not support multi-instance, the Public API throws an appropriate error.\n\nMulti-instance destinations and Device-mode\nYou can connect a source to up to 25 instances of a destination if all of the instances use cloud-mode. Destinations using cloud-mode receive data directly from the Segment servers.\nMobile sources, and the legacy Project source, can connect to multiple instances of destinations that operate only in cloud-mode. Mobile and Project sources cannot connect to multiple instances of destinations that operate in both cloud-mode and device-mode.\nWarning: If you bundle one instance of a destination in a mobile source but have other instances of that destination connected to that source you might see unexpected and inconsistent data.\nNon-mobile sources can only connect to one device-mode instance of a destination, in addition to up to 25 cloud-mode instances. A web browser sending to a destination in device-mode sends data directly from the user\u2019s browser (instead of through the Segment servers), by bundling a copy of destination\u2019s code with the Segment SDK. Segment can\u2019t bundle multiple copies of the destination SDK and so it can\u2019t send data to multiple instances of the destination from the browser.\nYou cannot connect a source to more than one instance of a destination that operates in device-mode only. These destinations can only accept data from code directly on the user\u2019s device, and Segment cannot include duplicates of that code for a single source.\nMulti-instance support is not available for most hybrid Actions destinations, and will not support Web Mode Actions destinations. Hybrid destinations are those that have some components that operate in device-mode and some that operate in cloud-mode. Actions destinations currently affected by this are Amplitude Actions and Braze Cloud Actions.\nAmplitude Actions does have multi-instance support for Analytics.js and server sources, but does not have multi-instance support for mobile sources.\nBraze Cloud Actions does not have multi-instance support because it includes a device-mode web plugin for the Debounce Middleware.\nOther multi-instance destination considerations\n\nMultiple Data Lakes: Segment does not currently support connecting a single source to multiple instances of a data lakes destination. Contact Segment Customer Success if this would be useful for your organization.\n\nProtocols transformations and multi-instance support: Protocols transformations are specific to each source, and operate the same on all instances of a specific destination. Segment does not currently support creating unique protocols transformations for each instance of a destination.\n\nIntegrations object considerations: A common part of a Segment message is the integrations object, which you can use to explicitly filter to which destinations the call is forwarded, as well as to specify options for different destination tools. If you use the integrations object to filter events or to send destination-specific options, Segment applies its values to all instances. For example:\n\n{\n  \"integrations\": {\n    \"Mixpanel\": false,\n    \"Adobe Analytics\": {\n      \"marketingCloudVisitorId\": \"12345\"\n    }\n  }\n}\n\n\nIn this example:\n\nEvents sent with this Mixpanel setting are not sent to instances of Mixpanel.\nEvents sent to any Adobe Analytics destinations with this Adobe Analytics setting use the same marketingCloudVisitorId value specified. You can not use the integrations object to send data to individual destination instances.\n\nThis page was last modified: 30 May 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nAdding a destination\nWhat happens when you add a destination\nDestination authentication\nDestination settings\nConnecting one source to multiple instances of a destination\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nUsage And Billing\n/\nSegment Startup Program\nSegment Startup Program\n\nSegment offers a Startup Program to enable early startups to track data and test the marketing and analytics tools necessary to grow their business. The program is open to any early-stage startup that meets the following eligibility requirements:\n\nIncorporated less than two years ago\nRaised no more than $5MM in total funding\nLocated in Google Cloud eligible territory\nhaven\u2019t previously received other Segment discounts\n\nThe Segment Startup Program includes three components:\n\nSegment\u2019s Startup Deal - Participating startups receive $25,000* in annual credit toward our monthly Team plan for as long as they meet our eligibility requirements (up to 2 years).\nPartner Startup Deals - Segment partners with other technology companies that offer valuable tools for startups to offer exclusive deals and promotions from marketing, data warehouse, and analytics tools.\nStartup Resources - Segment offers learning materials on topics like analytics, product-market fit, and more for founders to become experts on data analytics and making the most of Segment\u2019s technology.\n\nInterested companies can apply on the Startup Program site.\n\nApplication deadline\n\nEffective January 6, 2025, Segment will no longer accept applications for the Segment Startup Program. Applications submitted before 11:59 PM PT on December 5, 2024 will be reviewed and honored. Any applications received after this deadline won\u2019t be accepted. There will be no exceptions.\n\n*Can vary based on affiliated accelerator and VC partners.\n\nFrequently Asked Questions\n\nHow are the Segment credits applied? Credits are applied to your monthly bill, covering up to $25,000* in total usage per year. Any additional usage costs are not covered by the program.\n\nHow do I redeem the Segment credits? Eligible startups can apply directly for the Segment Startup Program.\n\nHow do I find out if I\u2019ve been accepted to the Segment Startup Program? If you\u2019ve been accepted to the program, you\u2019ll receive an email with a welcome message and next steps. If you haven\u2019t received an email, you can also check in your Segment workspace and look for a Startup Program icon in the top right corner.\n\nWhere can I view the credits applied to my Segment account? The Startup Program credits are reflected in the Workspace usage and billing page.\n\nDo I have to be a \u201cnew\u201d customer to receive a coupon? New and current Segment users who have not previously received any other coupon are eligible to apply.\n\nWhat happens if I go over my total credit applied? If you go over the total credit applied, you will be charged for the additional usage for that month.\n\nWhat happens when my credits expire? Once you\u2019ve used your total credits, you might be eligible to renew for another year at a discounted rate. Otherwise, we can talk about options for upgrading your plan.\n\nHow do I get the startup partner deals? Once you\u2019ve been accepted to the Segment Startup Program, you can apply for the partner deals using this Airtable form. (You can view a list of the available deals in a section of the Airtable form.)\n\nHow do I know if my accelerator/incubator/VC firm has a relationship with Segment? Ask your program manager to see if they participate in the Segment Startup Program. If they do not, you can request that they apply to become a partner.\n\nThis page was last modified: 19 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nFrequently Asked Questions\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nFunctions\n/\nFunctions Copilot\nFunctions Copilot\n\nFunctions Copilot helps you generate JavaScript code for functions using natural language prompts. For more information about the language model used to generate JavaScript code, see the Functions Copilot Nutrition Facts Label.\n\nFunctions Copilot benefits\n\nFunctions Copilot improves efficiency and productivity by streamlining the process of creating and managing custom functions.\n\nFunctions Copilot can help you:\n\nGenerate JavaScript code for custom integrations and data transformations.\nAnalyze existing code and provide optimization suggestions.\nSecure sensitive data with minimal effort.\nSimplify code testing and maintenance.\nExample prompts\n\nThis table lists example prompts you can use with Functions Copilot:\n\nFUNCTION TYPE\tEXAMPLE PROMPTS\nSource Functions\t\u201cTransform incoming data into a track event.\u201d\n\n\u201cEnrich user data with additional demographic details using an external API.\u201d\nDestination Functions\t\u201cCreate a function that enriches an Identify event using the Profile API.\u201d\n\n\u201cRemove PII data and hash email addresses in an Identify event.\u201d\nDestination Insert Functions\t\u201cEnrich an Identify event using an external API.\u201d\n\n\u201cTokenize PII data before sending it downstream.\u201d\nBest practices and limitations\n\nFollow this guidance when you use Functions Copilot:\n\nAvoid using personally identifiable information (PII) or sensitive data.\nWrite specific prompts. Specificity leads to more accurate CustomerAI function generation. Use the names of existing events, related attributes, and properties.\nIterate on your prompts. If you don\u2019t get the result you\u2019re looking for, try rewriting the prompt.\nLimitations\n\nKeep the following limitations in mind as you work with Functions Copilot:\n\nContext limitations: Functions Copilot generates code based on Segment-specific terminology and the prompts you write. As a result, the generated output may not always be accurate. If the function doesn\u2019t initially meet your needs, try to refine or rewrite your prompt.\nLanguage support: Functions Copilot only supports English prompts. Using other languages may impact the accuracy of the generated output.\n\nThis page was last modified: 27 Sep 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nFunctions Copilot benefits\nExample prompts\nBest practices and limitations\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nIdentity Resolution\n/\nSpace Setup\nSpace Setup\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\nStep one: Create a new Dev space\n\nWhen starting with Unify, begin by creating a Dev space. This will be your sandbox instance of Unify to test new Identity settings, audiences, and traits before applying the same changes to a Prod space that would immediately affect production data flowing to downstream destinations.\n\nStep two: Configure Identity settings\n\nBefore you connect any source to the Dev space, Segment recommends that you first start by reviewing and configuring your Identity settings, as changes to the Identity rules will only be applied to new events received following any updates. Read more on those settings in the Identity Resolution Settings docs.\n\nStep three: Set up a connection policy\n\nIf you haven\u2019t already, Segment highly recommends labeling all your sources with Dev or Prod environments. Once your sources have been labeled, visit the Connection Policy page by navigating to Unify > Unify settings > Space management. Here, you can enforce that only sources labeled Dev can be connected to your Dev Unify instance.\n\nNote: The Identity Resolution table can only be edited by workspace owners and users with the Identity Admin role.\n\nStep four: Connect sources and create test audiences\n\nOnce your connection policy is in place, select the Profile sources tab in Unify settings. Now you can connect a few sources that will automatically begin to replay.\n\nOnce the sources have finished replaying, check user profiles to ensure that profiles are merging as expected. This would also be an ideal time to create test audiences and confirm that these populate the expected number of users.\n\nStep five: Connect audiences to a Dev instance of a downstream destination\n\nConnect test audiences or traits to a dev instance of your downstream destination. Confirm that users are appearing as expected.\n\nStep six: Apply changes to Prod sources\n\nOnce everything looks good to go, create a new Prod space, following all the same steps above, and connect a live instance of your downstream destination to your Prod space.\n\nThis page was last modified: 12 Aug 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nStep one: Create a new Dev space\nStep two: Configure Identity settings\nStep three: Set up a connection policy\nStep four: Connect sources and create test audiences\nStep five: Connect audiences to a Dev instance of a downstream destination\nStep six: Apply changes to Prod sources\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nIdentity Resolution\n/\nIdentity Resolution ExternalIDs\nIdentity Resolution ExternalIDs\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\n\nThe steps in this guide pertain to spaces created before September 27th, 2020. For spaces created after September 27th, 2020, please refer to the Identity onboarding guide.\n\nDefault externalIDs\n\nThe Identity Graph creates or merges profiles based on externalIDs. ExternalIDs will become the identities attached to a user profile in the Profile explorer.\n\nNavigate to Unify > Profile explorer to view identities attached to a profile, along with custom traits, event history, and more.\n\nSegment automatically promotes the following traits and IDs in track and identify calls to externalIDs:\n\nEXTERNAL ID TYPE\tMESSAGE LOCATION IN TRACK OR IDENTIFY CALL\nuser_id\tuserId\nemail\ttraits.email, context.traits.email or properties.email\nandroid.id\tcontext.device.id when context.device.type = \u2018android\u2019\nandroid.idfa\tcontext.device.advertisingId when context.device.type = \u2018android\u2019 AND context.device.adTrackingEnabled = true\nandroid.push_token\tcontext.device.token when context.device.type = \u2018android\u2019\nanonymous_id\tanonymousId\nga_client_id\tcontext.integrations[\u2018Google Analytics\u2019].clientId when explicitly captured by users\nios.id\tcontext.device.id when context.device.type = \u2018ios\u2019\nios.idfa\tcontext.device.advertisingId when context.device.type = \u2018ios\u2019\nios.push_token\tcontext.device.token when context.device.type = \u2018ios\u2019\n\nThe Google clientID(ga_clientid) is a unique value created for each browser-device pair and will exist for 2 years if the cookie is not cleared. The analytics.reset() call should be triggered from Segment end when the user logs off. This call will clear the cookies and local Storage created by Segment. It doesn\u2019t clear data from other integrated tools. So on the next login, the user will be assigned with a new unique anonymous_id, but the same ga_clientid will remain if this cookie is not cleared. Hence, the profiles with different anonymous_id but with same ga_clientid will get merged.\n\nCustom externalIDs\n\nUnify resolves identity for any other externalIDs that you bind to users - such as a phone number or any custom identifier that you support.\n\nAs long as you\u2019ve configured custom externalIDs, such as phone, in your Space\u2019s Identity Resolution rules, you can include it with the context.externalIds array, the properties object, or the context.traits object.\n\nAs seen in the example below, you can send custom externalIds in the context object of any call to Segment\u2019s API.\n\nThe four fields below (id, type, collection, encoding) are all required:\n\nKEY\tVALUE\nid\tvalue of the externalID\ntype\tname of externalID type (app_id, ecommerce_id, shopify_id, and more)\ncollection\tusers if a user-level identifier or accounts if a group-level identifier\nencoding\tnone\n\nAs an example:\n\nanalytics.track('Subscription Upgraded', {\n   plan: 'Pro',\n   mrr: 99.99\n}, {\n  externalIds: [\n    {\n      id: '123-456-7890',\n      type: 'phone',\n      collection: 'users',\n      encoding: 'none'\n    }\n  ]\n})\n\n\nAdditionally, adding phone with the properties object gets picked up by Unify and applied as an externalID:\n\nanalytics.track('Subscription Upgraded', { plan: 'Pro', mrr: 99.99, phone: '123-456-7890'})\n\n\nYou can also include phone using the context.traits object and Unify adds it as an externalID to the profile.\n\nanalytics.track('Subscription Upgraded', { plan: 'Pro', mrr: 99.99}, {traits : {phone_number: '123-456-7890'}})\n\n\nUnify creates a user (user_id: use_123) with the custom externalID (phone: 123-456-7890). Query the user\u2019s phone record by using the externalID (phone: 123-456-7890), or update the profile with that externalID going forward. (Note: externalIDs must be lower-case.)\n\nViewing promoted externalIDs\n\nUsers can view which externalIDs are promoted on each event by viewing the raw payload on Events in the User Profile in the \u201cexternal_ids\u201d object.\n\nFor example, the following user had anonymous_id and user_id promoted as identifiers from the Course Clicked track call:\n\nExample\n\nFor example, a new anonymous user visits your Pricing page:\n\nanalytics.page('Pricing', {\n  anonymousId: 'anon_123'\n  title: 'Acme Pricing',\n  url: 'https://acme.com/pricing',\n  referrer: 'https://google.com/'\n});\n\n\nAt this point, the Identity Graph will create a new user with external id (anonymous_id: anon_123) and a persistent and globally unique segment_id, in this case: use_4paotyretuj4Ta2bEYQ0vKOq1e7.\n\nAny new events received with the same external id (anonymous_id: anon_123) are appended to same user use_4paotyretuj4Ta2bEYQ0vKOq1e7.\n\nNext, the user goes to a sign up form and signs up:\n\nanalytics.track('User Signup', {\n  userId: 'use_123',\n  anonymousId: 'anon_123'\n});\n\n\nAt this point, the Identity Graph associates external ID (user_id: use_123) with the same user use_4paotyretuj4Ta2bEYQ0vKOq1e7.\n\nThis page was last modified: 07 Nov 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nDefault externalIDs\nCustom externalIDs\nViewing promoted externalIDs\nExample\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nUnify\n/\nUnify FAQs\nUnify FAQs\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nUNIFY \u2713\n?\nDoes your identity model support multiple external ID types?\n\nYes, Identity Graph supports multiple external IDs.\n\nIdentity Graph automatically collects a rich set of external IDs without any additional code:\n\nDevice level IDs (example: anonymous_id, ios.idfa and android.id)\nDevice token IDs (example: ios.push_token and android_push_token)\nUser level IDs (example: user_id)\nCommon external IDs (email)\nCross-domain analytics IDs (cross_domain_id)\n\nIf you want Identity Graph to operate on a different custom ID, you can pass it in using context.externalIds on an Identify or Track call. If you\u2019re interested in this feature, contact your CSM to discuss the best way to implement this feature.\n\nHow does Unify handle identity merging?\n\nSegment analyzes each incoming event and extracts external IDs (like user_id, anonymous_id, email). The simplified algorithm works as follows:\n\nSegment first searches the Identity Graph for incoming external IDs.\nIf Segment finds no matching profile(s), it creates one.\nIf Segment finds one profile, it merges the incoming event with that profile. This means that Segment adds the external IDs on the incoming message and resolves the event to the profile.\nIf Segment finds multiple matching profiles, Segment applies the identity resolution settings for merge protection. Specifically, Segment uses identifier limits and priorities to add the correct identifiers to the profile.\nSegment then applies limits to ensure profiles remain under these limits. Segment doesn\u2019t add any further merges or mappings if the profile is at either limit, but event resolution for the profile will continue.\nIs all matching deterministic, or is there any support for probabilistic matching?\n\nAll Profile matching is deterministic and based on first-party data that you\u2019ve collected.\n\nSegment doesn\u2019t support probabilistic matching. Most marketing automation use cases require 100% confidence that a user is who you think they are (sending an email, delivering a recommendation, and so on). The best way to support this is through a deterministic identity algorithm.\n\nWhat happens to conflicting and non-conflicting profile attributes?\n\nIf two merged user profiles contain conflicting profile attributes, Segment selects the newest, or last updated, attributes when querying the profile.\n\nWhat identifiers can the merged profile be queried/updated with?\n\nAny of the external IDs can be used to query a profile. When a profile is requested, Segment traverses the merge graph and resolves all merged profiles. The result is a single profile, with the latest state of all traits, events, and identifiers.\n\nCan external IDs be changed or removed from the profiles?\n\nNo. As the Identity Graph uses external IDs, they remain for the lifetime of the user profile.\n\nCan I delete specific events from a user profile in Unify?\n\nNo. Alternatively, you may delete the entire user profile from Segment using a GDPR deletion request.\n\nHow does profile creation affect MTUs, particularly where a profile isn\u2019t merged with the parent profile due to exceeding the merge limit?\n\nSegment determines the Monthly Tracked Users (MTUs) count by the number of unique user IDs and anonymous IDs processed, regardless of how you manage these profiles in Unify and Engage. This count is taken as events are sent to Segment, before they reach Unify and Engage. Therefore, the creation of new profiles or the merging of profiles in Unify doesn\u2019t affect the MTU count. The MTU count only increases when you send new unique user or anonymous IDs to Segment.\n\nWhat is the event lookback period on the Profile Explorer?\n\nThe Profile Explorer retains event details for a period of up to 2 weeks. If you need event information beyond this timeframe, Segment recommends using Profiles Sync for comprehensive event analysis and retention.\n\nCan I remove a trait from a user profile?\n\nYes, you can remove a trait from a user profile by sending an Identify event with the trait value set to null in the traits object from one of your connected sources. For example:\n\n{\n  \"traits\": {\n    \"trait1\": null\n  }\n}\n\n\nSetting the trait value to an empty string won\u2019t remove the trait, like in this example:\n\n{\n  \"traits\": {\n    \"trait2\": \"\"\n  }\n}\n\n\nInstead, this updates the trait to an empty string within the user profile.\n\nThis page was last modified: 18 Nov 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nDoes your identity model support multiple external ID types?\nHow does Unify handle identity merging?\nIs all matching deterministic, or is there any support for probabilistic matching?\nWhat happens to conflicting and non-conflicting profile attributes?\nWhat identifiers can the merged profile be queried/updated with?\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nUser Subscriptions\n/\nSubscriptions with SQL Traits\nSubscriptions with SQL Traits\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE PREMIER \u2713\n?\n\nEngage Premier entered an End of Sale (EOS) period effective June 10, 2024. Existing Segment customers will continue to have access and support to Engage Premier until an end-of-life (EOL) date is announced. We recommend exploring the following pages in preparation of a migration or future MCM needs:\n\nTwilio Marketing Campaigns\n\nPreferred ISV Partners:\n\nAirship Blog\nBloomreach Blog\nBraze Blog\nInsider Blog\nKlaviyo Blog\nTwilio Engage Foundations Documentation\n\n\nUse Subscriptions with SQL Traits to connect to your data warehouse and query user subscription data to Engage on a scheduled basis. Use your data warehouse as a single source of truth for subscription statuses and query from warehouses such as BigQuery, Redshift, or Snowflake.\n\nOn this page, you\u2019ll learn how to use SQL to sync subscription data from your warehouse to Engage.\n\nUpdating subscription statuses with SQL creates new profiles and updates existing profiles. These profile updates may lead to users entering existing audiences or message campaigns.\n\nGetting started\n\nTo use Subscriptions with SQL Traits, you need the following:\n\nA warehouse connected to Segment\nA Segment workspace\nA user account with access to Engage in that workspace\n\nSegment supports Redshift, Postgres, Snowflake, Azure SQL, and BigQuery as data warehouse sources for SQL Traits. Visit Segment\u2019s warehouse docs for more on getting started with data warehouses.\n\nBefore you begin with Subscriptions with SQL Traits, you may also want to visit the subscription docs to learn more about user subscriptions in Engage.\n\nSync subscription data with SQL\n\nYou can sync with SQL from two locations in the Segment app. Navigate to Unify > Profile explorer or Engage > Audiences > Profile explorer, then:\n\nClick Manage subscription statuses, and select Update subscription statuses.\nSelect Sync with SQL, and click Configure.\nConfigure your SQL query\n\nTo configure Subscriptions with SQL Traits, you can write your own query or click Use Template to use one of the templates Engage provides. For any new users that your query returns, Engage adds a new profile.\n\nClick Reset Template to reset your SQL query.\n\nQueries must return at least one pair of the columns below with a value of subscribed, unsubscribed, or did_not_subscribe:\n\nemail and __segment_internal_email_subscription__\nphone and __segment_internal_sms_subscription__\n\nFor more subscription SQL best practices, view the query requirements below.\n\nSelect a warehouse and preview your query\n\nOnce you write your SQL query, click Select warehouse from the Configure screen to select the data warehouse you\u2019d like to query.\n\nBefore you schedule your sync intervals, click Preview to preview a subset of data and validate your results. To see subscription statuses for a particular profile, select a user row, then select the Identities tab.\n\nSchedule sync intervals\n\nYou can schedule sync intervals to import subscription data from your warehouse to Engage:\n\nFrom the Configure screen, click Schedule.\nAdd a SQL job name and description.\nSet the sync schedule.\nChoose a time to start the SQL job and how often to run syncs.\nClick Create to create and save the SQL job.\n\nSQL queries are executed directly to your data warehouse, which could generate additional costs for pay-per-query warehouses.\n\nView SQL job history\n\nUse the Update History page to view all SQL jobs.\n\nNavigate to Unify > Profile explorer or Engage > Audiences > Profile explorer.\nClick Manage subscription statuses.\nSelect View update history, then select the SQL Jobs tab.\n\nFrom the Update History page, you can view details for each SQL job including the creation date and time, compute status, and the number of users updated across all runs for a job. Click the job link to visit a particular SQL job Overview page.\n\nQuery requirements\n\nWhen you build your SQL query, keep the following requirements in mind for the data your query returns.\n\nYour query must:\n\nReturn at least one column with user_id, anonymous_id, email, phone (or group_id for account traits if Unify for B2B is enabled).\nReturn records less than 16KB in size.\n\nYour query must not:\n\nInclude values for both user_id and anonymous_id for a given record.\nReturn any user_ids, anonymous_ids, or group_ids with a null value.\nReturn any records with duplicate user_ids.\nReturn duplicate email or phone records that have different subscription statuses.\nReturn more than 25 million rows.\n\nThis page was last modified: 15 Jul 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nGetting started\nSync subscription data with SQL\nQuery requirements\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nPrivacy\n/\nConsent Management\n/\nConsent in Reverse ETL\nConsent in Reverse ETL\nFREE X\nTEAM X\nBUSINESS \u2713\nADDON X\n?\n\nWith Consent Management in Reverse ETL, you can enforce your end-users\u2019 consent preferences that are captured by your consent management platform (CMP) and stored in your warehouse.\n\nTo enforce consent stored in your warehouse, build a Reverse ETL model that identifies consent categories. You can create a \u201cconsent to\u201d column mapping in a new data model or update an existing data model to include a \u201cconsent to\u201d mapping.\n\nConsent in Reverse ETL supports Reverse ETL-supported Actions destinations and Segment Connections\n\nAt this time, Consent in Reverse ETL does not support adding consent to Segment Profiles using the Segment Profiles destination. To enforce consent data in your classic Segment destinations, use the Segment Connections destination.\n\nPrerequisites\n\nConsent management edit and update capabilities limited to Workspace Owners\n\nOnly users with the Workspace Owner role are able to create, edit, and disable consent categories. All other users have read-only access to Consent Management features.\n\nBefore you can enforce consent stored in your warehouse, take the following steps:\n\nSet up your third-party consent management tool and create consent categories. Take note of your consent categories and the key or ID associated with each category.\nKnow how your company uses each destination. You need to know which destinations to map to each category.\nStore your end user consent in a warehouse that Segment supports for Reverse ETL. Segment supports Reverse ETL capabilities in Azure, BigQuery, Databricks, Postgres, Snowflake, and Redshift data warehouses. Other data warehouses are not supported.\nStep 1: Create consent categories in the Segment app\n\nLimited availability of destinations\n\nReverse ETL supports the Actions destinations in the Reverse ETL catalog and Segment Connections.\n\nFrom the Segment homepage, select the Privacy tab and click Consent Management.\nOn the Consent management page, click Create categories.\nConfirm that you have completed the required prerequisites, and click Next.\nOn the Create consent categories page, add the following information to the category form:\nCategory name: Enter a name that describes your use case for the data sent to this destination. This field only accepts category names that are 20 characters or less.\nCategory ID: In OneTrust, this is a string of up to five alphanumeric characters, but other CMPs may have a different format. This field is case sensitive.\nMapped destinations: Select one or more of your Reverse ETL destinations to map to this category. Category mappings apply to all instances of a destination.\nAfter you\u2019ve finished setting up your category or categories, click Save.\n\nSegment recommends mapping all Reverse ETL destinations to a category\n\nSegment assumes all destinations without a mapping do not require user consent and will receive all events containing a consent object. If a destination is mapped to multiple categories, a user must consent to all categories for data to flow to the destination.\n\nTo edit or disable consent categories, view the Configure Consent Management documentation.\n\nStep 2: Add your Reverse ETL source\n\nIf you\u2019ve already added a Reverse ETL source to your workspace, you can proceed to Step 3: Identify consent columns.\n\nIf you haven\u2019t already configured a Reverse ETL source in your workspace, follow the instructions in the Reverse ETL: Add a source documentation to add your warehouse as a data source. When you\u2019ve configured your Reverse ETL source, proceed to Step 3: Identify consent columns.\n\nStep 3: Identify consent columns\n\nAfter you set up consent categories in the Segment app, you must identify the columns in your data warehouse that store end user consent by creating a model, or SQL query that defines the set of data you want to synchronize to your Reverse ETL destinations. When building your data model, Segment recommends that you represent consent as a boolean true or false value and map one consent category to one column.\n\nCreating a data model that does not include information about consent preferences results in no consent enforcement\n\nIf you create consent categories in your workspace but fail to identify columns that contain consent preferences in your data model, events flow to all destinations in your workspace regardless of end user consent preferences.\n\nIdentify consent when building your model\n\nTo identify consent when building your model:\n\nNavigate to Connections > Sources and select the Reverse ETL tab. Select your source and click Add Model.\nClick SQL Editor as your modeling method.\nCreate the SQL query that\u2019ll define your model. Your model is used to map data to your Reverse ETL destinations.\nChoose a column to use as the unique identifier for each record in the Unique Identifier column field. The Unique Identifier should be a column with unique values per record to ensure checkpointing works as expected. It can be a primary key. This column is used to detect new, updated, and deleted records.\nClick Preview to see a preview of the results of your SQL query. The data from the preview is extracted from the first 10 records of your warehouse.\nClick Next.\nEnter your Model Name.\nClick Create Model.\nSelect Add consent mapping.\nOn the Add consent mapping popup, identify the column in your model that holds the consent preferences for the consent category.\nSelect Add consent mapping to identify columns for all of your consent categories.\nWhen you\u2019re satisfied with your consent mappings, click Save.\nUpdate your Reverse ETL model to include consent\n\nTo update an existing Reverse ETL model to include consent enforcement:\n\nNavigate to Connections > Destinations and select the Reverse ETL tab.\nSelect the source and the model you want to edit.\nSelect the Query Builder tab to edit your query. When you\u2019re editing your query, include columns that store information about end user consent preferences. When you\u2019ve finished making changes, click Save Query.\nNavigate to Settings > Consent settings.\nSelect Add consent mapping.\nOn the Add consent mapping popup, identify the column in your model that holds the consent preferences for the consent category.\nSelect Add consent mapping to identify columns for all of your consent categories.\nWhen you\u2019re satisfied with your consent mappings, click Save.\n\nYou can select the Settings tab and click Consent settings to verify that the consent categories in your model match the consent categories you configured in your workspace.\n\nYou can store each consent category in its own column in your warehouse, or store your consent information in one single blob column. Segment requires your consent categories to be in their own column in your data model.\n\nThe following sample model maps consent categories from each column in your database:\n\nselect \n  USERID, \n  name, \n  email, \n  distinctid,\n  Ads, \n  Personalization, \n  Analytics, \n  \nfrom CONSENT_PREFERENCES;\n\n\nThe following sample model maps consent categories from one blob column in your database:\n\nselect \n  USERID, \n  name, \n  email, \n  distinctid, \n  CAST(CONSENT_OBJ:consent.cookie.Advertising as Boolean) as Ads,  \n  CAST(CONSENT_OBJ:consent.cookie.Personalization as Boolean) as Personalization, \n  CAST(CONSENT_OBJ:consent.cookie.Analytics as Boolean) as Analytics, \n   \nfrom CONSENT_PREFERENCES;\n\n\nFailing to identify consent columns in your warehouse might lead to unintentional data loss\n\nIf you have destinations mapped to consent categories in the Segment app but fail to identify a column in your warehouse that stores consent for a category, then consent preference for that category will be considered to be false and no data will flow to destinations mapped to the category.\n\nStep 4: Connect your downstream destinations\n\nAfter you set up categories in the Segment app and create a SQL model that extracts consent information, connect your downstream destinations to complete the consent enforcement process.\n\nConsent in Reverse ETL supports Reverse ETL-supported Actions destinations and Segment Connections\n\nAt this time, Consent in Reverse ETL does not support enforcing consent in the Segment Profiles destination. To enforce consent data in your classic Segment destinations, use the Segment Connections destination.\n\nTo add your first destination:\n\nNavigate to Connections > Destinations and select the Reverse ETL tab.\nClick Add Reverse ETL destination.\nSelect the destination you want to connect to and click Configure.\nSelect the Reverse ETL source you want to connect the destination to.\nEnter the Destination name and click Create Destination.\nEnter the required information on the Settings tab of the destination.\nNavigate to the destination settings tab and enable the destination. If the destination is disabled, then Segment won\u2019t be able to start a sync.\n\nSegment does not count Reverse ETL records filtered by Consent Management toward your Reverse ETL limits\n\nRecords filtered out by Consent Management are not counted as part of your Reverse ETL limits. For more information about Reverse ETL limits, see the Reverse ETL Limits documentation.\n\nValidate your consent mapping\n\nYou can validate that you successfully created your consent mapping in Segment Connections or supported Reverse ETL Actions destinations using the following methods.\n\nSegment Connections destination\n\nSegment automatically adds the consent object to every event that\u2019s routed downstream to your Segment Connections destination. Consent enforcement in Connections validates that only consenting data flows downstream to any classic Segment destinations connected to your Segment Connections instance.\n\nOpen the Source Debugger for your Reverse ETL source and confirm that the consent object appears on every event and that the consent object has the categories you mapped in Step 2: Identify consent columns.\n\nReverse ETL Actions destinations\n\nSegment automatically filters out data from users who have not consented to the category mapped to your destination.\n\nTo verify that this behavior is working as intended, open Delivery Overview for a RETL-supported Actions destination and view the events that were successfully delivered to the destination. The events in your destination should only come from users that consented to send data to the category that your supported Actions destination belongs to.\n\nThis page was last modified: 25 Jul 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nPrerequisites\nStep 1: Create consent categories in the Segment app\nStep 2: Add your Reverse ETL source\nStep 3: Identify consent columns\nStep 4: Connect your downstream destinations\nValidate your consent mapping\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nSegment for Data Users\nSegment for Data Users\n\nIf you aren\u2019t involved in setting up your Segment implementation, or are just starting to set up Destinations for your organization\u2019s workspace, this guide is for you.\n\nWhat is Segment?\n\nIf you read the detailed explanation of Segment on the previous page, you can skip ahead!\n\nSegment is a system for sending messages from your websites, mobile apps, and servers. These messages contain data about events on, or users of those systems, and these messages can sent on to other tools, and gathered together in a warehouse for later analysis. Segment can also bring in information about your users from external systems, such as helpdesks or CRM systems, and collate that information to help you analyze your data, build audiences of users, and personalize your users\u2019 experiences.\n\nOnce you (or your organizations\u2019 developers) have your Segment Sources set up and sending data, you can log in to the Segment App and set up Destinations, which are how Segment sends that data to other tools (like Google Analytics, Mixpanel, and many others).\n\nEnvironments and Labels\n\nDepending on your organization\u2019s configuration and access settings, you might be able to see one or multiple Environments (for example, \u201cProduction\u201d, \u201cTesting\u201d, \u201cDevelopment\u201d), or one or multiple Labels, which control access to different parts of your organization\u2019s Segment system. If you see several environments, contact your Segment administrator for more details so you can make sure you make your changes in the right place.\n\nData inside Segment\n\nData enters the Segment systems from Sources, but once data is in the system, your organization may have different tools configured to control and change it. This could change what data is available to you, or any destinations you set up.\n\nFor example, Protocols makes sure that data coming into Segment follows specific formats and patterns, and might block and discard malformed or unwanted data. The Privacy tool can be configured to remove Personally Identifiable Information (PII) from the data. And several different methods are available to filter data so that it doesn\u2019t send certain types of events, or reach specific destinations or warehouses.\n\nSet up a Destination\n\nDepending on the access level you have in your organization\u2019s Segment workspace, you might be able to create new Destinations, or you might only be able to edit existing ones.\n\nTo add a new Destination, you\u2019ll usually need some information (such as a token or API key) from the destination tool to start. You\u2019ll enter that into the Segment App so we can connect to and send data to that tool. You\u2019ll also need to know which Source you\u2019ll be sending data from.\n\nTo set up a destination:\n\nLog in to the Segment App, and click Add Destination to go to the catalog of available destinations.\nSearch for and select the destination you want to set up.\nOn the description page that appears, click Configure.\nOn the next page, select the source that you want the destination to get data from. You can only select one source at at time. The list displays only the sources that are compatible with the destination you chose. If you don\u2019t see a source that you expect, contact your administrator.\nClick Confirm Source.\nOn the next page, configure your destination by entering the API key, token, and any other information. The configuration page shows both required information, and any extra settings.\n\nTip: Segment usually is able to translate data into a format that the destination expects, however some destinations (such as Adobe Analytics) may require manual mapping steps to configure properly. If you see additional fields for mapping configuration, read the documentation for that destination to learn more.\n\nTroubleshooting\n\nIf you\u2019re setting up a destination to use cloud-mode data (data that\u2019s sent through Segment, rather than directly from a user\u2019s device), you can use the Event Tester and Event Delivery tools to check that data is arriving, and being correctly delivered to the destination.\n\nHave suggestions for things to add to this guide? Drop us a line!\n\nThis page was last modified: 14 Jul 2021\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nWhat is Segment?\nEnvironments and Labels\nData inside Segment\nSet up a Destination\nTroubleshooting\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nSegment App\n/\nIdentity & Access Management Overview\nIdentity & Access Management Overview\nFREE X\nTEAM X\nBUSINESS \u2713\nADD-ON X\n?\n\nSegment\u2019s access management tools let Workspace Owners manage which users can access different parts of their Segment workspaces. The Access Management page has three tabs: Users (team members), User Groups, and Tokens.\n\nAccess settings are applied at the workspace level. A Segment user can have access to one or more workspaces and can have different roles in each workspace. Users access their Segment account with either email/password credentials, their Twilio credentials, or by using Single Sign On.\n\nExporting a workspace\u2019s user list\n\nWorkspace Owners can download a CSV list of users who have access to a specific workspace (including their roles) from the Access Management page in the Segment app.\n\nYou can select a user in the table to see their roles. Check out the Roles documentation for more details.\n\nTwilio Unified Login\n\nWith Twilio Unified Login, Twilio users can use their Twilio email, password, and authentication settings to access several Twilio products, including Twilio Messaging, SendGrid, and Segment. You can also use Sign up With Google to create your Twilio account. Once you link your Segment account to your Twilio credentials, you can access Segment directly from the Twilio console using the Twilio Product Switcher.\n\nTwilio Sign Up\n\nSegment invitations and sign ups that are redirected to Twilio\u2019s sign up page must adhere to Twilio\u2019s minimum password and 2FA requirements. To learn more, view Twilio\u2019s Account Management documentation.\n\nAny existing Segment user must adhere to existing password requirements and 2FA settings set at the Workspace level.\n\nTwilio Product Switcher\n\nYou can access Segment from the Twilio Console using the Product Switcher. For more information, view the Twilio support article Getting Started with the Unified Login and Product Switcher.\n\nUser settings\n\nTwilio Unified Login users can manage their Segment user settings, including name, email, password, and 2FA settings, directly in their Twilio account. To learn more about Twilio\u2019s user and password policies, review Twilio\u2019s Account Management documentation.\n\nSegment Users and SSO/SCIM\n\nExisting Segment users can still use their credentials to access Segment.\n\nSegment continues to support SSO and SCIM, as users who need to access an SSO enabled workspace will be directed to authenticate through the configured Identity Provider.\n\nQuick links\nInvite a team member to your workspace\nCreate a User Group\nUpdate a team member\u2019s access\nRemove a team member from a workspace\nAdd a new user with Single Sign On\nInvite and manage workspace members\n\nLearn how to add members to your workspace, and manage their permissions.\n\nOrganize Users with User Groups\n\nLearn manage workspace members in bulk.\n\nThis page was last modified: 22 Feb 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nExporting a workspace\u2019s user list\nTwilio Unified Login\nQuick links\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nStorage\n/\nWarehouses\n/\nWarehouse Errors\nWarehouse Errors\nERROR: Schema \u201cXXX\u201d does not exist. (SQLSTATE 3F000)\n\nThis is a permissioning issue. To learn how to set up proper permissions, you can check out our\u00a0Postgres\u00a0and\u00a0Redshift guides.\n\nERROR: Cannot execute query because system is in resize mode (SQLSTATE 57014)\n\nThis error occurs when your cluster is currently resizing. The warehouse will continue on its scheduled run interval - once the resize is complete, we\u2019ll load all data from the failed run.\n\nERROR: 1040 (SQLSTATE XX000)\n\nThis is a Redshift 500 - an internal server error. This is usually caused by having too many tables or too many columns. If you\u2019re seeing this error,\u00a0contact the Segment Support team.\n\nread tcp XXX.XX.XX.XXXX:XXXX-XXX.XX.XX.XXXX:XXXX: read: connection timed out\n\nThis is a networking error that typically arises when Redshift doesn\u2019t close properly close the connection or gets rebooted.\n\nIf you see this error on consecutive syncs,\u00a0contact us.\n\npq: role \u201cXXX\u201d is not permitted to log in\n\nThis is a permissioning issue. To learn how to set up proper permissions, you can check out our\u00a0postgres\u00a0and\u00a0redshift guides.\n\npq: password authentication failed for user \u201cXXX\u201d;\n\nThis is a credential issue. To fix your credentials, head over to Warehouse > Settings > Connection.\n\ndial tcp: lookup XXX-hostname on 10.50.0.2:53: no such host\n\nThis is a credential issue. To fix your credentials, head over to Warehouse > Settings > Connection.\n\ndial tcp XX.XXX.XXX.XXX:XXXX: getsockopt: connection timed out / refused\n\nThis is a networking error. The connection times out because Redshift doesn\u2019t close properly or gets rebooted.\n\nIf you see this error on consecutive syncs, contact us.\n\nERROR: syntax error at or near \u201cENCODE\u201d; (SQLSTATE 42601)\n\nThis occurs when a Postgres database is incorrectly connected as Redshift. To resolve this, delete the warehouse and reconnect, using the Postgres set up option.\n\nError during deduping step for collectionXXX: EOF\n\nThis error is generally a network error when Redshift closes the connection. If the problem persists on multiple runs, contact us.\n\nERROR: permission denied for relation update (SQLSTATE 42501)\n\nThis is a permissioning issue. To learn how to set up proper permissions, you can check out our\u00a0postgres\u00a0and\u00a0redshift guides.\n\nEOF\n\nThis error is generally a network error when Redshift closes the connection. If the problem persists on multiple runs,\u00a0contact us.\n\nERROR: failed to create table: 002318 (42601): SQL compilation error: invalid column definition name \u201cXXX\u201d (ANSI reserved)\n\nThis error indicates that a column that is attempting to sync has the same title as a reserved keyword in Snowflake. More information regarding Snowflake\u2019s reserved keywords can be found in Snowflake\u2019s docs.\n\nThis page was last modified: 18 Oct 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nPrivacy\n/\nConsent Management\n/\nConsent in Segment Connections\nConsent in Segment Connections\nFREE X\nTEAM X\nBUSINESS \u2713\nADDON X\n?\n\nSegment Connections users can add the consent object to their sources to stamp events with the end user consent preferences captured by your consent management platform (CMP) and send them downstream to destinations in categories that an end user consented to share data with. Events without the consent object continue to flow to destinations without consent enforcement.\n\nWith the Destination Actions framework, you can send current end user consent preferences to flow to your destination alongside customer interactions so your destinations know when an end user revokes their consent.\n\nFor more information about sharing current end user consent preferences with your downstream destinations, see the Sharing consent with Actions destinations documentation.\n\nFor more information about configuring consent categories, see the Configure Consent Management documentation.\n\nIf your sources also contain the integrations object, Segment will look at the consent object first, and then take into account the integrations object according to the table in the Reconcile consent object and integrations object documentation.\n\nUnify users must send an additional event to add consent preferences to Profiles\n\nIf you use Unify, see the Consent in Unify documentation for more information about the Segment Consent Preference Updated event, which Segment uses with the consent object to add consent preference to Profiles.\n\nConsent object\n\nSegment requires every event from all of your sources to include the end user consent preferences, captured by your CMP or your application logic, in the form of the consent object. The consent object is a JSON object nestled inside of the context object with the following format:\n\nThe JSON keys in the consent object should represent the categoryId for each consent category.\n\n{\n\"context\": {\n  \"consent\": {\n    \"categoryPreferences\": {\n        \"Advertising\": true,\n        \"Analytics\": false,\n        \"Functional\": true,\n        \"DataSharing\": false\n      }\n    }\n  }\n}\n\n\n\nEvents without the consent object will continue to flow to destinations without consent enforcement.\n\nReconcile consent conflicts\n\nSegment resolves conflicts between your consent object and your integration object and between your CMP and the consent categories you configured in the Segment app.\n\nReconcile consent object and integrations object conflicts\n\nYou can add both the integrations object and the consent object to your Segment payloads for greater control over how Segment routes data to your downstream destinations.\n\nFor more information about the Integrations object, please see Filtering your Segment Data.\n\nIf an event includes both an integrations and consent object, Segment will look at the consent object first, and then take into account the integrations object according to the following table:\n\nCONSENT OBJECT\tINTEGRATION OBJECT\tRESULT\nNot provided or empty consent object\n\n\"context\": {\n}\nOR\n\"context\": {\n     \"consent\": {\n     }\n}\tNot provided or empty object\tData flows to all destinations.\nEmpty categoryPreferences object\n\n\"context\": {\n     \"consent\": {\n           \"categoryPreferences\": {\n           }\n     }\n}\tNot provided or empty object\tData does NOT flow to any mapped destinations - consent is considered to be false for all categories.\n\nData flows to all destinations NOT mapped to a consent category.\nNot provided\n\n\"context\": {\n}\t{facebook: true,\namplitude: false}\tData flows to the destinations that are true in the integrations object (Facebook). Any metadata provided in the integrations object also flows to your downstream destinations.\nEmpty consent object\n\n\"context\": {\n     \"consent\": {\n     }\n}\nOR\n\"context\": {\n     \"consent\": {\n           \"categoryPreferences\": {\n           }\n     }\n}\t{facebook: true,\namplitude: false}\tData does NOT flow to any mapped destinations - consent is considered to be false for all categories.\n\nData flows to all destinations NOT mapped to a consent category, destinations set to true in the integrations object, and destinations not included in the integrations object.\n{ad: true,\nanalytics: false}\n\nSegment has no category-to-destination mapping for ad and analytics\tProvided, not provided, or empty object\tData flows to all destinations, as all destinations are unmapped. If the integrations object is present, data flow may be impacted.\n{ad: true,\nanalytics: false}\n\nad = facebook, google-ads\n\tNot provided or empty object\tData flows to destinations that map to a consented purpose. In this case, data flows to all ad destinations (Facebook and Google Ads).\n\nNo data flows to analytics destinations.\n{ad: true,\nanalytics: false}\n\nad = facebook, google-ads\nanalytics = amplitude\t{facebook: true,\namplitude: false}\tData flows to all ad destinations, even though Google Ads is not present in the integrations object.\n\nData does NOT flow to analytics destinations.\n{ad: true,\nanalytics: false}\n\nad = facebook, google-ads\nanalytics = amplitude\t{facebook: false,\namplitude: false}\tData only flows to Google Ads and not to Facebook, which is false in the integrations object.\n\nData does NOT flow to analytics destinations.\n{ad: true,\nanalytics: false}\n\nad = facebook, google-ads\nanalytics = facebook, amplitude\t{facebook: true,\namplitude: false}\tWhen destinations are mapped to multiple categories, data only flows if consent is true for all categories. In this case, data only flows to Google Ads and not to Facebook.\n\nData does NOT flow to analytics destinations.\n{ad: true,\nanalytics: true}\n\nad = facebook, google-ads\nanalytics = facebook, amplitude\t{facebook: true,\namplitude: false}\tWhen destinations are mapped to multiple categories, data only flows if consent is true for all categories. In this case, data flows to Google Ads and Facebook. No data flows to Amplitude because it is false in the integrations object.\n{ad: false,\nanalytics: true}\n\nad = facebook, google-ads\nanalytics = facebook, amplitude\t{facebook: true,\namplitude: false}\tWhen destinations are mapped to multiple categories, data only flows if consent is true for all categories.\n\nIn this example, data does NOT flow to any destination because of the interaction between the integrations and consent objects.\nReconcile CMP and Segment consent category conflicts\n\nIf you have a category configured in your consent management tool (for example, advertising) and there is no category with the same ID in Segment, the data will flow to unmapped destinations. If destinations are mapped to a different category in the Segment app, data flow will honor end user consent for that category.\n\nIf there is a category configured in Segment (functional) that is not mapped in your CMP, data will not flow to destinations mapped to the functional category.\n\nConsent observability\n\nEvents discarded due to consent preferences appear in Delivery Overview at the \u201cFiltered at destination\u201d step with the discard reason Filtered by end user consent.\n\nThis page was last modified: 02 Aug 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nConsent object\nReconcile consent conflicts\nConsent observability\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nAudiences\n/\nLinked Audiences Limits\nLinked Audiences Limits\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nLinked Audiences is an add-on to Twilio Engage. To use Linked Audiences, you must have access to Engage.\n\nTo provide consistent performance and reliability at scale, Segment enforces default use limits for Linked Audiences.\n\nUsage limits\n\nThe Linked Audiences module provides you the flexibility to create and publish unlimited Linked Audiences within each billing cycle. This means you won\u2019t encounter any limitations or pauses in service related to the number of Linked Audiences you generate.\n\nLinked Audience limits are measured based on Activation Events, which is the number of times profiles are processed to each destination, including audience entered, audience exited, and entity change events. This includes both successful and failed attempts. For example, if you processed an audience of 50k to Braze and Google Ads Conversions, then your total Activation Event usage is 100k records.\n\nYour plan includes a high limit of Activation Events, which ensures that the vast majority of users can use Linked Audiences freely without needing to worry about the limit.\n\nTo see how many Activation Events you\u2019ve processed using Linked Audiences, navigate to Settings > Usage & billing and select the Linked Audiences tab. If your limit is reached before the end of your billing period, your syncs won\u2019t automatically pause to avoid disruptions to your business. You may be billed for overages in cases of significant excess usage. If you consistently require a higher limit, contact your sales representative to upgrade your plan with a custom limit.\n\nPLAN\tLINKED AUDIENCES LIMIT\tHOW TO INCREASE YOUR LIMIT\nFree\tNot available for purchase\tN/A\nTeam\tNot available for purchase\tN/A\nBusiness\t40 x the number of MTUs or 0.4 x the number of monthly API calls\tContact your sales rep to upgrade your plan\n\nIf you have a non-standard or high volume usage plan, you have unique Linked Audiences limits or custom pricing.\n\nProduct limits\nNAME\tLIMIT\tDETAILS\nRETL row limit\t150 million\tThe audience compute fails if the total output exceeds the limit.\nRETL column limit\t500 columns\tThe audience compute fails if the number of columns exceeds the limit.\nGlobal concurrent audience runs\t5 total within any given space\tNew audience runs are queued once the limit is reached and will start execution once prior audience runs complete. If you need a higher global concurrent audience runs limit, contact friends@segment.com.\nEvent Size\t32 KB\tSegment doesn\u2019t emit messages for profiles whose total related entities and enrichments exceed the limit.\nData Graph depth\t6\tYou can\u2019t save a Data Graph if you exceed the limit.\nPreview size\t3K rows\tThe maximum number of rows you can have to generate a preview. The preview fails if you bring back too many entities.\nEntity value type ahead cache\tUp to 100 unique values\tThe maximum number of entity values Segment stores in cache.\nEntity columns\tUp to 1000 unique values\tThe maximum number of entity property columns Segment surfaces in the condition builder.\nRun frequency\t15 minutes (this is the fastest time)\tYou can\u2019t configure more frequency syncs. You can select Run Now to trigger runs, but you\u2019re limited by Profiles Sync for when new data syncs back to the data warehouse.\nDestination Mappings\tUp to 100 mappings\tYou can set up to 100 action destination mappings per destination instance.\n\nThis page was last modified: 19 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nUsage limits\nProduct limits\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSources\n/\nCatalog\n/\nLibraries\n/\nWebsite\n/\nJavascript\n/\nSelf-Managed Custom Proxy\nSelf-Managed Custom Proxy\n\nCustom proxies allow you to proxy Analytics.js and all tracking event requests through your own domain.\n\nYou cannot use custom proxy setup for Analytics.js CDN or Tracking API with device-mode destinations because it requires that the destination\u2019s native scripts are loaded onto the client, and the requests are sent directly to the destination.\n\nBusiness Tier customers can also use Custom Domain\n\nCustom Domain is a fully-managed service that enables you to configure a first-party subdomain over HTTPS to track event requests through your domain instead of tracking events through Segment\u2019s default domain. For more information, see the Custom Domain documentation.\n\nSegment\u2019s domain delegation solutions\n\nSegment offers two domain delegation solutions: Custom Proxy and Custom Domain. If you use Custom Domain, you can choose to use either DNS delegation or a Cannonical Name (CNAME). Segment recommends using Custom Domain with DNS delegation, which leads to easy setup, maintenance, and monitoring.\n\nSERVICE\tHOW IT WORKS\tINFRASTRUCTURE MANAGEMENT\tAVAILABILITY\nCustom Domain with DNS Delegation\tA Segment service that allows your website to use your own subdomain to load Analytics.js securely over HTTPS and send event data. It is not limited to Analytics.js and is also compatible with server libraries. It uses a DNS subdomain that you delegate to Segment.*\tSegment manages all related infrastructure, including applying security updates, managing the SSL certificate lifecycle, and monitoring.\tBusiness Tier\n\nRecommended for reliable data collection.\nCustom Domain with CNAME\tThis approach uses a Canonical Name (CNAME) to map an alias name on your domain name to Analytics.js. It is not limited to Analytics.js and is also compatible with server libraries.\tCustomers are responsible for maintaining CNAME.\tBusiness Tier\n\nNot recommended due to evolving and persistent browser privacy measures.\nCustom Proxy\tThis approach uses a proxy or wrapper where all data is first collected by a proxy on your domain and then forwarded to Segment.\tCustomers are responsible for maintaining their own proxy infrastructure.\tAvailable to all Segment users.\n\nNot recommended because it adds a point of failure, but remains an option if Custom Domain with sub-domain delegation is unavailable to you.\n\n*If it\u2019s not possible for you to delegate subdomains to Segment, you can use a CNAME instead. Segment encourages users to delegate a DNS subdomain rather than use use CNAME aliasing due to the evolving privacy standards in browsers, but CNAME aliasing remains an option for users not interested in using nameservers.\n\nCustom Proxy prerequisites\n\nTo set up a custom proxy, you need:\n\nAccess to your site DNS settings\nA CDN you can serve assets from\nAccess to the CDN settings\nA security certificate for the proxy domain\n\nCustom Proxy Troubleshooting\n\nIf you experience issues configuring a custom proxy, contact your organization\u2019s IT department for help. Segment does not have access to the resources you need to configure a custom proxy.\n\nThis guide explains how to set up a custom proxy in CloudFront. You can apply these principles to almost any modern CDN that supports proxies.\n\nYou need to set up two important parts, regardless of the CDN provider you use:\n\nProxy to Segment CDN (cdn.segment.com)\nProxy to Segment tracking API (api.segment.io)\n\nIf you are using a Regional Workspace, please note that instead of using api.segment.io to proxy the Tracking API, you\u2019ll be using events.eu1.segmentapis.com\n\nSegment only has the ability to enable the proxy setting for the Web (Analytics.js) source. Details for mobile source proxies are in the Analytics-iOS and Analytics-Android documentation. It is not currently possible to set up a proxy for server sources using the Segment UI.\n\nSegment loads most integrations through the proxy, except for third-party SDKs\n\nThird-party SDKs are loaded by a partner\u2019s CDN, even with a Segment proxy configured. For example, if you have a Segment custom proxy enabled and send data to a FullStory destination, FullStory\u2019s CDN would load the FullStory SDK.\n\nCustom Proxy setup\n\nThere are two options you can choose from when you set up your custom domain proxy.\n\nCloudFront\nCustom CDN or API proxy\n\nFollow the directions listed for CloudFront or use your own CDN setup. Once you complete those steps and verify that your proxy works for both cdn.segment.com and api.segment.io, contact Segment Product Support with the following template email:\n\nHi,\n\nThis is {person} from {company}. I would like to configure a proxy for the following source(s):\n\n**Source URL**: link to the source in your Segment workspace (for example: https://app.segment.com/<your_slug>/sources/<source>/overview)\n**Source ID**: navigate to **API Keys** on the left-hand side of the source **Settings** and provide the Source ID \n\n\nDouble-check the Source URL and the Source ID.\n\nA Segment Customer Success team member will respond that they have enabled this option for your account. When you receive this confirmation, open the source in your workspace, and navigate to Settings > Analytics.js. Update the Host Address setting from api.segment.io/v1 to [your proxy host]/v1.\n\nThe Host Address field does not appear in source settings until it\u2019s enabled by Segment Customer Success.\n\nThere should be no downtime once the setup is complete, as the default Segment domains continue to work alongside the customer\u2019s domains.\n\nCustom CDN / API Proxy\n\nFollow these instructions after setting up a proxy such as CloudFront. Choose between the snippet instructions or the npm instructions.\n\nIf you\u2019ve followed the instructions above to have a Segment team member enable the apiHost settings in the UI, you can skip the instructions in this section.\n\nSnippet instructions\n\nIf you\u2019re a snippet user, modify the analytics snippet located inside the <head> of your website:\n\nTo proxy CDN settings and destination requests that typically go to https://cdn.segment.com, replace:\n\n- t.src=\"https://cdn.segment.com/analytics.js/v1/\" + key + \"/analytics.min.js\"\n+ t.src=\"https://MY-CUSTOM-CDN-PROXY.com/analytics.js/v1/\" + key + \"/analytics.min.js\"\n\n\nTo proxy API tracking calls that typically go to api.segment.io/v1, replace:\n\n- analytics.load(\"<MY_WRITE_KEY>\")\n+ analytics.load(\"<MY_WRITE_KEY>\", { integrations: { \"Segment.io\": { apiHost: \"MY-CUSTOM-API-PROXY.com/v1\" }}})\n\nnpm instructions\n\nIf you\u2019re using the npm library, make the following changes directly in your code:\n\nTo proxy settings and destination requests that typically go to https://cdn.segment.com through a custom proxy:\n\nconst analytics = AnalyticsBrowser.load({\n  writeKey,\n  // GET https://MY-CUSTOM-CDN-PROXY.com/v1/projects/<writekey>/settings --> proxies to\n  // https://cdn.segment.com/v1/projects/<writekey>/settings\n\n  // GET https://MY-CUSTOM-CDN-PROXY.com/next-integrations/actions/...js  --> proxies to\n  // https://cdn.segment.com/next-integrations/actions/...js\n  cdnURL: 'https://MY-CUSTOM-CDN-PROXY.com'\n })\n\n\nTo proxy tracking calls that typically go to api.segment.io/v1, configure the integrations['Segment.io'].apiHost:\n\nconst analytics = AnalyticsBrowser.load(\n    {\n      writeKey,\n      cdnURL: 'https://MY-CUSTOM-CDN-PROXY.com'\n    },\n    {\n      integrations: {\n        'Segment.io': {\n          // POST https://MY-CUSTOM-API-PROXY.com/v1/t --> proxies to\n          // https://api.segment.io/v1/t\n          apiHost: 'MY-CUSTOM-API-PROXY.com/v1',\n          protocol: 'https' // optional\n        }\n      }\n    }\n  )\n\nCustom Proxy CloudFront\n\nThese instructions refer to Amazon CloudFront, but apply more generally to other providers as well.\n\nCDN Proxy\n\nTo set up your CDN Proxy:\n\nLog in to the AWS console and navigate to CloudFront.\nClick Create Distribution.\nConfigure the distribution settings. In the Origin section, update the following values:\nOrigin Domain Name: cdn.segment.com\nProtocol: HTTPS only\nIn the Default cache behavior section, configure the following values:\nViewer protocol policy: Redirect HTTP to HTTPS\nAllowed HTTP Methods: GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE\nIn the Settings section, configure the following values:\nAlternate domain name (CNAME): analytics.<yourdomain>.com\nCustom SSL certificate: Select an existing or new certificate to validate the authorization to use the Alternate domain name (CNAME) value. For more information, see Amazon\u2019s documentation Requirements for using alternate domain names.\nClick Create Distribution.\n\nTake note of the Domain Name for use in the next step.\n\nAdd CNAME Record to DNS\n\nTo add a CNAME record for the Segment proxy to your organizations DNS settings:\n\nUse a name that makes it clear what you are using the subdomain for, for example analytics.mysite.com.\nGo to your domain registrar and add a new record to your DNS of type \u201cCNAME\u201d.\nConfigure these values:\nName: <subdomain_name>.yourdomain.com\nValue: The Domain Name value from CloudFront\nSave your record. This might take some time to take effect, depending on your TTL settings.\nMake a curl request to your domain to verify that the proxy works.\nTracking API Proxy\n\nAs events travel through the proxy before reaching the tracking API, set up a proxy for the tracking API so that all calls proxy through your domain. To do this, set up a CloudFront distribution that\u2019s similar to the one in the previous section, with the exception of the Origin Domain Name:\n\nFIELD\tVALUE\tDESCRIPTION\nOrigin Domain Name\tapi.segment.io\tThe domain name to which the proxy is served\nAdd CNAME Record to DNS\n\nTo add a CNAME record to your DNS settings:\n\nGo to your domain registrar and add a new record to your DNS of type \u201cCNAME\u201d. This time use the CloudFront distribution for the tracking API proxy.\nEnter values for these fields:\nName: <subdomain_name>.yourdomain.com\nValue: Tracking API CloudFront Distribution Domain Name\nSave your record. This might take some time to take effect, depending on your TTL settings.\nRun curl on your domain to check if the proxy is working correctly.\nCommon issues\n\nThese are some common issues that occur for customers implementing a custom proxy. This is not an exhaustive list, and these CloudFront or Cloudflare settings may change.\n\nCloudflare returning a 403 error\n\nA 403 error can mean that you\u2019ve misconfigured your Cloudflare CDN distribution. Try one of the following options to fix the error:\n\nIf you have a Cloudflare enterprise plan, create a Page Rule in Cloudflare so that Segment\u2019s CDN doesn\u2019t refuse the requests made through the Cloudflare Proxy. If cdn.segment.com is another CNAME that resolves to xxx.cloudfront.net, you will need to use a Page Rule in Cloudflare to override the host header to match the hostname for proxy requests. For more information about overriding the host header, see Cloudflare\u2019s Rewrite Host headers docs.\n\nFor customers who are not on the Cloudflare Enterprise plan, use Cloudflare Workers. Workers usually run on the main domain (for example, www.domain.com), but if you want Workers to run on a subdomain, like http://segment.domain.com, you must record the subdomain in your DNS. For more information, see Cloudflare\u2019s Routes and domains documentation.\n\nWhen creating a Worker you can use this example provided by Cloudflare in their Bulk origin override documentation with the origins set to:\n\nconst ORIGINS = {\n\"yourcdndomain.com\": \"cdn.segment.com\",\n}\n\nCloudflare CORS issue\n\nIn order to resolve a CORS OPTIONS pre-request fetch error, you must specify \u201cStrict (SSL-Only Origin Pull)\u201d as a Cloudflare Page rule for the api.segment.io proxy. Please see Cloudflare\u2019s Encryption modes documentation for more details.\n\nCloudFront Proxy returning a 403 error\n\nIf your CloudFront Proxy is returing a 403 error, the following change in CloudFront might resolve the issue:\n\nBefore:\nCache Based on Selected Request Headers: All\n\nAfter:\nCache Based on Selected Request Headers: None\n\n\nAlternatively, this setting may solve your issue:\n\nBefore:\nOrigin request policy: AllViewer\n\nAfter:\nOrigin request policy: None\n\nCloudFront CORS issue\n\nTo resolve a CORS issue, you might need to add a referrer header in the request you send to Segment. Follow AWS\u2019s How do I resolve the \u201cNo \u2018Access-Control-Allow-Origin\u2019 header is present on the requested resource\u201d error from CloudFront? guide, which explains how to add a referrer header.\n\nSelf-hosting Analytics.js\n\nTo reduce fetching assets from Segment\u2019s CDN, you can bundle Analytics.js with your own code.\n\nTo bundle Analytics.js with your own code, you can:\n\nUse Analytics.js as an npm package.\n\nUse npm to install your destinations.\n\nHardcode your settings instead of fetching from the CDN (Segment doesn\u2019t recommend this as it completely bypasses the Segment source GUI).\n\n// npm-only\nexport const analytics = new AnalyticsBrowser()\nanalytics.load({\n ...\n cdnSettings: {...} // object from https://cdn.segment.com/v1/projects/<YOUR_WRITE_KEY>/settings'\n })\n\nRestore the API host to the Segment default\n\nIf you wish to restore the proxied API host to it\u2019s original value:\n\nNavigate to the Source > Settings > Analytis.js tab\nScroll down until you see the Host address field.\nUnder the field, there is a small blue text that says \u2018Restore to a default value\u2019. Click Restore and then Save.\n\nAny changes made to the CDN host must be update manually in your code.\n\nThis page was last modified: 02 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSegment\u2019s domain delegation solutions\nCustom Proxy prerequisites\nCustom Proxy setup\nCustom CDN / API Proxy\nCustom Proxy CloudFront\nCommon issues\nSelf-hosting Analytics.js\nRestore the API host to the Segment default\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nSegment App\n/\nIam\n/\nUsing Label-Based Access Control\nUsing Label-Based Access Control\nFREE X\nTEAM X\nBUSINESS \u2713\nADD-ON X\n?\n\nLabels allow workspace owners to assign permissions to users to grant them access to groups. Groups represent collections of Sources, or collections of Spaces.\n\nTo create or configure labels, go to the Labels tab in your workspace settings. Only workspace Owners can manage labels for the entire workspace.\n\nAll workspaces include labels for Dev (development) and Prod (production) environments. Business Tier customers can create an unlimited number of labels.\n\nCustom Environments\n\nBy default, all workspaces include labels for Dev (development) and Prod (production) environments. Workspace owners can configure what these labels are applied to, and can create up to five custom environments.\n\nLabels must be in key:value format, both the key and value must begin with a letter, and they can only contain letters, numbers, hyphens or dashes.\n\nTo apply labels to Sources and Spaces, click the Assign Labels tab from the Labels screen. In the screen that appears, select the Sources and Spaces to apply the label to.\n\nOnce a label is in use (either assigned to a resource or used to restrict permissions on a user), the label cannot be deleted. You must first manually remove the label from any resources and permissions before you can delete it.\n\nWhile only Workspace Owners can bulk-edit labels, Source and Space admins can edit the labels on the sources and spaces they have access to. To do this, go to the Settings tab for each item.\n\nWorkspace owners can also grant specific Roles access to specific labels. For example, you might give a Source Admin access to only Sources that have the Prod label.\n\nPermissions can then be assigned to users in Access Management by label, on the Source Admin, Source Read-Only, Engage Admin, Engage User and Engage Read-Only users.\n\nCustom Labels\n\nNote: All Segment workspaces can create up to five custom labels. Additional label types (in addition to environment labels) are available to Segment Business Tier accounts.\n\nTo create additional custom labels, a workspace owner can create new key types in the Labels screen. The workspace owner can customize any combination of labels to mirror how resources should be partitioned in their organization. For example, some organizations may prefer to restrict access on their Sources and Spaces by brand or product area while other organizations may find it more useful to restrict their resources by tech stack or engineering department.\n\nWhen you create a new key, it becomes available in the Sources page as a column type that can be used to organize sources.\n\nLabels FAQ\nWhere can I create labels?\n\nWorkspace owners can create labels for sources and Spaces from the Segment workspace Settings -> Admin -> Labels.\n\nWhat resources can I assign a label to?\n\nLabels currently only apply to Sources and Spaces.\n\nWhere can I assign labels?\n\nWorkspace owners can assign bulk assign labels to sources and Spaces using the \u201cAssign Labels\u201d tab in the Labels screen. Source admins and Space admins can edit the labels on their individual resources in the \u201cSettings\u201d tab.\n\nWhere can labels be used?\n\nOnce a label has been created and has been assigned to resources within the workspace, workspace owners can use these labels to restrict permissions on user access, restrict which sources can be connected to a space through a Connection Policy, and organize sources by viewing these labels as columns in the Sources page.\n\nCan I delete a label?\n\nWorkspace owners can only delete a label if it is not being used (either assigned to a resource or used to restrict permissions on a user). First, manually remove the label from any resources or user permissions.\n\nCan I rename a label?\n\nNo, a label cannot be renamed. If you need to rename a label, we recommend you create the new label, and then assign it to all resources named the old label before deleting the old label.\n\nCan I assign a resource multiple values from the same category?\n\n(for example, a source as both brand:A and brand:B))\n\nNo, you can only assign one value per category. This is to ensure there is no confusion in logic around permissions. For example, if a user is assigned permission to brand:A, it would be unclear to the workspace owner if this user gets access to a source labeled both brand:A and brand:B or only sources with the sole label brand:A.\n\nHow does assigning a user permissions based on labels work?\n\nLabels are additive, so you can only further restrict a user\u2019s permissions by adding more labels. If a user has access to everything labeled environment:production, we assume no restrictions on any other category of label. This user has less restricted permissions than another user who has access to everything with environment:production AND region:apac.\n\nFor example, if the following sources had these set of labels:\n\nSOURCE\tLABELS\nA\tenvironment:prod, product:car\nB\tenvironment:prod, product:truck\nC\tenvironment:dev, product: car\n\nThen the following through users with Source Admin restricted with Labels will only have access to the following Sources:\n\nUSER\tSOURCE ADMIN WITH LABELS\tACCESS TO SOURCES\nSally\tenvironment:prod\tA, B\nBob\tenvironment:prod, product:truck\tB\nJane\tproduct: car\tA, C\nCan I grant a user permissions with OR statements?\n\nYou can only assign one set of additive labels on a per-user basis. However, to give a user who needs access to all sources labeled brand:a or brand:b, we recommend that you use Group permissions and assign this user to two separate groups, where one group has Source Admin access to brand:a and the other has Source Admin access to brand:b.\n\nThis page was last modified: 30 Oct 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nCustom Environments\nCustom Labels\nLabels FAQ\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nJourneys\n/\nJourneys Step Types\nJourneys Step Types\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nOn this page, you\u2019ll find information about the steps you can add to a Journey.\n\nConditions and delays\n\nJourneys has two steps that you can use to determine how and when users move to the following step.\n\nAdd a condition\n\nThe Add a condition step defines the conditions that a user must satisfy to move from one step to the next. You can define new conditions or import conditions from an existing audience.\n\nAdd a delay\n\nThe Add a delay step defines the length of time in minutes, hours, days, or weeks that a user must wait before moving to the next step.\n\nFlow control steps\n\nJourneys offers four steps that help you control how users flow through your Journey.\n\nTrue/false splits\n\nA true/false split divides the previous step\u2019s user group into two branches, based on Boolean logic against a defined condition. Users who satisfy the condition(s) move to the True branch. Otherwise, they move to the False branch. To enforce mutual exclusivity, Journeys evaluates true/false conditions when a user reaches the relevant step.\n\nYou can add Step Names to describe the users who end up in both the True and False branches.\n\nMulti-branch splits\n\nMulti-branch split divides the group of users from a previous step into two or more branches based on each branch\u2019s defined conditions.\n\nDefine the number of branches you want to create, then add an Add a condition step to define each branch\u2019s condition.\n\nJourneys doesn\u2019t enforce mutual exclusivity in branch conditions. For more information about ensuring branch exclusivity, see Best Practices.\n\nRandomized splits\n\nA randomized split lets you experiment with and test the performance of a Journey\u2019s branches. When you create a randomized split, you add up to five Journey branches, each with a different step. Journeys then sends eligible users down one of the branches at random. Each branch receives a portion of the eligible users based on percentages that you assign to the branches.\n\nIf the Journey has a re-entry condition, users will join the same split branches upon re-entry.\n\nTo test your messaging channels, for example, you might create a randomized split with three different branches, assigning 40% of users to an email campaign, 40% to an SMS campaign, and 20% to a control group. Once users flow through the split, you can determine the success of the email and SMS campaigns compared to each other and the control group.\n\nAdd a randomized split\n\nFollow these steps to add a randomized split to a Journey:\n\nCreate a new Journey, and add an entry condition.\nSelect the + icon to add a step, then select Create a randomized split.\nName the randomized split step, then add up to five branches.\nSet the distribution percentage for each branch, then select Save.\nFor each branch in the split, select the child + icon and add a step.\nSave and publish your Journey.\n\nUsers who meet the Journey\u2019s entry condition will then enter the Journey and flow through the randomized split.\n\nAct on the split\u2019s results\n\nOnce users complete your Journey\u2019s randomized split step, you\u2019ll have insight into how each split performed. You can take action on the results by cloning the Journey and sending a new set of users through the highest performing branch.\n\nConnecting to existing steps\n\nYou can merge split Journey branches by using the Connect to existing steps option. Connecting to existing steps lets you apply a single step to more than one group. For example, you may want to target some Journey group members with email campaigns while targeting others with ad campaigns. Instead of duplicating steps, you can connect these steps to steps that already exist.\n\nKeep the following in mind when connecting to existing steps:\n\nYou can only connect the end of a branch to another branch.\nYou cannot link back or loop back to previous steps.\nIf you connect multiple non-exclusive branches, the user will only be sent to a Destination the first time they reach it.\n\nFollow the instructions below to connect branches to an existing step:\n\nWithin an existing Journey, click the Edit button.\nClick the + icon below an existing step to add a new step.\nFrom the Select a Step window, select Connect to existing step.\nChoose the existing step you want to connect.\nClick Save to confirm.\nActions steps\n\nWith Journey actions steps, you can send marketing campaigns to groups of users and deliver Journey information to downstream tools.\n\nShow an ad\n\nThe Show an ad step lets you send users to an advertising destination. You can also configure exit settings that remove users from the ad step after specific periods of time.\n\nFor example, you may want to show an ad for only one week to users who abandoned a cart during a purchase. With the Show an ad step, you can remove users from the ad destination seven days after they enter it.\n\nAd-based exit settings\n\nAd step exit settings don\u2019t impact other Journey steps. A user can exit an ad step but remain in the overall Journey. For more on Journeys exit settings, view Journey exit and re-entry times.\n\nFollow these steps to add a Show an ad step to a Journey:\n\nFrom the Journey builder, select the + icon to add a step, then select Show an ad.\nName the step, then select + Add destination.\nChoose the ad destination that Segment will sync to.\nTo specify how long users will remain in the step, choose one of the following options:\nIf you want users to exit the destination, select the checkbox next to Remove users from the destination after. Set a time frame in minutes, hours, days, or weeks.\nIf you want users to stay in the ad destination indefinitely, leave the checkbox empty.\nSelect Save to finish creating the step.\nChannels steps\n\nThe Send an email, Send an SMS, and Send a WhatsApp steps are only available on Engage Premier.\n\nSend an email\n\nUse Twilio Engage to send email as a step in a Journey.\n\nTo send email in Engage, you must connect a SendGrid subuser account to your Segment space. Visit the onboarding steps for more information.\n\nFrom the Add step window, Send an email.\nBuild an email from scratch, or use an existing template as a starting point. You can use an existing template as a base to build the email, but any changes made from within Journeys won\u2019t be saved in the original email template. Click Manage Templates to visit the Email Templates page.\nConfigure the email step.\nAdd a step name.\nAdd the sender\u2019s email address and name. Emails can only be sent from a verified domain.\nIndicate if you want to send replies back to the sender. If not, add a reply to email and name.\nAdd email addresses to receive a blind carbon copy of your email.\nAdd preview text and the subject line. Use merge tags to personalize the email template with user profile traits.\nDesign and test the email in the Body section. Be sure to include an unsubscribe link in your message.\nAdd conversion goals.\nClick Save to add the email step to your Journey.\n\nSubscribed users will receive an email upon entering the step. To send an email to users regardless of their subscription state, you can use Engage to send a message to all users. Visit Email Campaigns for more information.\n\nSend an SMS\n\nUse Twilio Engage to send an SMS message as a step in a Journey.\n\nTo send SMS in Engage, you must connect a Twilio messaging service to your Segment workspace. Visit the onboarding steps for more information.\n\nFrom the Add step window, click Send an SMS.\nBuild an SMS template from scratch, or select a previously built template. Click Manage Templates to visit the SMS Templates page.\nConfigure the Send SMS step.\nAdd a name to describe the step.\nSelect a Twilio Engage messaging service to use.\nAdd the body of the SMS. Include an opt-out message such as \u201cReply STOP to unsubscribe\u201d in the text.\nUse merge tags to personalize your text, and test the SMS message.\nAdd a conversion goal to track message success.\nClick Save to add the SMS step to your Journey.\n\nAs soon as a subscribed user enters the Send SMS step, they\u2019ll receive the text. Visit SMS Campaigns for more information.\n\nSend a WhatsApp\n\nUse Twilio Engage to send a WhatsApp message as a step in a Journey.\n\nWhatsApp Public Beta\n\nWhatsApp as an Engage channel is in public beta.\n\nFrom the Add step window, click Send a WhatsApp.\nPick an approved template from the template list, then choose Select.\nGive the WhatsApp message step a name.\nIn the Sender field, choose WhatsApp, then click Save to add the WhatsApp message to your Journey.\nSend to Destinations\n\nThe Send to Destinations step delivers information about the Journey to the selected Destination. For more information, see Send data to Destinations.\n\nThis page was last modified: 29 Sep 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nConditions and delays\nFlow control steps\nActions steps\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSpec Overview\nSpec Overview\n\nThe Segment Spec provides guidance on meaningful data to capture, and the best format for it, across all of Segment\u2019s libraries and APIs. If you implement Segment using these formats, it\u2019s simple to translate your data to downstream tools.\n\nSegment University: The Segment Methods\n\nCheck out our high-level overview of these APIs in Segment University. (Must be logged in to access.)\n\nEvent and Product Limits\n\nEvents ingested by Segment are subject to defined Product Limits.\n\nThe Segment Spec has three components.\n\nFirst, it outlines the semantic definition of the customer data Segment captures across all libraries and APIs. There are six API calls in the Spec. They each represent a distinct type of semantic information about a customer. Every call shares the same common fields.\n\nAPIs\nIdentify: who is the customer?\nTrack: what are they doing?\nPage: what web page are they on?\nScreen: what app screen are they on?\nGroup: what account or organization are they part of?\nAlias: what was their past identity?\n\nSecond, it details the event data Segment captures across some cloud sources and destinations.\n\nCloud Sources and Destinations\nEmail\nLive Chat\nA/B Testing\n\nThird, it shares the events Segment recommends you track for a particular industry based on experience working with thousands of customers. When you respect these specs, Segment maps these events to particular features within end destinations like Google Analytics and Facebook Ads.\n\nIndustry Specs\nMobile\nE-Commerce\nVideo\nB2B SaaS\nAI Copilot\n\nThis page was last modified: 18 Apr 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nEngage\n/\nProfiles\n/\nAdd or Update Profiles and Traits with a CSV\nAdd or Update Profiles and Traits with a CSV\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nENGAGE FOUNDATIONS \u2713\n?\n\nYou can use the Profiles CSV Uploader to add or update user profiles and traits. This page contains guidelines for your CSV upload and explains how to upload a CSV file to Engage.\n\nWhen you upload a CSV file, Engage generates internal Identify calls using Segment\u2019s Tracking API and sends them into the Engage output source.\n\nCSV file upload guidelines\n\nKeep the following guidelines in mind as you upload CSV files to Twilio Engage:\n\nYou can only upload .csv files.\nFiles can\u2019t be empty and must have at least one header and one row.\nYou can\u2019t have multiple columns with the same header.\nCSV files cannot exceed 1 million rows (plus one header row), 299 columns, or 100 MB in file size.\nYou can only upload one file at a time.\nAdd an identifier column or anonymous_id in your identity resolution configuration.\nLeave any unknown values blank to avoid bad data. Engage can create a user profile from a single identifier in your CSV.\nThe template won\u2019t include duplicate custom traits, traits with trailing, leading, or multiple consecutive spaces between characters, or unallowed characters.\nCustom traits column headers are case-sensitive. For example, first Name, FIRST Name, and First Name would all be different traits in the template.\nTrailing, leading, or multiple consecutive spaces between characters are not allowed.\nThe CSV uploader shares Unify product limits.\nUpload a CSV file\n\nUse the Upload CSV page to upload a CSV file in your Segment space:\n\nNavigate to Unify > Profile explorer or Engage > Audiences > Profile explorer.\nClick +Add Profiles.\nDownload and fill out the CSV template.\nUpload your CSV file.\n1. Download your CSV template\n\nClick Download Template to download a CSV template with identifier columns from your identity resolution configuration.\n\n2. Fill out your CSV file\n\nEnter values for the identifiers in your CSV file.\n\n3. Upload your CSV file\n\nUpload a CSV file to Twilio Engage in two ways:\n\nDrag and drop the CSV file in the dropzone.\nClick Browse to locate the CSV file.\nWork with the CSV template\n\nKeep the following in mind as you fill out your CSV template.\n\nAllowed CSV file characters\n\nYou can use these characters in your CSV file:\n\nAlphabetic English characters in both upper and lower case\nThe numerals 0-9\nThese special characters: !@#$%^&*()_+-=[]{}:\\\\|.`~<>\\/?\nThe following non-English characters:\n\n\u00e0\u00e1\u00e2\u00e4\u01ce\u00e6\u00e3\u00e5\u0101\u00e7\u0107\u010d\u010b\u010f\u00f0\u1e0d\u00e8\u00e9\u00ea\u00eb\u011b\u1ebd\u0113\u0117\u0119\u011f\u0121gg\u035fh\u0127\u1e25h\u0324\u00ec\u00ed\u00ee\u00ef\u01d0\u0129\u012b\u0131\u012f\u0137k\u035fh\u0142\u013c\u013el\u0325\u1e41m\u0310\u00f2\u00f3\u00f4\u00f6\u01d2\u0153\u00f8\u00f5\u014d\u0159\u1e5br\u0325\u027d\u00df\u015f\u0219\u015b\u0161\u1e63s\u0324s\u0331s\u021b\u0165\u00fe\u1e6dt\u0324\u0288\u00f9\u00fa\u00fb\u00fc\u01d4\u0169\u016b\u0171\u016f\u0175\u00fd\u0177\u00ff\u017a\u017e\u017c\u1e93z\u0324\u00c0\u00c1\n\u00c4\u01cd\u00c6\u00c3\u00c5\u0100\u00c7\u0106\u010c\u010a\u010e\u00d0\u1e0c\u00c8\u00c9\u00ca\u00cb\u011a\u1ebc\u0112\u0116\u0118\u011e\u0120GG\u035fH\u0126\u1e24H\u0324\u00cc\u00cd\u00ce\u00cf\u01cf\u0128\u012aI\u012e\u0136K\u035fH\u0141\u013b\u013dL\u0325\u1e40M\u0310\u00d2\u00d3\u00d4\u00d6\u01d1\u0152\u00d8\u00d5\u014c\u0158\u1e5aR\u0325\u024cS\u1e9e\u015a\u0160\u015e\u0218\u1e62S\u0324S\u0331\u021a\u0164\u00de\u1e6cT\u0324\u01ae\u00d9\u00da\u00db\u00dc\u01d3\u0168\u016a\u0170\u016e\u0174\u00dd\u0176\u0178\u0179\u017d\u017b\u1e92Z\n\nView Update History\n\nUse the Update History page to view CSV file uploads in your workspace over the last 30 days.\n\nTo view the Update History page:\n\nNavigate to Unify > Profile explorer or Engage > Audiences > Profile explorer.\nClick View update history.\nValidation errors\n\nThe following table lists validation errors you may run into with your profiles and traits CSV upload:\n\nERROR\tERROR MESSAGE\nInvalid file types\tYou can upload only .csv files. Change your file format, then try again.\nEmpty files\tThis file contains no data. Add data to your CSV, then try again.\nCSV parsing error\tWe encountered an issue while parsing your CSV file. Validate the CSV file and try again.\nUnexpected/fallback\tSomething went wrong. Try again later.\nEmpty header row\tThis file contains empty header(s). Remove the empty header(s), then try again.\nFile exceeds one million rows\tToo many rows. You can upload up to 1000000 rows.\nFile exceeds 299 columns\tYour CSV file is exceeding the limit of 299 columns.\nFile exceeds 100 MB\tFiles can be up to 100 MB.\nFile contains a header with unallowed spaces\tThis file contains leading, trailing or consecutive spaces. Remove leading, trailing or consecutive spaces, then try again.\nFile contains duplicate headers\tThis file contains duplicate header(s). Remove duplicate header(s), then try again.\nFile contains invalid characters\tThis file contains invalid character(s). Remove invalid character(s), then try again.\nUnconfigured anonymous_id or missing Identifier column\tThis file is missing an identifier column and does not have anonymous_id configured. Add an identifier column or add anonymous_id in your identity resolution configuration, then try again.\n\nThis page was last modified: 03 Oct 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nCSV file upload guidelines\nUpload a CSV file\nWork with the CSV template\nView Update History\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nSegment App\n/\nIam\n/\nPicking a secure password\nPicking a secure password\n\nPicking a strong password is one of the most important things you can do to protect your account.\n\nTwilio Unified Login users can manage their password in their Twilio account\n\nTwilio Unified Login users can manage their user settings, including name, email, password, and 2FA settings, directly in their Twilio account. To learn more about Twilio\u2019s user and password policies, review Twilio\u2019s Account Management documentation.\n\nUnder the Hood\n\nWhen you first create a Segment account, or when you reset or change the password of an existing account, you\u2019ll see some tools which Segment uses to help you choose a strong password. Segment uses zxcvbn to show your password strength, and Have I Been Pwned to notify you if your password has been found in any data breaches. Your password is never stored in plaintext, and is securely stored using the bcrypt password hashing function in Segment\u2019s database.\n\nGeneral Guidance\n\nHere are some general password guidelines:\n\nUse a password manager like 1Password or LastPass to generate and store passwords.\n\nPasswords should be 8 or more characters. Consider using pass-phrases (for example, \u00a0customer data infrastructure), combinations of random characters (for example, \u00a09;ske%t!u9jdckd#s>), or other strategies that are difficult to guess, such as icOnsTent CaPitaliZation.\n\nUse a different password for every website. If you use the same password on multiple websites and one is breached, your accounts on all of these websites may be affected.\n\nDo not share your password with anyone, even your co-workers. Once shared, they may use that password on another site or share it with another co-worker without telling you. If one of them leaves the company they will still be able to take actions under your account.\n\nHas my password been compromised?\n\nIf you see a message that says \u201cThis password is known to have been previously compromised in a data breach\u201d, it means that the password you typed has been used before, and was in a database that was compromised and put on the internet. This does not mean that Segment has been compromised, or that someone has accessed your Segment account. Check out Have I Been Pwned for more information, and choose a different password.\n\nThis page was last modified: 04 Dec 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nProtocols\n/\nEnforce\n/\nForward blocked events\nForward blocked events\nFREE X\nTEAM X\nBUSINESS \u2713\n+\nPROTOCOLS \u2713\n?\n\nIf you\u2019re concerned about permanently discarding blocked events, you can enable blocked event forwarding on a Segment Source. To set up forwarding, navigate to the settings tab of the Source, then Schema Configuration.\n\nSelect the source you\u2019ll forward events to from the Blocked Events and Traits dropdown. Segment recommends that you create a new Source for forwarded events to avoid contaminating production data and enable blocking only when you are confident about the quality of your data.\n\nSince forwarding happens server to server, Segment recommends creating a HTTP Tracking API source, though any server-side source will work.\n\nOnly blocked events are forwarded to the source. Events with omitted traits are not forwarded. Instead, Segment inserts a context.protocols object into the event payload which contains the omitted properties or traits.\n\nBilling Note: Events forwarded to another Source count towards to your MTU counts. Blocking and discarding events does not contribute to your MTU counts.\n\nThis page was last modified: 03 Aug 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nSegment App\n/\nVerifying Your Email Address\nVerifying Your Email Address\n\nBefore you can use your Segment account, you first need to verify your email address. We automatically send a verification message to the address you used to sign, but you can re-send the verification email if you didn\u2019t receive it.\n\nIf a team member invited you to a Segment workspace, your email is automatically verified if you set up your account from the link in the invitation email.\n\nTroubleshooting why you didn\u2019t get the verification email\n\nYou should receive a verification email from Segment within 20 minutes of signing up for a Segment account.\n\nIf you don\u2019t receive the verification email, check out some of the suggestions below:\n\nTypos: Check the spelling of your email address. If the spelling is incorrect, sign up again using the correct spelling of the address.\n\nSpam or Junk Folder: Check your Spam or Junk folder. Verification emails may be filtered directly into your email provider\u2019s spam or junk mail folder. Your ISP or corporate domain may be configured to deliver commercial mail to this folder by default.\n\nWeb Browser Needs a Refresh: Try requesting another verification email, and refresh the page of your email web browser.\n\nBlocked or Bounced Address: If you tried to verify a specific email address but did not receive the verification email, your ISP or corporate domain may have blocked the email. If you haven\u2019t received any Segment emails, try verifying an alternative email address or get help.\n\nRole, Group, or Alias Address: Some role addresses can be verified, but many can\u2019t, because more than one person may have access to a role email address (such as admin or dev-ops emails). You should use a personal email address for your contact and billing address.\n\nThis page was last modified: 21 Apr 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nSegment App\n/\nIam\n/\nAudit Trail\nAudit Trail\nFREE X\nTEAM X\nBUSINESS \u2713\nADD-ON X\n?\n\nThe Audit Trail allows you to view the last 90 days of user and system activity, filter activity for specific actions or actors, and export your data to an event streams source or CSV file.\n\nFor any requests exceeding the 90-day timeframe, contact Segment Support for assistance.\n\nViewing the Audit Trail requires Workspace Owner permissions\n\nYou must have the Workspace Owner role to view the Audit Trail page. For more information about roles and permissions within Segment, see the Roles documentation.\n\nTo view the Audit Trail:\n\nFrom the Segment app, select Settings.\nFrom the Settings tab, select Admin.\nAudit Trail events\n\nThe Audit Trail returns information about the following Segment product areas:\n\nSources\nFunctions\nWarehouses\nDestinations\nStorage\nConsent Management\nTracking Plans\nDestination Filters\nTransformations\nAudiences\nComputed Traits\nEngage Warehouse Sources\nProfiles Sync\nSpaces\nUsers\nJourneys\nBroadcasts\nWorkspace\n\nTo view a list of all events Segment surfaces in the Audit Trail, open the Audit Trail, click Filters, and select the Events dropdown.\n\nFiltering events\n\nUse the Filters dropdown to refine your search results and filter by actions or actors to see who made changes on specific resources in the app. Actors include both logged-in users and access tokens.\n\nAudit forwarding\n\nYou can forward events in your workspace to an event streams source to set up real-time alerts and quickly revert changes (like a user unintentionally disabling a warehouse) that could cause unwanted downstream effects.\n\nSegment recommends creating a dedicated source for Audit Trail events\n\nSegment recommends forwarding all events to an instance of the HTTP API source. Segment passes all forwarded events through its entire processing pipeline. This ensures that Tracking Plans, Filters, and other features work with the audit events, and also ensures you can send those events to multiple downstream destinations.\n\nTo forward Audit Trail events to an event streams source:\n\nNavigate to Settings > Workspace Settings > Audit Forwarding.\nSelect or create an event streams source to which you\u2019ll forward workspace events.\nToggle the setting to On and click Save Changes.\n\nWhen you forward audit events to a source, Segment passes those events through its entire processing pipeline. This ensures that tracking plans, filters, and other features work with the audit events, and also ensures you can send those events to multiple downstream destinations.\n\nFrequently asked questions\nEngage\nWhy am I getting alerts about an audience/computed trait sync failure, but when I look at the specific audience/computed trait it shows a successful sync?\n\nAn audience/computed trait Run or a Sync may fail on its first attempt, but Engage will retry up to 5 times before considering it a hard failure and display on that audience/compute trait\u2019s Overview page. As long as the runs/syncs within the specific Audience\u2019s Overview page say they are successful, then these can be safely ignored.\n\nHow things work internally: Segment Engage scheduler fetches audiences/traits from compute service and then handles the logic of generating tasks. These compute/sync tasks get scheduled and executed by another worker. Essentially, these tasks are a list of steps to be executed. Each task has a series of steps that are marked as complete by saving a timestamp for the completion. If the worker is disrupted, it picks up at the latest step, which has no completed_at timestamp. In some cases, the step may fail or the entire task may fail (for example, due to timeout or the worker disruption as there are many moving parts). In either case, these failures will be retried.\n\nThese tasks are a part of internal Segment process, and there are systems in place to retry failed tasks. In most cases, it is not necessary to track these failures, as long as there are no actual computation or sync failures.\n\nThe Audit Trail logic, however, is configured to notify you about every task failure, even if it then later succeeds.\n\nIf you would like to avoid receiving the notifications for transient failures, reach out to support to request enabling a setting to reduce the number of notifications your workspace receives.\n\nThis page was last modified: 06 Aug 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nAudit Trail events\nFiltering events\nAudit forwarding\nFrequently asked questions\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSpec\n/\nSpec: Identify\nSpec: Identify\n\nThe Segment Identify call lets you tie a user to their actions and record traits about them. It includes a unique User ID and any optional traits you know about the user, like their email, name, and more.\n\nSegment University: The Identify Method\n\nCheck out our high-level overview of the Identify method in Segment University. (Must be logged in to access.)\n\nSegment recommends that you make an Identify call:\n\nAfter a user first registers\nAfter a user logs in\nWhen a user updates their info (for example, they change or add a new address)\n\nThe first three examples are pretty self-explanatory, but many might ask: why you would call Identify on every page load if you\u2019re storing the userId in the cookie/local storage?\n\nCalling Identify in one of Segment\u2019s libraries is one of the first steps to getting started with Segment. Refer to library-specific documentation for more details.\n\nHere\u2019s the payload of a typical Identify call with most common fields removed:\n\n{\n  \"type\": \"identify\",\n  \"traits\": {\n    \"name\": \"Peter Gibbons\",\n    \"email\": \"peter@example.com\",\n    \"plan\": \"premium\",\n    \"logins\": 5\n  },\n  \"userId\": \"97980cfea0067\"\n}\n\n\nAnd here\u2019s the corresponding JavaScript event that would generate the above payload:\n\nanalytics.identify(\"97980cfea0067\", {\n  name: \"Peter Gibbons\",\n  email: \"peter@example.com\",\n  plan: \"premium\",\n  logins: 5\n});\n\n\nBased on the library you use, the syntax in the examples might be different. You can find library-specific documentation on the Sources Overview page.\n\nBeyond the common fields, an Identify call has the following fields:\n\nFIELD\t\tTYPE\tDESCRIPTION\ntraits\toptional\tObject\tFree-form dictionary of traits of the user, like email or name. See the Traits field docs for a list of reserved trait names.\nuserId\trequired; optional if anonymousID is set instead\tString\tUnique identifier for the user in your database. A userId or an anonymousId is required. See the Identities docs for more details.\n\nNote that these traits coming in from your source events are called custom traits.\n\nExample\n\nHere\u2019s a complete example of an Identify call:\n\n{\n  \"anonymousId\": \"507f191e810c19729de860ea\",\n  \"channel\": \"browser\",\n  \"context\": {\n    \"ip\": \"8.8.8.8\",\n    \"userAgent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36\"\n  },\n  \"integrations\": {\n    \"All\": false,\n    \"Mixpanel\": true,\n    \"Salesforce\": true\n  },\n  \"messageId\": \"022bb90c-bbac-11e4-8dfc-aa07a5b093db\",\n  \"receivedAt\": \"2015-02-23T22:28:55.387Z\",\n  \"sentAt\": \"2015-02-23T22:28:55.111Z\",\n  \"timestamp\": \"2015-02-23T22:28:55.111Z\",\n  \"traits\": {\n    \"name\": \"Peter Gibbons\",\n    \"email\": \"peter@example.com\",\n    \"plan\": \"premium\",\n    \"logins\": 5,\n    \"address\": {\n      \"street\": \"6th St\",\n      \"city\": \"San Francisco\",\n      \"state\": \"CA\",\n      \"postalCode\": \"94103\",\n      \"country\": \"USA\"\n    }\n  },\n  \"type\": \"identify\",\n  \"userId\": \"97980cfea0067\",\n  \"version\": \"1.1\"\n}\n\nCreate your own Identify call\n\nUse the following interactive code pen to see what your Identify calls would look like with user-provided information:\n\nSample Identify\nName:\nEmail:\nPlan:\nLogins:\nStreet:\nCity:\nState:\nZip Code:\nCountry:\nSample Identify Call\nSample output goes here!\nIdentities\n\nThe Identify call specifies a customer identity that you can reference across the customer\u2019s whole lifetime. Every Identify call must have a User ID or an Anonymous ID, depending on how much you know about the user in question.\n\nAnonymous ID\n\nThere are certain cases where you don\u2019t actually know who the user is according to your database, but you still want to be able to tie them to traits, events, or page views. For example, you may not know who a user is when tracking newsletter signups or anonymous page views.\n\nIn these cases, you should use an Anonymous ID.\n\nThe Anonymous ID can be any pseudo-unique identifier. For example, on your servers you can use a session id. If you don\u2019t have any readily available identifier, you can always generate a new random one \u2014 Segment recommends UUIDv4 format.\n\nSegment\u2019s browser and mobile libraries automatically use Anonymous IDs to keep track of users as they navigate around your website or app, so you don\u2019t need to worry about them when using those libraries.\n\nHere\u2019s an example of a JavaScript event for an anonymous user:\n\nanalytics.identify({\n  subscriptionStatus: 'inactive'\n});\n\nUser ID\n\nUser IDs are a more permanent and robust identifier, like a database ID. Since these IDs are consistent across a customer\u2019s lifetime, Identify calls should include a User ID as often as possible.\n\nA User ID is usually the unique identifier that you recognize a user by in your own database. For example, if you\u2019re using MongoDB, User IDs might look something like this: 507f191e810c19729de860ea.\n\nSegment recommends using database IDs, in uuidv4 format, instead of email addresses or usernames because database IDs never change. That guarantees that even if the user changes their email address, you can still recognize them as the same person in all of your analytics tools, and you\u2019ll be able to correlate analytics data with your own internal database.\n\nInstead of using an email address or a username as a User ID, send them along as custom traits.\n\nCustom traits\n\nCustom traits are pieces of information you know about a user that are included in an Identify call. These could be demographics like age or gender, account-specific like plan, or even things like whether a user has seen a particular A/B test variation.\n\nSegment has reserved some custom traits that have semantic meanings for users, and will handle them in special ways. For example, Segment always expects email to be a string of the user\u2019s email address. Segment sends this on to destinations like Mailchimp that require an email address for their tracking.\n\nOnly use reserved traits for their intended meaning.\n\nReserved custom traits Segment has standardized:\n\nTRAIT\tTYPE\tDESCRIPTION\naddress\tObject\tStreet address of a user optionally containing: city, country, postalCode, state, or street\nage\tNumber\tAge of a user\navatar\tString\tURL to an avatar image for the user\nbirthday\tDate\tUser\u2019s birthday\ncompany\tObject\tCompany the user represents, optionally containing: name (String), id (String or Number), industry (String), employee_count (Number) or plan (String)\ncreatedAt\tDate\tDate the user\u2019s account was first created. Segment recommends using ISO-8601 date strings.\ndescription\tString\tDescription of the user\nemail\tString\tEmail address of a user\nfirstName\tString\tFirst name of a user\ngender\tString\tGender of a user\nid\tString\tUnique ID in your database for a user\nlastName\tString\tLast name of a user\nname\tString\tFull name of a user. If you only pass a first and last name Segment automatically fills in the full name for you.\nphone\tString\tPhone number of a user\ntitle\tString\tTitle of a user, usually related to their position at a specific company. Example: \u201cVP of Engineering\u201d\nusername\tString\tUser\u2019s username. This should be unique to each user, like the usernames of Twitter or GitHub.\nwebsite\tString\tWebsite of a user\n\nYou might be used to some destinations recognizing special traits by slightly different names. For example, Mixpanel recognizes a $created trait when the user\u2019s account was first created, while Intercom recognizes the same trait as created_at instead. Segment attempts to handle all the destination-specific conversions for you automatically. If you need help understanding if a specific field will be converted to a destination, take a look at Segment\u2019s open source integration code, view the destination\u2019s documentation, or contact Segment support.\n\nYou can pass these reserved traits using camelCase or snake_case, so in JavaScript you can match the rest of your camelCase code by sending firstName, while in Ruby you can match your snake-case code by sending first_name. That way the API never seems alien to your code base. Keep in mind that not all destinations support these reserved traits, so sending these traits in camelCase and snake_case can result in two sets of traits in other destinations.\n\nThis page was last modified: 23 Apr 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nExample\nIdentities\nCustom traits\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nHandling Duplicate Data\nHandling Duplicate Data\n\nSegment guarantees that 99% of your data won\u2019t have duplicates within an approximately 24 hour look-back window. Warehouses and Data Lakes also have their own secondary deduplication process to ensure you store clean data.\n\n99% deduplication\n\nSegment has a special deduplication service that sits behind the api.segment.com endpoint and attempts to drop 99% of duplicate data. Segment stores at least 24 hours\u2019 worth of event\u00a0messageIds, which allows Segment to deduplicate any data that appears with the same messageId within the stored values.\n\nSegment deduplicates on the event\u2019s\u00a0messageId, not on the contents of the event payload. Segment doesn\u2019t have a built-in way to deduplicate data for events that don\u2019t generate messageIds. The message de-duplication is not scoped to a specific source or a workspace, and applies to all events being received by Segment.\n\nKeep in mind that Segment\u2019s libraries all generate\u00a0messageIds for each event payload, with the exception of the Segment HTTP API, which assigns each event a unique\u00a0messageId\u00a0when the message is ingested. You can override these default generated IDs and manually assign a\u00a0messageId\u00a0if necessary. The messageId field is limited to 100 characters.\n\nWarehouse deduplication\n\nDuplicate events that are more than 24 hours apart from one another deduplicate in the Warehouse. Segment deduplicates messages going into a Warehouse (including Profiles Sync data) based on the\u00a0messageId, which is the\u00a0id\u00a0column in a Segment Warehouse.\n\nData Lake deduplication\n\nTo ensure clean data in your Data Lake, Segment removes duplicate events at the time your Data Lake ingests data. The Data Lake deduplication process dedupes the data the Data Lake syncs within the last 7 days with Segment deduping the data based on the messageId.\n\nThis page was last modified: 02 Aug 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\n99% deduplication\nWarehouse deduplication\nData Lake deduplication\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nOAuth 2.0\nOAuth 2.0\nFREE X\nTEAM X\nBUSINESS \u2713\nADD-ON X\n?\n\nOAuth 2.0 is an online authorization standard that uses tokens to grant access to API resources like Segment\u2019s tracking API. You can use OAuth 2.0 as a security requirement for connections to third-party tools.\n\nPermissions\n\nDepending on your workspace permissions, your access to OAuth apps is limited.\n\nSEGMENT ROLE\tPERMISSION\nWorkspace Owner\tYou can view, create, and edit OAuth apps.\nWorkspace Member\tYou cannot view, create, or edit OAuth apps.\nSource Admin\tYou can view and edit OAuth apps.\nYou can connect and disconnect OAuth apps.\nYou can enable or disable OAuth enforcement.\nSource Read-only\tYou can only view OAuth apps.\nFunction Admin\tYou can view and edit OAuth apps.\nYou can connect and disconnect OAuth apps.\nYou can enable and disable OAuth enforcement.\nFunction Read-only\tYou can only view OAuth apps.\nCreate an OAuth app\n\nYou must have already created a workspace in Segment to use OAuth.\n\nTo create a new OAuth application:\n\nNavigate to Settings > Workspace settings and select the Access Management tab.\nSelect the OAuth application tab within the Access Management page.\nClick Create OAuth app.\n\nEnter the configuration settings:\n\nSETTINGS\tDETAILS\nApplication name\tThe name of the OAuth app.\nPublic key\tUpload a public key in PEM format to authenticate through the OAuth application. You can upload a second public key after you create the OAuth application. You can create a public key by running the script: openssl rsa -in private.pem -pubout -outform PEM -out public.pem\nPublic key name\tEnter a name for your public key.\nToken expiration period\tYou can choose between: 1 day, 2 days, 3 days, 1 week, 2 weeks, 3 weeks, 30 days.\nScope\tThis specifies what type of access you need for each API. See the list of supported scopes.\nClick Create.\n\nOnce you create your OAuth app, you can now connect a source to your OAuth app.\n\nConnect a source to OAuth\n\nOAuth only supports server-side sources. See the list of supported sources.\n\nTo connect a source to OAuth:\n\nNavigate to Connections > Sources.\nSelect the source you want to enable OAuth for.\nGo to the Settings tab of the source page and select OAuth app.\nClick Connect OAuth app.\nSelect the OAuth app you want to connect the source to.\nClick Connect.\n\nTo disconnect your source from OAuth, click Disconnect.\n\nEnable a source to OAuth\n\nOnce you\u2019ve connected your source to OAuth, you can enable it. To enable your source:\n\nNavigate to Connections > Sources and select your source.\nGo to the Settings tab of the source and select OAuth app.\nTurn the toggle on for Enable OAuth.\n\nTo disable your source from OAuth, turn the toggle off for Enable OAuth.\n\nObtain the access token\n\nYou can obtain an access token once you create an OAuth application and enable a source to OAuth.\n\nAccess tokens are only valid within a region. The supported regional authorization servers are:\n\nOregon - https://oauth2.segment.io\nDublin - https://oauth2.eu1.segmentapis.com\n\nTo obtain the access token:\n\nCreate a JWT token with the header and payload as below:\n\nHeader\n\n {\n     \"alg\":\"RS256\", \n     \"typ\":\"JWT\", \n     \"kid\":\"<<KID>>\"\n }\n\n\nPayload\n\n {\n     \"iss\":\"<<ISS>>\",\n     \"sub\":\"<<SUB>>\",\n     \"aud\":\"<<AUD>>\", \n     \"iat\":\"<<IAT>>\",\n     \"exp\":\"<<EXP>>\",\n     \"jti\":\"<<JTI>>\"\n }\n\nFIELD\tDESCRIPTION\nKID\tThe key ID of the public key in the OAuth application.\nISS\tThe identifier of the JWT issuer.\nSUB\tThe OAuth application ID.\nIAT\tThe epoch time in seconds when the token was issued.\nEXP\tThe expiry time in seconds. This is expected to be valid only for a short duration under a minute.\nJTI\tThe unique identifer for the token.\n\nSend a form-url-encoded POST request to the regional authorization server\u2019s \\token route with the following parameters:\n\n grant_type=client_credentials\n client_assertion_type=urn:ietf:params:oauth:client-assertion-type:jwt-bearer\n client_assertion=<<JWT>>\n scope=<<SCOPE>>\n\nFIELD\tDESCRIPTION\nJWT\tThe signed JWT token string from Step 1.\nSCOPE\tScopes for which token is requested. See supported scopes.\n\nTo use the access token, see an example of how to use the access token in the HTTP API source.\n\nEdit an OAuth application\n\nTo edit an existing OAuth application:\n\nNavigate to Settings > Workspace settings and select the Access Management tab.\nSelect the OAuth application tab within the Access Management page.\nClick the application name of the OAuth application you want to edit.\nOn the Overview tab you can:\nRevoke a token\nCopy the Application ID and the Public key\nDelete the OAuth application\nSelect the Settings tab on the right window where you can:\nEdit the Application name\nDelete a public key\nAdd a new public key\nChange the token expiration period\nEdit your scope\nClick Save changes.\nDelete an OAuth app\n\nTo delete an OAuth app, you must remove all connected sources from the app.\n\nTo delete an OAuth app:\n\nNavigate to Settings > Workspace settings and select the Access Management tab.\nSelect the OAuth application tab within the Access Management page.\nSelect the App name of the OAuth app you want to delete.\nSelect Delete OAuth app.\nEnter the name of the OAuth app you want to delete.\nClick Delete OAuth app.\nRevoke a token\n\nWhen security incidents expose access tokens, you can revoke your access token. To revoke a token:\n\nNavigate to Settings > Workspace settings and select the Access Management tab.\nSelect the *OAuth application tab within the Access Management page.\nSelect the App name with the token you want to delete.\nEnter the complete token\nClick Revoke token.\nSupported sources\n\nOAuth 2.0 currently supports these sources:\n\nHTTP Tracking API\nNode.js\nPublic API\nPython\nSource Functions\nSupported scopes\n\nOAuth 2.0 currently supports these scopes:\n\nTracking API scopes\n\ntracking_api:write\n\nSource Functions scopes\n\nfunctions:write\n\nPublic API scopes\n\npublic_api:read_write\n\nThis page was last modified: 03 Sep 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nPermissions\nCreate an OAuth app\nConnect a source to OAuth\nEnable a source to OAuth\nObtain the access token\nEdit an OAuth application\nDelete an OAuth app\nRevoke a token\nSupported sources\nSupported scopes\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nStorage\n/\nWarehouses\n/\nWarehouse Schemas\nWarehouse Schemas\n\nA schema describes the way that the data in a warehouse is organized. Segment stores data in relational schemas, which organize data into the following template: <source>.<collection>.<property>, for example segment_engineering.tracks.user_id, where source refers to the source or project name (segment_engineering), collection refers to the event (tracks), and the property refers to the data being collected (user_id). All schemas convert collection and property names from CamelCase to snake_case using the go-snakecase package.\n\nWarehouse column creation\n\nNote: Segment creates tables for each of your custom events in your warehouse, with columns for each event\u2019s custom properties. Segment does not allow unbounded event or property spaces in your data. Instead of recording events like \u201cOrdered Product 15\u201d, use a single property of \u201cProduct Number\u201d or similar. Segment creates and populates a column only when it receives a non-null value from the source.\n\nHow warehouse tables handle nested objects and arrays\n\nSegment\u2019s libraries pass nested objects and arrays into tracking calls as properties, traits, and tracking calls. To preserve the quality of your events data, Segment uses the following methods to store properties and traits in database tables:\n\nThe warehouse connector stringifies all properties that contain a nested array\nThe warehouse connector stringifies all context fields that contain a nested array\nThe warehouse connector stringifies all traits that contain a nested array\nThe warehouse connector \u201cflattens\u201d all properties that contain a nested object\nThe warehouse connector \u201cflattens\u201d all traits that contain a nested object\nThe warehouse connector optionally stringifies arrays when they follow the Ecommerce spec\nThe warehouse connector \u201cflattens\u201d all context fields that contain a nested object (for example, context.field.nestedA.nestedB becomes a column called context_field_nestedA_nestedB)\nFIELD\tCODE (EXAMPLE)\tSCHEMA (EXAMPLE)\nObject (Context): Flatten\t\ncontext: {\n  app: {\n    version: \"1.0.0\"\n  }\n}\n\n\tColumn Name:\ncontext_app_version\n\nValue:\n\u201c1.0.0\u201d\nObject (Traits): Flatten\t\ntraits: {\n  address: {\n    street: \"6th Street\"\n  }\n}\n\n\tColumn Name:\naddress_street\n\nValue:\n\u201c6th Street\u201d\nObject (Properties): Flatten\t\nproperties: {\n  product_id: {\n    sku: \"G-32\"\n  }\n}\n\n\tColumn Name:\nproduct_id_sku\n\nValue:\n\u201cG-32\u201d\nArray (Any): Stringify\t\nproducts: {\n  product_id: [\n    \"507f1\", \"505bd\"\n  ]\n}\n\n\tColumn Name:\nproduct_id\n\nValue: \u201c[507f1, 505bd]\u201d\nWarehouse tables\n\nThe table below describes the schema in Segment Warehouses:\n\nSOURCE\tPROPERTY\n<source>.aliases\tA table with your alias method calls. This table includes the traits you identify users by as top-level columns, for example <source>.aliases.email.\n<source>.groups\tA table with your group method calls. This table includes the traits you record for groups as top-level columns, for example <source>.groups.employee_count.\n<source>.accounts\tIN BETA A table with unique group method calls. Group calls are upserted into this table (updated if an existing entry exists, appended otherwise). This table holds the latest state of a group.\n<source>.identifies\tA table with your identify method calls. This table includes the traits you identify users by as top-level columns, for example <source>.identifies.email.\n<source>.users\tA table with unique identify calls. identify calls are upserted on user_id into this table (updated if an existing entry exists, appended otherwise). This table holds the latest state of a user. The id column in the users table is the same as the user_id column in the identifies table. Also note that this table won\u2019t have an anonymous_id column since a user can have multiple anonymousIds. To retrieve a user\u2019s anonymousId, query the identifies table. If you observe any duplicates in the users table contact Segment support (unless you are using BigQuery, where this is expected).\n<source>.pages\tA table with your page method calls. This table includes the properties you record for pages as top-level columns, for example <source>.pages.title.\n<source>.screens\tA table with your screen method calls. This table includes properties you record for screens as top-level columns, for example <source>.screens.title.\n<source>.tracks\tA table with your track method calls. This table includes standardized properties that are all common to all events: anonymous_id, context_*, event, event_text, received_at, sent_at, and user_id. This is because every event that you send to Segment has different properties. For querying by the custom properties, use the <source>.<event> tables instead.\n<source>.<event>\tFor track calls, each event like Signed Up or Order Completed also has its own table (for example. initech.clocked_in) with columns for each of the event\u2019s distinct properties (for example. initech.clocked_in.time).\nIdentifies table\n\nThe identifies table stores the .identify() method calls. Query it to find out user-level information. It has the following columns:\n\nMETHOD\tPROPERTY\nanonymous_id\tThe anonymous ID of the user.\ncontext_<key>\tNon-user-related context fields sent with each identify call.\nid\tThe unique ID of the identify call itself.\nreceived_at\tWhen Segment received the identify call.\nsent_at\tWhen a user triggered the identify call.\nuser_id\tThe unique ID of the user.\n<trait>\tEach trait of the user you record creates its own column, and the column type is automatically inferred from your data. For example, you might have columns like email and first_name.\nQuerying the Identifies table\n\nTo see a list of the columns in the identifies table for your <source>, run the following:\n\nSELECT column_name AS Columns\nFROM columns\nWHERE schema_name = '<source>'\nAND table_name = 'identifies'\nORDER by column_name\n\n\nThe identifies table is where you can query information about your users and their traits. For example, this query returns unique users you\u2019ve seen on your site each day:\n\nSELECT DATE(sent_at) AS Day, COUNT(DISTINCT(user_id)) AS Users\nFROM <source>.identifies\nGROUP BY day\nORDER BY day\n\nGroups table\n\nThe groups table stores the group method calls. Query it to find out group-level information. It has the following columns:\n\nMETHOD\tPROPERTY\nanonymous_id\tThe anonymous ID of the user.\ncontext_<key>\tNon-user-related context fields sent with each group call.\ngroup_id\tThe unique ID of the group.\nid\tThe unique ID of the group call itself.\nreceived_at\tWhen Segment received the groups call.\nsent_at\tWhen a user triggered the group call.\nuser_id\tThe unique ID of the user.\n<trait>\tEach trait of the group you record creates its own column, and the column type is automatically inferred from your data. For example, you might have columns like email and name.\nQuerying the Groups table\n\nTo see a list of the columns in the groups table for your <source>, run the following:\n\nSELECT column_name AS Columns\nFROM columns\nWHERE schema_name = '<source>'\nAND table_name = 'groups'\nORDER by column_name\n\n\nTo see a list of the groups using your product, run the following:\n\nSELECT name AS Company\nFROM <source>.groups\nGROUP BY name\n\nPages and Screens tables\n\nThe pages and screens tables store the page and screen method calls. Query it to find out information about page views or screen views. It has the following columns:\n\nMETHOD\tPROPERTY\nanonymous_id\tThe anonymous ID of the user.\ncontext_<key>\tNon-user-related context fields sent with each page or screen call.\nid\tThe unique ID of the page or screen call itself.\nreceived_at\tWhen Segment received the page or screen call.\nsent_at\tWhen a user triggered the page or screen call.\nreceived_at\tWhen Segment received the track call.\nuser_id\tThe unique ID of the user.\nproperty\tEach property of your pages or screens creates its own column, and the column type is automatically inferred from your data. For example, you might have columns like referrer and title.\nQuerying the Pages and Screens tables\n\nTo see a list of the columns in the pages table for your <source>, run the following:\n\nSELECT column_name AS Columns\nFROM columns\nWHERE schema_name = '<source>'\nAND table_name = 'pages'\nORDER by column_name\n\n\nThe pages table can give you interesting information about page views that happen on your site. The following query, for example, shows page views grouped by day:\n\nSELECT DATE(sent_at) AS Day, COUNT(*) AS Views\nFROM <source>.pages\nGROUP BY day\nORDER BY day\n\nDAY\tVIEWS\n2015-01-14\t2,203,198\n2015-01-15\t2,393,020\n2015-07-21\t1,920,290\n\u2026\t\u2026\nTracks table\n\nThe tracks table stores the track method calls. Query it to find out information about the events your users have triggered. It has the following columns:\n\nMETHOD\tPROPERTY\nanonymous_id\tThe anonymous ID of the user.\ncontext_<key>\tNon-user-related context fields sent with each track call.\nevent\tThe slug of the event name, mapping to an event-specific table.\nevent_text\tThe name of the event.\nid\tAn ID attached to the event at execution time and used for deduplication at the server level.\nreceived_at\tWhen Segment received the track call.\nsent_at\tWhen a user triggered the track call.\nuser_id\tThe unique ID of the user.\nQuerying the Tracks table\n\nYour tracks table is a rollup of the different event-specific tables, for quick querying of just a single type. For example, you could see the number of unique users signed up each day:\n\nSELECT DATE(sent_at) AS Day, COUNT(DISTINCT(user_id)) AS Users\nFROM segment.tracks\nWHERE event = 'signed_up'\nGROUP BY day\nORDER BY day\n\nDAY\tVIEWS\n2015-01-14\t25,198\n2015-01-15\t31,020\n2015-07-21\t19,290\n\u2026\t\u2026\nEvent Tables\n\nYour event tables are a series of table for each custom event you record to Segment. We break them out into their own tables because the properties, and, as a result, the columns, differ for each event. Query these tables to find out information about specific properties of your custom events. They have the following columns:\n\nEVENT\tPROPERTY\nanonymous_id\tThe anonymous ID of the user.\ncontext_<key>\tNon-user-related context fields sent with each track call.\nevent\tThe slug of the event name, so you can join the tracks table.\nevent_text\tThe name of the event.\nid\tThe unique ID of the track call itself.\nreceived_at\tWhen Segment received the track call.\nsent_at\tWhen a user triggered the track call.\nuser_id\tThe unique ID of the user.\n<property>\tEach property of your track calls creates its own column, and the column type is automatically inferred from your data.\nQuerying the Events tables\n\nTo see a list of the event tables for a given <source>, run the following:\n\nSELECT schema as source, \"table\" as Event\nFROM disk\nWHERE schema = '<source>'\n  AND \"table\" != 'aliases'\n  AND \"table\" != 'groups'\n  AND \"table\" != 'identifies'\n  AND \"table\" != 'pages'\n  AND \"table\" != 'screens'\n  AND \"table\" != 'tracks'\nORDER BY \"table\"\n\nSOURCE\tEVENT\nproduction\tsigned_up\nproduction\tcompleted_order\n\u2026\t\u2026\n\nTo see a list of the columns in one of your event tables, run the following:\n\nSELECT column_name AS Columns\nFROM columns\nWHERE schema_name = '<source>'\nAND table_name = '<event>'\nORDER by column_name\n\nTracks vs. Events Tables\n\nTo see the tables for your organization, you can run this query:\n\nSELECT schema || '.' || \"table\" AS table, rows\nFROM disk\nORDER BY 1\n\n\nThe source.event tables have the same columns as the source.track tables, but they also include columns specific to the properties of each event.\n\nIf you\u2019re recording an event like:\n\nanalytics.track('Register', {\n  plan: 'Pro Annual',\n  accountType: 'Facebook'\n});\n\n\nThen you can expect to see columns named plan and account_type as well as the default event, id, and so on. That way, you can write queries against any of the custom data sent in track calls.\n\nNote\n\nBecause Segment adds properties and traits as un-prefixed columns to your tables, there is a chance the names can collide with the reserved column names. For this reason, Segment discards properties with the same name as the reserved column name (for example, user_id).\n\nYour event tables are one of the more powerful datasets in Segment SQL. They allow you to see which actions users perform when interacting with your product.\n\nBecause every source has different events, what you can do with them will vary. Here\u2019s an example where you can see the number of \u201cEnterprise\u201d users signed up for each day:\n\nSELECT DATE(sent_at) AS Day, COUNT(DISTINCT(user_id)) AS Users\nFROM <source>.signed_up\nWHERE account_type = 'Enterprise'\nGROUP BY day\nORDER BY day\n\nDAY\tUSERS\n2015-01-14\t258\n2015-01-15\t320\n2015-07-21\t190\n\u2026\t\u2026\n\nHere\u2019s an example that queries the daily revenue for an ecommerce store:\n\nSELECT DATE(sent_at) AS Day, SUM(total) AS Revenue\nFROM <source>.completed_order\nGROUP BY day\nORDER BY day\n\nDAY\tREVENUE\n2014-07-19\t$2,630\n2014-07-20\t$1,595\n2014-07-21\t$2,350\nSchema Evolution and Compatibility\nNew Columns\n\nNew event properties and traits create columns. Segment processes the incoming data in batches, based on either data size or an interval of time. If the table doesn\u2019t exist we lock and create the table. If the table exists but new columns need to be created, we perform a diff and alter the table to append new columns.\n\nWhen Segment process a new batch and discover a new column to add, we take the most recent occurrence of a column and choose its datatype.\n\nData Types\n\nThe data types that Segment currently supports include:\n\ntimestamp\ninteger\nfloat\nboolean\nvarchar\n\nData types are set up in your warehouse based on the first value that comes in from a source. For example, if the first value that came in from a source was a string, Segment would set the data type in the warehouse to string.\n\nIn cases where a data type is determined incorrectly, the support team can help you update the data type. As an example, if a field can include float values as well as integers, but the first value we received was an integer, we will set the data type of the field to integer, resulting in a loss of precision.\n\nTo update the data type, reach out to the Segment support team. They will update the internal schema that Segment uses to infer your warehouse schema. Once the change is made, Segment will start syncing the data with the correct data type. However, if you want to backfill the historical data , you must drop the impacted tables on your end so that Segment can recreate them and backfill those tables.\n\nTo request data types changes, please reach out to Segment Support for assistance, and provide with these details for the affected columns in the following format: <schema_name>.<table_name>.<column_name>.<current_datatype>.<new_datatype>\n\nColumn Sizing\n\nAfter analyzing the data from dozens of customers, we set the string column length limit at 512 characters. Longer strings are truncated. We found this was the sweet spot for good performance and ignoring non-useful data.\n\nSegment uses special-case compression for some known columns, like event names and timestamps. The others default to LZO. Segment may add look-ahead sampling down the road, but from inspecting the datasets today this would be unnecessarily complex.\n\nTimestamps\n\nThe Segment API associates four timestamps with every call: timestamp, original_timestamp, sent_at and received_at.\n\nAll four timestamps pass through to your Warehouse for every ETL\u2019d event. In most cases the timestamps are close together, but they have different meanings which are important.\n\ntimestamp is the UTC-converted timestamp which is set by the Segment library. If you are importing historical events using a server-side library, this is the timestamp you\u2019ll want to reference in your queries.\n\noriginal_timestamp is the original timestamp set by the Segment library at the time the event is created. Keep in mind, this timestamp can be affected by device clock skew. You can override this value by manually passing in a value for timestamp which will then be relabeled as original_timestamp. Generally, this timestamp should be ignored in favor of the timestamp column.\n\nsent_at is the UTC timestamp set by library when the Segment API call was sent. This timestamp can also be affected by device clock skew.\n\nreceived_at is UTC timestamp set by the Segment API when the API receives the payload from client or server. All tables use received_at for the sort key.\n\nSegment recommends using the received_at timestamp for all queries based on time. The reason for this is two-fold. First, the sent_at timestamp relies on a client\u2019s device clock being accurate, which is generally unreliable. Secondly, Segment sets received_at as the sort key in Redshift schemas, which means queries will execute much faster when using received_at. You can continue to use timestamp or sent_at timestamps in queries if received_at doesn\u2019t work for your analysis, but the queries will take longer to complete.\n\nFor Business Tier customers, Segment suggests enabling received_at in the Selective Sync settings to ensure syncs and backfills complete successfully.\n\nreceived_at does not ensure chronology of events. For queries based on event chronology, timestamp should be used.\n\nISO-8601 date strings with timezones included are required when using timestamps with Engage. Sending custom traits without a timezone included in the timestamp will result in the value not being saved.\n\nTo learn more about timestamps in Segment, read our timestamps overview in the Segment Spec.\n\nid\n\nEach row in your database will have an id which is equivalent to the messageId which is passed through in the raw JSON events. The id is a unique message id associated with the row.\n\nuuid, uuid_ts, and loaded_at\n\nThe uuid column is used to prevent duplicates. You can ignore this column.\n\nThe uuid_ts column is used to keep track of when the specific event was last processed by our connector, specifically for deduping and debugging purposes. You can generally ignore this column.\n\nThe loaded_at column contains the UTC timestamp reflecting when the data was staged by the processor. This column is created only in BigQuery warehouse.\n\nSort Key\n\nAll tables use received_at for the sort key. Amazon Redshift stores your data on disk in sorted order according to the sort key. The Redshift query optimizer uses sort order when it determines optimal query plans.\n\nMore Help\n\nHow do I send custom data to my warehouse?\n\nHow do I give users permissions to my warehouse?\n\nHow frequently does data sync to my warehouse?\n\nCheck out our Frequently Asked Questions about Warehouses and a list of helpful Redshift queries to get you started.\n\nThis page was last modified: 17 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nWarehouse tables\nIdentifies table\nGroups table\nPages and Screens tables\nTracks table\nEvent Tables\nTracks vs. Events Tables\nSchema Evolution and Compatibility\nTimestamps\nid\nuuid, uuid_ts, and loaded_at\nSort Key\nMore Help\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nReverse Etl\n/\nManage Reverse ETL Syncs\nManage Reverse ETL Syncs\n\nView your sync history, reset your syncs, or subscribe to alerts.\n\nSync overview\n\nThe Reverse ETL sync overview tab, located under Connections > Destinations, gives you an overview of your latest Reverse ETL syncs.\n\nYou can view the following information about each sync:\n\nLatest sync: The status of your latest sync. Syncs can either be In progress, Successful, or Failed.\nMapping: The named mapping.\nModel: The model that extracts data from your warehouse.\nAction: The action that your destination uses to map information from your warehouse to your downstream destination.\nMapping status: The status of your mapping - either Enabled or Disabled.\nSync history\n\nCheck the status of your data extractions and see details of your syncs. Click into failed records to view additional details on the error, sample payloads to help you debug the issue, and recommended actions.\n\nTo check the status of your extractions:\n\nNavigate to Connections > Destinations and select the Reverse ETL tab.\nSelect the destination you want to view.\nSelect the mapping you want to view.\nClick the sync you want to view to get details of the sync. You can view:\nThe status of the sync.\nDetails of how long it took for the sync to complete.\nHow many total records were extracted, as well as a breakdown of the number of records added, updated, and deleted.\nThe load results - how many successful records were synced as well as how many records were updated, deleted, or are new.\nIf your sync failed, click the failed reason to get more details on the error and view sample payloads to help troubleshoot the issue.\nAutomatic retry handling\n\nAutomatic retry handling might not yet be available in your workspace\n\nTo ensure overall system stability and performance, Segment is releasing automatic retry handling to all workspaces in a phased rollout program. Segment expects this feature to be available to all customers by January 31, 2025.\n\nSegment automatically retries events that were extracted from your data warehouse but failed to load for up to 14 days or 5 syncs following a partially successful sync or a sync failure.\n\nSegment checks for the latest changes in your data before loading the failed records on a subsequent (automatically scheduled or manually triggered) sync to ensure the data loaded into Segment isn\u2019t stale and only the latest version of the data is loaded to destination. If the error causing the load failure is coming from an upstream tool, you can fix the error in the upstream tool to resolve the load error on a subsequent sync.\n\nSyncs with intervals less than or equal to two hours may not see failed events on the sync immediately following failed record\n\nSyncs with intervals less than or equal to two hours may not see failed events right away, as Segment\u2019s internal systems take up to two hours to retry events that initially failed.\n\nReset syncs\n\nReverse ETL uses the Unique Identifier column to detect data changes, like new, updated, and deleted records. If you encounter an error, you can reset Segment\u2019s tracking of this column and force Segment to manually add all records from your dataset.\n\nTo reset a sync:\n\nSelect the three dots next to Sync now.\nSelect Reset sync.\nClick I understand what happens when I reset a sync state.\nClick Reset sync.\nCancel syncs\n\nYou can cancel syncs when your sync is currently running during the extraction and load phase.\n\nTo cancel a sync:\n\nNavigate to Connections > Destinations > Reverse ETL.\nSelect the mapping with a sync that is in progress.\nSelect the sync that is in progress.\nClick Cancel sync to cancel the sync.\nSelect the reason for canceling the sync.\n\nYour canceled syncs with have a status as Canceled, and any syncs that are in the process of being canceled will have a status of Canceling.\n\nOnce you cancel a sync, the record count under Extraction Results reflects the records already processed. These records won\u2019t be included in future syncs. To reprocess these records, you can reset or replay the sync.\n\nReplays\n\nYou can choose to replay syncs. To replay a specific sync, contact friends@segment.com. Keep in mind that triggering a replay resyncs all records for a given sync.\n\nAlerting\n\nYou can opt in to receive email, Slack, and in-app alerts about Reverse ETL sync failures and fluctuations in the volume of events successfully delivered to your mapping.\n\nThe notification channels that you select for one alert will apply to all alerts in your workspace.\n\nFailed or partially successful syncs\n\nTo subscribe to alerts for a failed or partially successful sync:\n\nNavigate to Settings > User Preferences.\nSelect Reverse ETL in the Activity Notifications section.\nClick the Reverse ETL sync status that you\u2019d like to receive notifications for. You can select one or more of the following sync statuses:\nReverse ETL sync failed: Receive a notification when your Reverse ETL sync fails.\nReverse ETL sync partial success: Receive a notification when your Reverse ETL sync is partially successful.\nSelect one or more of the following alert options:\nEnable email notifications: Enter an email address or alias that should receive alerts.\nEnable Slack notifications: Enter a webhook URL and Slack channel name.\nEnable in-app notifications: Select this option to see an in-app notification.\nClick Create alert.\n\nIf you opted to receive notifications by email, you can click View active email addresses to see the email addresses that are currently signed up to receive notifications.\n\nMapping-level successful delivery rate fluctuations\n\nYou can create an alert that notifies you when the volume of events successfully received by your mapping in the last 24 hours falls below a percentage you set. For example, if you set a percentage of 99%, Segment notifies you if your destination had a successful delivery rate of 98% or below.\n\nTo receive a successful delivery rate fluctuation alert in a Slack channel, you must first create a Slack webhook. For more information about Slack webhooks, see Slack\u2019s Sending messages using incoming webhooks documentation.\n\nTo subscribe to alerts for successful delivery fluctuations at the mapping level:\n\nNavigate to your intended mapping and select the Alerts tab.\nClick Create alert.\nSet an alert threshold, or the percentage of successfully delivered events that would prompt an alert.\nSelect one or more of the following notification channels:\nEmail: Enter an email address or alias that should receive alerts.\nSlack notification: Enter a Webhook URL and a Slack channel name to receive alerts in a Slack channel.\nIn-app notifications: Select this to receive notifications in the Segment app. To view your notifications, select the bell next to your user icon in the Segment app.\nToggle the Enable alert setting on and click Create.\n\nTo edit or disable your alert, navigate to your mapping\u2019s Alerts tab and select the Actions menu for the alert you\u2019d like to edit.\n\nThis page was last modified: 12 Dec 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nSync overview\nSync history\nAutomatic retry handling\nReset syncs\nCancel syncs\nReplays\nAlerting\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nPrivacy\n/\nHIPAA Eligible Segment\nHIPAA Eligible Segment\nFREE X\nTEAM X\nBUSINESS \u2713\nADDON \u2713\n?\n\nSegment is a HIPAA eligible platform, and meets the data privacy and security requirements of healthcare customers and their stakeholders. For more information about Segment becoming HIPAA eligible, see the announcement blog post.\n\nBusiness Associate Addendum\n\nTwilio BAA are available to customers on a Business Tier plan.\n\nBefore you begin, check that the Segment products and services you\u2019ll use for your HIPAA workflows are on the list of Twilio\u2019s HIPAA Eligible Products and Services. After you\u2019ve verified availability, contact your Account Expert to request a demo.\n\nVerify your Workspace\n\nEnsure your Workspace is eligible for HIPAA before you configure and send any personal health information (PHI).\n\nIn your Workspace, navigate to Settings > Workspace Settings.\nOn the General Settings tab, ensure that the HIPAA badge appears. This badge confirms that the Workspace is HIPAA eligible.\n\nWith the BAA signed and Workspace confirmed as eligible, you can start building. For more information about starting a HIPAA compliant implementation, see Twilio\u2019s Architecting for HIPAA on Twilio, which outlines the shared responsibilities and requirements for building and maintaining HIPAA-compliant workflows in Segment.\n\nHIPAA Auditing\n\nSegment maintains audit logs of every read and update action a user performs in the Segment app that may involve PHI/PII.\n\nData captured in the HIPAA audit logs includes:\n\nworkspace_id: unique identifier of the workspace\nactor_user_id: unique identifier Segment assigns to the logged in user\nevent_type: The action performed by the user. For example, Source Debugger Raw Viewed, Destination Filter Modified, or other events\nend_user_id: Segment sometimes assigns this unique identifier to an end-user, event, audience, or journey, depending on the event type\ntimestamp: Time in UTC when the action occurred\n\nThese logs can be provided upon request. For specific requests, please reach out to friends@segment.com.\n\nData encryption\n\nSegment encrypts the data in select fields marked as yellow in the Privacy Portal before sending them to event stream, cloud mode destinations, further supporting HIPAA compliance in your destinations. Segment encrypts data using a RSAES OAEP SHA 256 algorithm.\n\nData encryption does not support \u201cfuzzy matching\u201d. You can encrypt Default PII matchers, Custom PII matchers, and any Synonyms you\u2019ve created for keys.\n\nData encryption only supports event-stream, cloud-mode destinations\n\nOnly data fields in context, traits, and property objects can be encrypted.\n\nAfter Segment encrypts the data, the encrypted data value is always a string. Any downstream validation that looks for integer or boolean data types will fail for encrypted values.\n\nConfigure data encryption for a new destination\n\nTo configure data encryption while setting up a new destination:\n\nFrom the Destinations page in the Segment App, click Add destination.\nSelect a destination from the catalog and click Configure.\nOn the destination\u2019s overview page, click Add destination.\nOn the Select data source page, select the source you want to connect to your destination and click Next.\nOn the Setup page, give your destination a name, fill in any optional settings, and select the Have Segment encrypt sensitive data checkbox.\nOpen the Fields dropdown, select one or more fields you\u2019d like to encrypt and click the Generate Encryption Keys button. You can select Default PII matchers, Custom PII matchers, and any Synonyms you\u2019ve created for keys. Data encryption does not support \u201cfuzzy matching\u201d.\nIf you don\u2019t see all of the fields that you want to encrypt, change the classification of your missing data fields to Yellow in the Privacy Portal.\n\nSecurely store your private key.\nNote: Once you finish setting up the destination, you cannot retrieve the key.\nClick Create destination.\n\nPrivate Key is not recoverable\n\nSegment does not save the private key created during the data encryption setup flow, and cannot retrieve the key after you finish setting up your destination. You can generate a new key using the instructions in the Configure new key pairs section. Any data encrypted prior to generating a new key pair cannot be decrypted with the new key.\n\nConfigure data encryption for an existing destination\n\nTo configure data encryption for an existing destination:\n\nOpen the My destinations page in the Segment app.\nSelect a destination, and click the Data Encryption tab.\nOn the Data Encryption page, select the Have Segment encrypt sensitive data checkbox.\nOpen the Fields dropdown, select one or more fields you\u2019d like to encrypt and click the Generate Encryption Keys button. You can select Default PII matchers, Custom PII matchers, and any Synonyms you\u2019ve created for keys. Data encryption does not support \u201cfuzzy matching\u201d.\nIf you don\u2019t see all of the fields that you want to encrypt, change the classification of your missing data fields to Yellow in the Privacy Portal.\n\nSecurely store your private key.\nNote: Once you finish setting up the destination, you cannot retrieve the key.\nClick Save.\n\nPrivate Key is not recoverable\n\nSegment does not save the private key created during the data encryption setup flow, and cannot retrieve the key after you finish setting up your destination. You can generate a new key using the instructions in the Configure new key pairs section. Any data encrypted prior to generating a new key pair cannot be decrypted with the new key.\n\nConfigure new key pairs\n\nIf you lose access to your private key, you can generate a new key pair in your destination\u2019s Data Encryption tab. Any data previously encrypted using the previous key pair is unaffected, but cannot be decrypted using the new key.\n\nTo generate a new key pair:\n\nOpen the My destinations page in the Segment app.\nSelect the destination you\u2019d like to create new keys for and click Data Encryption.\nClick Regenerate Encryption Keys.\nSecurely store your private key.\nNote: Once you finish setting up the destination, you cannot retrieve the key.\nClick Save Changes to update the key pair.\nEdit encrypted fields\n\nAfter enabling encryption for a destination, you can add or remove encrypted data fields in your destination\u2019s Data Encryption tab. All changes made to fields are forward-looking. You may experience some latency between making the changes and having the changes take effect.\n\nTo make changes to your selected fields:\n\nOpen the My destinations page in the Segment app.\nSelect the destination you\u2019d like to edit your selected fields for and click Data Encryption.\nAdd or remove fields. You can select Default PII matchers, Custom PII matchers, and any Synonyms you\u2019ve created for keys. Data encryption does not support \u201cfuzzy matching\u201d.\nTo add fields, click the Fields box to open the dropdown and select the fields you\u2019d like to add.\nTo remove fields, click the x icon next to the name of the field you\u2019d like to remove.\nClick Save Changes.\nRemove encryption\n\nDisabling the data encryption setting removes encryption on all previously configured data.\n\nTo remove encryption from incoming data:\n\nOpen the My destinations page in the Segment app.\nSelect a destination, and click Data Encryption.\nOn the Data Encryption page, deselect the Have Segment encrypt sensitive data checkbox.\nOn the Turn off data encryption? popup, click Confirm.\n\nDisabling the data encryption setting does not decrypt existing data, but does prevent any future data from being encrypted.\n\nUser session timeouts\n\nSegment automatically logs out all users with access to HIPAA eligible workspaces after 15 minutes of inactivity.\n\nThis page was last modified: 25 Jan 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nBusiness Associate Addendum\nVerify your Workspace\nHIPAA Auditing\nData encryption\nUser session timeouts\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nGuides\n/\nUsage And Billing\n/\nMTUs, Throughput and Billing\nMTUs, Throughput and Billing\n\nThe graphic illustrates an example billing model with data flowing through Segment within a monthly period. Each event on the different touch points (for example, Sign-ups or Product Added) is calculated as one API call.\n\nSegment detects that the user across two of the touch points is the same user based on their userID (userID 123) and deduplicates them, counting for one MTU.\n\nWith Engage, this user falls into one audience, has one computed trait, and falls into one Journeys step, accounting for three compute credits total. Compute credits are not tied to an individual user, so multiple people could fall into these buckets, still accounting for a single compute credit for each trait/audience/journey step.\n\nFinally, the example sends some user events to a destination function, which is charged according to function execution time.\n\nWhat is an MTU?\n\nMTU stands for \u201cmonthly tracked user\u201d. (Keep reading to learn how Segment counts MTUs.)\n\nWhat is an API call?\n\nWhen you use Segment to collect your data, you use the Segment tracking methods (Track, Page, Screen, Identify, Group, and Alias) which collect the data in a structured way, and then send it to api.segment.io. If you are using the Segment HTTP API, or sending batched data using a cloud-source, that data also goes through this Segment API endpoint.\n\nEach data blob (with its properties or traits) goes through this endpoint, and is considered one \u201cAPI call\u201d.\n\nWhat is throughput?\n\nDepending on your Segment account type, your plan might include a throughput limit. The throughput limit tells you how many API calls and objects Segment allows you per MTU.\n\nFor example, if your workspace\u2019s throughput limit is set to 250, this means that you can send a combined total of 250 API calls and objects to Segment each month per MTU you\u2019ve paid for in your plan. If you have a 10,000 MTU plan, this means you can send up to a total of 2.5 million API calls and objects each month.\n\nThese objects and API calls are not tied to a specific user, but are an aggregate number applied to your workspace. Most customers never hit this limit, and Business tier plans often have custom limits.\n\nBatching and throughput limits\n\nYou can sometimes \u201cbatch\u201d API calls to reduce send times, however batching doesn\u2019t reduce your throughput usage. Batched calls are unpacked as they are received, and the objects and calls the batch contains are counted individually. While batching does not reduce your throughput, it does reduce the possibility of rate limit errors.\n\nHow does Segment calculate MTUs?\n\nSegment counts the number of unique userIds, and then adds the number of unique anonymousIds that were not associated with a userId during the billing period. Segment counts these IDs over all calls made from all sources in your workspace, over a billing month. Segment only counts each user once per month, even if they perform more than one action or are active across more than one source.\n\nExample MTU counts\n\nImagine that you have both a website and a mobile app. Both the website and mobile app have pages that you can use without being logged in, and both send Identify calls when a user does log in.\n\nDeduplication across sources\n\nAs a simple example, imagine that a user is already logged in on both the website and the mobile app. When the user\u2019s activity generates events on the website, these events are sent using Analytics.js, and include the user\u2019s userId. When they do things on the mobile app, these events are sent from a mobile source, and also include the userId. When Segment counts the MTUs, all the events from the same userId only generate one MTU, regardless of the source it came from.\n\nDeduplication after log-in\n\nNow imagine a new user, who has never logged in. At first, they have two anonymousIds, one for the mobile app and one for the website. However, if they log in during the course of the month, you now know who they are, and can attach their anonymousId to a userId.\n\nIf the user logs in on just the app, you would still see two MTUs: one anonymousId for the website source, and one anonymousId with an attached userId from the mobile app source. If the user logs in on both the app and website, they would count as one MTU: two different anonymousIds attached to one userId.\n\nHow do I see my usage data?\n\nIf you have questions about your data usage or how it relates to your bill, log into your Segment workspace, click Settings > Usage and Billing > Usage.\n\nThe Usage page shows what plan the workspace is on, what data volume that plan includes, and how much data you have already used in the current billing period. If you have used more data volume than your plan includes, the page shows information about how much data is in overage.\n\nClick the billing period dropdown at the top of the page to see a cumulative daily report of data volumes (by source) for the current billing period. The last five billing periods are also available, along with an overview of the last twelve months of data volumes.\n\nWhat is the difference between an event and an object?\n\nUnderstanding the difference between events and objects helps you understand how MTUs are calculated.\n\nAn event is a data collection triggered in response to a user action: a Track call (or a Page/Screen call if the action was to navigate to a new page). Events take place in a single moment in time, and include a name, timestamp, and properties. When an event happens more than once, it creates a new Event record (with a new timestamp) rather than updating an existing one. For example, a user browsing a product catalog might generate several \u201cProduct Viewed\u201d events, which might include the product name, price, and category.\n\nThis is in contrast to \u201cObjects\u201d which represent a single thing that persists over time and can be updated. Objects have \u201ctraits\u201d (instead of properties) which record information about that object, and which can change over time. For example a \u201cuser\u201d object could have a trait of \u201cemail\u201d which doesn\u2019t change often, but could also have a computed trait like logged_in_last_7_days that changes between true and false based on how much they use your site.\n\nHow is object throughput calculated?\n\nObject Cloud Sources retrieve records from integration partners on a scheduled basis. Segment processes these records before writing them out to connected Storage Destinations. Segment counts one object for each record retrieved from a Cloud Source. The number of objects ingested during a billing period has a direct impact on throughput, which is calculated as (objects ingested + API calls received) / MTU allowance.\n\nDepending on the capabilities of the partner\u2019s API, Segment may need to retrieve all available records and then deduplicate them prior to writing them out to a connected storage destination. In such cases, all retrieved records are still counted as ingested objects, even if the same records are retrieved multiple times in a given billing period. If you experience overages due to high object throughput, contact friends@segment.com to request a less frequent sync cadence.\n\nMTUs, object throughput, and Cloud sources\n\nIf you use Cloud sources to pull in data from your third party services (in addition to tracking your users with Segment library sources), the data from these cloud apps can increase your MTU counts and object counts.\n\nThere are two types of cloud sources: object sources, and event sources. Object sources bring in information about entities, such as a person or company, which can change and have their properties updated over time. Events happen once in time, so while their properties don\u2019t change, they can also happen more than once over time. (See above for more on objects vs events.)\n\nObject sources do not increase your MTU count because the data included doesn\u2019t usually contain an ID. Object sources can only send to Warehouses, and do affect the total object count which is used to calculate your throughput. Some examples of object-sources are Salesforce, Zendesk, and Stripe.\n\nEvent sources can create new MTUs because each event coming from this source includes either a userId or an anonymousId associated with the event. Some examples of event sources are Vero, Drip, and Youbora.\n\nTip! You can check the Collections section of a cloud-source\u2019s Segment documentation to see what type of data it sends. The Collections table lists each data type sent from the cloud source, and tells you if that data is an Object or an Event.\n\nMTUs and Protocols\n\nProtocols is a Business Tier feature. If you are on a Free or Team plan, this section does not apply to you.\n\nSegment\u2019s Protocols product allows you to selectively filter and block your incoming data to prevent malformed data from reaching destinations including your data warehouses and other storage solutions.\n\nTracking plan blocking: Blocked events are blocked from sending to all Segment Destinations, including warehouses and streaming Destinations. They\u2019re blocked from reaching the entire Segment data pipeline. When you block an Event using a Tracking Plan, it does not count towards your MTU limit.\n\nBlocked events (sometimes called \u201cviolations\u201d) only count toward your MTU limit if you enable blocked event forwarding in your Source settings. You might do this to monitor issues with your incoming data as you continue to develop your tracking.\n\nIf you enable violation forwarding, it generates one (1) additional MTU in your workspace, total. If you are on an API billing plan, you are charged for the increased API volume generated by the forwarded violations. Forwarded violations might also generate costs in downstream destinations and data warehouses connected to the violations source.\n\nMTUs and Engage\n\nEngage is a Business Tier addon feature. If you are on a Free or Team plan this section does not apply to you (because you do not have this feature).\n\nAll Engage data are omitted from billing MTU and API throughput calculations, including computed traits, SQL traits, and audiences.\n\nMTUs and Replays\n\nReplay is a Business Tier feature. If you are on a Free or Team plan, this section does not apply to you.\n\nReplays only affect your MTU count if you are using a Repeater destination, which might send data that hasn\u2019t yet been seen this month back through a source.\n\nMTUs and Reverse ETL\n\nSee the Reverse ETL usage limits to see how MTUs affect your Reverse ETL usage limits.\n\nWhy is my MTU count different from what I see in my destinations and other tools?\n\nDifferent tools count users under different conditions, so comparing numbers between any two tools, or between Segment and a tool, rarely produces the same number. Each tool accepts slightly different incoming data, and they often reject or process the incoming data differently. Included are some example explanations of why you might see differing numbers below.\n\nContact Segment Product Support if for more information about a specific tool, or if you\u2019re concerned that differing numbers might be due an implementation error.\n\nGoogle Analytics\n\nGoogle Analytics requires that you include a url in any Page\u00a0calls from a Segment server library. If you don\u2019t include a url, Google Analytics silently rejects the call, which can reduce the number of users you see in GA.\n\nSegment does not pass data from\u00a0Identify calls to Google because it is against Google\u2019s terms of service to pass Personally Identifiable Information (PII) to the Google Analytics reporting interface. If you need to pass data from an Identify call, you can set up a Custom Dimension mapping to override this.\n\nTo pass the\u00a0userId\u00a0from your\u00a0Identify calls to Google Analytics, go to the Google Analytics destination settings in the Segment web app, locate the Advanced Google Analytics settings, and enable\u00a0Send User-ID to GA.\n\nAmplitude\n\nBy default, Segment doesn\u2019t send standard\u00a0Page calls\u00a0or\u00a0Screen calls to Amplitude, which might reduce the number of unique users Amplitude sees.\n\nTo send Page and Screen calls to Amplitude, go to the Amplitude destination settings in the Segment web app, and locate the Advanced Options tab.\n\nAmplitude can only automatically link an anonymous user to their logged-in userId if the events or traits come from a device-mode source (such as Analytics.js or a mobile library). If you use a server library or the Segment HTTP API, Amplitude can\u2019t automatically connect the anonymous user to their logged-in identity. To work around this so Amplitude can connect the anonymous and identified user, make your Identify call when the user logs in, and include both the\u00a0anonymousId\u00a0from before the user logged in and the userId the user provided at log-in.\n\nFor Amplitude to associate both client-side and server-side activity with the same user, you must pass the same\u00a0deviceId\u00a0to Amplitude. Otherwise, Amplitude creates two users - one associated with the user\u2019s deviceId\u00a0and another user associated with the user\u2019s Segment\u00a0anonymousId.\n\nWhat might cause a spike in my MTU count?\n\nThere are several reasons why you might see a sudden increase in MTUs. Most of them are due to traffic fluctuations, however, some changes you make in code might also increase your MTU count, usually because you are (unexpectedly) generating a new anonymousId or userId for a single user.\n\nIf you think an implementation problem is causing an increase in your MTU count, contact Segment Product Support as soon as possible for help troubleshooting and resolving the issue.\n\nChanges in traffic\n\nMTU counts usually increase when the number users or visitors to parts of your site or application that use Segment tracking increase. Sometimes you\u2019ll see a spike when you post a big press release, or marketing campaign that leads to an influx of visitors. Another potential cause of big increases is adding tracking to parts of your site or app that didn\u2019t have tracking before.\n\nChanges to imported sources\n\nAnother possibility is an increase in the number of interactions with your users outside your app (emails, help desk, push notifications, etc) that you are importing using cloud sources. Tracking users you weren\u2019t tracking before increases your MTU count unless you are able to pass a userId so they can be resolved with existing users. If you\u2019re already tracking those users elsewhere with Segment, they are not counted a second time.\n\nUser behavior\n\nUsers who are very privacy-conscious might cause your tracking to generate more MTUs; however in most cases these users are a fraction of a percentage of total traffic.\n\nIf the user visits the website from a different browser, each browser generates a different anonymousId. If these are not linked to a userId they continue to count as new MTUs.\nIf the user visits the page in Incognito mode, the browser generates a new anonymousId for each incognito session. These IDs are discarded at the end of the session.\nIf the user manually clears their browser cookies, this removes any Segment tracking data they may have gathered, including the userId and anonymousIds. When they next visit your site they generate all new anonymousIds and tracking information. This new information isn\u2019t resolved with existing tracked user records until you can attach a userId to them.\nCalling reset\n\nCheck to see if you changed how you call analytics.reset(). This utility method clears the old user identity information, and generates a new anonymousId each time you call it. This creates a user that Segment cannot resolve with an existing user until they are further identified.\n\nOverwriting an existing identity\n\nSegment\u2019s analytics libraries include methods that allow you to overwrite both the userId (using identify(xxx)) and anonymousId (using analytics.user().anonymousId(xxx)). Using these methods on a user whose tracking information already includes an ID can cause the user to be counted more than once.\n\nIf you find you need to use one of these overwrite methods, you should check to make sure that the field you are changing is null first. If the field is not null, you probably don\u2019t want to overwrite it and lose the user\u2019s original tracked identity.\n\nCross-domain issues\n\nIf the pages you track are on more than one domain (for example, mydomain.com and mydomain.net), the user generates a new anonymousId for each domain. However, if the domain is a subdomain (for example mydomain.com and app.mydomain.com), they can share a user cookie and so share identity data and count as only one MTU.\n\nIf the user goes from one page to another and the second page loads in an iFrame, the page in the iFrame generates its own anonymousId.\n\nWhere can I find information about Twilio Engage Channels billing?\n\nSegment does not bill for SMS and Email sends from Engage Channels. For actual billed usage, refer to the Twilio and SendGrid accounts that you\u2019ve linked to Engage.\n\nThis page was last modified: 28 Mar 2023\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nWhat is an MTU?\nWhat is an API call?\nWhat is throughput?\nHow does Segment calculate MTUs?\nHow do I see my usage data?\nWhat is the difference between an event and an object?\nHow is object throughput calculated?\nMTUs, object throughput, and Cloud sources\nMTUs and Protocols\nMTUs and Engage\nMTUs and Replays\nMTUs and Reverse ETL\nWhy is my MTU count different from what I see in my destinations and other tools?\nWhat might cause a spike in my MTU count?\nWhere can I find information about Twilio Engage Channels billing?\n\nWas this page helpful?\n\n Yes\n No",
        "Log in\nSign Up\nHome\n/\nConnections\n/\nSpec\n/\nNative Mobile Spec\nNative Mobile Spec\n\nOne of the core components of the Segment Spec is the Track method. It records any arbitrary event that the user has triggered. For Mobile tracking, in addition to Screen calls, you\u2019ll want to send specific event names that Segment recognizes semantically. That way, Segment can transform them correctly before sending them off to downstream destinations.\n\nBy standardizing the events that comprise the core mobile application lifecycle and associated mobile campaign and referral events, Segment and its partners can, wherever possible, forward these events on your behalf and build downstream destinations that take full advantage of the semantic meaning associated with these events and their properties.\n\nIf you\u2019re already collecting similar events, Segment recommends migrating to these event names so that you can take advantage of available features in Segment destinations that depend on the spec as they become available.\n\nThese events pair nicely with Segment\u2019s ecommerce spec for mobile marketplaces to take full advantage of features like dynamic ads in Facebook and the ability to take full advantage of server-side destinations with Mobile Attribution Platforms like Tune and Kochava.\n\nPer the Privacy Policy and applicable terms, don\u2019t send Segment sensitive personal information about your users. Certain features from Segment and its partners allow you to opt-in to automatically track data (for example: Application Installed or Deep Link Clicked). When working with these features and Segment in general, be cognizant of the data that is being tracked to ensure its matching both your obligations under your agreement with Segment and the privacy expectations of your users.\n\nOverview of events\n\nThe Segment Native Mobile Spec includes the following semantic events:\n\nApplication Lifecycle Events\n\nApplication Installed\nApplication Opened\nApplication Backgrounded\nApplication Foregrounded\nApplication Updated\nApplication Uninstalled\nApplication Crashed\n\nCampaign Events\n\nInstall Attributed\nPush Notification Received\nPush Notification Tapped\nPush Notification Bounced\nDeep Link Opened\nDeep Link Clicked\n\nSegment recommends using the above event names if you\u2019re going to be integrating the events yourself. This will ensure that they can be mapped effectively in downstream tools.\n\nLifecycle events\n\nMobile applications live within a fairly bounded lifecycle. To understand and communicate effectively with your users, it\u2019s crucial to instrument the core flows associated with installing and opening your app. The following events allow you to get a picture of top-line metrics such as DAUs, MAUs, and Screen Views per session. Automatic lifecycle event tracking is optional - you can learn how to enable and disable them in Segment\u2019s docs for each library below:\n\niOS\nSwift\nAndroid\nKotlin\nReact Native\n\nThe following events will be tracked automatically when lifecycle events are enabled in all mobile libraries:\n\nApplication Installed\nApplication Opened\nApplication Updated\n\nIn Kotlin, Swift, and React Native, the following additional events are tracked:\n\nApplication Backgrounded\n\nIn Swift, the following event is also tracked:\n\nApplication Foregrounded\nApplication Installed\n\nThis event fires when a user first opens your mobile application. Note, if the user never opens your app after installing, Segment will not collect this event. This event doesn\u2019t wait for attribution or campaign information to be received, and is collected automatically by Segment\u2019s SDKs. Advertising providers like Facebook and Google require discrete install events to correctly attribute installs to ads served through their platform.\n\n{\n  \"userId\": \"019mr8mf4r\",\n  \"type\": \"track\",\n  \"event\": \"Application Installed\",\n  \"properties\": {\n    \"version\": \"1.2.3\", \"build\": \"1234\"\n  }\n}\n\nPROPERTY\tTYPE\tDESCRIPTION\nversion\tString\tThe version installed.\nbuild\tString\tThe build number of the installed app.\nApplication Opened\n\nThis event fires when a user launches or foregrounds your mobile application after the first open. It will fire after the Application Installed event and again after the app is re-opened after being closed. This event does not wait for attribution information to be received but may include information about referring applications or deep link URLs if available to the application upon open.\n\n{\n  \"userId\": \"019mr8mf4r\",\n  \"type\": \"track\",\n  \"event\": \"Application Opened\",\n  \"properties\": {\n    \"from_background\": false,\n    \"referring_application\": \"GMail\",\n    \"url\": \"url://location\"\n  }\n}\n\nPROPERTY\tTYPE\tDESCRIPTION\nfrom_background\tBoolean\tIf application transitioned from \u201cBackground\u201d to \u201cInactive\u201d state prior to foregrounding (as opposed to from \u201cNot Running\u201d state).\nurl\tString\tThe value of UIApplicationLaunchOptionsURLKey from launchOptions. Collected on iOS only.\nreferring_application\tString\tThe value of UIApplicationLaunchOptionsSourceApplicationKey from launchOptions.\nversion\tString\tThe version installed.\nbuild\tString\tThe build number of the installed app.\nApplication Backgrounded\n\nThis event should be sent when a user backgrounds the application upon applicationDidEnterBackground.\n\n{\n  \"userId\": \"019mr8mf4r\",\n  \"type\": \"track\",\n  \"event\": \"Application Backgrounded\",\n  \"properties\": {}\n}\n\nApplication Foregrounded\n\nThis event is fired when a user opens the app or brings it back into the foreground of their device. This is only collected by the Swift library.\n\n{\n  \"userId\": \"019mr8mf4r\",\n  \"type\": \"track\",\n  \"event\": \"Application Foregrounded\",\n  \"properties\": {}\n}\n\nApplication Updated\n\nThis event fires when a user updates the application. Segment\u2019s SDK will automatically collect this event instead of an \u201cApplication Opened\u201d event when we determine that the Open is first since an update.\n\n{\n  \"userId\": \"019mr8mf4r\",\n  \"type\": \"track\",\n  \"event\": \"Application Updated\",\n  \"properties\": {\n    \"previous_version\": \"1.1.2\",\n    \"previous_build\": \"1234\",\n    \"version\": \"1.2.0\",\n    \"build\": \"1456\"\n  }\n}\n\nPROPERTY\tTYPE\tDESCRIPTION\nprevious_version\tString\tThe previously recorded version.\nprevious_build\tString\tThe previously recorded build.\nversion\tString\tThe new version.\nbuild\tString\tThe new build.\nApplication Uninstalled\n\nFire this event when a user uninstalls the application. Some destination partners will detect this for you using Silent Push Notifications through their SDK. You might be able to send these events to Segment using a callback. Visit the partner docs to see if this is available.\n\n{\n  \"userId\": \"019mr8mf4r\",\n  \"type\": \"track\",\n  \"event\": \"Application Uninstalled\",\n  \"properties\": {}\n}\n\nApplication Crashed\n\nYou can send this event when you receive a crash notification from your app, but it is not meant to supplant traditional crash reporting tools. By tracking crashes as an analytics event with device and user information, you can analyze the which types of users are impacted by crashes and how those crashes, in turn, affect their engagement. You may also want to target those customers with tailored communications in other channels if they\u2019ve encountered several crashes. Segment does not collect this event. To capture the event, use a destination that collects this data and route that event back to Segment through a webhook or some other callback.\n\n{\n  \"userId\": \"019mr8mf4r\",\n  \"type\": \"track\",\n  \"event\": \"Application Crashed\",\n  \"properties\": {}\n}\n\nCampaign events\n\nAs the walls between apps become increasingly lowered, capturing information about the content and campaigns that drive users to engage with your app is critical to building more targeted, relevant, personalized experiences for your users.\n\nSegment does not collect any campaign events automatically unless configured to do so.\n\nInstall Attributed\n\nWhen Segment or an integrated partner can discern the source of an install, we\u2019ll collect an Install Attributed event. This event may be sent to Segment using server-to-server connection from your attribution provider, or directly on the device using packaged destinations. In either case, this will happen after install, and does not apply to all installs, which is why it is a discrete event.\n\n{\n  \"userId\": \"019mr8mf4r\",\n  \"type\": \"track\",\n  \"event\": \"Install Attributed\",\n  \"properties\": {\n    \"provider\": \"Tune/Kochava/Branch/AppsFlyer\",\n    \"campaign\": {\n      \"source\": \"Network/FB/AdWords/MoPub/Source\",\n      \"name\": \"Campaign Name\",\n      \"content\": \"Organic Content Title\",\n      \"ad_creative\": \"Red Hello World Ad\",\n      \"ad_group\": \"Red Ones\"\n    }\n  }\n}\n\nPROPERTY\tTYPE\tDESCRIPTION\nprovider\tString\tThe attribution provider.\ncampaign[source]\tString\tCampaign source \u2014 attributed ad network.\ncampaign[name]\tString\tThe name of the attributed campaign.\ncampaign[medium]\tString\tIdentifies what type of link was used.\ncampaign[content]\tString\tThe content of the campaign.\ncampaign[ad_creative]\tString\tThe ad creative name.\ncampaign[ad_group]\tString\tThe ad group name.\nPush Notification Received\n\nThis event can be sent when a push notification is received in the app. It can be automatically enabled on iOS.\n\n{\n  \"userId\": \"019mr8mf4r\",\n  \"type\": \"track\",\n  \"event\": \"Push Notification Received\",\n  \"properties\": {\n    \"campaign\": {\n      \"medium\": \"Push\",\n      \"source\": \"Vendor Name\",\n      \"name\": \"Referral Flow\",\n      \"content\": \"Your friend invited you to play a match.\"\n    }\n  }\n}\n\nPROPERTY\tTYPE\tDESCRIPTION\ncampaign[name]\tString\tCampaign name.\ncampaign[medium]\tString\tIdentifies what type of link was used (Push Notification).\ncampaign[content]\tString\tPush notification content.\ncampaign[source]\tString\tDesignates the push provider. (Optional)\nPush Notification Tapped\n\nThis event can be sent when a user taps on a push notification associated with your app. It can be automatically enabled on iOS.\n\n{\n  \"userId\": \"019mr8mf4r\",\n  \"type\": \"track\",\n  \"event\": \"Push Notification Tapped\",\n  \"properties\": {\n    \"action\": \"Accept\",\n    \"campaign\": {\n      \"medium\": \"Push\",\n      \"source\": \"Vendor Name\",\n      \"name\": \"Referral Flow\",\n      \"content\": \"Your friend invited you to play a match.\"\n    }\n  }\n}\n\nPROPERTY\tTYPE\tDESCRIPTION\naction\tString\tIf this notification is \u201cactionable\u201d, the custom action tapped. Default: \u201cOpen\u201d\ncampaign[name]\tString\tCampaign name.\ncampaign[medium]\tString\tIdentifies what type of link was used (Push Notification).\ncampaign[content]\tString\tPush notification content.\ncampaign[source]\tString\tDesignates the push provider. (Optional)\nPush Notification Bounced\n\nThis event fires when a push notification from a provider bounces. If your push notification provider forwards push lifecycle events to Segment, they should include this event in their suite.\n\n{\n  \"userId\": \"019mr8mf4r\",\n  \"type\": \"track\",\n  \"event\":\"Push Notification Bounced\",\n  \"properties\": {\n    \"action\": \"Accept\",\n    \"campaign\": {\n      \"medium\": \"Push\",\n      \"source\": \"Vendor Name\",\n      \"name\": \"Referral Flow\",\n      \"content\": \"Your friend invited you to play a match.\"\n    }\n  }\n}\n\nPROPERTY\tTYPE\tDESCRIPTION\naction\tString\tIf this notification is \u201cactionable\u201d, the custom action tapped. Default: \u201cOpen\u201d\ncampaign[name]\tString\tCampaign name.\ncampaign[medium]\tString\tIdentifies what type of link was used (Push Notification).\ncampaign[content]\tString\tPush notification content.\ncampaign[source]\tString\tDesignates the push provider. (Optional)\nDeep Link Opened\n\nWhen your application is opened using a referring link, Segment or your packaged deep link partner can fire this event on your behalf. If the deep link has additional data associated with it, either passed through the third party service or as annotations in launchOption, you may want to include those values as properties here as well.\n\nThis event is fired in addition to the associated Application Opened event.\n\n{\n  \"userId\": \"019mr8mf4r\",\n  \"type\": \"track\",\n  \"event\": \"Deep Link Opened\",\n  \"properties\": {\n    \"provider\": \"Branch Metrics\",\n    \"url\": \"app://landing\"\n  }\n}\n\nPROPERTY\tTYPE\tDESCRIPTION\nprovider\tString\tThe deep link provider.\nurl\tString\tThe App URL opened.\nDeep Link Clicked\n\nThis event may be provided by deep link providers postback mechanisms or an internal redirect service if you use one in order to provide a waypoint funnel step between your content or advertisement and the resulting app open.\n\n{\n  \"userId\": \"019mr8mf4r\",\n  \"type\": \"track\",\n  \"event\": \"Deep Link Clicked\",\n  \"properties\": {\n    \"provider\": \"Branch Metrics\",\n    \"url\": \"brnch.io/1234\"\n  }\n}\n\nPROPERTY\tTYPE\tDESCRIPTION\nprovider\tString\tThe deep link provider.\nurl\tString\tThe deep link URL clicked.\n\nThis page was last modified: 31 May 2024\n\nNeed support?\n\nQuestions? Problems? Need more info? Contact Segment Support for assistance!\n\nVisit our Support page\nHelp improve these docs!\n Edit this page\n Request docs change\nWas this page helpful?\n Yes\n No\nGet started with Segment\nSegment is the easiest way to integrate your websites & mobile apps data to over 300 analytics and growth tools.\nRequest Demo\nCreate free account\n Edit this page\n Request docs change\n\nOn this page\n\nOverview of events\nLifecycle events\nCampaign events\n\nWas this page helpful?\n\n Yes\n No"
    ],
    "mparticle": [
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nGuides\n\nThis section contains in-depth information about the mParticle platform and features.\n\nFor information about our SDKs, APIs, and tools, visit the Developers section.\nFor information about our latest product releases, visit the mParticle Changelog.\n\nPlatform Guide\n\nLearn to manage your data in the mParticle dashboard.\n\nActivity\nConnections\nData Filter\n\nLearn More\n\nGetting Started\n\nA from-scratch guide to get you started sending data to mParticle and forwarding it on to Event and Audience outputs.\n\nCreate Inputs\nStart capturing data\nConnect an Event Output\nCreate an Audience\n\nLearn More\n\nAnalytics\n\nUncover actionable insights across the entire customer journey without SQL or writing a single line of code.\n\nIntroduction\nSettings\n\nLearn More\n\nIDSync\n\nLearn about mParticle\u2019s premium Identity Management Framework\n\nIntroduction\nUse Cases for IDSync\nComponents of IDSync\n\nLearn More\n\nData Master\n\nExplore every data point in your workspace and manage your data quality.\n\nData Master Introduction\nCatalog\nLive Stream\nData Plan\n\nLearn More\n\nPersonalization\n\nCreate audiences and orchestrate customer journeys.\n\nProfiles\nCalculated Attributes\nAudiences\nJourneys\nPredictive Audiences\n\nLearn More\n\nWarehouse Sync\n\nIngest data from third-party warehouses using mParticle's reverse-ETL solution.\n\nLearn More\n\nData Privacy Controls\n\nCollect and leverage consumer consent and opt-outs towards compliance with GDPR and CCPA.\n\nLearn More\n\nData Subject Requests\n\nExplore how to respond to data subject requests as mandated by the GDPR and CCPA regulations.\n\nLearn More\n\nDefault Service Limits\n\nLearn about the default limits mParticle imposes on incoming data in order to protect the performance of the mParticle dashboard and your app.\n\nLearn More\n\nFeeds\n\nHarness third-party data sources with Feeds.\n\nLearn More\n\nCross-Account Audience Sharing\n\nShare audience data with other accounts within your organization.\n\nLearn More\n\nImport Data with CSV Files\n\nImport bulk data from a data warehouse or legacy system.\n\nLearn More\n\nGlossary\n\nList of mParticle-specific terms and definitions.\n\nLearn More\n\nVideo Index\n\nList of embedded videos.\n\nLearn More\n\nAnalytics (Deprecated)\n\nResources for legacy Analytics customers.\n\nLearn More\n\nDOCS\nLast Updated:\n1/8/2025, 6:32:28 PM\nCDP\nmParticle Docs\nHome\nGuides\nDevelopers\nIntegrations\nAnalytics\nAnalytics Docs\nOverview\nGetting Started\nDevelopers\nExpert Training\nPredictive AI\nCortex Docs\nOverview\nGetting Started\nDevelopers\nUsing Cortex\nLearn More\nBlog\nPodcast\nResources\nCommunity\nChange log\nGet in Touch\nHelp Center\nDemo\nBecome a Partner\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nDocumentation\nDevelopers\nAPI References\nClient SDKs\nMedia SDKs\nServer SDKs\nQuickstart\nTools\nGuides\n\nAPI Credential Management\n\nThe Developer's Guided Journey to mParticle\n\nGuides\nGetting Started\nPlatform Guide\nAnalytics\nIDSync\n\nIDSync Overview\n\nUse Cases for IDSync\n\nComponents of IDSync\n\nStore and Organize User Data\n\nIdentify Users\n\nDefault IDSync Configuration\n\nProfile Conversion Strategy\n\nProfile Link Strategy\n\nProfile Isolation Strategy\n\nBest Match Strategy\n\nAliasing\n\nData Master\nPersonalization\n\nWarehouse Sync\n\nData Privacy Controls\n\nData Subject Requests\n\nDefault Service Limits\n\nFeeds\n\nCross-Account Audience Sharing\n\nApproved Sub-Processors\n\nImport Data with CSV Files\n\nGlossary\n\nVideo Index\n\nAnalytics (Deprecated)\nIDSync Overview\n\nIDSync is the mParticle identity framework, enabling you to create a unified view of your customers, with improved data governance, policy, and security.\n\nIDSync gives visibility into, and control over, the management of known and anonymous user identities across apps and platforms. It allows marketers to accurately identify customers in key moments of their journey in order to support robust targeting and personalization, and to deliver consistent user experiences across all devices, touchpoints, and channels.\n\nWas this page helpful?\n\nYes\nNo\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nDocumentation\nDevelopers\nAPI References\nClient SDKs\nMedia SDKs\nServer SDKs\nQuickstart\nTools\nGuides\n\nAPI Credential Management\n\nThe Developer's Guided Journey to mParticle\n\nGuides\nGetting Started\nPlatform Guide\nAnalytics\nIDSync\nData Master\nPersonalization\n\nWarehouse Sync\n\nData Privacy Controls\n\nData Subject Requests\n\nDefault Service Limits\n\nFeeds\n\nCross-Account Audience Sharing\n\nApproved Sub-Processors\n\nImport Data with CSV Files\n\nGlossary\n\nVideo Index\n\nAnalytics (Deprecated)\nIntegrations\n24i\nAarki\nABTasty\nAbakus\nActable\nAdChemix\nAdikteev\nAdjust\nAdMedia\nAdobe Marketing Cloud\nAdobe Audience Manager\nAdobe Campaign Manager\nAdobe Target\nAdPredictive\nAgilOne\nAlgolia\nAirship\nAlgoLift\nAlooma\nAmazon Kinesis\nAmazon Advertising\nAmazon Kinesis Firehose\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAmazon SNS\nAdobe Marketing Cloud\nAmobee\nAmpush\nAmplitude\n\nForwarding Data Subject Requests\n\nEvent\n\nAnalytics\nAntavo\nAppLovin\nAnodot\nApptentive\nApptimize\nAttractor\nAttentive\nApteligent\nMicrosoft Azure Blob Storage\nBidease\nBing Ads\nBatch\nBluecore\nBluedot\nBranch\nBlueshift\nBugsnag\nBraze\nBranch S2S Event\nCadent\nButton\nCensus\nciValue\nCleverTap\ncomScore\nCortex\nConversant\nCordial\nCriteo\nCrossing Minds\nCustom Feed\nCustomerGlu\nDatabricks\nCustomer.io\nDatadog\nDidomi\nDynalyst\nEdge226\nDynamic Yield\nEmarsys\nEpsilon\nEverflow\nFacebook Offline Conversions\nFiksu\nFacebook\nFlurry\nGoogle Analytics for Firebase\nFlybits\nForeSee\nFormation\nFoursquare\nFreeWheel Data Suite\nGoogle Ad Manager\nGoogle Ads\nGoogle Analytics\nGoogle BigQuery\nGoogle Analytics 4\nGoogle Enhanced Conversions\nGoogle Cloud Storage\nGoogle Marketing Platform\nGoogle Marketing Platform Offline Conversions\nGoogle Pub/Sub\nHerow\nGoogle Tag Manager\nHeap\nHightouch\nIbotta\nHyperlocology\nIndicative\nImpact\nInMarket\nInMobi\nInspectlet\nInsider\niPost\nIntercom\nironSource\nIterable\nJampp\nKayzen\nKafka\nKlaviyo\nKochava\nKissmetrics\nKubit\nLaunchDarkly\nLeanplum\nLiftoff\nLifeStreet\nLiveLike\nLiveramp\nLocalytics\nMadHive\nmAdme Technologies\nMailchimp\nMarigold\nMediaMath\nMautic\nMediasmart\nMicrosoft Azure Event Hubs\nMintegral\nMixpanel\nMoEngage\nMovable Ink\nMoloco\nMonetate\nMovable Ink - V2\nmyTarget\nMultiplied\nNarrative\nNanigans\nNami ML\nNeura\nOneTrust\nNCR Aloha\nOptimizely\nOracle BlueKai\nOracle Responsys\nPersona.ly\nPaytronix\nPersonify XP\nPieEye\nPilgrim\nPinterest\nPlarin\nPostie\nPrimer\nPunchh\nPushwoosh\nQuantcast\nRadar\nReddit\nRegal\nRetina AI\nRemerge\nReveal Mobile\nRevenueCat\nRTB House\nSailthru\nRokt\nSalesforce Email\nSalesforce Mobile Push\nSamba TV\nScalarr\nSendGrid\nShareThis\nSignal\nSessionM\nShopify\nSimpleReach\nSingular-DEPRECATED\nSingular\nSkyhook\nSlack\nSmadex\nSmarterHQ\nSnapchat\nSnapchat Conversions\nSnowflake\nSnowplow\nSplunk MINT\nAppsFlyer\nSplit\nStartApp\nSprig\nStatsig\nStormly\nSwrve\nTalon.One\nTapad\nTapjoy\nTaptica\nTaplytics\nTeak\nThe Trade Desk\nTikTok Event\nTicketure\nTreasure Data\nQuadratic Labs\nTriton Digital\nTUNE\nTwitter\nValid\nVoucherify\nVkontakte\nWebtrends\nVungle\nWebhook\nWhite Label Loyalty\nWootric\nXandr\nYahoo (formerly Verizon Media)\nYouAppi\nYotpo\nZ2A Digital\nQualtrics\nZendesk\nEvent\n\nAmplitude provides product analytics that helps companies leverage cross-platform behavioral data to drive user growth.\n\nSupported Features\nAnalytics\nData Export\nReal-Time Dashboards\nRetroactive Funnels\nPrerequisites\n\nTo activate mParticle\u2019s integration with Amplitude, you need the Amplitude API Key for each app that you\u2019d like to set up. Your API key is listed on the Amplitude Project Settings page.\n\nThe Amplitude integration requires the mParticle Amplitude Kit when connected to a web input. If you are initializing the web SDK using the Snippet Option then the Kit is automatically included for you. If you are Self Hosting the SDK then you need to add this kit manually into your source code.\n\nData Processing Notes\n\nmParticle will forward User Identities and Attributes to Amplitude, even if there are no events in the batch.\n\nDevice/User ID Mapping\n\nEvery event in Amplitude has a main Device ID field. mParticle populates this field as follows:\n\nFor Android, the Android ID (falling back to the Android Advertising ID if unavailable)\nFor iOS, the IDFA (falling back to the IDFV if unavailable)\nFor Roku, the Roku Advertising ID (falling back to the Roku Publisher ID if not available)\nFor FireTV, the Fire Advertising ID.\n\nAmplitude also has dedicated fields for particular Device IDs, such as IDFA and Android ID. These will be populated if the ID is available. See Field Mappings, for more information.\n\nAmplitude requires either a Device ID or a User ID. User ID can be mapped as Email, Customer ID or mParticle ID. If no accepted identifiers are present, data will not be forwarded.\n\nForwarding Web Data\n\nBy default, mParticle forwards web data to Amplitude client-side, by directly invoking Amplitude\u2019s Javascript methods. Optionally, you can choose to forward web data server-to-server in the Connection Settings. Note that if you choose this option, your incoming data must have your selected User ID to be forwarded.\n\nData Localization\n\nBy default, mParticle sends data to the Amplitude organization\u2019s US endpoint for the HTTP API, but Amplitude offers a different endpoint for EU organizations. In mParticle, the Amplitude Organization Configuration Setting allows you to select a target Amplitude organization location.\n\nEvent Data Mapping\nScreen Views\n\nmParticle will forward all screen views to Amplitude with the Amplitude Event Type set to \u201cViewed ScreenName\u201d, where ScreenName is the screen name passed to the logScreen SDK method (or the name of the screen\u2019s Activity class if you\u2019re using automatic screen tracking on Android).\n\nSession Forwarding\n\nmParticle will forward all session start and session end events to Amplitude with the Amplitude Event Type set to session_start and session_end.\n\neCommerce Event Forwarding\n\nEach valid eCommerce event sent to mParticle is expanded into multiple events before being translated and forwarded to Amplitude. While this expansion applies to all eCommerce transactions, the resulting events are dependent on the type of eCommerce event being forwarded and the number of products in the event.\n\nNote: mParticle will not attempt to forward eCommerce Events to Amplitude which contain no Products.\n\nThe eCommerce events resulting from the expansion that may be forwarded to Amplitude are:\n\nTransaction level events (e.g. eCommerce - Purchase)\nItem level events (e.g. eCommerce - Purchase - Item)\nRevenue events (e.g. [Amplitude] Revenue)\nTRANSACTION EVENT\nOne Transaction Event is forwarded to Amplitude for every valid eCommerce event provided.\nContains a summary of the transaction, including a JSON array of the products sent in the initial eCommerce event.\nITEM EVENTS\nOne Item Event is forwarded to Amplitude for every product inside the provided eCommerce event.\nEach Item event contains information about one of the products in the provided eCommerce event.\nREVENUE EVENT\nOne Revenue Event is forwarded to Amplitude for every valid Purchase or Refund eCommerce event provided. Otherwise none is sent.\nThe name of the Revenue Event is set by Amplitude, and contains revenue information about the Purchase or Refund event.\nECOMMERCE EVENT NAMING\n\nEach valid eCommerce event sent to mParticle will result in a transaction level event being sent to Amplitude. This event contains a summary of the transaction, including a JSON array of the products sent in the initial eCommerce event.\n\nThe name given to the Transaction and Item Level Events is determined from the Product Action provided on the eCommerce Event sent to mParticle.\n\nEvent Product Action\tTransaction Level Event Name\tItem Level Event Names\tRevenue Event Name\nadd_to_cart\teCommerce - AddToCart\teCommerce - AddToCart - Item\tN/A\nremove_from_cart\teCommerce - RemoveFromCart\teCommerce - RemoveFromCart - Item\tN/A\ncheckout\teCommerce - Checkout\teCommerce - Checkout - Item\tN/A\ncheckout_option\teCommerce - CheckoutOption\teCommerce - CheckoutOption - Item\tN/A\nclick\teCommerce - Click\teCommerce - Click - Item\tN/A\nview_detail\teCommerce - ViewDetail\teCommerce - ViewDetail - Item\tN/A\npurchase\teCommerce - Purchase\teCommerce - Purchase - Item\t[Amplitude] Revenue\nrefund\teCommerce - Refund\teCommerce - Refund - Item\t[Amplitude] Revenue\nadd_to_wishlist\teCommerce - AddToWishlist\teCommerce - AddToWishlist - Item\tN/A\nremove_from_wish_list\teCommerce - RemoveFromWishlist\teCommerce - RemoveFromWishlist - Item\tN/A\nCustom Event Forwarding\n\nCustom events logged via mParticle\u2019s logEvent SDK method and their attributes will be forwarded to Amplitude, with the event name passed to logEvent as the Amplitude Event Type. An event name must be specified or an error will be returned.\n\nAttribution Custom Event Forwarding\n\nAttribution Custom events will be forwarded to Amplitude prefixed with the attribution provider in the event name. For example, [AppsFlyer] attribution. Event Attributes that are included with the event are forwarded to Amplitude in user_properties, also prefixed with the attribution provider.\n\nPush Registration\n\nmParticle will forward all push registration events to Amplitude with the Amplitude Event Type set to Push Registration. All Field Mappings defined in this documentation will be forwarded too.\n\nApplication State Transition Forwarding\n\nIf the Send Application State Transitions setting is enabled, Application State Transition events will be forwarded to Amplitude as follows:\n\nApplication State Transition\tAmplitude event type\ninitialized, is_first_run = true\tInstall\ninitialized, is_upgrade = true\tUpgrade\ninitialized\tApplication Initialized\nexit\tApplication Exit\nbackground\tApplication Background\nforeground\tApplication Foreground\nField Mappings\nParameter\tAmplitude Field\tmParticle Details\nAndroid ID\tandroid_id\tPassed if OS is Android\nAndroid Advertising ID\tadid\tPassed if OS is Android\nApplication Version\tapp_version\tApplication Version\nBrand\tdevice_brand\tThe device brand the user is on. This is not passed for Apple devices.\nCarrier\tdevice_carrier\tDevice Carrier\nCity\tcity\tCity the user is in; this is also included in User Properties\nCountry\tCountry the user is in; this is also included in User Properties\tCountry\nDesignated Market Area\tDMA\tIf you wish to forward this property to Amplitude, you must set it as a custom user attribute, labeled dma.\nDevice ID\tdevice_id\tSet based on Operating System; see Device/User ID Mapping\nEmail\tIf the Include Email in User Properties setting is enabled, email is included in user_properties\tEmail\nEvent Properties\tevent_properties\tAll event attributes included with eCommerce, Custom and Screen View events. See above for Attribution Custom Events.\nEvent Type\tevent_type\tDescribed above for each supported event\nIDFA\tidfa\tPassed if OS is iOS or tvOS\nInsert ID\tinsert_id\tA unique id for the event derived from the event name and the event and session_start timestamps\nIP Address\tip\tIP address of the user\nLanguage\tlanguage\tLanguage the user has set\nLatitude\tlocation_lat\tLatitude of the user\nLibrary\tlibrary\tA label for the source of data which is visible in the Amplitude dashboard. This will always be set to \u2018mParticle\u2019\nLongitude\tlocation_lng\tLongitude of the user\nManufacturer\tdevice_manufacturer\tDevice Manufacturer\nModel\tdevice_model\tDevice Model\nOS Name\tos_name\tiOS, tvOS, Android, Roku\nOS Version\tos_version\tThe version of the mobile OS or browser the user is on\nPlatform\tplatform\tiOS, Android, Apple TV, Web, Roku\nRegion\tregion\tRegion (or State) the user is in; this is also included in User Properties\nSession Start Time\tsession_id\tSession Start Timestamp\nTime\ttime\tEvent Timestamp, in milliseconds\nUser ID\tuser_id\tSet based on the value of the User Identification setting\nUser Properties\tuser_properties\tAll user attributes included with the event. See above for Attribution Custom Events.\nServer to Server Web Requests\n\nOnly for web requests, mParticle will extract OS and browser info from HTTP user agent. Similar to Amplitude\u2019s SDK behavior, os_name and os_version will be populated with browser info. For that reason, mParticle will send 2 additional Custom User Properties, web_os_name and web_os_version, that will contain OS info. See Amplitude\u2019s doc.\n\nmParticle will also populate device_brand and device_model from the HTTP user agent if the Extract Device Family from User Agent setting is enabled. With this setting enabled, if mParticle cannot determine the device brand or model, it will populate device_brand with the same value as is set for web_os_name. Note, the device values may differ slightly between S2S events and events sent through the Amplitude web kit.\n\nConfiguration Settings\nSetting Name\tData Type\tDefault Value\tDescription\nAPI Key\tstring\tNULL\tThe Amplitude API Key for each app that you set up.\nAmplitude Organization\tstring\tUS Organization\tThe Amplitude datacenter that is configured for your Amplitude organization.\nUse Batch API Endpoint\tbool\tFalse\tIf enabled, the Amplitude batch API endpoint will be used. The endpoint has a higher rate limit but may have a slight delay in delivering events. Please note that if the request is replayed, the batch API endpoint will always be used regardless of this configuration value.\nConnection Settings\nSetting Name\tData Type\tDefault Value\tPlatform\tDescription\nUser Identification\tstring\tcustomerId\tAll\tTo identify users, choose \u201cCustomer ID\u201d to send Customer ID if provided, \u201cEmail\u201d to send Email addresses if provided, or \u201cMPID\u201d to send mParticle ID.\nYou can map other IDs by selecting any of the \u201cOther\u201d fields from the User Identification drop-down. These fields can be used to map Other IDs as Customer IDs.\nAnonymous ID forwarding\tenum\tUnselected\tAll\tIf enabled, mParticle will send an identifier derived from either Device Application Stamp or a combination of Device Application Stamp and MPID when another device ID does not exist on the batch. This setting is only supported for server-side forwarding. Note, for partner feeds, the value of Device Application Stamp is the same for all MPIDs.\nInclude Email in User Properties\tbool\tFalse\tAll\tIf enabled, the email user identity will be forwarded in the Amplitude user_properties.\nAllow unset user attributes\tbool\tTrue\tAll\tAllow user attributes to be removed in Amplitude using the $unset operation.\nPrefix Attribution with Source\tbool\tTrue\tAll\tIf enabled, the attribution source name will be prefixed for attribution events.\nInclude UTM in User Properties\tbool\tdefault\tWeb\tIf enabled, Amplitude will find the standard UTM parameters from either the URL or the browser cookie and set them as user properties.\nForward Web Requests Server Side\tbool\tFalse\tWeb\tIf enabled, mParticle will not initialize the full Amplitude integration on the web client. Instead, web data will be forwarded to Amplitude via server-to-server API.\nInstance Name\tstring\tdefault\tWeb\tThe name to use for the client-side Amplitude instance configured by mParticle. This should be unique for each Amplitude connection.\nInclude Enriched User Attributes\tbool\tTrue\tAll\tIf enabled, mParticle will forward enriched user attributes from the existing user profile.\nSend Application State Transitions\tbool\tFalse\tAll\tIf enabled, application state transitions will be forwarded to Amplitude.\nSend Event Attributes as Objects\tbool\tFalse\tAll\tIf enabled, mParticle will attempt to send event attributes as objects. Attributes should be string values containing serialized JSON. If we are unable to parse JSON from the attribute, we will send it to Amplitude as is. We will parse all valid JSON including objects, arrays, numbers, booleans, and nulls. Note, Amplitude event properties do not support all nested object formats - please see their docs here for details.\nGenerate Insert ID From Event ID\tbool\tFalse\tAll but Web\tIf enabled, mParticle will generate insert ID from the event ID. If disabled, insert ID will be generated from a combination of device ID, user ID, event ID, event ID, and time. Insert ID is used by Amplitude for deduplication.\nEnable Apple Search Ads\tbool\tFalse\tiOS, tvOS\tIf enabled, the Apple Search Ads attributes will be forwarded in the Amplitude user_properties.\nExtract Device Family from User Agent\tbool\tFalse\tWeb\tIf enabled, mParticle will attempt to extract device family information from the provided user agent string. Note, this is only used for server-side web requests. See Server to Server Web Requests for more information.\nDOCS\nLast Updated:\n1/8/2025, 6:32:28 PM\nCDP\nmParticle Docs\nHome\nGuides\nDevelopers\nIntegrations\nAnalytics\nAnalytics Docs\nOverview\nGetting Started\nDevelopers\nExpert Training\nPredictive AI\nCortex Docs\nOverview\nGetting Started\nDevelopers\nUsing Cortex\nLearn More\nBlog\nPodcast\nResources\nCommunity\nChange log\nGet in Touch\nHelp Center\nDemo\nBecome a Partner\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nDocumentation\nDevelopers\nAPI References\nClient SDKs\nMedia SDKs\nServer SDKs\nQuickstart\nTools\nGuides\n\nAPI Credential Management\n\nThe Developer's Guided Journey to mParticle\n\nGuides\nGetting Started\nPlatform Guide\nAnalytics\nIDSync\nData Master\nPersonalization\nPredictive Attributes\nPredictive Audiences\n\nIntroduction\n\nProfiles\n\nCalculated Attributes\nAudiences\n\nAudiences Overview\n\nReal-time Audiences\n\nStandard Audiences\n\nJourneys\n\nWarehouse Sync\n\nData Privacy Controls\n\nData Subject Requests\n\nDefault Service Limits\n\nFeeds\n\nCross-Account Audience Sharing\n\nApproved Sub-Processors\n\nImport Data with CSV Files\n\nGlossary\n\nVideo Index\n\nAnalytics (Deprecated)\nAudiences Overview\n\nmParticle allows you to define audiences and connect them to integrations for the purpose of engaging with your users. This can be very powerful when it comes to user engagement and monetization scenarios.\n\nYou can define audiences from any user-associated data you capture with mParticle, whether from platform inputs or partner feeds.\n\nWhen you fully instrument your app using the mParticle SDK, you send data from apps to the mParticle platform. You can also enrich that data stream with other data sources. For example, in addition to sending app data, you may want to send in data that is not collected in the app (server side data) and have the mParticle platform match the data based on a user identifier, and then create audiences based upon this superset of data. Examples of data sent server side might include CRM data, purchase or revenue data from other non-mobile channels. Make sure all the data that you need is captured is important; audience creation is the first step.\n\nIf you need to send data via our Events API, please contact our customer support team.\nExamples\n\nUse audiences to drive user engagement, encourage app downloads, and more.\n\nDrive user engagement\n\nLet\u2019s say you want to engage with users that have recently installed your app but haven\u2019t used your app very much. Your objective is to drive higher engagement and convert those new users to high lifetime value users. You want to accomplish this across multiple channels: push notification and email. Therefore, your audience qualification criteria is that the user has installed your app in the last 72 hours and has less than three sessions.\n\nYou can easily and visually define this audience, then configure audience integrations to push notification and email partners - in this example, let\u2019s use Button for push and Mailchimp for email. Once you configure the respective integrations, mParticle instantiates a corresponding audience in Button and updates the corresponding email marketing list in Mailchimp. No coding is necessary.\n\nDrive app downloads\n\nLet\u2019s say you want to find more users like your currently highly engaged users and run an app download campaign in Facebook against that target audience. You start by defining your highly engaged users, using whatever criteria is important to you: lifetime value metrics, session activity, event activity, or any other data points you capture.\n\nOnce your audience is defined, configure the Facebook integration and corresponding custom audiences in your Facebook account. From there you can leverage the custom audiences like any other custom audience in Facebook.\n\nBecause we want to target users that look like our highly engaged users, we will create a Facebook lookalike audience from our highly engaged user audience and run a Facebook app install campaign that targets that lookalike audience.\n\nmParticle provides two types of audiences:\n\nReal-time audiences, available in all mParticle accounts\nStandard audiences, a premium feature\n\nWas this page helpful?\n\nYes\nNo\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nDocumentation\nDevelopers\nAPI References\nClient SDKs\nMedia SDKs\nServer SDKs\nQuickstart\nTools\nGuides\n\nAPI Credential Management\n\nThe Developer's Guided Journey to mParticle\n\nGuides\nGetting Started\nPlatform Guide\nAnalytics\nIDSync\nData Master\nPersonalization\n\nWarehouse Sync\n\nData Privacy Controls\n\nData Subject Requests\n\nDefault Service Limits\n\nFeeds\n\nCross-Account Audience Sharing\n\nApproved Sub-Processors\n\nImport Data with CSV Files\n\nGlossary\n\nVideo Index\n\nAnalytics (Deprecated)\nIntegrations\n24i\nAarki\nABTasty\nAbakus\nActable\nAdChemix\nAdikteev\nAdjust\nAdMedia\nAdobe Marketing Cloud\nAdobe Audience Manager\nAdobe Campaign Manager\nAdobe Target\nAdPredictive\nAgilOne\nAlgolia\nAirship\nAlgoLift\nAlooma\nAmazon Kinesis\nAmazon Advertising\nAmazon Kinesis Firehose\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAmazon SNS\nAdobe Marketing Cloud\nAmobee\nAmpush\nAmplitude\nAnalytics\nAntavo\nAppLovin\nAnodot\nApptentive\nApptimize\nAttractor\nAttentive\nApteligent\nMicrosoft Azure Blob Storage\nBidease\nBing Ads\nBatch\nBluecore\nBluedot\nBranch\nBlueshift\nBugsnag\nBraze\nBranch S2S Event\nCadent\nButton\nCensus\nciValue\nCleverTap\ncomScore\nCortex\nConversant\nCordial\nCriteo\nCrossing Minds\nCustom Feed\nCustomerGlu\nDatabricks\nCustomer.io\nDatadog\nDidomi\nDynalyst\nEdge226\nDynamic Yield\nEmarsys\nEpsilon\nEverflow\nFacebook Offline Conversions\nFiksu\nFacebook\n\nAudience\n\nEvent\n\nFlurry\nGoogle Analytics for Firebase\nFlybits\nForeSee\nFormation\nFoursquare\nFreeWheel Data Suite\nGoogle Ad Manager\nGoogle Ads\nGoogle Analytics\nGoogle BigQuery\nGoogle Analytics 4\nGoogle Enhanced Conversions\nGoogle Cloud Storage\nGoogle Marketing Platform\nGoogle Marketing Platform Offline Conversions\nGoogle Pub/Sub\nHerow\nGoogle Tag Manager\nHeap\nHightouch\nIbotta\nHyperlocology\nIndicative\nImpact\nInMarket\nInMobi\nInspectlet\nInsider\niPost\nIntercom\nironSource\nIterable\nJampp\nKayzen\nKafka\nKlaviyo\nKochava\nKissmetrics\nKubit\nLaunchDarkly\nLeanplum\nLiftoff\nLifeStreet\nLiveLike\nLiveramp\nLocalytics\nMadHive\nmAdme Technologies\nMailchimp\nMarigold\nMediaMath\nMautic\nMediasmart\nMicrosoft Azure Event Hubs\nMintegral\nMixpanel\nMoEngage\nMovable Ink\nMoloco\nMonetate\nMovable Ink - V2\nmyTarget\nMultiplied\nNarrative\nNanigans\nNami ML\nNeura\nOneTrust\nNCR Aloha\nOptimizely\nOracle BlueKai\nOracle Responsys\nPersona.ly\nPaytronix\nPersonify XP\nPieEye\nPilgrim\nPinterest\nPlarin\nPostie\nPrimer\nPunchh\nPushwoosh\nQuantcast\nRadar\nReddit\nRegal\nRetina AI\nRemerge\nReveal Mobile\nRevenueCat\nRTB House\nSailthru\nRokt\nSalesforce Email\nSalesforce Mobile Push\nSamba TV\nScalarr\nSendGrid\nShareThis\nSignal\nSessionM\nShopify\nSimpleReach\nSingular-DEPRECATED\nSingular\nSkyhook\nSlack\nSmadex\nSmarterHQ\nSnapchat\nSnapchat Conversions\nSnowflake\nSnowplow\nSplunk MINT\nAppsFlyer\nSplit\nStartApp\nSprig\nStatsig\nStormly\nSwrve\nTalon.One\nTapad\nTapjoy\nTaptica\nTaplytics\nTeak\nThe Trade Desk\nTikTok Event\nTicketure\nTreasure Data\nQuadratic Labs\nTriton Digital\nTUNE\nTwitter\nValid\nVoucherify\nVkontakte\nWebtrends\nVungle\nWebhook\nWhite Label Loyalty\nWootric\nXandr\nYahoo (formerly Verizon Media)\nYouAppi\nYotpo\nZ2A Digital\nQualtrics\nZendesk\nEvent\n\nThis section describes the configuration settings necessary to activate mParticle\u2019s event tracking integration with Facebook. The event tracking integration enables conversion tracking for Facebook advertising campaigns.\n\nSupported Features\nEvent Forwarding\nData Processing Notes\n\nFacebook has limits around the number of unique event names and attributes their platform can process as noted here: https://developers.facebook.com/docs/reference/android/current/class/AppEventsLogger/\n\n1000 unique event names\n25 attributes per event\nBetween 2 and 40 characters in an event name and attribute\nIt may takes up to 24 hours to see refreshed stats in Facebook Analytics\nPrerequisites\n\nEvent data from mParticle to Facebook is typically sent server side. However, Web data can be sent client or server side based on your implementation and settings. These different options require different settings:\n\niOS, tvOS, and Android - data is sent S2S, and you\u2019ll need your app\u2019s Facebook Application ID and Application Secret\nData Feeds - data is sent S2S, and you\u2019ll need your Facebook Pixel ID\nWeb - for client side kit, you\u2019ll need your Facebook Pixel ID\nWeb - for S2S, you\u2019ll need your Facebook Pixel ID and Facebook Marketing API Access Token. You also need to enable both the Use Pixel Server-Side Forwarding and Forward Web Requests Server Side settings.\nRoku, Xbox, SmartTV, FireTV, Alexa - you\u2019ll need your Facebook Pixel ID and Facebook Marketing API Access Token.\nos_version is required for iOS/tvOS events. If os_version is not present, the event will be rejected and an error message will be shown in System Alerts.\nConfiguring Facebook Pixel Server-to-Server\n\nYou need to perform a few steps in Facebook to create a Facebook Pixel S2S connection.\n\nNavigate to the Facebook Events Manager\nConnect a New Data Source: Select Web with a connection method of Conversions API.\nCreate an Access Token: Open the settings for the new Pixel Data Source, scroll to the Conversions API > Set up manually section and click Create Access Token. Follow the steps described and copy the Access Token for setup in mParticle.\nConfiguring Duplicate Events\n\nFacebook recommends sending the same pixel event twice - once from the browser and once from the server. If done incorrectly, this can result in duplicate events appearing in Facebook. The following steps walk through how to configure this within mParticle.\n\nUSING ONLY THE MPARTICLE WEB SDK (RECOMMENDED)\n\nTo do this using mParticle, you will need to perform the following steps:\n\nSetup the mParticle Web SDK in your application\nSetup two configurations for mParticle\u2019s Facebook integration. One of these configurations must have Use Pixel Server-Side Forwarding enabled and the other must not. Both must have an access token set.\nConnect both of these configurations to your Web input. The configuration with server-side Pixel forwarding enabled, must also have Forward Web Requests Server Side enabled. Similarly, the connection with Pixel server-side forwarding disabled, must have Forward Web Requests Server Side disabled.\nOTHER SCENARIOS\n\nIdeally, all Web data sent to mParticle will be sent through the mParticle Web SDK, but it is still possible to deduplicate events outside of this flow if necessary. To ensure redundant events sent through Facebook Pixel and the Facebook Conversions API are correctly deduplicated when they reach Facebook, two conditions must be met:\n\nEvents must have consistent event_name values.\nEvents must have consistent event_id values. For this, the field source_message_id may be used to manually set the Event ID sent to Facebook.\n\nIf you use the mParticle Web SDK and server-side web integration, then this will be automatically handled.\n\nIf you need to manually assign a source_message_id via the web SDK and server-side separately, use the following API:\n\nmParticle.logBaseEvent({\n    messageType: 4, // This is the messageType for custom events.  Use 3 for page views\n    name: 'Test Event',\n    data: {attr1: 'value1'}, // custom attributes\n    eventType: mParticle.EventType.Navigation, // optional for custom events. Do not set for page views\n    customFlags: {flag1: 'flagValue1'},\n    sourceMessageId: 'custom_source_message_id'\n});\n\nYou can also assign a source_message_id to commerce events:\n\n// 1. Create the product\n\nconst product1 = mParticle.eCommerce.createProduct(\n    'Double Room - Econ Rate',  // Name\n    'econ-1',                   // SKU\n    100.00,                     // Price\n    4,                          // Quantity\n);\n\n// 2. Summarize the transaction\nconst transactionAttributes = {\n    Id: 'foo-transaction-id',\n    Revenue: 430.00,\n    Tax: 30\n};\n\n// 3. Log the purchase event\n// Optional custom attributes for a product action can be defined as key/value pairs witin an object\nconst customAttributes = {\n    'sale': true\n};\n\n// Optional custom flags can be defined as key/value pairs within an object\nconst customFlags = {\n    'Google.Category': 'travel'\n};\n\nconst options = {\n    sourceMessageId: 'custom_source_message_id'\n}\n\nmParticle.eCommerce.logProductAction(\n    mParticle.ProductActionType.Purchase,\n    [product1],\n    customAttributes,\n    customFlags,\n    transactionAttributes,\n    options\n);\n\nNote that your workspace will need to be on event batching in order to leverage passing a custom source_message_id to the Web SDK for this to work. If you are not on web batching, please contact your customer success manager.\n\nVisit the Facebook Business Help Center and Facebook For Developers for more information on the subject of deduplication.\n\nTROUBLESHOOTING FACEBOOK PIXEL ISSUES\n\nPlease run through the following steps to confirm your settings are correct:\n\nVerify your access token is of type System User and will never expire using this page: https://developers.facebook.com/tools/debug/accesstoken/\nVerify your Pixel ID is valid using this page. Please enter the ID and confirm the Send To Test Events works: https://developers.facebook.com/docs/marketing-api/conversions-api/payload-helper/\n\nIf you run into issues with either of the above steps, please repeat the steps described in Configuring Facebook Pixel Server-to-Server.\n\nEvent Data Mapping\nThe iOS/tvOS and Android integrations forward App State Transition, Commerce, Custom, Screen View, and Session Start events.\nThe Web integration forwards Commerce, Custom, Screen View, and Session Start / End events.\niOS14 Update for Device Data Mapping\n\nThe Facebook advertiser_tracking_enabled field is set based on the att_authorization_status and limit_ad_tracking fields as defined below. Check the iOS14 Implementation guide for more information.\n\nIf att_authorization_status is available:\n\natt_authorization_status\tadvertiser_tracking_enabled\nauthorized\t1\nAll other values\t0\n\nIf att_authorization_status is not available, the limit_ad_tracking field is evaluated:\n\nlimit_ad_tracking\tadvertiser_tracking_enabled\nNot available or false\t1\ntrue\t0\nUser Data Mappings\n\nmParticle will send a variety of user data fields to Facebook for advanced matching. The specific fields sent depends on if Facebook Pixel server-side forwarding is enabled or not.\n\nFacebook will not accept event data forwarded from mParticle unless it includes at least one user data attribute from the list below.\n\nFacebook Pixel Server-Side Forwarding Disabled\n\nmParticle will hash and send the following fields to Facebook when they are set for a user.\n\nmParticle Field\tFacebook Field\tDescription\nemail User Identity\tem\tmParticle will send the email identity based on the value of the Email Type setting. The email user identity will be hashed before forwarding to Facebook, other user identities will not be hashed prior to forwarding.\nIdentity as defined by External User Identity Type setting\texternal_id\tmParticle will hash and send a single identity based on the value of the External User Identity Type setting.\nPhone Number User Identity\tph\tmParticle will hash and send a single phone number identity. mParticle will use the mobile_number identity if it is provided. If not, mParticle will use phone_number_2 if it is provided. If neither of those are provided, mParticle will use phone_number_3 if it is provided. If none of those are provided, mParticle will use the $mobile user attribute if it is provided.\nDevice Identity\tadvertiser_id\tmParticle will send an Advertiser ID if the user has an IDFA (Apple Identifier for Advertisers), GAID (Google Advertising ID), or RAID (Roku Advertising Identifier)\n$gender User Attribute\tge\t\n$firstname User Attribute\tfn\t\n$lastname User Attribute\tln\t\n$city User Attribute\tct\t\n$state User Attribute\tst\t\n$zip User Attribute\tzp\t\n$country User Attribute\tcountry\t\n\nFacebook Pixel Server-Side Forwarding Enabled\n\nmParticle Field\tFacebook Field Name\tHashed?\tDescription\nemail User Identity\tem\tYes\tmParticle will send the email identity based on the value of the Email Type setting. The email user identity will be hashed before forwarding to Facebook, other user identities will not be hashed prior to forwarding.\nFacebook.BrowserId Custom Flag\tfbp\tNo\tFacebook Browser ID\nFacebook.ClickId Custom Flag\tfbc\tNo\tFacebook Click ID\nFacebook.ActionSource Custom Flag\taction_source\tNo\tThis field allows you to specify where your conversions occurred. Knowing where your events took place helps ensure your ads go to the right people. The accepted values are email, website, app, phone_call, chat, physical_store, system_generated and other. The website action source requires that the Facebook.EventSourceUrl custom flag is set on the event. Please see Facebook\u2019s documentation for details.\nPhone Number User Identity\tph\tYes\tmParticle will hash and send a single phone number identity. mParticle will use the mobile_number identity if it is provided. If not, mParticle will use phone_number_2 if it is provided. If neither of those are provided, mParticle will use phone_number_3 if it is provided. If none of those are provided, mParticle will use the $mobile user attribute if it is provided.\nIdentity as defined by External User Identity Type setting\texternal_id\tYes\tmParticle will hash and send a single identity based on the value of the External User Identity Type setting.\n$gender User Attribute\tge\tYes\t\n$firstname User Attribute\tfn\tYes\t\n$lastname User Attribute\tln\tYes\t\n$city User Attribute\tct\tYes\t\n$state User Attribute\tst\tYes\t\n$zip User Attribute\tzp\tYes\t\n$country User Attribute\tcountry\tYes\t\nip\tclient_ip_address\tNo\t\nDevice Info http_header_user_agent\tclient_user_agent\tNo\tThe http_header_user_agent is required if the Default Action Source is set to Website in your configuration settings. Please see Facebook\u2019s documentation for details.\nCustom Mappings\n\nmParticle\u2019s Facebook integration supports custom mappings which allows you to map your events and attributes for Facebook. mParticle provides mappings for the following Facebook event types:\n\nFor web connections, custom mappings are only available for Pixel server-side forwarding.\nEvent Name\tStandard Mapping\tPixel Mapping\nAchieved Level\tfb_mobile_level_achieved\tN/A\nAd Click\tAdClick\tN/A\nAd Impression\tAdImpression\tN/A\nAdded Payment Info\tfb_mobile_add_payment_info\tAddPaymentInfo\nAdded to Cart\tfb_mobile_add_to_cart\tAddToCart\nAdded to Wishlist\tfb_mobile_add_to_wishlist\tAddToWishlist\nAdded to Wishlist - Automotive\tfb_mobile_add_to_wishlist\tAddToWishlist\nCompleted Registration\tfb_mobile_complete_registration\tCompleteRegistration\nCompleted Tutorial\tfb_mobile_tutorial_completion\tN/A\nContact\tContact\tContact\nCustomize Product\tCustomizeProduct\tCustomizeProduct\nDonate\tDonate\tDonate\nFind Location\tFindLocation\tFindLocation\nInitiated Checkout\tfb_mobile_initiated_checkout\tInitiateCheckout\nInitiated Checkout - Travel\tfb_mobile_initiated_checkout\tInitiateCheckout\nLead\tN/A\tLead\nLead - Automotive\tN/A\tLead\nPage View\tPageView\tPageView\nPurchased\tfb_mobile_purchase\tPurchase\nPurchased - Travel\tfb_mobile_purchase\tPurchase\nRated\tfb_mobile_rate\tN/A\nSchedule\tSchedule\tSchedule\nSearched\tfb_mobile_search\tSearch\nSearched - Automotive\tfb_mobile_search\tSearch\nSearched - Travel\tfb_mobile_search\tSearch\nSpent Credits\tfb_mobile_spent_credits\tN/A\nStart Trial\tStartTrial\tStartTrial\nSubmit Application\tN/A\tSubmitApplication\nSubscribe\tSubscribe\tSubscribe\nUnlocked Achievement\tfb_mobile_achievement_unlocked\tN/A\nViewed Content\tfb_mobile_content_view\tViewContent\nViewed Content - Automotive\tfb_mobile_content_view\tViewContent\nViewed Content - Travel\tfb_mobile_content_view\tViewContent\nFB_CONTENT_TYPE\n\nWhen setting up the custom mappings, the fb_content_type can provide additional information on the event to be used for Collaborative Ads. The acceptable values for fb_content_type are:\n\nproduct\nproduct_group\ndestination\nflight\nhotel\nvehicle\n[\u201cproduct\u201d, \u201clocal_service_business\u201d]\n\nIf a value provided for fb_content_type is not in the above list, the value sent will be product.\n\nProduct Events\nYou need to turn on sending Purchase Product data through the Facebook Connection Settings modal in the mParticle app to forward product events.\n\nmParticle forwards the mParticle product events Added to Cart and Added to Wishlist to Facebook using Facebook\u2019s corresponding pre-defined event names. mParticle Product Views are forwarded to Facebook as the pre-defined event \u201cViewed Content\u201d. The unit price, currency, product category, and SKU are passed to Facebook as well. See below for a sample Added to Cart event call using the Facebook SDK, and an equivalent call using the mParticle SDK.\n\nObjective-CJava\n//Facebook SDK call\n[FBAppEvents logEvent:FBAppEventNameAddedToCart\n           valueToSum:54.23\n           parameters:@{ FBAppEventParameterNameCurrency    : @\"USD\",\n                         FBAppEventParameterNameContentType : @\"shoes\",\n                         FBAppEventParameterNameContentID   : @\"HDFU-8452\" } ];\n\n//Equivalent mParticle SDK call\nMPProduct *product = [[MPProduct alloc] initWithName:@\"A Shoe\"\n                                            category:@\"shoes\"\n                                            quantity:1\n                                       revenueAmount:54.23];\nproduct.unitPrice = 54.23;\nproduct.sku = @\"HDFU-8452\";\n\n[[MParticle sharedInstance] logProductEvent:MPProductEventAddedToCart product:product];\nPurchase Events\n\nPurchase events logged through mParticle\u2019s eCommerce SDK methods will be forwarded to Facebook using Facebook\u2019s \u201cPurchased\u201d pre-defined event name.\n\nFor server-to-server Web connections, Facebook requires Purchase events to have valid values for both \"value\" and \"currency\". If either of these is invalid or not set, mParticle will log an error to the System Alerts page.\nCustom Events\n\nAll custom app events, which are logged via mParticle\u2019s logEvent SDK method, will be forwarded to Facebook as custom app events, using the event name passed to mParticle\u2019s logEvent SDK method.\n\nObjective-CJava\n//Facebook SDK call\n[FBAppEvents logEvent:@\"battledAnOrc\"];\n\n//Equivalent mParticle SDK call\n[[MParticle sharedInstance] logEvent:@\"battledAnOrc\"];\nScreen Views\n\nFor the Web platform, mParticle will forward screen views as \u2018PageView\u2019 events.\n\nNote: This also applies to Pixel server-side forwarding.\n\nFor the iOS/tvOS and Android platforms, screen views are supported by custom mappings. Reference the custom mappings section for more.\n\nWeb Server-to-Server Fields\n\nThere are several fields only accepted by server-to-server Web connections. These fields and the mParticle fields they are set from are listed below:\n\nFacebook Field Name\t\nDescription\n\tmParticle Field\nclient_user_agent\tThe user agent for the browser corresponding to the event. This flag is required if the Default Action Source is set to Website in the connection configuration.\tFacebook.ClientUserAgent custom flag\nevent_source_url\tThe browser URL where the event happened. This flag is required if the Default Action Source is set to Website in the connection configuration.\tFacebook.EventSourceUrl custom flag\nopt_out\tA flag that indicates Facebook should not use this event for ads delivery optimization.\tCCPA Opt Out Status\nCustom Data Fields\tAdditional data used for ads delivery optimization.\tCustom attributes*\n\n*Custom data fields can also be set via custom mappings or E-Commerce event fields. See the relevant sections for more details.\n\nConfiguration Settings\nSetting Name\tData Type\tDefault Value\tDescription\nAccess Token\tstring\t\tThe Facebook Access Token used to make Marketing API calls. Required for Web. Facebook recommends using a System User Access Token.\nUse Pixel Server-Side Forwarding\tbool\tFalse\tIf enabled, mParticle will use Facebook\u2019s Pixel Server-Side API to forward events for Web and Out-of-Band connections. Notes: this setting is read-only. To enable it, create a new configuration.\nConnection Settings\nSetting Name\tData Type\tDefault Value\tPlatform\tDescription\nSend Purchase Product Data\tbool\tFalse\tAll\tIf enabled, additional product data will be forwarded for purchase events\nFacebook Application ID\tstring\t\tiOS, Android, tvOS\tThe App ID found on your Facebook application\u2019s dashboard\nFacebook Application Secret\tstring\t\tiOS, Android, tvOS\tThe App Secret found on your Facebook application\u2019s dashboard\nSend Activate On\tstring\tast\tiOS, Android, tvOS\tSpecifies whether to send Facebook activate and deactivate messages based on mParticle application state transition messages or session start messages\nLimit Event and Data Usage\tbool\tFalse\tiOS, Android, tvOS\tSets whether data sent to Facebook should be restricted from being used for purposes other than analytics and conversions, such as targeting ads\nPixel ID\tstring\t\tWeb\tFacebook Pixel ID\nDefault Action Source\tstring\tother\tWeb, Feed\tThe default value for a conversion\u2019s action source. This value will be used if the Facebook.ActionSource custom flag is not set on the event. Please see the documentation for information on setting the custom flag.\nForward Web Requests Server Side\tbool\tFalse\tWeb\tIf enabled, requests will only be forwarded server-side\nDisable Automatic Page Logging\tbool\tFalse\tWeb\tSingle Page Applications Only - By default, the Facebook Pixel has an HTML5 History State API listener activated. Whenever a new state is pushed into History, it will automatically fire a PageView event (outside of mParticle). This could lead to duplicate PageViews in Facebook. If you want all PageViews to be tracked via mParticle instead, check this box to disable this listener.\nExternal User Identity Type\tstring\tCustomer ID\tAll\tThe User Identity to send to Facebook as an External ID. The identity is sent hashed for every selection except for MPID, which is sent raw.\nSend CCPA Limited Data Use\tenum\tNever\tAll\tWhen should mParticle send the CCPA limited data use flag to Facebook. Note: the flag can only be sent for batches with country and state user attributes defined or for Pixel connections with client IP defined.\nEmail Type\tenum\tEmail\tAll\tThe mParticle User Identity type to forward as an Email to Facebook. The email user identity will be hashed before forwarding to Facebook, other user identities selected from this dropdown will not be hashed prior to forwarding.\nEnrich Facebook Cookie Identifiers\tbool\tFalse\tWeb, Feed\tIf enabled, cookie-based identifiers are stored on a user\u2019s profile, and batches are enriched with the most-recently seen identifiers when missing from the batch. Currently, cookie identifiers include only fbc and fbp.\nEnrich IP Address\tbool\tFalse\tWeb, Feed\tIf enabled, a batch will be enriched with the most-recently seen IP address when missing from the batch.\nDOCS\nLast Updated:\n1/8/2025, 6:32:28 PM\nCDP\nmParticle Docs\nHome\nGuides\nDevelopers\nIntegrations\nAnalytics\nAnalytics Docs\nOverview\nGetting Started\nDevelopers\nExpert Training\nPredictive AI\nCortex Docs\nOverview\nGetting Started\nDevelopers\nUsing Cortex\nLearn More\nBlog\nPodcast\nResources\nCommunity\nChange log\nGet in Touch\nHelp Center\nDemo\nBecome a Partner\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nNot Found\n\nYou hit a page that does not exist.\n\nReturn to the home page\nDOCS\nLast Updated:\n1/8/2025, 6:32:28 PM\nCDP\nmParticle Docs\nHome\nGuides\nDevelopers\nIntegrations\nAnalytics\nAnalytics Docs\nOverview\nGetting Started\nDevelopers\nExpert Training\nPredictive AI\nCortex Docs\nOverview\nGetting Started\nDevelopers\nUsing Cortex\nLearn More\nBlog\nPodcast\nResources\nCommunity\nChange log\nGet in Touch\nHelp Center\nDemo\nBecome a Partner\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nDocumentation\nDevelopers\nAPI References\nClient SDKs\nMedia SDKs\nServer SDKs\nQuickstart\nTools\nGuides\n\nAPI Credential Management\n\nThe Developer's Guided Journey to mParticle\n\nGuides\nGetting Started\nPlatform Guide\nAnalytics\nIDSync\nData Master\nPersonalization\n\nWarehouse Sync\n\nData Privacy Controls\n\nData Subject Requests\n\nDefault Service Limits\n\nFeeds\n\nCross-Account Audience Sharing\n\nApproved Sub-Processors\n\nImport Data with CSV Files\n\nGlossary\n\nVideo Index\n\nAnalytics (Deprecated)\nIntegrations\n24i\nAarki\nABTasty\nAbakus\nActable\nAdChemix\nAdikteev\nAdjust\nAdMedia\nAdobe Marketing Cloud\nAdobe Audience Manager\nAdobe Campaign Manager\nAdobe Target\nAdPredictive\nAgilOne\nAlgolia\nAirship\nAlgoLift\nAlooma\nAmazon Kinesis\nAmazon Advertising\nAmazon Kinesis Firehose\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAmazon SNS\nAdobe Marketing Cloud\nAmobee\nAmpush\nAmplitude\nAnalytics\nAntavo\nAppLovin\nAnodot\nApptentive\nApptimize\nAttractor\nAttentive\nApteligent\nMicrosoft Azure Blob Storage\nBidease\nBing Ads\nBatch\nBluecore\nBluedot\nBranch\nBlueshift\nBugsnag\nBraze\nBranch S2S Event\nCadent\nButton\nCensus\nciValue\nCleverTap\ncomScore\nCortex\nConversant\nCordial\nCriteo\nCrossing Minds\nCustom Feed\nCustomerGlu\nDatabricks\nCustomer.io\nDatadog\nDidomi\nDynalyst\nEdge226\nDynamic Yield\nEmarsys\nEpsilon\nEverflow\nFacebook Offline Conversions\nFiksu\nFacebook\nFlurry\nGoogle Analytics for Firebase\nFlybits\nForeSee\nFormation\nFoursquare\nFreeWheel Data Suite\nGoogle Ad Manager\nGoogle Ads\nGoogle Analytics\nGoogle BigQuery\nGoogle Analytics 4\nGoogle Enhanced Conversions\nGoogle Cloud Storage\nGoogle Marketing Platform\nGoogle Marketing Platform Offline Conversions\nGoogle Pub/Sub\nHerow\nGoogle Tag Manager\nHeap\nHightouch\nIbotta\nHyperlocology\nIndicative\nImpact\nInMarket\nInMobi\nInspectlet\nInsider\niPost\nIntercom\nironSource\nIterable\nJampp\nKayzen\nKafka\nKlaviyo\nKochava\nKissmetrics\nKubit\nLaunchDarkly\nLeanplum\nLiftoff\nLifeStreet\nLiveLike\nLiveramp\nLocalytics\nMadHive\nmAdme Technologies\nMailchimp\nMarigold\nMediaMath\nMautic\nMediasmart\nMicrosoft Azure Event Hubs\nMintegral\nMixpanel\nMoEngage\nMovable Ink\nMoloco\nMonetate\nMovable Ink - V2\nmyTarget\nMultiplied\nNarrative\nNanigans\nNami ML\nNeura\nOneTrust\nNCR Aloha\nOptimizely\nOracle BlueKai\nOracle Responsys\nPersona.ly\nPaytronix\nPersonify XP\nPieEye\nPilgrim\nPinterest\nPlarin\nPostie\nPrimer\nPunchh\nPushwoosh\nQuantcast\nRadar\nReddit\nRegal\nRetina AI\nRemerge\nReveal Mobile\nRevenueCat\nRTB House\nSailthru\nRokt\nSalesforce Email\nSalesforce Mobile Push\nSamba TV\nScalarr\nSendGrid\nShareThis\nSignal\nSessionM\nShopify\nSimpleReach\nSingular-DEPRECATED\nSingular\nSkyhook\nSlack\nSmadex\nSmarterHQ\nSnapchat\nSnapchat Conversions\nSnowflake\nSnowplow\nSplunk MINT\nAppsFlyer\n\nFeed\n\nForwarding Data Subject Requests\n\nEvent\n\nSplit\nStartApp\nSprig\nStatsig\nStormly\nSwrve\nTalon.One\nTapad\nTapjoy\nTaptica\nTaplytics\nTeak\nThe Trade Desk\nTikTok Event\nTicketure\nTreasure Data\nQuadratic Labs\nTriton Digital\nTUNE\nTwitter\nValid\nVoucherify\nVkontakte\nWebtrends\nVungle\nWebhook\nWhite Label Loyalty\nWootric\nXandr\nYahoo (formerly Verizon Media)\nYouAppi\nYotpo\nZ2A Digital\nQualtrics\nZendesk\nEvent\n\nAppsFlyer is a leading Mobile Attribution & Marketing Analytics platform that allows app marketers to easily measure the performance of all their marketing channels - paid, organic and social - from a single real-time dashboard.\n\nmParticle\u2019s integration with AppsFlyer includes a required Kit integration and an optional server-side integration. The Kit forwards event data from your app to AppsFlyer on the client side, and also handles app install attribution, uninstall tracking, and deeplinking, including deferred deeplinking. Supplementary data from outside your app can be forwarded to mParticle via our Events API and sent on to AppsFlyer server-to-server.\n\nConsent\nTo adhere to the EU user consent policy, starting March 6, 2024, Google is enforcing the consent field to be populated for EU users. AppsFlyer is adjusting to this requirement. You can find more details in AppsFlyer's announcement here.\n\nGoogle has several notions of user-provided consent. Two of these apply to AppsFlyer events: ad_user_data and ad_personalization. These represent consent for ad user data and ad personalization.\n\nUser-specified Consent\n\nTo configure user consent forwarding under this value, a mapping should be set-up leveraging mParticle\u2019s notion of Consent Purposes. To learn more about handling user consent within mParticle\u2019s platform, see the following docs: Data Privacy Controls.\n\nOnce a Consent Purpose is set up, user consent information can be associated with it in subsequent Event Batches. The Consent Purpose data mapping can then be configured for downstream forwarding via the User Consent Data Mapping connection setting.\n\nConsent Defaults\n\nIn the absence of a user-defined consent value for the ad_user_data and ad_personalization fields via the Consent Purpose mapping, a default value can be optionally configured via a separate drop-down setting for each consent type. When no user consent is provided, the default status is used, if specified.\n\nCaution: It is recommended that in the long term, you set up user-specified consent through the Consent Purpose mapping, such that the user consent is correctly forwarded to Google. It is your responsibility as a Data Controller to stay compliant under the GDPR, and set up user consent collection for downstream forwarding. The consent default setting can be deprecated in the future.\n\nEnabling AppsFlyer\n1. Configure\nAdd the mParticle SDKs to your iOS and/or Android apps. See the docs for your platform here.\nConnect your iOS and/or Android workspaces to AppsFlyer. You will need to provide your Dev Key, which you can access from the Settings page of your organization\u2019s AppsFlyer dashboard. For an iOS connection, you will also need your Apple App ID. Make sure you remove the \u2018ID\u2019 prefix. For example, if your App ID is id12345, just enter 12345. For more information on setting up a new mParticle connection, see the Platform Guide.\nCONFIGURATION SETTINGS\nSetting Name\tData Type\tDefault Value\tDescription\nDev Key\tstring\t\tYou can find your Dev Key within the AppsFlyer settings page.\nCONNECTION SETTINGS\nSetting Name\tData Type\tDefault Value\tPlatform\tDescription\nSend Additional Product Data\tbool\tfalse\tS2S\tIf enabled, additional product data will be forwarded for purchase and add-to-cart events. This was designed for a downstream Facebook product.\nApple App ID\tstring\t\tiOS\tYou can find your app\u2019s Apple ID in the app page in iTunes Connect.\nGDPR Applies\tbool\tfalse\tAll\tIf enabled, the integration will be considered as European regulations do apply to these users and conversions.\nConsent Data Mapping\tmapping\tnull\tAll\tA mapping of mParticle consents to Google consents.\nAd User Data Default Consent Value\tstring\tUnspecified\tAll\tSelect the Ad User Data Consent Status to be forwarded to AppsFlyer.\nAd Personalization Default Consent Value\tstring\tUnspecified\tAll\tSelect the Ad Personalization Consent Status to be forwarded to AppsFlyer.\n2. Add the iOS/Android Kits\n\nmParticle\u2019s AppsFlyer integration requires that you add the AppsFlyer Kit to your iOS or Android app. Just by adding the binary to your app:\n\nThe mParticle SDK will initialize AppsFlyer\u2019s SDK per their documentation, using the AppsFlyer dev key provided above, and will also forward all required UIApplication (iOS) and Activity (Android) lifecycle events to the AppsFlyer SDK.\nEvents that you log via mParticle\u2019s API will be automatically mapped onto AppsFlyer analogous event tracking APIs.\nThe customer ID and email of the current user, set via mParticle\u2019s Identity APIs, will be mapped onto AppsFlyer\u2019s analogous setUser APIs.\nGoogle Play Install Referrer will be forwarded (Android only - see below).\n\nmParticle publishes the AppsFlyer Kit as separate iOS and Android libraries which have a transitive dependency on the mParticle core libraries. You can add them to your app via Carthage, Cocoapods, Swift Package Manager, or Gradle:\n\nCocoaPods\n# Sample Podfile\n\nsource 'https://github.com/CocoaPods/Specs.git'\n\ntarget '<Your Target>' do\n    pod 'mParticle-AppsFlyer'\nend\nCarthage\nSwift\nGradle\n\nRefer to the Apple SDK and Android SDK guides to read more about kits.\n\n3. Install Referrer (Android only)\n\nAppsFlyer\u2019s SDK requires the forwarding of the Google Play Install Referrer Intent. The AppsFlyer kit will take care of this for you as long as you\u2019ve added mParticle\u2019s ReferrerReceiver to you manifest, or you are manually forwarding the Intent to our ReferrerReceiver class. The mParticle SDK will then forward the Install Referrer intent to each configured kit, including AppsFlyer.\n\nPlease see the Android SDK setup guide to ensure your manifest is configured properly.\n\nDeep Linking and Attribution\n\nThe AppsFlyer SDK exposes client-side deep linking and attribution APIs, all of which are supported by the mParticle AppsFlyer kit and are covered in this section. The core use-cases are:\n\nRetrieving install attribution information (deferred deep linking)\nRetrieve app-open attribution information (non-deferred deep linking)\nOneLink, AppsFlyer\u2019s unified deeplinking hosting service\n\nEach platform has very specific requirements to ensure that the above functionality is supported. The minimum requirements are:\n\nVerify the AppsFlyer connection is enabled for the workspace key and secret, and mParticle environment (development or production) that you\u2019re testing.\nVerify that the devKey configured above matches your AppsFlyer app.\nVerify that you have correctly configured your Apple App ID (retrieve from iTunes connect, this is not your iOS development team prefix) both in AppsFlyer and in mParticle.\nVerify that the proper associated domains and entitlements are configured (iOS only)\nVerify that your AndroidManifest.xml has the proper Intent Filters to match any deep link domains that you\u2019re testing.\nRetrieving Attribution Information\n\nmParticle\u2019s SDKs will automatically initialize the AppsFlyer SDK, forwarding the required UIApplication lifecycle events and the continueUserAcivity events for iOS and Android, as well as registering a delegate such that you can retrieve deep linking and attribution parameters on the client-side to customize your user\u2019s in-app experience.\n\nSpecifically, the AppsFlyer SDK exposes one distinct callback:\n\ndidResolveDeepLink\n\nmParticle\u2019s API includes a wrapper for this callback, and both the iOS and Android kits expose a constant to inform you when the callback has been fired and if it was successful. On both platforms, the iOS/Android kit will register a delegate/callback with the AppsFlyer SDK on initialization and for the lifetime of the app\u2019s process, and will call your completion handler block (iOS) or AttributionListener (Android) whenever there is new conversion data available.\n\nWhen a user clicks a OneLink link, the AppsFlyer SDK retrieves available OneLink data from the AppsFlyer servers. Then, the UDL API calls the didResolveDeepLink() method which returns a DeepLinkResult object containing a status and (if successful) a DeepLink object containing the deep_link_value and deep_link_sub1-10 parameters you can use to customize the resulting in-app outcome.\n\nHowever, when users deep link directly using universal or app links, the didResolveDeepLink method returns the full link unparsed since the app opens directly without going through AppsFlyer first. You can read more about AppsFlyer\u2019s deep linking methods here.\n\nThe parameters returned in these results will match the result of the AppsFlyer SDK, as documented for Android and iOS.\n\nSee the deep linking documentation for iOS and Android for more information.\n\nTest Scenarios\n\nThe following core test scenarios that should be verified:\n\nNew user-install (deferred deeplink)\nApplication not running, and an existing user-install invokes a deep link (\u201ccold start\u201d scenario)\nApplication running in the background, and an existing user install invokes a deep link (\u201cwarm start\u201d scenario)\nTrack Events\n\nThe AppsFlyer Kit will forward app events and eCommerce events to AppsFlyer. Custom Mappings are available to map your custom app events onto AppsFlyer\u2019s event names. Refer to the iOS and Android SDK docs for details on instrumenting your app for event tracking:\n\neCommerce\n\niOS SDK\nAndroid SDK\n\nApp Events\n\niOS SDK\nAndroid SDK\n\nNote that AppsFlyer enforces a limit of 45 characters for event names.\n\nKit Source and Sample Applications\n\nThe source code for each kit, as well as sample apps, are available on Github:\n\niOS\nAndroid\nServer Integration\n\nmParticle can forward app events and commerce events received via our Events API to AppsFlyer\u2019s server-API. Note that AppsFlyer requires data to be linked to a unique AppsFlyer ID, generated client-side by the AppsFlyer Kit. mParticle stores the AppsFlyer ID in its internal user profile. This means that:\n\nYou must have the AppsFlyer Kit included in your app to be able to forward data, and you can only forward data associated with your app users.\nThe data you send server-side must include a device ID, so that mParticle can lookup the AppsFlyer ID for the user.\nThe AppsFlyer Kit must be included in your app - the server integration may only be used as a complement to the Kit integration.\niOS 14 Update for ApplicationTrackingTransparency\n\nFor iOS 14, mParticle will send the att field based on the att_authorization_status to AppsFlyer in their expected format. Check the iOS14 Implementation guide for more information.\n\nIf att_authorization_status is available:\n\natt_authorization_status\tatt\nauthorized\t3\ndenied\t2\nnot_determined\t0\nrestricted\t1\nDOCS\nLast Updated:\n1/8/2025, 6:32:28 PM\nCDP\nmParticle Docs\nHome\nGuides\nDevelopers\nIntegrations\nAnalytics\nAnalytics Docs\nOverview\nGetting Started\nDevelopers\nExpert Training\nPredictive AI\nCortex Docs\nOverview\nGetting Started\nDevelopers\nUsing Cortex\nLearn More\nBlog\nPodcast\nResources\nCommunity\nChange log\nGet in Touch\nHelp Center\nDemo\nBecome a Partner\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nDocumentation\nDevelopers\nAPI References\nClient SDKs\nMedia SDKs\nServer SDKs\nQuickstart\nTools\nGuides\n\nAPI Credential Management\n\nThe Developer's Guided Journey to mParticle\n\nGuides\nGetting Started\nPlatform Guide\nAnalytics\nIDSync\nData Master\nPersonalization\n\nWarehouse Sync\n\nData Privacy Controls\n\nData Subject Requests\n\nDefault Service Limits\n\nFeeds\n\nCross-Account Audience Sharing\n\nApproved Sub-Processors\n\nImport Data with CSV Files\n\nGlossary\n\nVideo Index\n\nAnalytics (Deprecated)\nIntegrations\n24i\nAarki\nABTasty\nAbakus\nActable\nAdChemix\nAdikteev\nAdjust\nAdMedia\nAdobe Marketing Cloud\nAdobe Audience Manager\nAdobe Campaign Manager\nAdobe Target\nAdPredictive\nAgilOne\nAlgolia\nAirship\nAlgoLift\nAlooma\nAmazon Kinesis\nAmazon Advertising\nAmazon Kinesis Firehose\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAmazon SNS\nAdobe Marketing Cloud\nAmobee\nAmpush\nAmplitude\nAnalytics\nAntavo\nAppLovin\nAnodot\nApptentive\nApptimize\nAttractor\nAttentive\nApteligent\nMicrosoft Azure Blob Storage\nBidease\nBing Ads\nBatch\nBluecore\nBluedot\nBranch\nBlueshift\nBugsnag\nBraze\nBranch S2S Event\nCadent\nButton\nCensus\nciValue\nCleverTap\ncomScore\nCortex\nConversant\nCordial\nCriteo\nCrossing Minds\nCustom Feed\nCustomerGlu\nDatabricks\nCustomer.io\nDatadog\nDidomi\nDynalyst\nEdge226\nDynamic Yield\nEmarsys\nEpsilon\nEverflow\nFacebook Offline Conversions\nFiksu\nFacebook\n\nAudience\n\nEvent\n\nFlurry\nGoogle Analytics for Firebase\nFlybits\nForeSee\nFormation\nFoursquare\nFreeWheel Data Suite\nGoogle Ad Manager\nGoogle Ads\nGoogle Analytics\nGoogle BigQuery\nGoogle Analytics 4\nGoogle Enhanced Conversions\nGoogle Cloud Storage\nGoogle Marketing Platform\nGoogle Marketing Platform Offline Conversions\nGoogle Pub/Sub\nHerow\nGoogle Tag Manager\nHeap\nHightouch\nIbotta\nHyperlocology\nIndicative\nImpact\nInMarket\nInMobi\nInspectlet\nInsider\niPost\nIntercom\nironSource\nIterable\nJampp\nKayzen\nKafka\nKlaviyo\nKochava\nKissmetrics\nKubit\nLaunchDarkly\nLeanplum\nLiftoff\nLifeStreet\nLiveLike\nLiveramp\nLocalytics\nMadHive\nmAdme Technologies\nMailchimp\nMarigold\nMediaMath\nMautic\nMediasmart\nMicrosoft Azure Event Hubs\nMintegral\nMixpanel\nMoEngage\nMovable Ink\nMoloco\nMonetate\nMovable Ink - V2\nmyTarget\nMultiplied\nNarrative\nNanigans\nNami ML\nNeura\nOneTrust\nNCR Aloha\nOptimizely\nOracle BlueKai\nOracle Responsys\nPersona.ly\nPaytronix\nPersonify XP\nPieEye\nPilgrim\nPinterest\nPlarin\nPostie\nPrimer\nPunchh\nPushwoosh\nQuantcast\nRadar\nReddit\nRegal\nRetina AI\nRemerge\nReveal Mobile\nRevenueCat\nRTB House\nSailthru\nRokt\nSalesforce Email\nSalesforce Mobile Push\nSamba TV\nScalarr\nSendGrid\nShareThis\nSignal\nSessionM\nShopify\nSimpleReach\nSingular-DEPRECATED\nSingular\nSkyhook\nSlack\nSmadex\nSmarterHQ\nSnapchat\nSnapchat Conversions\nSnowflake\nSnowplow\nSplunk MINT\nAppsFlyer\nSplit\nStartApp\nSprig\nStatsig\nStormly\nSwrve\nTalon.One\nTapad\nTapjoy\nTaptica\nTaplytics\nTeak\nThe Trade Desk\nTikTok Event\nTicketure\nTreasure Data\nQuadratic Labs\nTriton Digital\nTUNE\nTwitter\nValid\nVoucherify\nVkontakte\nWebtrends\nVungle\nWebhook\nWhite Label Loyalty\nWootric\nXandr\nYahoo (formerly Verizon Media)\nYouAppi\nYotpo\nZ2A Digital\nQualtrics\nZendesk\nAudience\n\nThe mParticle Facebook Audiences integration allows you to forward mParticle audience data directly to a Facebook Ads Manager account for use with Facebook Custom Audiences, making it easy to target existing or prospective customers.\n\nPrerequisites\nAccess to a Facebook Ad account\n\nYou must have Full Access to the Facebook Ads account you want to forward your audience data to. To check if you have Full Access:\n\nLog into the Meta Business Suite at business.facebook.com\nSelect the Meta business portfolio you want to send your audience data to.\n\nGo to Settings > Accounts > Ad Accounts, and select the Ad account you want to use. Click Ad account access.\nUnder \u201cPeople with full control\u201d, look for the Facebook account you plan to use when setting up the mParticle integration.\nTo assign full access to someone, click Manage next to their name. You can also assign a new account access by clicking Assign people and selecting their name from the list of people with access to your Meta Business portfolio.\n\nAccept the Facebook Audiences Terms of Service\n\nThe Facebook Ads account you are connecting to must have accepted the following terms of service:\n\nCustom Audiences Terms of Service\nValue Based Custom Audience Terms of Service\nWeb Custom Audience Terms of Service\n\nTo see if you\u2019ve accepted the terms of service:\n\nLog into your Facebook Ad account at adsmanager.facebook.com and click Audiences in the left nav bar.\nMake sure that you have the correct ad account selected using the dropdown menu in the top right.\nYou should be prompted to accept the Facebook Audience terms of service. If you already have accepted these terms of service, you will be presented with the options to create new audiences:\n\nAccess to your Facebook Application ID\n\nIf you want to forward users\u2019 Facebook IDs with your audience data, you must have the Facebook Application ID number for your app or integration. To find your Facebook Application ID, you need to access the Facebook Developer Dashboard. To find your Facebook App ID:\n\nVisit the Meta for Developers website at https://developers.facebook.com/ and log in with your Facebook account credentials.\nClick My Apps in the top nav bar.\nUnder the list of apps on your dashboard, you can find your App ID listed under the name of your app:\n\nOnce you have satisfied the prerequisites described above, follow the steps below to set up the mParticle Facebook audience integration.\n\nFacebook audience integration setup instructions\n\nSetting up and using the audience integration for Facebook includes three stages:\n\nCreating a Facebook audience output in mParticle.\nConnecting an audience in mParticle to Facebook.\nFinding your new mParticle audiences in Facebook for use in a campaign.\n1) Create Facebook audience output\nLog into your mParticle account at app.mparticle.com, and navigate to the Integrations Directory.\nSearch for \u201cFacebook\u201d, hover your cursor over the Facebook tile, and click Setup.\n\nCheck the box labeled Audience, and click Configure.\n\nUnder Account Credentials, click Log In. This opens a separate window where you can connect your Facebook account to mParticle. Click Connect.\n\nEnter a descriptive name under Configuration Name.\nUse the Facebook Account ID dropdown menu to select the Facebook account that is tied to the Facebook Ads account you want to forward your audiences to.\n\nClick Save & Open in Connections.\n2) Connect an audience to Facebook\nIf you just completed step 1.7, then you will be brought to Audiences > Real-time in mParticle. Otherwise, from your mParticle account, navigate to Audiences in the left-hand navigation and select either Real-time or Standard.\nFrom the list of audiences, find the one you want to forward to Facebook and click the \u201cplus sign\u201d + button under Connected Outputs.\n\nClick Connect Output.\n\nSelect Facebook, and use the Configuration Name dropdown menu to select the new configuration you want to send your audience to. Click Next.\n\nToggle the Connection Status to either Inactive or Active. Only active connections will forward audiences. After completing the setup process, it may take up to 24 hours for your audience data to fully populate in Facebook.\nEnter your Facebook Application ID. This is only required if you intend to forward Facebook IDs in the next step.\n\nCheck the box next to each of the following user identities you want to forward to Facebook with your audience:\n\nEmail address (hashed as SHA256)\nPhone number (hashed as SHA256)\nFacebook ID\nApple IDFA\nGoogle GAID\n\nEmail addresses and Facebook IDs result in higher match rates with Facebook users because they are more unique across the Facebook platform, so it\u2019s recommended to use these identities whenever they are available.\n\nCheck Enable Multi-Key Audience to have an additional audience forwarded to Facebook that includes all of the user identities you selected in the previous step.\n\nmParticle creates a new custom audience in your Facebook Ads Manager for each identity that you select. For example, if you select both phone and email, you will see two new custom audiences added: one using user phone numbers and one using their email addresses. If you enable Multi-Key Audiences, you will still see a separate audience for each identity, but you will also see an additional audience that includes all of the identities you selected.\n\nCheck Match on User Attributes to have mParticle forward all available attributes from the following list for each user. This can increase the match rate for your audience. All user attributes are hashed as SHA256 before being forwarded to Facebook.\n\nGender\nFirst name\nLast name\nCity\nState\nCountry\n\nTo create a value-based lookalike audience, check Is Value Based Audience, and use the User Attribute Representing Value dropdown menu to select an mParticle user attribute to forward to Facebook.\n\nThe attribute you select is what Facebook refers to when determining the relative value of a user, and whether or not to include it in the audience. Users with higher values for this attribute are considered more valuable.\nTo allow attributes with a value of 0 to be forwarded, check Allow Zero Values. Negative values are never forwarded.\n\nUnder Customer File Source, select one of the following as the source of your audience data:\n\nDirectly from customers: all audience data was collected directly from the audience members\nCustomers and partners: the audience data was collected both from audience members and from a partner, like a third-party agency\nDirectly from partners: the audience data was collected only from third-party partners\nStarting on July 2, 2018, Facebook requires the disclosure of the origin of audience data. You can read more about this requirement from Facebook.\n\nSelect the identity you want to forward to Facebook as an external email address from the External Email Identity Type dropdown.\n\nIncluding an external email address can help improve your match rate in Facebook.\n\nSelect the identity you want to be hashed and forwarded to Facebook for use as an external unique ID from the Multi-Key External ID Type dropdown.\n\nThis can be any unique ID, like an advertising ID, loyalty membership ID, user ID, or even a cookie ID, and can help increase your audience\u2019s match rate.\nClick Add Connection.\n3) Find your new mParticle audience in Facebook\n\nTo find your new custom audience(s):\n\nNavigate to your Facebook Ads Manager at adsmanager.facebook.com and log in.\nSelect the business portfolio containing the ad account that you used when setting up your audience connection using the left-hand navigation.\nIn the left-hand navigation, click Audiences.\nAfter creating and activating your audience connection between mParticle and Facebook, you will see one or more (depending on your configuration settings) custom audiences added to your Audience list in Facebook Ads Manager.\n\nIt can take up to 24 hours for an mParticle audience to completely sync to Facebook.\nTo edit, share, or create an ad for one of your custom audiences, click the audience name from the list to view the details panel. From the details panel, click the Actions dropdown menu. Certain actions may be unavailable due to missing payment info or a compliance requirement that hasn\u2019t been configured yet for your Ads account.\n\nDeleting an audience\n\nmParticle deletes the downstream audience when you delete an audience from mParticle.\n\nSupported identities\n\nFollowing are the user identities and device identities that can be forwarded to Facebook. Identities that are hashed as SHA256 are noted.\n\nFacebook ID\nEmail address (SHA256)\nPhone number (SHA256)\nApple IDFA (SHA256)\nGoogle Advertising ID (SHA256)\nAndroid Device ID (SHA256)\nUnderstanding your match rate\n\nThe \u201cmatch rate\u201d for a Facebook audience refers to the percentage of your forwarded user data that Facebook successfully matches to active Facebook users.\n\nWhen you connect an mParticle audience with Facebook, several user identifiers are included for each audience member (such as email addresses, phone numbers, Facebook user IDs, or mobile advertiser IDs). After you send an audience from mParticle to Facebook, Facebook attempts to match these identifiers to user profiles on Facebook. The match rate indicates how successful Facebook is in finding corresponding user profiles for the identifiers you provided.\n\nThere are several factors that can impact your match rate:\n\nData quality: more accurate, up-to-date, and complete user data leads to a higher match rate.\nPrivacy settings: Facebook users with stricter privacy or data sharing settings may also impact your match rate.\nIdentifiers used: certain identifiers, like email addresses and Facebook user IDs, have higher match rates due to their uniqueness.\nAudience size: larger audiences have higher match rates because of the increased chance in finding matches within the set of Facebook users. Facebook suggests a minimum audience size of 1,000 people for best results.\n\nA higher match rate means that more of your user data was successfully matched to Facebook users, allowing you to reach a larger audience with your ads or other marketing efforts. However, it\u2019s important to keep in mind that not all of your user data may match with Facebook users, and the match rate may vary from one audience to another.\n\nValue-based audiences\n\nYou can use the mParticle audiences integration to create value-based lookalike audiences. These audiences target users who share similar characteristics and behaviors with your existing high-value customers.\n\nHowever, unlike traditional lookalike audiences, which are based solely on demographic or interest similarities, value-based lookalike audiences are generated using an attribute that reflects the monetary value or profitability of your customers. For example, if you select a lifetime value attribute, Facebook will create a lookalike audience composed of users that have a higher lifetime value.\n\nBy default, mParticle only forwards users in a value-based audience to Facebook if the value of their \u201cvalue reprepresenting attribute\u201d is greater than 0. You can include values equaling zero by checking Allow Zero Values when creating your value-based audience.\n\nValue-based audiences can only be created during the process of first connecting an mParticle audience with Facebook. Audiences that have already been connected to Facebook can\u2019t be turned into value-based audiences.\n\nYou can learn more about value-based lookalike audiences in Facebook\u2019s documentation.\n\nCustomer File Source Requirements\n\nAs of July 2, 2018, Facebook requires that all new audiences disclose whether their data was collected from customers directly, collected from partners, or both. You can provide this information in the connection settings when connecting an audience to Facebook via the Customer File Source setting.\n\nYou can learn more about the Customer File Source Requirement in Facebook\u2019s documentation.\n\nData forwarding\nLimits\n\nFacebook does not allow more than 500 custom audiences to be created per Facebook Ads account. If you attempt to forward more than 500 audiences to Facebook, an error will be returned.\n\nUpload frequency\n\nThe Facebook Audience integration uses bulk forwarding. This means that instead of forwarding audience data in real time, mParticle adds updates to a queue. By default, mParticle forwards data to Facebook from this queue whenever one of the following conditions is met:\n\n3 hours have passed since the last update\nAt least 75,000 messages are in the queue\nVerify and troubleshoot your connection\n\nBefore activating your new connection, or when troubleshooting issues, please be aware of the following:\n\nAre you forwarding audience data to the correct Facebook Ads account?\n\nDuring steps 4-5 of the first stage of the setup process, you connect your Facebook account to mParticle. Make sure that you use a Facebook account that has been granted access to the Facebook Ads account you will forward your audiences to, and make sure that after connecting your Facebook account that you use the Facebook Account ID dropdown menu to select the Facebook Ads account.\n\nIt is possible to have access to multiple Ads accounts, so make sure that you selected the correct Ads account when creating your output.\n\nRead Access to a Facebook Ads account for instructions on how to make sure your Facebook account has the correct access. If you do not have the correct access, reach out to your Facebook Ads account administrator.\n\nIf you are forwarding Facebook IDs, did you add the correct Facebook app ID?\n\nIf you are trying to forward Facebook IDs from mParticle to Facebook, you must provide your Facebook application ID.\n\nOften referred to as an app ID, a Facebook Application ID is a unique ID assigned to your application when you register it on the Facebook Developers platform. This App ID is required to integrate your application with various Facebook functions, such as logging in through Facebook, accessing Facebook\u2019s Graph API, or incorporating social plugins like the Like button.\n\nSee Access to your Facebook Application ID for instructions on how to find your application ID.\n\nData latency\n\nWhile you may see your forwarded mParticle audiences listed in Facebook Ads right away, it can take up to 24 hours before your audience data begins to fully populate. This time can vary depending on how large your audiences are.\n\nWhen looking for your audience data in Facebook, select the correct Business Portfolio\n\nAfter logging into Facebook Ads Manager, make sure that you select the correct Business Portfolio from the left-hand navbar. You should select the Business Portfolio containing the Ads Account you chose to forward your audiences to.\n\nForwarding multi-key audiences to Facebook creates multiple audiences\n\nWhen you select more than one user identity to forward to Facebook, a separate audience will be created in Facebook for each identity you select. If you want to have a single audience containing all of the identities you selected, make sure that you check Enable Multi-Key Audiences when connecting your audience to Facebook.\n\nHowever, even with multi-key audiences enabled, you will still see an additional audience in Facebook for each identity selected.\n\nSettings reference\nConfiguration Settings\nSetting Name\tData Type\tDefault Value\tDescription\nFacebook Account ID\tstring\t\tThis setting is your Facebook account id. You can find it in the Ads Powertool.\nFacebook AccessToken\tstring\t\tThe Facebook access token used to make Graph API calls.\nConnection Settings\nSetting Name\tData Type\tDefault Value\tDescription\nForward Emails\tbool\tTrue\tIf enabled, and the user\u2019s e-mail address is available, the SHA-256 hash of that e-mail address will be added to the audience \u201d<Audience Name> (email)\u201c.\nForward Facebook IDs\tbool\tTrue\tIf enabled, the user\u2019s Facebook ID is available, and the Facebook Application ID property is set, it will be added to the audience \u201d<Audience Name> (fb id)\u201c.\nForward IDFAs\tbool\tTrue\tIf enabled, and the user\u2019s IDFA is available, it will be added to the audience \u201d<Audience Name> (IDFA/GAID)\u201c.\nForward Google Advertising IDs\tbool\tTrue\tIf enabled, and the user\u2019s Google Advertising ID is available, it will be added to the audience \u201d<Audience Name> (IDFA/GAID)\u201c.\nForward Phones\tbool\tTrue\tIf enabled, and the user\u2019s phone number is available, it will be added to the audience \u201d<Audience Name> (phone)\u201c.\nEnable Multi-Key Audience\tbool\tFalse\tIf enabled, all selected identities will also be sent to a single audience for higher match rates.\nMatch on User Attributes\tbool\tFalse\tIf enabled, mParticle will send all possible user attribute values to Facebook for multi-key matching. See user identity matching for more details.\nFacebook Application ID\tstring\t\tThe App ID found on your Facebook application\u2019s dashboard.\nIs Audience Value Based\tbool\tFalse\tIf enabled, the audiences created in Facebook will be value-based.\nUser Attribute Representing Value\tstring\t\tThe user attribute to be used as the basis for setting value in Facebook. Only non-negative numbers will be forwarded to Facebook. This setting only applies to value-based audiences.\nAllow Zero Values\tbool\tFalse\tIf enabled, user data either missing the above user attribute, or having a user attribute value of zero, will be forwarded to Facebook. This setting only applies to value based audiences.\nCustomer File Source\tenum\tUNSELECTED\tIndicates whether the information was collected directly from customers, provided by partners or a combination of the two. Starting July 2, 2018, Facebook requires this setting on all new audiences created.\nExternal Email Identity Type\tenum\tEmail\tThe mParticle User Identity type to forward as an External Email to Facebook.\nMulti-Key External ID Type\tenum\tNone\tThe user identity to be sent to Facebook as the external identity. Note: this identity\u2019s value will have whitespace trimmed, be converted to lowercase, and be hashed prior to sending to Facebook.\nThe Facebook Application ID setting is required if you enable the Forward Facebook IDs setting.\nDOCS\nLast Updated:\n1/8/2025, 6:32:28 PM\nCDP\nmParticle Docs\nHome\nGuides\nDevelopers\nIntegrations\nAnalytics\nAnalytics Docs\nOverview\nGetting Started\nDevelopers\nExpert Training\nPredictive AI\nCortex Docs\nOverview\nGetting Started\nDevelopers\nUsing Cortex\nLearn More\nBlog\nPodcast\nResources\nCommunity\nChange log\nGet in Touch\nHelp Center\nDemo\nBecome a Partner\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nDocumentation\nDevelopers\nAPI References\nClient SDKs\nMedia SDKs\nServer SDKs\nQuickstart\nTools\nGuides\n\nAPI Credential Management\n\nThe Developer's Guided Journey to mParticle\n\nGuides\nGetting Started\nPlatform Guide\nAnalytics\nIDSync\nData Master\nPersonalization\n\nWarehouse Sync\n\nData Privacy Controls\n\nData Subject Requests\n\nDefault Service Limits\n\nFeeds\n\nCross-Account Audience Sharing\n\nApproved Sub-Processors\n\nImport Data with CSV Files\n\nGlossary\n\nVideo Index\n\nAnalytics (Deprecated)\nIntegrations\n24i\nAarki\nABTasty\nAbakus\nActable\nAdChemix\nAdikteev\nAdjust\nAdMedia\nAdobe Marketing Cloud\nAdobe Audience Manager\nAdobe Campaign Manager\nAdobe Target\nAdPredictive\nAgilOne\nAlgolia\nAirship\nAlgoLift\nAlooma\nAmazon Kinesis\nAmazon Advertising\nAmazon Kinesis Firehose\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAmazon SNS\nAdobe Marketing Cloud\nAmobee\nAmpush\nAmplitude\nAnalytics\nAntavo\nAppLovin\nAnodot\nApptentive\nApptimize\nAttractor\nAttentive\nApteligent\nMicrosoft Azure Blob Storage\nBidease\nBing Ads\nBatch\nBluecore\nBluedot\nBranch\nBlueshift\nBugsnag\nBraze\nBranch S2S Event\nCadent\nButton\nCensus\nciValue\nCleverTap\ncomScore\nCortex\nConversant\nCordial\nCriteo\nCrossing Minds\nCustom Feed\nCustomerGlu\nDatabricks\nCustomer.io\nDatadog\nDidomi\nDynalyst\nEdge226\nDynamic Yield\nEmarsys\nEpsilon\nEverflow\nFacebook Offline Conversions\nFiksu\nFacebook\nFlurry\nGoogle Analytics for Firebase\nFlybits\nForeSee\nFormation\nFoursquare\nFreeWheel Data Suite\nGoogle Ad Manager\nGoogle Ads\nGoogle Analytics\nGoogle BigQuery\nGoogle Analytics 4\nGoogle Enhanced Conversions\nGoogle Cloud Storage\nGoogle Marketing Platform\nGoogle Marketing Platform Offline Conversions\nGoogle Pub/Sub\nHerow\nGoogle Tag Manager\nHeap\nHightouch\nIbotta\nHyperlocology\nIndicative\nImpact\nInMarket\nInMobi\nInspectlet\nInsider\niPost\nIntercom\nironSource\nIterable\nJampp\nKayzen\nKafka\nKlaviyo\nKochava\nKissmetrics\nKubit\nLaunchDarkly\nLeanplum\nLiftoff\nLifeStreet\nLiveLike\nLiveramp\nLocalytics\nMadHive\nmAdme Technologies\nMailchimp\nMarigold\nMediaMath\nMautic\nMediasmart\nMicrosoft Azure Event Hubs\nMintegral\nMixpanel\nMoEngage\nMovable Ink\nMoloco\nMonetate\nMovable Ink - V2\nmyTarget\nMultiplied\nNarrative\nNanigans\nNami ML\nNeura\nOneTrust\nNCR Aloha\nOptimizely\nOracle BlueKai\nOracle Responsys\nPersona.ly\nPaytronix\nPersonify XP\nPieEye\nPilgrim\nPinterest\nPlarin\nPostie\nPrimer\nPunchh\nPushwoosh\nQuantcast\nRadar\nReddit\nRegal\nRetina AI\nRemerge\nReveal Mobile\nRevenueCat\nRTB House\nSailthru\nRokt\nSalesforce Email\nSalesforce Mobile Push\nSamba TV\nScalarr\nSendGrid\nShareThis\nSignal\nSessionM\nShopify\nSimpleReach\nSingular-DEPRECATED\nSingular\nSkyhook\nSlack\nSmadex\nSmarterHQ\nSnapchat\n\nAudience\n\nEvent\n\nSnapchat Conversions\nSnowflake\nSnowplow\nSplunk MINT\nAppsFlyer\nSplit\nStartApp\nSprig\nStatsig\nStormly\nSwrve\nTalon.One\nTapad\nTapjoy\nTaptica\nTaplytics\nTeak\nThe Trade Desk\nTikTok Event\nTicketure\nTreasure Data\nQuadratic Labs\nTriton Digital\nTUNE\nTwitter\nValid\nVoucherify\nVkontakte\nWebtrends\nVungle\nWebhook\nWhite Label Loyalty\nWootric\nXandr\nYahoo (formerly Verizon Media)\nYouAppi\nYotpo\nZ2A Digital\nQualtrics\nZendesk\nAudience\n\nSnap is a camera company whose flagship application Snapchat, the Spectacles product, and a variety of publisher tools provide brands a unique platform to reach targeted audience segments with engaging and personalized content.\n\nPrerequisites\n\nIn order to enable the mParticle integration with Snapchat you will need the account credentials for a Snapchat account. The integration activation process will prompt you to log into your Snapchat account, and once logged in, mParticle will automatically retrieve the credentials that it needs to forward audience data to Snapchat. If you intend to use the optional Value Based Audience feature, you will need to request Snap to enable the feature for your ad account and ensure they complete such request before you create a Value Based Audience connection in mParticle.\n\nActivate Snapchat\nSelect Directory, and click the Snapchat tile.\nClick Add Snapchat to Setup.\nSelect the Output Audience Integration Type and click Add to Setup.\nSelect the Snapchat output configuration group and click Configure.\nClick Login to Snapchat - A pop-up window will appear allowing you to enter your account credentials to access your Snapchat account.\n\nEnter a Configuration Name.\nSelect your Account Credentials and click Save.\nClick Accept to authorize mParticle to access your Snapchat account.\n\nYou can now connect your audiences to Snapchat.\nData Processing Notes\nMinimum - An audience is selectable for targeting in Snapchat as long as it has a minimum of 1000 users.\nSnapchat has a limit of 1000 custom audiences per ad account.\nUser Identity Mapping\n\nWhen forwarding audience data to Snapchat, mParticle will send SHA-256 hash of the following identities based on the values of the Connection Settings:\n\nEmails\nDevice IDs (IDFA or Google Advertising ID)\nPhone Numbers (Mobile Telephone Number, Phone Number 2, Phone Number 3, and $mobile reserved user attribute)\nWhen enabling Phone Numbers, please make sure to include country code in phone numbers since it's required by Snap. Ex: +1 800-555-1111.\nUpload Frequency\n\nThe Snapchat Audience Integration uses Bulk Forwarding. Bulk Forwarding means that, instead of uploading updates to an audience in real time, mParticle compiles updates into a queue until either a given amount of time has passed since the last upload, or until a certain number of updates are waiting to be sent.\n\nBy default, mParticle uploads to Snapchat whenever at least one of the following conditions is met:\n\n3 hours have passed since the last update.\nAt least 500000 messages are in the queue.\n\nUpload frequency can sometimes be adjusted. Reach out to your mParticle Customer Success Manager if you need to discuss upload frequency.\n\nDeleting an Audience\n\nWhen you delete a connection or audience, the downstream segment in Snap will be deleted.\n\nValue-based audiences\n\nYou can use the mParticle audience integration to create value-based audiences with Snap. As a prerequisite, Snap must enable value-based audience support for your ad account.\n\nBy default, mParticle will forward all users in a value-based audience to Snap even if their user attribute representing value is missing or negative. You can exclude users with negative, missing, and zero values by unchecking the Allow Zero And Negative Values connection setting when creating your value-based audience.\n\nValue-based audiences can only be created during the process of first connecting an mParticle audience with Snapchat. Audiences that have already been connected to Snapchat can\u2019t be turned into value-based audiences.\nConnection Settings\nSetting Name\tData Type\tDefault Value\tDescription\nForward Emails\tbool\tTrue\tIf enabled, and the user\u2019s e-mail address is available, the SHA-256 hash of that e-mail address will be added to the audience \u201d<Audience Name> (Email)\u201d\nForward Device IDs\tbool\tTrue\tIf enabled, mParticle will forward the SHA-256 hash of that users\u2019 device IDs (IDFA for Apple OS or Google Advertising ID) to the audience \u201d<Audience Name> (Device Id)\u201c.\nForward Phone Numbers\tbool\tFalse\tIf enabled, and any of the user\u2019s phone numbers are available, the SHA-256 hash of those phone numbers will be added to the audience \u201d<Audience Name> (Phone)\u201d\nIs Value Based Audience\tbool\tFalse\tIf enabled, the audience created in Snapchat will be value-based. This can not be changed after the audience has been created.\nUser Attribute Representing Value\tUser Attribute\tUnselected\tThe user attribute to be used as the basis for setting a value in Snapchat. Only positive numbers will be forwarded to Snapchat, unless Allow Zero Values is checked. This setting only applies to value-based audiences.\nAllow Zero And Negative Values\tbool\tTrue\tIf disabled, only user data with positive values will be forwarded to Snapchat. This setting only applies to value-based audiences.\nCurrency\tCurrency Code\tUSD\tThe currency code to be associated with the customer value. This setting only applies to value-based audiences and can not be changed after the audience has been created.\nDOCS\nLast Updated:\n09/01/2025, 00:02:28\nCDP\nmParticle Docs\nHome\nGuides\nDevelopers\nIntegrations\nAnalytics\nAnalytics Docs\nOverview\nGetting Started\nDevelopers\nExpert Training\nPredictive AI\nCortex Docs\nOverview\nGetting Started\nDevelopers\nUsing Cortex\nLearn More\nBlog\nPodcast\nResources\nCommunity\nChange log\nGet in Touch\nHelp Center\nDemo\nBecome a Partner\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nDocumentation\nDevelopers\nAPI References\nClient SDKs\nMedia SDKs\nServer SDKs\nQuickstart\nTools\nGuides\n\nAPI Credential Management\n\nThe Developer's Guided Journey to mParticle\n\nGuides\nGetting Started\n\nCreate an Input\n\nStart capturing data\n\nConnect an Event Output\n\nCreate an Audience\n\nConnect an Audience Output\n\nTransform and Enhance Your Data\n\nPlatform Guide\nAnalytics\nIDSync\nData Master\nPersonalization\n\nWarehouse Sync\n\nData Privacy Controls\n\nData Subject Requests\n\nDefault Service Limits\n\nFeeds\n\nCross-Account Audience Sharing\n\nApproved Sub-Processors\n\nImport Data with CSV Files\n\nGlossary\n\nVideo Index\n\nAnalytics (Deprecated)\nStart capturing data\n\nAfter you create an input, you can begin capturing data.\n\nPrerequisites\n\nBefore you start this activity, you should have already:\n\nCreated an input.\nData in mParticle\n\nmParticle collects two important kinds of data:\n\nEvent data\n\nEvent data is about actions taken by a user in your app. Some events are collected automatically by mParticle\u2019s native SDKs. These include the Session Start events you saw in the Live stream when you first set up your input. Other events need to be captured by writing code in your app. Of these, the most significant are:\n\nScreen/Page Views - keep track of where a user is in your app\nCustom Events - track any actions the user might take in your app, such as clicking a button or watching a video.\nCommerce Events - track purchases and other product-related activity.\nUser data\n\nmParticle also captures data about your user, including their identities, information about the device they are using and any custom attributes you set. As with event data, some user data, such as information about the devices they use, is captured automatically by mParticle\u2019s native SDKs. Two important types of user data must be captured by writing code in your app:\n\nUser identities are unique identifiers for your user, like an email address or customer ID. These are different from the device identities collected automatically by the SDKs, which don\u2019t identify an individual person but a particular cell phone, browser session, or some other device.\n\nUser identities help mParticle keep track of unique users of your app and allow you to track a user\u2019s activity over time and across different devices. To learn a lot more about user and device identities, read our IDSync guide. For now, you just need to know that a user identity is a way of identifying a person, independently of the device they are currently using.\n\nUser Attributes are key-value pairs that can store any custom data about your user. The value of a user attribute can be:\n\nA string\nA number\nA list\nA boolean value (true or false)\nnull - attributes with a null value function as \u2018tags\u2019, and can be used to sort your users into categories.\nCapture User and Event Data\n\nTo start capturing data you will need to go back to your app code. In the previous step you should have installed and initialized the mParticle SDK in at least one of your app platforms. This means you\u2019re already set up to capture Session Start and Session End events, as well as basic data about the device. Grab a friendly developer again, if you need one, and try to add some additional user and event data to your implementation. Here are a few things you might try, with links to the appropriate developer docs:\n\nAdd a Customer ID or Email Address for a user.\nAndroid / iOS / Web\n\n\nCreate a custom user attribute that tells you something about a user. For example: status: \"premium\".\nAndroid / iOS / Web\n\n\nCreate a page or screen view event that captures the name of a page or screen being viewed.\nAndroid / iOS / Web\n\n\nCreate a custom event to track a user action in your app. Include some custom attributes. For example, the mPTravel app sends a custom event when a user views one of its content videos. The event is called \u201cPlay Video\u201d and it has two custom attributes: the category of content, and the travel destination the video promotes. Later on, you\u2019ll see how events like these can be used to target custom messaging.\nAndroid / iOS / Web\n\n\nCreate a purchase event - track a purchase using mParticle\u2019s commerce APIs.\nAndroid / iOS / Web\nVerify: Look for incoming data in the Live Stream\n\nOnce you\u2019ve added code to your app to start collecting some basic data, start up a development build of your app again and trigger some events. Have another look at the Live Stream. You should start to see new event batches, with the individual events you have added to your app.\n\nTroubleshoot\n\nIf you have at least some data in your Live Stream, such as the session start and session end messages generated in the previous step, but your screen views, custom events or purchases aren\u2019t showing, it\u2019s likely that there is an issue with your app code.\n\nCheck that your code is correctly formed, and that the methods which send events to mParticle are actually triggered by user activity in your app.\nReview your development environment\u2019s logs. These will show when an event is sent to mParticle.\nNext steps\n\nExcellent, you\u2019ve started collecting real custom datapoints from your app. At this point you might want to take a quick break to:\n\nExplore the capabilities of the Live Stream\nLearn more about the importance of user identities in mParticle.\n\nNow that you\u2019re collecting data, it\u2019s time to send it on by connecting an event output.\n\nWas this page helpful?\n\nYes\nNo\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nNot Found\n\nYou hit a page that does not exist.\n\nReturn to the home page\nDOCS\nLast Updated:\n1/8/2025, 6:32:28 PM\nCDP\nmParticle Docs\nHome\nGuides\nDevelopers\nIntegrations\nAnalytics\nAnalytics Docs\nOverview\nGetting Started\nDevelopers\nExpert Training\nPredictive AI\nCortex Docs\nOverview\nGetting Started\nDevelopers\nUsing Cortex\nLearn More\nBlog\nPodcast\nResources\nCommunity\nChange log\nGet in Touch\nHelp Center\nDemo\nBecome a Partner\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nDocumentation\nDevelopers\nAPI References\nClient SDKs\nMedia SDKs\nServer SDKs\nQuickstart\nTools\nGuides\n\nAPI Credential Management\n\nThe Developer's Guided Journey to mParticle\n\nGuides\nGetting Started\nPlatform Guide\nAnalytics\nIDSync\nData Master\nPersonalization\n\nWarehouse Sync\n\nData Privacy Controls\n\nData Subject Requests\n\nDefault Service Limits\n\nFeeds\n\nCross-Account Audience Sharing\n\nApproved Sub-Processors\n\nImport Data with CSV Files\n\nGlossary\n\nVideo Index\n\nAnalytics (Deprecated)\nIntegrations\n24i\nAarki\nABTasty\nAbakus\nActable\nAdChemix\nAdikteev\nAdjust\nAdMedia\nAdobe Marketing Cloud\nAdobe Audience Manager\nAdobe Campaign Manager\nAdobe Target\nAdPredictive\nAgilOne\nAlgolia\nAirship\nAlgoLift\nAlooma\nAmazon Kinesis\nAmazon Advertising\nAmazon Kinesis Firehose\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAmazon SNS\nAdobe Marketing Cloud\nAmobee\nAmpush\nAmplitude\nAnalytics\nAntavo\nAppLovin\nAnodot\nApptentive\nApptimize\nAttractor\nAttentive\nApteligent\nMicrosoft Azure Blob Storage\nBidease\nBing Ads\nBatch\nBluecore\nBluedot\nBranch\nBlueshift\nBugsnag\nBraze\nBranch S2S Event\nCadent\nButton\nCensus\nciValue\nCleverTap\ncomScore\nCortex\nConversant\nCordial\nCriteo\nCrossing Minds\nCustom Feed\nCustomerGlu\nDatabricks\nCustomer.io\nDatadog\nDidomi\nDynalyst\nEdge226\nDynamic Yield\nEmarsys\nEpsilon\nEverflow\nFacebook Offline Conversions\nFiksu\nFacebook\nFlurry\nGoogle Analytics for Firebase\nFlybits\nForeSee\nFormation\nFoursquare\nFreeWheel Data Suite\nGoogle Ad Manager\nGoogle Ads\nGoogle Analytics\nGoogle BigQuery\nGoogle Analytics 4\nGoogle Enhanced Conversions\nGoogle Cloud Storage\nGoogle Marketing Platform\nGoogle Marketing Platform Offline Conversions\nGoogle Pub/Sub\nHerow\nGoogle Tag Manager\nHeap\nHightouch\nIbotta\nHyperlocology\nIndicative\nImpact\nInMarket\nInMobi\nInspectlet\nInsider\niPost\nIntercom\nironSource\nIterable\nJampp\nKayzen\nKafka\nKlaviyo\nKochava\nKissmetrics\nKubit\nLaunchDarkly\nLeanplum\nLiftoff\nLifeStreet\nLiveLike\nLiveramp\nLocalytics\nMadHive\nmAdme Technologies\nMailchimp\nMarigold\nMediaMath\nMautic\nMediasmart\nMicrosoft Azure Event Hubs\nMintegral\nMixpanel\nMoEngage\nMovable Ink\nMoloco\nMonetate\nMovable Ink - V2\nmyTarget\nMultiplied\nNarrative\nNanigans\nNami ML\nNeura\nOneTrust\nNCR Aloha\nOptimizely\nOracle BlueKai\nOracle Responsys\nPersona.ly\nPaytronix\nPersonify XP\nPieEye\nPilgrim\nPinterest\nPlarin\nPostie\nPrimer\nPunchh\nPushwoosh\nQuantcast\nRadar\nReddit\nRegal\nRetina AI\nRemerge\nReveal Mobile\nRevenueCat\nRTB House\nSailthru\nRokt\nSalesforce Email\nSalesforce Mobile Push\nSamba TV\nScalarr\nSendGrid\nShareThis\nSignal\nSessionM\nShopify\nSimpleReach\nSingular-DEPRECATED\nSingular\nSkyhook\nSlack\nSmadex\nSmarterHQ\nSnapchat\nSnapchat Conversions\nSnowflake\nSnowplow\nSplunk MINT\nAppsFlyer\nSplit\nStartApp\nSprig\nStatsig\nStormly\nSwrve\nTalon.One\nTapad\nTapjoy\nTaptica\nTaplytics\nTeak\nThe Trade Desk\nTikTok Event\nTicketure\nTreasure Data\nQuadratic Labs\nTriton Digital\nTUNE\nTwitter\n\nEvent\n\nAudience\n\nValid\nVoucherify\nVkontakte\nWebtrends\nVungle\nWebhook\nWhite Label Loyalty\nWootric\nXandr\nYahoo (formerly Verizon Media)\nYouAppi\nYotpo\nZ2A Digital\nQualtrics\nZendesk\nAudience\n\nmParticle\u2019s Twitter Tailored Audience integration enables app owners to push audiences created in Audience Manager to Twitter. Once the integration is activated and your audience is pushed to Twitter, you can then target Promoted Tweets to your Tailored Audiences for engagement, cross-promotion, exclusion targeting or monetization use cases.\n\nPrerequisites\n\nIn order to enable audience forwarding to Twitter, you will need the account credentials for a Twitter account that is linked to your app\u2019s Twitter Ads account. The integration activation process in Audience Manager will prompt you to log into your Twitter account, and once authorized, mParticle will automatically retrieve the credentials that it needs to forward audiences to Twitter. The account you login with will need a permission level of Account Administrator to connect an affiliated Ad Account.\n\nData Processing Notes\nTiming - Real Time Audiences are activated for targeting in near real time, however please note that the true estimate for the audience size may still take up to 60 hours. It is possible to view audience status in the Ads UI via \u201cAudience Manager\u201d dashboard.\nActivate the Integration\nAdd the Twitter Audience integration to your setup from the Directory.\nFrom Setup > Outputs Add a new Twitter Audience Configuration.\nLog into Twitter from the popup, select your advertising account and click Save.\nFrom your audience\u2019s Connect page, add Twitter as an output and select the configuration you created. You can choose whether or not to forward emails, Twitter handles and device IDs to Twitter. Click Add Connection to finalize the connection.\nUser Identity Mapping\n\nDepending on the Configuration Settings that you select (see below), Twitter will use one or more of the following IDs to match users:\n\nEmail address\nTwitter handle\nDevice IDs (IDFA for Apple OS, Google Ad ID for Android)\n\nmParticle sends these IDs as a SHA-256 hash.\n\nDeleting an Audience\n\nmParticle deletes the downstream audience when you delete an audience from mParticle.\n\nConfiguration Settings\nSetting Name\tData Type\tDefault Value\tDescription\nAccount ID\tstring\t\tYour Twitter advertising-enabled account ID\nConnection Settings\nSetting Name\tData Type\tDefault Value\tDescription\nForward Emails\tbool\tTrue\tIf enabled, and the user\u2019s e-mail address is available, the SHA-256 hash of that e-mail address will be added to the audience \u201d<Audience Name> (email)\u201d\nForward Twitter Handles\tbool\tTrue\tIf enabled, mParticle will forward users\u2019 Twitter handles to Twitter for this audience.\nForward Device IDs\tbool\tTrue\tIf enabled, mParticle will forward users\u2019 device IDs (IDFA for Apple OS, Google Ad ID for Android) to Twitter for this audience.\nAdditional Information\n\nFor more information about advertising through Twitter, see Twitter Business.\n\nDOCS\nLast Updated:\n1/8/2025, 6:32:28 PM\nCDP\nmParticle Docs\nHome\nGuides\nDevelopers\nIntegrations\nAnalytics\nAnalytics Docs\nOverview\nGetting Started\nDevelopers\nExpert Training\nPredictive AI\nCortex Docs\nOverview\nGetting Started\nDevelopers\nUsing Cortex\nLearn More\nBlog\nPodcast\nResources\nCommunity\nChange log\nGet in Touch\nHelp Center\nDemo\nBecome a Partner\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nDocumentation\nDevelopers\nAPI References\nClient SDKs\nMedia SDKs\nServer SDKs\nQuickstart\nTools\nGuides\n\nAPI Credential Management\n\nThe Developer's Guided Journey to mParticle\n\nGuides\nGetting Started\nPlatform Guide\nThe New mParticle Experience\nObservability\n\nIntroduction\n\nData Retention\n\nConnections\n\nActivity\n\nLive Stream\n\nData Filter\n\nRules\n\nTiered Events\n\nmParticle Users and Roles\n\nAnalytics Free Trial\n\nTroubleshooting mParticle\n\nUsage metering for value-based pricing (VBP)\n\nAnalytics\nIDSync\nData Master\nPersonalization\n\nWarehouse Sync\n\nData Privacy Controls\n\nData Subject Requests\n\nDefault Service Limits\n\nFeeds\n\nCross-Account Audience Sharing\n\nApproved Sub-Processors\n\nImport Data with CSV Files\n\nGlossary\n\nVideo Index\n\nAnalytics (Deprecated)\nRules\n\nRules allow you to cleanse, enrich and transform your incoming and outgoing data. A rule is a script which accepts an incoming mParticle Events API \u201cbatch\u201d object and modifies it according to your business logic. Some example use-cases for a rule are:\n\nModify a batch\u2019s data\nDrop a batch\nModify an event\u2019s data\nDrop an event from a batch\nAdd events to a batch\nRule execution\n\nEach of your Inputs, such as for your web, mobile, or server-to-server data, has an individually configured data pipeline, and each Input\u2019s pipeline contains the same key stages. Rules are therefore applied for a specific Input\u2019s pipeline, and it\u2019s up to you choose where in that Input\u2019s pipeline each Rule is executed. A single Input pipeline may contain multiple Rules each stage.\n\nmParticle pipeline stages\n\nStage 1 - Input\n\nData is received by mParticle for a specific Input (such as Web, iOS, or a custom server feed).\n\nStage 2 - Storage and Processing\n\nThe Input\u2019s data is stored and processed by mParticle, including:\n\nmParticle\u2019s profile system, which stores user data and enriches the Input\u2019s data based on the existing profile of that user.\nmParticle Data Master tool\n\nStage 3 - Output\n\nThe Input\u2019s data is sent individually to the mParticle Audience system and 300+ partner integrations. In this stage the pipeline actually branches out with a single Input potentially being connected to many Outputs.\n\nRule application\n\nRules are applied to a specific Input\u2019s pipeline. There are two places in the pipeline where rules can be applied:\n\nIn between Stage 1 and Stage 2\n\nRules executed between Stage 1 and Stage 2 affect the data sent to both Stage 2 and then Stage 3, including the mParticle profile store, audience store, and all outputs. These are labeled \u201cAll Output\u201d Rules in your mParticle dashboard.\n\nWarning: If you specify a subtractive rule as All Outputs and apply it between stages 1 and 2, such as dropping events from a batch, you may remove data that you don't intend to remove and the data may not be recoverable because it's dropped before storage and processing.\n\nIn between Stage 2 and Stage 3\n\nYou can also apply a rule right before it\u2019s sent to a specific Output. This lets you mutate data to handle specific requirements or mappings that need to occur for a given partner integration.\n\nRule requirements\nAll rules accept an mParticle Events API batch object and can return a modified or null batch object.\nThere are some differences in error handling and available fields depending on pipeline location. See Rules Developer Guide for details.\nA 200ms timeout applies to all rules. You can choose if a batch should be dropped or continue unprocessed by the rules in the case of a timeout.\n\nRules are executed on the server and only act on data forwarded downstream server-to-server. A warning is shown in the dashboard if you set up one of the following rules:\n\nA rule for integrations that forwards data client-side via a kit.\nA rule for hybrid integrations that support forwarding via client-side and server-to-server.\nIf you are using a rule to modify user identities or user attributes, you must include a \u201cUser Identity Change Event\u201d (user_identity_change) or a \u201cUser Attribute Change Event\u201d (user_attribute_change). See Rules Developer Guide for an example of user_attribute_change in a rule.\nCreate a function in AWS\n\nmParticle rules are hosted in your AWS account as Lambda functions. To do this, you need to be able to provide an Amazon Resource Number (ARN) for your rule. See the AWS Lambda documentation for help creating a function. The Lambda functions used for rules must be hosted in the same AWS region as your mParticle account.\n\nThe name of the function must begin with \u201cmpr\u201d\nYour development rule must have an alias of \u201c$LATEST\u201d\nYour production rule must have an alias of \u201cPROD\u201d\n\nYour ARNs should look something like this:\n\narn:aws:lambda:us-east-1:999999999999:function:mprmylambdafunction:PROD\n\narn:aws:lambda:us-east-1:999999999999:function:mprmylambdafunction:$LATEST\n\nWhen providing an Amazon Resource Number (ARN), you must specify the correct ARN for the localized data center, or pod, for your mParticle organization. Refer to Data Hosting Locations to determine the correct ARN for your pod. If do not know which pod to specify for your organization, contact your account representative.\nIAM user\n\nTo connect to your AWS Lambda function, you must provide the AWS Access Key ID and Secret Access Key for an IAM user.\n\nIn the IAM dashboard, add the following permissions policy for the user:\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"mpRulesLogs\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:DescribeLogGroups\",\n                \"logs:DescribeLogStreams\",\n                \"logs:FilterLogEvents\",\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:us-east-1:*:log-group:/aws/lambda/mpr*\"\n            ]\n        },\n        {\n            \"Sid\": \"mpRulesMetrics\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"cloudwatch:GetMetricStatistics\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        },\n        {\n            \"Sid\": \"mpRulesLambda\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"lambda:InvokeFunction\",\n                \"lambda:GetAlias\"\n            ],\n            \"Resource\": [\n                \"arn:aws:lambda:us-east-1:*:function:mpr*\"\n            ]\n        }\n    ]\n}\nIAM role\n\nYou will also need to create a role in IAM. Assign this role the same policy document created above.\n\nAssign this role to each Lambda function you plan to deploy as an mParticle rule.\n\nCreating a rule in the dashboard\nCreate a rule by navigating to Data Master > Rules\nClick New Rule.\n\nEnter your Development and Production ARNs and click Test.\n\nError handling\n\nWhen you first test a rule, you must select a Failure Action. This determines what happens if your rule throws an unhandled exception. There is no default action, you must select one of the following:\n\nIf you choose Discard, an unhandled exception causes your rule return null, effectively dropping the batch.\nIf you choose Proceed, an unhandled exception causes your rule to return the unaltered batch object, proceeding as if the rule had not been applied.\n\nRegardless of which option you choose, it\u2019s best practice to handle all exceptions in your code, rather than falling back on the above defaults. This is especially true if your rule deals with events, where an unhandled exception from just one event could lead to all events in the batch being dropped.\n\nJavascript syntax\nRule examples and samples use Javascript syntax, but it is possible to use any language supported by AWS Lambda.\nexports.handler=(batch,context,callback)=>{\n    //do something with batch\n    callback(null, batch)\n}\n\nYour code must be a valid Lambda function.\n\nbatch is the complete incoming batch object.\ncontext is a required argument for Lambda functions, but is effectively null for mParticle rules.\nTesting rules\n\nThe first time you test a rule, you are asked to provide a name, description and failure action. After naming a rule, you can test it by using one of the sample templates provided in the Test rule dialog. You can also copy and paste batch JSON from your Live Stream. If you do this, be sure to pick a full batch to copy, not a single event. Click Test to run. Optionally, check a box to save your JSON template in local storage for future testing.\n\nYou must enter valid batch JSON in the code editor.\n\nIf there are any syntactical errors in your code, warning or error icons will display next to the line number with details of the problem so you can correct.\n\nAfter clicking Test, you can examine the JSON output from your function to see that the input has been modified as expected.\n\nAfter a successful test you can click Save to save the Rule. Due to recent updates in AWS Lambda, it may be necessary to wait one minute after a successful test in order to save the Rule.\n\nIf your test fails, try examining the logs for any console output.\n\nVersioning\n\nWhen you first create a rule, by default it will only be applied to DEV data. As well as testing a rule with sample JSON you should test the rule in your dev environment to make sure data reaching your output services is as expected. When you are ready to apply a rule to your production data, click Promote to Prod on the rule page. This will create a \u201cv1\u201d production rule.\n\nNote that before a rule can be promoted to Prod, you must remove all console.log() statements.\n\nIf you need to make changes, choose $LATEST from the Version dropdown. All other versions are read only. Test your changes with your dev environment and, when you are ready, click Promote to Prod to create \u201cv2\u201d of your production rule.\n\nNote that you can have a maximum of 50 versions per rule. If you have too many versions, select a version and click the trash can icon to the right of the version number to delete it.\n\nStatus\n\nEach rule has a master switch in the Settings panel. If there is a problem with your rule, you can switch it off and it will be disabled for all connections until you enable it again. To disable, click Edit in the right sidebar and set the Status slider to inactive.\n\nMetrics\n\nThe following metrics are available:\n\nInvocations - how many times the rule was invoked\nThrottles - how many times a 429 throttling response was returned when calling the rule\nErrors - how many errors have occurred when calling the rule\n\nThese metrics are for the last 24 hours and apply to all connections. Summaries for each rule can be seen on the main rules page. Detailed graph of the previous 24 hours is available on the Monitoring tab of the individual rule page.\n\nLogs\n\nTo help you with troubleshooting rules, mParticle maintains logs for each rule where you can view all console output. From an individual rule page, select the Logs tab. You can filter messages by date range or search for keywords.\n\nDeleting rules\n\nFrom the rules listing, select the Delete action to delete the rule. If the rule is applied to any connections, it will be removed from those connections.\n\nWas this page helpful?\n\nYes\nNo\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\n\nmParticle is a customer data platform (CDP) that simplifies how you collect and connect your user data to hundreds of vendors without needing to manage multiple integrations. We simplify the entire process for you, so you can do more with your data without the hassle of complex integrations.\n\nNew to mParticle? Explore our UI with our interactive demo.\n\nExplore Demo\nGet Started\n1. Collect\n\nSend your first event to mParticle\n\nGo to 'Create an input'\n2. Validate\n\nEnsure data quality\n\nGo to 'Verify'\n3. Connect\n\nForward data to a downstream services\n\nGo to 'Connect an event output'\nDo more with mParticle\nIdentity\n\nManage user identities with IDSync\n\nData Master\n\nView and enforce your data quality\n\nAudiences\n\nEngage customer cohorts\n\nCustom Rules\n\nTransform data as it enters and leaves mParticle\n\nUser Activity View\n\nGet a complete view of your users\n\nUser Privacy\n\nEnsure compliance with GDPR, CCPA, and your privacy policies\n\nEvents API\n\nSend events directly to mParticle\n\nProfile API\n\nReal-time API to drive user personalization\n\nFirehose API\n\nBuild your own custom integrations\n\nInputs\nClient SDKs\n\nAndroid\n\niOS\n\nWeb\n\nView all\nEvents API\n\nHTTP\n\nNode\n\nPython\n\nRuby\n\nJSON Reference\n\nView all\nPartner Feeds\n\nBranch\n\nAppsFlyer\n\nBraze\n\nView all\nCustom CSV Feed\n\nCustom CSV Feed\n\nView all\nOutputs\nEvents\n\nMixpanel\n\nAmplitude\n\nFacebook\n\nBraze\n\nAppsFlyer\n\nView all\nAudience\n\nFacebook\n\nSnapchat\n\nTwitter\n\nBraze\n\nPinterest\n\nView all\nDOCS\nLast Updated:\n09/01/2025, 00:02:28\nCDP\nmParticle Docs\nHome\nGuides\nDevelopers\nIntegrations\nAnalytics\nAnalytics Docs\nOverview\nGetting Started\nDevelopers\nExpert Training\nPredictive AI\nCortex Docs\nOverview\nGetting Started\nDevelopers\nUsing Cortex\nLearn More\nBlog\nPodcast\nResources\nCommunity\nChange log\nGet in Touch\nHelp Center\nDemo\nBecome a Partner\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nDocumentation\nDevelopers\nAPI References\nClient SDKs\nMedia SDKs\nServer SDKs\nQuickstart\nTools\nGuides\n\nAPI Credential Management\n\nThe Developer's Guided Journey to mParticle\n\nGuides\nGetting Started\nPlatform Guide\nAnalytics\nIDSync\nData Master\nPersonalization\n\nWarehouse Sync\n\nData Privacy Controls\n\nData Subject Requests\n\nDefault Service Limits\n\nFeeds\n\nCross-Account Audience Sharing\n\nApproved Sub-Processors\n\nImport Data with CSV Files\n\nImport Data with CSV Files\n\nCSV File Reference\n\nGlossary\n\nVideo Index\n\nAnalytics (Deprecated)\nImport Data with CSV Files\n\nYou can import bulk data, both user attribute and events, from your data warehouse or legacy system using comma separated values (CSV) files. You can use this technique in all regions and with all outputs.\n\nUse the following process to load user attribute or event data from CSV files:\n\n1. Create CSV files\n\nFile guidelines\nData guidelines\n2. Get credentials for the mParticle SFTP server\n3. Configure the Custom CSV Feed\n4. Drop CSV files on the SFTP server\n1. Create CSV files\n\nPrepare the CSV files for import. Files must follow the guidelines in this section.\n\nFile guidelines\nFiles must adhere to the RFC4180 standards for CSV formatting.\n\nFiles must be sent in one of the following formats:\n\nA plain CSV (.csv)\nA ZIP file containing one or more CSV files (.zip)\nA gzipped CSV (.csv.gz).\nA PGP/GPG-encrypted file with the additional extension .gpg appended, for example, .csv.gpg or .csv.gz.gpg). Only encrypted OR unencrypted files can be accepted, but not both. You must use PGP encryption with mParticle\u2019s public key. See Encrypted files for additional instructions.\nFile sizes should be between 5 MB and 2 GB. If you upload files outside these limits, processing speed is slower. If possible, split the data across multiple small files, because their processing can be parallelized.\nEach file can only contain events of the same event type. You can\u2019t mix events of different types in the same file.\nDon\u2019t use subfolders.\nEach row size should be under 80 KB. Larger rows may impact performance.\nAll column names must be unique.\nEach CSV file must contain fewer than 50 columns.\n\nFile name requirements:\n\nDo not use any dashes ( - ) or dots ( . ) in your file name, other than what is described below.\nEnd the file name based on the event content in your file: -custom_event.csv -commerce_event.csv -screen_view.csv -eventless.csv for eventless uploads of user identities and attributes\n\nColumn names: specify fields according to our JSON Schema, using dot notation.\n\nColumn names described are case sensitive.\nData guidelines\nEnvironment: include a column name environment set to development or production. If an environment column is not included, data is ingested into the production environment.\n\nUser and Device IDs: as with any data sent to mParticle, you must include a column with at least one user ID or device ID.\n\nDevice IDs\n\ndevice_info.android_advertising_id\ndevice_info.android_uuid\ndevice_info.ios_advertising_id\ndevice_info.att_authorization_status\ndevice_info.ios_idfv\ndevice_info.roku_advertising_id\ndevice_info.roku_publisher_id\ndevice_info.fire_advertising_id\ndevice_info.microsoft_advertising_id\n\nUser IDs\n\nmpid\nuser_identities.customerid\nuser_identities.email\nuser_identities.facebook\nuser_identities.microsoft\nuser_identities.twitter\nuser_identities.yahoo\nuser_identities.other\nuser_identities.other2\nuser_identities.other3\nuser_identities.other4\nImportant: CSV files must have all the required identity columns, and the rows must have valid values in those columns to prevent processing errors.\n\nUser attributes:\n\nIf you include user attributes, for each, include a column named as user_attributes.key, where key is a user attribute key. For example:\n\nuser_attributes.$FirstName\nuser_attributes.communication_preference\nuser_attributes.Member Tier\n\nAttribute names with spaces are allowed and do not require quotes. All the keys listed in the JSON Reference are supported.\n\nEvents:\n\nUse a column named events.data.timestamp_unixtime_ms to set the event time.\nUse a column named events.data.custom_attributes.key, where key is an event attribute key, to set custom event attributes.\nAttribute names with spaces are allowed and do not require quotes. All the keys listed in the JSON Reference are supported.\nScreen view events: use a column named events.data.screen_name if you want to include the screen name.\nCustom events: use columns named events.data.event_name and events.data.custom_event_type to include custom events.\n\nCommerce events: use columns with the following names for commerce events.\n\nevents.data.product_action.action\nevents.data.product_action.products.id\nevents.data.product_action.products.name\nevents.data.product_action.products.category\nevents.data.product_action.products.brand\nevents.data.product_action.products.variant\nevents.data.product_action.products.position\nevents.data.product_action.products.price\nevents.data.product_action.products.quantity\nevents.data.product_action.products.coupon_code\nevents.data.product_action.products.added_to_cart_time_ms\nevents.data.product_action.products.total_product_amount\nevents.data.product_action.products.custom_attributes\n\nOnly one product per event can be included for commerce events uploaded via CSV.\n\nData types:\n\nAll data in the CSV is converted to a string. The only exceptions to this are values that require a particular data type, such as MPID or IDFA.\n\nOnly standard custom events and screen views, and eventless batches (eventless drops of user identity and attributes), have been tested.\nAttributes sent as arrays are not fully supported. When the entire array is present in a single cell of the CSV file, it is supported and is converted to string. Because there is no way of specifying anything but the first item in an array, repeated header columns, each subsequent column overwrites the previous one. Multiple columns don\u2019t append to the array. This is why you can only include one product for ecommerce events. Commerce events in the Events API support arrays in multiple places, but with CSV files, you can only populate a single item in each of these arrays.\nCustom manifest: You can use a custom manifest to drop files created in another system without transforming them. For details, see Use a custom manifest.\n2. Get credentials for the mParticle SFTP server\n\nmParticle maintains an SFTP server where you will drop your CSV files. Use the following instructions to securely retrieve your credentials and find the hostname and path to use when you drop your files on the SFTP server.\n\nTo get your SFTP username and password:\n\nSign up for a Keybase account with your work email at https://keybase.io/. Keybase is a secure tool which includes end-to-end encrypted chat.\nProvide your Keybase account name to your Customer Success Manager or your mParticle Solutions Consultant so that they can pass it on to our Ops team.\nExpect to receive your SFTP access credentials in a Keybase chat from mParticle. Note that if you need to use credentials that you already have, you\u2019ll share those credentials in the Keybase chat.\n3. Configure the Custom CSV Feed\n\nConfigure the Custom CSV Feed as input. This step provides the hostname and folder path on the SFTP server where your CSV files must be dropped.\n\nSubdirectories within this drop folder are not supported. To maintain multiple directories, configure a different input for each directory.\n\nTo configure the Custom CSV Feed:\n\nVisit SETUP > Inputs > Feeds in the mParticle UI and click the Add Feed Input button, then select Custom CSV Feed from the list. \n\nIf you\u2019ve already added the Custom CSV Feed, it won\u2019t show up in the list. Scroll through the list of feeds until you see Custom CSV Feed, and then click the large plus sign in the gray bar to create a new feed. You need one feed for each different event type.\n\nEnter the following values:\n\nConfiguration Name: enter a name that makes this feed easy to recognize in your list of feeds.\nCustom Event Name: if you are importing a custom event, enter the name that will be used for the custom event.\nCustom Event Type: if you entered a custom event name, select the event type.\nCustom Manifest: if you are using a custom manifest, paste it in the text box provided.\nExpect Encrypted Files: if you will import a PGP/GPG-encrypted file, select this option. \nAfter you complete the connection configuration, click Issue SFTP Details. mParticle displays your hostname and path for mParticle\u2019s SFTP server. \n4. Drop CSV files on the SFTP server\nConnect to the mParticle SFTP server using the credentials provided. Once you have connected, the mParticle creates the drop folder. If you don\u2019t see one, create a folder named drop.\nCreate a new folder inside the drop folder, and name it using the pathname provided in the mParticle UI as shown in the previous section. For example, based on the previous example, the folder path and name is sftp.mparticle.com:4422:drop/us1-123456789123456789/. Hint: Verify that there are no trailing spaces in the name.\nUse your credentials to upload your CSV files to mParticle\u2019s SFTP server, using the correct path and folder name from the previous step.\n\nFiles on the SFTP location are added to the processing queue nearly immediately. Depending on file count and file size, a backlog may develop. You can observe how much data has been processed using Data Master and your outbound connections. There is no notification of processing progress or completion.\n\nCSV File Reference provides more information about processing behavior and the advanced techniques of working with custom manifests or encrypted files.\n\nWas this page helpful?\n\nYes\nNo\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nNot Found\n\nYou hit a page that does not exist.\n\nReturn to the home page\nDOCS\nLast Updated:\n1/8/2025, 6:32:28 PM\nCDP\nmParticle Docs\nHome\nGuides\nDevelopers\nIntegrations\nAnalytics\nAnalytics Docs\nOverview\nGetting Started\nDevelopers\nExpert Training\nPredictive AI\nCortex Docs\nOverview\nGetting Started\nDevelopers\nUsing Cortex\nLearn More\nBlog\nPodcast\nResources\nCommunity\nChange log\nGet in Touch\nHelp Center\nDemo\nBecome a Partner\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nDocumentation\nDevelopers\nAPI References\nClient SDKs\nMedia SDKs\nServer SDKs\nQuickstart\nTools\nGuides\n\nAPI Credential Management\n\nThe Developer's Guided Journey to mParticle\n\nGuides\nGetting Started\n\nCreate an Input\n\nStart capturing data\n\nConnect an Event Output\n\nCreate an Audience\n\nConnect an Audience Output\n\nTransform and Enhance Your Data\n\nPlatform Guide\nAnalytics\nIDSync\nData Master\nPersonalization\n\nWarehouse Sync\n\nData Privacy Controls\n\nData Subject Requests\n\nDefault Service Limits\n\nFeeds\n\nCross-Account Audience Sharing\n\nApproved Sub-Processors\n\nImport Data with CSV Files\n\nGlossary\n\nVideo Index\n\nAnalytics (Deprecated)\nCreate an Input\nIntroduction\n\nThe purpose of this guide is to walk you through the basic steps of setting up mParticle in your app, unlocking core functionality, and troubleshooting common issues. Along the way, you\u2019ll cover some important concepts you need to understand to be successful with mParticle.\n\nThis is not a complete guide to all of mParticle\u2019s features and capabilities. If you already know your way around mParticle and you\u2019re looking for in-depth docs, head to our Developers or Guides sections.\n\nMost of this guide is aimed at users of the mParticle Dashboard, but to follow along with the tutorials, you will need to set up the mParticle SDK in your iOS, Android or web app, so you may need support from a developer to complete some steps.\nExamples\n\nThe tutorials in this guide follow the process of setting up mParticle in the mPTravel app: a mobile and web app that sells luxury travel packages to its users.\n\nLater on in this guide, you\u2019ll learn about sending data from mParticle to some of our many integration partners. As examples, the tutorials use services which are simple to set up and verify, and which offer a free account tier, so that you will be able follow the examples exactly if you wish. However, mParticle is agnostic about which integrations you choose and you can follow the same basic steps from this guide to implement any of our integrations.\n\nInputs and Outputs\n\nOne of the key functions of mParticle is to receive your data from wherever it originates, and send it wherever it needs to go. The sources of your data are inputs and the service or app where it is forwarded are outputs. A connection is a combination of an input and output.\n\nInputs include:\n\nApps or services built on any platform we support, such as iOS, Android, or Web. You can view the full list in SETUP > Inputs in the PLATFORMS tab.\nData feeds of any other data you want to send into mParticle. This could be data you have collected yourself or from a feed partner. Once configured, feed inputs are listed in SETUP > Inputs on the FEEDS tab.\nOutputs may be configured for events, audiences, cookie syncs, or data subject requests depending on what the output supports. You can see the list of configured outputs in SETUP > Outputs or SETUP > Data Warehouses. Outputs include:\nAnalytics partners such as Indicative\nAdvertising partners such as Facebook\nIn-app messaging partners such as Braze\nData Warehouse partners, such as Amazon Redshift, Google BigQuery, or Snowflake\n\nTo get started with mParticle, you need some data, which means you need to create at least one input.\n\nCreate Access Credentials\n\nThe first thing you need to do is to to create a set of access credentials that will allow a client-side SDK or a server-side application to forward data to this workspace.\n\nLogin to your mParticle account. If you\u2019re just getting started, your first workspace is created for you. The first screen you see is an overview of activity in the workspace. Since you haven\u2019t yet sent any data, there\u2019s nothing to report, so far. \nNavigate to Setup > Inputs in the left column. Here you can see each platform type accepted by mParticle. Different platforms are associated with different sets of metadata, such as device identifiers, and most outputs only accept data from a limited set of platforms, so it is important to select the right platform. To capture data from your native Android app, choose Android. Just click the + next to your chosen platform. \nClick Issue Keys.\n\nCopy and save the generated Key and Secret. \nAbout Access Credentials\n\nmParticle labels the credentials you create for an integration the key and secret, but they are not exactly like an API key and secret, since you embed these credentials in the app. However, this is not the security risk that exposing API credentials would be:\n\nThe client-side key and secret can\u2019t read data from the system.\nYou can block bad data to stop any traffic that doesn\u2019t match the data you expect as defined in your schema.\n\nMost anonymous client-server architectures, including Adobe, Braze, Firebase, Google Analytics, and Segment don\u2019t have per-session or per-instance credentials, nor does mParticle.\n\nInstall and Initialize an mParticle SDK\n\nYou need a developer to help you install and initialize an SDK. See the Getting Started guides for the iOS, Android or Javascript SDKs to get set up before continuing.\n\nVerify: Look for Incoming Data in the Live Stream\nNavigate to Activity > Live Stream in the left column. The Live Stream lets you inspect all incoming data from your development environments. It\u2019s an easy way to check that you have correctly initialized mParticle in your app. When you first open up the Live Stream, it will be empty, as we haven\u2019t yet started sending data. \nStart up a development build of your app (get a developer to help you if necessary). The mParticle SDKs automatically collect and forward data about installs and user sessions, so just by opening a development build of your app, you should start to see data in the Live Stream. \nAdvanced Platform Configuration Settings\n\nFor the iOS, Android, tvOS, and Web platforms, some advanced configuration settings are available. To change these settings, navigate to Setup > Inputs in the left column and select either iOS, Android, tvOS, or Web from the list of platforms.\n\nExpand the Advanced Settings by clicking the + icon.\n\nRestrict Device ID by Limit Ad Tracking\n\niOS, Android, and tvOS (Apple TV) devices allow users to limit the collection of advertising IDs. Advertising IDs are unique identifiers you may use to associate event and user data with a specific device. For both iOS and Android devices, if a user has not provided explicit consent to share their device\u2019s advertising ID, then the value of that ID is set to an all-zero value.\n\nBy checking Restrict Device ID by Limit Ad Tracking, mParticle will not collect advertising IDs from users who have enabled the Limit Ad Tracking setting on their device.\n\nRemember, mParticle will collect advertising IDs for both iOS and Android devices, regardless of whether or not a user has enabled the Limit Ad Tracking setting on their device. However, the IDs collected from users who have opted out will be all-zero values.\n\nFollowing are descriptions of Apple and Google\u2019s policies for device advertising IDs:\n\nIOS ADVERTISING IDS\n\nAfter the release of iOS 14.5, Apple introduced the App Tracking Transparency (ATT) framework, which requires app developers to request users\u2019 explicit consent to share their advertising IDs. If a user of your app has not provided this consent, Apple\u2019s advertising ID (IDFA) will be set to all an all-zero value: 00000000-0000-0000-0000-000000000000. Read more about Apple advertising identifiers in their documentation.\n\nFor more information about the ATT framework, visit the iOS 14 Guide.\n\nANDROID ADVERTISING IDS\n\nGoogle allows Android users to opt out from sharing their devices\u2019 advertising IDs. Similar to Apple\u2019s policy, Google will set a user\u2019s advertising ID (GAID or AAID) to an all-zero value if that user has opted out from sharing their ID. Read more about Google\u2019s advertising identifiers in their documentation.\n\nCollect Integration-Specific Identifiers\n\nThe Web SDK can collect integration-specific identifiers to enrich the user data forwarded to your connected outputs.\n\nWhen Collect Integration-Specific Identifiers is checked, these integration-specific identifiers are collected and used to enrich your user data to help optimize the match rate of your audiences in downstream tools. Currently, these identifiers include Facebook\u2019s fbc and fbp fields.\n\nSUPPORTED INTEGRATIONS\nVendor\tIdentifier\tCollection Method\tMaps to\nFacebook Click ID\tfbclid\turl query parameter\tFacebook.ClickId\nFacebook Click ID\tfbc\tbrowser cookie\tFacebook.ClickId\nFacebook Browser ID\tfbp\tbrowser cookie\tFacebook.BrowserId\nGoogle GCLID\tgclid\turl query parameter\tGoogleEnhancedConversions.Gclid\nGoogle GWBRAID\tgwbraid\turl query parameter\tGoogleEnhancedConversions.Gbraid\nGoogle WBRAID\twbraid\turl query parameter\tGoogleEnhancedConversions.Wbraid\nTroubleshoot\n\nIf you don\u2019t see data appearing in the Live Stream within the first few minutes after opening a development build:\n\nCheck that you have copied your Key and Secret correctly\nCheck that you have properly included the mParticle SDK in your project and it is correctly initialized. The necessary steps will differ depending on the platform. Check our iOS, Android and Web docs for more.\nNext Steps\n\nCongratulations, you have created a working data input. Now it\u2019s time to start capturing some data.\n\nWas this page helpful?\n\nYes\nNo\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nNot Found\n\nYou hit a page that does not exist.\n\nReturn to the home page\nDOCS\nLast Updated:\n1/8/2025, 6:32:28 PM\nCDP\nmParticle Docs\nHome\nGuides\nDevelopers\nIntegrations\nAnalytics\nAnalytics Docs\nOverview\nGetting Started\nDevelopers\nExpert Training\nPredictive AI\nCortex Docs\nOverview\nGetting Started\nDevelopers\nUsing Cortex\nLearn More\nBlog\nPodcast\nResources\nCommunity\nChange log\nGet in Touch\nHelp Center\nDemo\nBecome a Partner\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nDocumentation\nDevelopers\nAPI References\nClient SDKs\nMedia SDKs\nServer SDKs\nQuickstart\nAndroid\nHTTP Quick Start\niOS Quick Start\nJava Quick Start\nNode Quick Start\nPython Quick Start\nWeb\n\nOverview\n\nStep 1. Create an input\n\nStep 2. Verify your input\n\nStep 3. Set up your output\n\nStep 4. Create a connection\n\nStep 5. Verify your connection\n\nStep 6. Track events\n\nStep 7. Track user data\n\nStep 8. Create a data plan\n\nTools\nGuides\n\nAPI Credential Management\n\nThe Developer's Guided Journey to mParticle\n\nGuides\nGetting Started\nPlatform Guide\nAnalytics\nIDSync\nData Master\nPersonalization\n\nWarehouse Sync\n\nData Privacy Controls\n\nData Subject Requests\n\nDefault Service Limits\n\nFeeds\n\nCross-Account Audience Sharing\n\nApproved Sub-Processors\n\nImport Data with CSV Files\n\nGlossary\n\nVideo Index\n\nAnalytics (Deprecated)\nOverview\n\nmParticle is a customer data platform that makes it easy to collect and organize data before sending it to product analytics, A/B testing, marketing automation, and data warehousing tools.\n\nmParticle provides several SDKs and APIs allowing you to collect data from a variety of sources, like a mobile app, web app, or data feeds from other SaaS providers.\n\nYou can collect two types of data with mParticle:\n\nEvent data: data that describes what your users are doing\nUser data: data that describes who your users are\nLearn how to integrate the Web SDK from start to finish\n\nTo demonstrate how mParticle works, you will learn how to track basic event data like page views and purchase events in a web app using the mParticle web SDK. Then, you will learn how to send that data to a webhook. You will also learn how to manage your data quality by creating a data plan.\n\nmParticle is extremely flexible. There are thirteen SDKs for specific platforms like web, iOS, and Android, in addition to several APIs. mParticle also provides over 250 integrations with data warehouse, analytics, and marketing automation tools.\n\nTo keep your first steps with mParticle quick and easy, this tutorial uses the web SDK and a sample ecommerce app built using React called The Higgs Shop.\n\nThis is a technical tutorial for developers. If you aren\u2019t a developer, you can find a general introduction to mParticle in the Platform Guide.\nAbout the Web SDK\n\nmParticle\u2019s Web SDK supports commonly used web browsers, connected TVs, and other client-side environments using JavaScript and TypeScript.\n\nBy initializing the Web SDK in your app, you gain access to useful methods you can call in your app\u2019s code to send events and user data to mParticle.\n\nPrerequisites\nAccess to an mParticle instance\n\nIn order to begin sending data from your app to mParticle, you will need access to an mParticle account and an API key.\n\nDownload the mParticle sample app\n\nThis tutorial demonstrates how mParticle works using The Higgs Shop, a sample web application built using React.\n\nOpen your terminal or command prompt, and clone the web sample app repository on GitHub.\nInstall the sample app\nWhile in your terminal or command prompt, navigate to the root folder of the web sample app: /mparticle-sample-apps/mparticle-web-sample-apps/core-sdk-samples/higgs-shop-sample-app\nIf you don\u2019t have Node.js or npm installed, install them now. Optionally, you may also install Node Version Manager (nvm) which is a helpful tool for installing and managing different versions of Node.\nInstall the sample app\u2019s dependencies with npm install\nOpen the folder /higgs-shop-sample-app in your IDE or text editor. Rename the file .env.sample to .env. We will replace the environment variable REACT_APP_MPARTICLE_API_KEY with the API Key in step 5\nTo run the sample app from your computer, run npm start. This will start a development server before automatically opening The Higgs Shop in a new browser window.\nIt may take up to 2 minutes to launch the sample app.\nSince we haven\u2019t added our API key to the sample app\u2019s config file yet, we will receive a warning message. We will add our API key in the next step.\nFor now, ignore the API key warning and close the page and stop the development server in your terminal or command prompt. In the next step, we\u2019ll create and add our API key before restarting the sample app.\n\nNext >> Create an input\n\nWas this page helpful?\n\nYes\nNo\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nDocumentation\nDevelopers\nAPI References\nClient SDKs\nMedia SDKs\nServer SDKs\nQuickstart\nTools\nGuides\n\nAPI Credential Management\n\nThe Developer's Guided Journey to mParticle\n\nGuides\nGetting Started\nPlatform Guide\nAnalytics\nIDSync\nData Master\nPersonalization\n\nWarehouse Sync\n\nData Privacy Controls\n\nData Subject Requests\n\nDefault Service Limits\n\nFeeds\n\nCross-Account Audience Sharing\n\nApproved Sub-Processors\n\nImport Data with CSV Files\n\nGlossary\n\nVideo Index\n\nAnalytics (Deprecated)\nIntegrations\n24i\nAarki\nABTasty\nAbakus\nActable\nAdChemix\nAdikteev\nAdjust\nAdMedia\nAdobe Marketing Cloud\nAdobe Audience Manager\nAdobe Campaign Manager\nAdobe Target\nAdPredictive\nAgilOne\nAlgolia\nAirship\nAlgoLift\nAlooma\nAmazon Kinesis\nAmazon Advertising\nAmazon Kinesis Firehose\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAmazon SNS\nAdobe Marketing Cloud\nAmobee\nAmpush\nAmplitude\nAnalytics\nAntavo\nAppLovin\nAnodot\nApptentive\nApptimize\nAttractor\nAttentive\nApteligent\nMicrosoft Azure Blob Storage\nBidease\nBing Ads\nBatch\nBluecore\nBluedot\nBranch\nBlueshift\nBugsnag\nBraze\n\nFeed\n\nAudience\n\nForwarding Data Subject Requests\n\nEvent\n\nBranch S2S Event\nCadent\nButton\nCensus\nciValue\nCleverTap\ncomScore\nCortex\nConversant\nCordial\nCriteo\nCrossing Minds\nCustom Feed\nCustomerGlu\nDatabricks\nCustomer.io\nDatadog\nDidomi\nDynalyst\nEdge226\nDynamic Yield\nEmarsys\nEpsilon\nEverflow\nFacebook Offline Conversions\nFiksu\nFacebook\nFlurry\nGoogle Analytics for Firebase\nFlybits\nForeSee\nFormation\nFoursquare\nFreeWheel Data Suite\nGoogle Ad Manager\nGoogle Ads\nGoogle Analytics\nGoogle BigQuery\nGoogle Analytics 4\nGoogle Enhanced Conversions\nGoogle Cloud Storage\nGoogle Marketing Platform\nGoogle Marketing Platform Offline Conversions\nGoogle Pub/Sub\nHerow\nGoogle Tag Manager\nHeap\nHightouch\nIbotta\nHyperlocology\nIndicative\nImpact\nInMarket\nInMobi\nInspectlet\nInsider\niPost\nIntercom\nironSource\nIterable\nJampp\nKayzen\nKafka\nKlaviyo\nKochava\nKissmetrics\nKubit\nLaunchDarkly\nLeanplum\nLiftoff\nLifeStreet\nLiveLike\nLiveramp\nLocalytics\nMadHive\nmAdme Technologies\nMailchimp\nMarigold\nMediaMath\nMautic\nMediasmart\nMicrosoft Azure Event Hubs\nMintegral\nMixpanel\nMoEngage\nMovable Ink\nMoloco\nMonetate\nMovable Ink - V2\nmyTarget\nMultiplied\nNarrative\nNanigans\nNami ML\nNeura\nOneTrust\nNCR Aloha\nOptimizely\nOracle BlueKai\nOracle Responsys\nPersona.ly\nPaytronix\nPersonify XP\nPieEye\nPilgrim\nPinterest\nPlarin\nPostie\nPrimer\nPunchh\nPushwoosh\nQuantcast\nRadar\nReddit\nRegal\nRetina AI\nRemerge\nReveal Mobile\nRevenueCat\nRTB House\nSailthru\nRokt\nSalesforce Email\nSalesforce Mobile Push\nSamba TV\nScalarr\nSendGrid\nShareThis\nSignal\nSessionM\nShopify\nSimpleReach\nSingular-DEPRECATED\nSingular\nSkyhook\nSlack\nSmadex\nSmarterHQ\nSnapchat\nSnapchat Conversions\nSnowflake\nSnowplow\nSplunk MINT\nAppsFlyer\nSplit\nStartApp\nSprig\nStatsig\nStormly\nSwrve\nTalon.One\nTapad\nTapjoy\nTaptica\nTaplytics\nTeak\nThe Trade Desk\nTikTok Event\nTicketure\nTreasure Data\nQuadratic Labs\nTriton Digital\nTUNE\nTwitter\nValid\nVoucherify\nVkontakte\nWebtrends\nVungle\nWebhook\nWhite Label Loyalty\nWootric\nXandr\nYahoo (formerly Verizon Media)\nYouAppi\nYotpo\nZ2A Digital\nQualtrics\nZendesk\nAudience\n\nBraze is a comprehensive customer engagement platform that powers relevant experiences between consumers and brands they love. Braze helps brands foster human connection through interactive conversations across channels.\n\nmParticle audiences correspond to Braze\u2019s Segments feature and can be used to target engagement campaigns in Braze.\n\nPrerequisites\n\nIn order to activate our Braze integration, you\u2019re going to need the API key for each app that you\u2019d like to set up, which can be found by logging into your Braze account and navigating to your app\u2019s settings. You will also need to create an App Group REST API Key in the Developer Console.\n\nBraze Instance\n\nBraze maintains several instances. As part of the Configuration Settings, you need to specify which one your data should be forwarded to. You can tell your Braze Instance from the URL of your Braze Dashboard.\n\nInstance\tDashboard URL\nUS 01 Cluster\thttps://dashboard-01.braze.com\nUS 02 Cluster\thttps://dashboard-02.braze.com\nUS 03 Cluster\thttps://dashboard-03.braze.com\nUS 04 Cluster\thttps://dashboard-04.braze.com\nUS 05 Cluster\thttps://dashboard-05.braze.com\nUS 06 Cluster\thttps://dashboard-06.braze.com\nUS 08 Cluster\thttps://dashboard-08.braze.com\nEU 01 Cluster\thttps://dashboard-01.braze.eu\nEU 02 Cluster\thttps://dashboard-02.braze.eu\n\nCheck with your Braze account manager if you are unsure which Braze instance you are using.\n\nThere is also the ability to specify a Custom instance, which allows you to specify separate endpoints for REST, SDK and Javascript.\n\nImportant: Your Custom Endpoint settings should be your URL's Authority. For example: sdk.iad-01.braze.com, not https://sdk.iad-01.braze.com.\n\nUsing https:// or a trailing / in your endpoint address will cause errors.\n\nUser Identity Mapping\n\nmParticle will attempt to match users in Braze based on IDFAs, Android Device IDs, and a customizable External User ID field, which uniquely identifies a user in Braze. You can set which Identity Type to use as the External User ID in the Configuration Settings. Your External User ID should be a unique, permanent identifier \u2014 usually Customer ID or Email. If you are also using the Event integration, make sure you set the same External User ID across both integrations.\n\nForwarding Audiences\n\nThe Braze API does not allow mParticle to directly create and maintain membership of segments in Braze, so the Audience integration works by setting attributes on a user, which you can then use to define a corresponding segment in Braze. Like mParticle, Braze populates it\u2019s Segment Manager based on the actual data points received, so you need to create audiences in mParticle and connect them to Braze first. Then, provided your audience is not empty, the segment membership attributes should become available in the Braze Segment Manager within a few minutes.\n\nmParticle offers seven ways to set segment membership attributes, controlled by the Send Segments As Configuration Setting. The drop-down has the following options:\n\nSingle String Attribute (default)\nSingle Array Attribute\nOne Attribute Per Segment\nBoth Single Array Attribute and Single String Attribute\nBoth Single Array Attribute and One Attribute Per Segment\nBoth Single String Attribute and One Attribute Per Segment\nSingle Array Attribute, Single String Attribute, and One Attribute Per Segment\n\nThese options will be explained in further detail below.\n\nSample Audience Membership\n\nNote that Audience IDs can be found in the main Audience list view.\n\nFor example, note the Audience IDs for these three audiences:\n\n13053\n13052\n13051\nSingle String Attribute\nWarning: Braze string attributes have a max length of 255 characters. If your users' audience memberships are being truncated, choose one of the options involving `Single Array Attribute` instead.\n\nThis is the default behavior. mParticle creates a single custom attribute in Braze for each user, called SegmentMembership. The value of this attribute is a comma-separated string of mParticle audience IDs that match the user.\n\nUsing the above examples, a user who is a member of Test Audience 1, Test Audience 2, and Test Audience 3 will show the attribute SegmentMembership with a value of \"'13053','13052', '13051'\" in Braze. See the Sample Braze Profile for an example.\n\nTo target members of Test Audience 1, for example, you need to create a matching segment in Braze using the mParticle Audience ID, with the filter SegmentMembership \u2014 matches regex \u2014 13051. It\u2019s important to choose the matches regex option, and not equals, or users with membership in more than one audience will not be matched.\n\nWarning: Do not select `Single Attribute` for multiple configurations using the same credentials and cluster. Each configuration will send its own set of audience IDs to the same profile in Braze which will cause them to overwrite each other. If you need to use multiple configurations, please use the `Single Attribute` for at most one configuration and use `One Attribute Per Segment` for others.\nSingle Array Attribute\nWarning: Braze array attributes have a max length of 25. If any of your users are members of >25 audiences, membership info will be truncated by Braze. To work around this, contact your Braze representative to increase your max array length threshold.\n\nmParticle creates a single custom array attribute in Braze for each user, called SegmentMembershipArray. The value of this attribute is an array of mParticle audience IDs that match the user.\n\nUsing the above examples, a user who is a member of Test Audience 1, Test Audience 2, and Test Audience 3 will show the attribute SegmentMembershipArray with a value of [\"13053\",\"13052\", \"13051\"] in Braze. Note, however, that Braze parses array attributes to make them more readable.\n\nSee the Sample Braze Profile for an example.\n\nTo target members of Test Audience 1, for example, you need to create a matching segment in Braze with the filter SegmentMembershipArray \u2014 includes value \u2014 13051\n\nOne Attribute Per Segment\n\nmParticle creates a custom attribute in Braze for each audience that a user belongs to, based on the External Name of the audience.\n\nUsing the above examples, a user who is a member of audience Test Audience 1 will show the attribute In Test Audience 1 is true in Braze.\n\nTo target members of Test Audience 1, for example, you need to create a matching segment in Braze with the filter In Test Audience 1 \u2014 equals \u2014 true.\n\nBoth Single Array Attribute and Single String Attribute\n\nmParticle will send attributes as described by both Single Array Attribute and Single String Attribute.\n\nBoth Single Array Attribute and One Attribute Per Segment\n\nmParticle will send attributes as described by both Single Array Attribute and One Attribute Per Segment.\n\nBoth Single String Attribute and One Attribute Per Segment\n\nmParticle will send attributes as described by both Single String Attribute and One Attribute Per Segment.\n\nSingle Array Attribute, Single String Attribute, and One Attribute Per Segment\n\nmParticle will send attributes as described by Single Array Attribute, Single String Attribute and One Attribute Per Segment.\n\nSample Braze Profile\n\nHere is a sample Braze profile with all options enabled. \n\nDeactivating and Deleting Connections\n\nSince mParticle does not directly maintain segments in Braze, it will not delete segments when the corresponding mParticle audience connection is deleted or deactivated. When this happens, mParticle will not update the audience user attributes in Braze to remove the audience from each user.\n\nDeleting an Audience\n\nDeleting an audience does not remove the custom attributes in Braze.\n\nConfiguration Settings\nSetting Name\tData Type\tDefault Value\tDescription\nAPI Key\tstring\t\tYour app\u2019s API Key can be found in your Braze dashboard.\nAPI Key Operating System\tenum\tUnselected\tSelect which operating system your Braze API key corresponds to. This selection will limit the types of push tokens that will be forwarded on an audience update.\nSend Segments As\tenum\tSingle String Attribute\tThe method of sending audiences to Braze. Audience membership can be uploaded as a custom attribute containing comma-separated audience IDs, a custom array attribute containing audience IDs, and / or one attribute per audience with a boolean value to indicate membership. Warning: If multiple configurations use the same credentials and cluster, options involving single attributes may cause values to be overwritten on Braze\u2019s end.\nApp Group REST API Key\tstring\t\tThe App Group REST API Key can be found in the developer console section of the Braze dashboard.\nExternal Identity Type\tenum\tCustomer ID\tThe mParticle User Identity type to forward as an External ID to Braze.\nEmail Identity Type\tenum\tEmail\tThe mParticle User Identity Type to forward as the Email to Braze.\nBraze Instance\tenum\tUS 03 Cluster\tSpecify which cluster your Braze data will be forwarded to. Please ensure you are contractually authorized to use the EU cluster if you select that option. If you choose \u2018Custom\u2019, you will need to provide separate endpoints for your SDK, Server, and Web data.\nForward Push Tokens\tbool\tTrue\tIf enabled, mParticle will forward user Push Tokens to Braze.\nDOCS\nLast Updated:\n1/8/2025, 6:32:28 PM\nCDP\nmParticle Docs\nHome\nGuides\nDevelopers\nIntegrations\nAnalytics\nAnalytics Docs\nOverview\nGetting Started\nDevelopers\nExpert Training\nPredictive AI\nCortex Docs\nOverview\nGetting Started\nDevelopers\nUsing Cortex\nLearn More\nBlog\nPodcast\nResources\nCommunity\nChange log\nGet in Touch\nHelp Center\nDemo\nBecome a Partner\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nDocumentation\nDevelopers\nAPI References\nClient SDKs\nMedia SDKs\nServer SDKs\nQuickstart\nTools\nGuides\n\nAPI Credential Management\n\nThe Developer's Guided Journey to mParticle\n\nGuides\nGetting Started\nPlatform Guide\nAnalytics\nIDSync\nData Master\nPersonalization\n\nWarehouse Sync\n\nData Privacy Controls\n\nData Subject Requests\n\nDefault Service Limits\n\nFeeds\n\nCross-Account Audience Sharing\n\nApproved Sub-Processors\n\nImport Data with CSV Files\n\nGlossary\n\nVideo Index\n\nAnalytics (Deprecated)\nData Privacy Controls\n\nManage your consent and opt-out privacy obligations under the GDPR and CCPA with Data Privacy Controls. This feature is not prescriptive and there is no single way to implement consent or opt-out. Instead, mParticle gives you a simple, standard technique for storing and applying consent and opt-out choices. Consent state powers both GDPR consent and CCPA data sale opt-out.\n\nThis guide introduces you to mParticle\u2019s data privacy control functionality and shows you how to collect an individual\u2019s consent and apply it to your data flows.\n\nCommon uses of data privacy controls\n\nData privacy controls are flexible and customizable, allowing you to build any data flow or consent-based logic you need.\n\nUse mParticle\u2019s data privacy controls to help comply with CCPA\u2019s \u201cdo not sell my data\u201d requirement by collecting users who opt-out and blocking those users\u2019 data from flowing to any \u2018data sale\u2019 output by:\n\nRecording a CCPA data sale opt-out as a user consent (more information below)\nIdentify which outputs count as \u2018data sale\u2019 and apply the below forwarding rule to them\nApplying a forwarding rule of: Do not forward if CCPA Data sale opt out is present\n\nGDPR defines consent as one method of lawful data processing. One common setup is to:\n\nDefine a processing purpose of \u2018marketing\u2019\nPrompt users for affirmative consent for \u2018marketing\u2019\nIdentify which outputs would perform \u2018analytics\u2019 processing\nApply a forwarding rule of: Only forward user data if GDPR Consent for \u2018marketing\u2019 is true\nData privacy and the mParticle platform\n\nOnce enabled and configured, data privacy work with the mParticle platform to ingest and pass on consent state:\n\nDefine categories of data collection called consent purposes.\nStore the consent state in a user\u2019s profile.\nControl data flow based on stored consent.\nSend user consent state to your integrations (outputs).\n\nEnabling data privacy controls\n\nData privacy controls save user consent decisions and applies them to data flows.\n\nEnable GDPR and/or CCPA compliance features on your workspace from Workspace Settings > Workspace > Regulation.\n\nFor GDPR, create a set of purposes from Privacy > Privacy Settings in the dashboard.\nOnce a purpose has been added it cannot be removed.\nA purpose defines the scope of data collection and processing to which the user may consent. GDPR recognizes several different purposes for data collection including the possibility of a user consenting to some purposes of data collection but not others. mParticle does not limit you to a specific set of purposes, but rather lets you define your own purposes when you set up a workspace.\nFor CCPA, once it is enabled in your workspace, the purpose data_sale_opt_out is automatically created. The SDKs and mParticle UIs facilitate using this purpose, so you don\u2019t need to hardcode it anywhere.\n\nConsent properties\n\nThe mParticle format for a single record of a user decision on a privacy prompt, .consent, is our consent_state object. This is used for both GDPR-style opt-in consent and for CCPA-style opt-out.\n\nFor each user or workspace, consent state can be stored for each possible combination of regulation and purpose. For each purpose, the following fields are supported.\n\nAll fields are optional, except consented, timestamp_unixtime_ms, regulation and purpose. The regulation and purpose fields are built into the structure. Be sure to include your privacy and compliance experts when deciding how to implement optional fields.\n\nThe `consented` field indicates the user's decision to the data processing defined in the purpose. It does not imply any additional permissions.\nProperty\n\t\nType, Required\n\t\nExample Values\n\t\nDescription\n\t\nregulation\tstring Required\tgdpr\tThe regulation under which a user consent or preference is being saved. Currently gdpr and ccpa are supported.\t\npurpose\tstring Required\tgeolocation\tA data processing purpose that describes the type of processing done on the data subject\u2019s data. For GDPR, purposes must be defined in mParticle before using them in a consent_state object. For CCPA, this is not required as the default CCPA purpose is data_sale_opt_out\t\nconsented\tbool \u00a0 \u00a0 Required\ttrue / false\tFor GDPR, this records the user\u2019s decision on the prompted consent purpose. If the user agreed (true) or rejected (false). For CCPA, set this to true if the user has opted-out of data sale and false if they have not opted-out of data sale.\t\ntimestamp_unixtime_ms\tnumber Required\t1510949166\tA timestamp for the creation of the consent state. mParticle\u2019s SDKs send this field automatically. If using the HTTP API, this field must be set manually.\t\ndocument\tstring \u00a0 \u00a0 \u00a0 \u00a0Not required\t\"v23.2b\"\tAn identifier for the document, document version or experience that the user may have consented to.\t\nlocation\tstring \u00a0 \u00a0 \u00a0 \u00a0Not required\t\"example.com/\", \"17 Cherry Tree Lane\"\tA location where the user gave consent. This property exists only to provide additional context. It may be a physical or digital location.\t\nhardware_id\tstring Required\t\"IDFA:a5d934n0-232f-4afc-2e9a-3832d95zc702\"\tA hardware ID for the device or browser used to give consent. This property exists only to provide additional context and is not used to identify users.\t\nExample consent state\n\nConsent state can be logged via the HTTP API simply by including a consent state object in a batch, mirroring the structure of the user profile (above):\n\n\"consent_state\": {\n \"gdpr\": {\n   \"location_collection\": {\n     \"document\": \"location_collection_agreement.v43\",\n     \"consented\": true,\n     \"timestamp_unixtime_ms\": 1523039002083,\n     \"location\": \"dtmgbank.com/signup\",\n     \"hardware_id\": \"IDFA:a5d934n0-232f-4afc-2e9a-3832d95zc702\"\n   },\n   \"parental\": {\n     \"document\": \"standard_parental_consent.v2\",\n     \"consented\": true,\n     \"timestamp_unixtime_ms\": 1523039002083,\n     \"location\": \"dtmgbank.com/signup\",\n     \"hardware_id\": \"IDFA:a5d934n0-232f-4afc-2e9a-3832d95zc702\"\n   }\n },\n \"ccpa\":{\n   \"data_sale_opt_out\":{\n     \"consented\": true,\n     \"timestamp_unixtime_ms\": 1579198790480\n   }\n }\n}\nCollecting consent state\n\nFor detailed definitions of how to report consent state, review the sections of our API and SDK references that cover data privacy controls:\n\nWeb SDK\niOS SDK\nAndroid SDK\nAMP SDK\nHTTP API\n\nAdditionally, our integration with OneTrust allows you to ingest customer consent states into mParticle.\n\nUsing consent state\nUser profiles\n\nConsent state is maintained per person on the User Profile using the structure defined above.\n\nFor testing consent, you can use User Activity View to check that a consent was recorded correctly. Here is an example of how CCPA data sale opt-out will appear:\n\nAudiences\n\nConsent state can be used to create conditions in the Audience Builder to check a users\u2019 consent state as a requirement for audience inclusion or exclusion.\n\nFor example, for CCPA you may want to include only users who have NOT opted out of data sale, by adding a criteria like this:\n\nFor GDPR, you may want to include only users that have an opt-in consent for a given purpose, shown here as \u2018Advertising\u2019:\n\nConnections and forwarding rules\n\nConsent state can be used to create forwarding rules that selectively filter data based on a users consent state, in real time and per-person.\n\nFor example, you can choose to only forward data from users who have given consent for a particular purpose. \n\nFor CCPA, you may want a forwarding rule to apply a data sale opt-out. In this example, users\u2019 who have a consent state of true for the CCPA purpose of data_sale_opt_out will NOT have their data forwarded (if the consent state is missing or false for that purpose, data will flow): \n\nFor GDPR, you may want a forwarding rule to only send data when a single purpose is consented: \n\nKITS AND FORWARDING RULES\n\nIf you set up a Forwarding Rule for an embedded kit integration, the iOS and Android SDKs will check consent status for the user on initialization. If the rule condition fails, the kit will not be initialized. Note that kits are only initialized when a session begins or on user change, so if consent status changes in the course of a session, while mParticle will immediately stop forwarding data to the kit, it is possible that an embedded kit may remain active and independently forwarding data to a partner from the client until the session ends.\n\nForwarding consent state to partners\n\nWhen the consent state of a profile changes, that change can be communicated to mParticle event integrations. If the consent_state object on an incoming event batch contains changes from the existing profile, mParticle adds a \u2018system notification\u2019 to the batch for each consent state change before the batch is sent to incoming forwarders. This notification contains the full old and new consent state objects:\n\n\"system_notifications\": [\n {\n   \"data\": {\n     \"purpose\": \"location_collection\",\n     \"current\": {\n       \"regulation\": \"GDPR\",\n       \"document\": \"location_collection_agreement_v4\",\n       \"consented\": false,\n       \"timestamp_unixtime_ms\": 1523045332033,\n       \"location\": \"17 Cherry Tree Lane\",\n       \"hardware_id\": \"IDFA:a5d934n0-232f-4afc-2e9a-3832d95zc702\"\n     },\n     \"old\": {\n       \"regulation\": \"GDPR\",\n       \"document\": \"location_collection_agreement_v4\",\n       \"consented\": true,\n       \"timestamp_unixtime_ms\": 1523039002083,\n       \"location\": \"17 Cherry Tree Lane\",\n       \"hardware_id\": \"IDFA:a5d934n0-232f-4afc-2e9a-3832d95zc702\"\n     }\n   },\n   \"type\": \"gdpr_change\"\n }\n]\n\nThere are currently two ways that consent state changes are forwarded to mParticle event integrations:\n\nSome partners accept raw event batch data from mParticle, mostly for data storage or custom analytics use cases. For these partners, mParticle will forward the \u2018system_notifications\u2019 object with each relevant event batch. Forwarding of system notifications can be turned off with a user setting. Integrations that can currently receive the system notifications object include:\n\nAmazon Kinesis\nAmazon S3\nAmazon SNS\nAmazon SQS\nGoogle Pub/Sub\nGoogle Cloud Storage\nMicrosoft Azure Event Hubs\nSlack\nWebhook\n\nmParticle is working with other partners to support forwarding consent state changes as a Custom Event. These events contain the new consent state information as custom attributes, a custom event type of \"Consent\", and an event name of \"Consent Given\" or \"Consent Rejected\". These consent events are forwarded to supporting partners as standard custom events.\n\n{\n \"data\": {\n   \"event_name\": \"Consent Given\",\n   \"custom_event_type\": \"Consent\",\n   \"custom_attributes\": {\n     \"consented\": \"true\",\n     \"document\": \"location_collection_agreement_v4\",\n     \"hardware_id\": \"IDFA:a5d934n0-232f-4afc-2e9a-3832d95zc702\",\n     \"purpose\": \"location_collection\",\n     \"location\": \"17 Cherry Tree Lane\",\n     \"regulation\": \"GDPR\",\n     \"timestamp_unixtime_ms\": 1523039002083\n   },\n   \"event_type\": \"custom_event\"\n }\n}\n\nPartners that currently accept these custom consent state events include:\n\nAmplitude\nSnowplow\nMixpanel\nGoogle Analytics\nSalesforce DMP\n\n\u201cGDPR Consent Change\u201d is listed as a data type in the Integrations directory and we will update this list as more partners add support. Please reach out to your success manager if you would like to distribute consent to an additional partner.\n\nData subject requests\n\nmParticle helps you respond to data subject requests as mandated by the GDPR and CCPA regulations.\n\nYou can search for integrations that support data subject requests in the Integrations page. Search on category Data Subject Request.\n\nIngest GPC signals\n\nThe California Consumer Protection Act (CCPA) and the upcoming CPRA (California Privacy Rights Act) require that users can signal their privacy choices. In support of that requirement, you can ingest Global Privacy Control (GPC) signals with mParticle.\n\nBrowsers append the GPC signal to HTTP requests and make it queryable via the DOM. This signal is designed to convey a person\u2019s request to websites and services to not sell or share their personal information with third parties, per the Global Privacy Control specification. This opt-out is at the browser level, allowing users to turn on the GPC signal for all or specific websites.\n\nThe workflow for ingesting and forwarding GPC signals via SDK or Events API:\n\nSample code for GPC\n\nThis sample code show two options: mapping to a GDPR purpose and mapping to a user attribute.\n\n/*\n First, grab the GPC signal. \"true\" indicates the user has signaled an opt-out\n See here for more details on querying the GPC signal:\n https://globalprivacycontrol.github.io/gpc-spec/\n*/\nvar gpcSignal = navigator.globalPrivacyControl;\n\n\n/*\n Option 1:\n In this example, the GPC signal is mapped to a \"targeting_collection\" GDPR purpose.\n This is only an example, you determine the GDPR purposes and how GPC maps to them.\n You can do the same mapping to CPPA.\n*/\nvar targeting_consent = mParticle.Consent.createGDPRConsent(\n   !gpcSignal, // note that this is inverted\n   Date.now(), // Timestamp\n   \"browser_gpc_signal\", // Document\n   \"17 Cherry Tree Lane\", // Location\n   \"browser-id:a5d934n0-232f-4afc-2e9a-3832d95zc702\" // Hardware ID\n);\n\n\n// Add to your consent state\nvar consentState = mParticle.Consent.createConsentState();\nconsentState.addGDPRConsentState(\"targeting_collection\", targeting_consent);\nvar user = mParticle.Identity.getCurrentUser();\nuser.setConsentState(consentState);\n\n\n/*\n Option 2:\n In this example, the GPC signal is mapped to a user attribute\n*/\nvar user = mParticle.Identity.getCurrentUser();\nuser.setUserAttribute(\"gpc_signal\", gpcSignal);\n\nWas this page helpful?\n\nYes\nNo\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nDocumentation\nDevelopers\nAPI References\nClient SDKs\nMedia SDKs\nServer SDKs\nQuickstart\nTools\nGuides\n\nAPI Credential Management\n\nThe Developer's Guided Journey to mParticle\n\nGuides\nGetting Started\nPlatform Guide\nAnalytics\nIDSync\nData Master\nPersonalization\n\nWarehouse Sync\n\nData Privacy Controls\n\nData Subject Requests\n\nDefault Service Limits\n\nFeeds\n\nCross-Account Audience Sharing\n\nApproved Sub-Processors\n\nImport Data with CSV Files\n\nGlossary\n\nVideo Index\n\nAnalytics (Deprecated)\nIntegrations\n24i\nAarki\nABTasty\nAbakus\nActable\nAdChemix\nAdikteev\nAdjust\nAdMedia\nAdobe Marketing Cloud\nAdobe Audience Manager\nAdobe Campaign Manager\nAdobe Target\nAdPredictive\nAgilOne\nAlgolia\nAirship\nAlgoLift\nAlooma\nAmazon Kinesis\nAmazon Advertising\nAmazon Kinesis Firehose\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAmazon SNS\nAdobe Marketing Cloud\nAmobee\nAmpush\nAmplitude\nAnalytics\nAntavo\nAppLovin\nAnodot\nApptentive\nApptimize\nAttractor\nAttentive\nApteligent\nMicrosoft Azure Blob Storage\nBidease\nBing Ads\nBatch\nBluecore\nBluedot\nBranch\n\nEvent\n\nForwarding Data Subject Requests\n\nFeed\n\nBlueshift\nBugsnag\nBraze\nBranch S2S Event\nCadent\nButton\nCensus\nciValue\nCleverTap\ncomScore\nCortex\nConversant\nCordial\nCriteo\nCrossing Minds\nCustom Feed\nCustomerGlu\nDatabricks\nCustomer.io\nDatadog\nDidomi\nDynalyst\nEdge226\nDynamic Yield\nEmarsys\nEpsilon\nEverflow\nFacebook Offline Conversions\nFiksu\nFacebook\nFlurry\nGoogle Analytics for Firebase\nFlybits\nForeSee\nFormation\nFoursquare\nFreeWheel Data Suite\nGoogle Ad Manager\nGoogle Ads\nGoogle Analytics\nGoogle BigQuery\nGoogle Analytics 4\nGoogle Enhanced Conversions\nGoogle Cloud Storage\nGoogle Marketing Platform\nGoogle Marketing Platform Offline Conversions\nGoogle Pub/Sub\nHerow\nGoogle Tag Manager\nHeap\nHightouch\nIbotta\nHyperlocology\nIndicative\nImpact\nInMarket\nInMobi\nInspectlet\nInsider\niPost\nIntercom\nironSource\nIterable\nJampp\nKayzen\nKafka\nKlaviyo\nKochava\nKissmetrics\nKubit\nLaunchDarkly\nLeanplum\nLiftoff\nLifeStreet\nLiveLike\nLiveramp\nLocalytics\nMadHive\nmAdme Technologies\nMailchimp\nMarigold\nMediaMath\nMautic\nMediasmart\nMicrosoft Azure Event Hubs\nMintegral\nMixpanel\nMoEngage\nMovable Ink\nMoloco\nMonetate\nMovable Ink - V2\nmyTarget\nMultiplied\nNarrative\nNanigans\nNami ML\nNeura\nOneTrust\nNCR Aloha\nOptimizely\nOracle BlueKai\nOracle Responsys\nPersona.ly\nPaytronix\nPersonify XP\nPieEye\nPilgrim\nPinterest\nPlarin\nPostie\nPrimer\nPunchh\nPushwoosh\nQuantcast\nRadar\nReddit\nRegal\nRetina AI\nRemerge\nReveal Mobile\nRevenueCat\nRTB House\nSailthru\nRokt\nSalesforce Email\nSalesforce Mobile Push\nSamba TV\nScalarr\nSendGrid\nShareThis\nSignal\nSessionM\nShopify\nSimpleReach\nSingular-DEPRECATED\nSingular\nSkyhook\nSlack\nSmadex\nSmarterHQ\nSnapchat\nSnapchat Conversions\nSnowflake\nSnowplow\nSplunk MINT\nAppsFlyer\nSplit\nStartApp\nSprig\nStatsig\nStormly\nSwrve\nTalon.One\nTapad\nTapjoy\nTaptica\nTaplytics\nTeak\nThe Trade Desk\nTikTok Event\nTicketure\nTreasure Data\nQuadratic Labs\nTriton Digital\nTUNE\nTwitter\nValid\nVoucherify\nVkontakte\nWebtrends\nVungle\nWebhook\nWhite Label Loyalty\nWootric\nXandr\nYahoo (formerly Verizon Media)\nYouAppi\nYotpo\nZ2A Digital\nQualtrics\nZendesk\nFeed\n\nBranch helps mobile apps grow with deep links that power referral systems, sharing links, invites and marketing links with full attribution and analytics.\n\nInput Data Details\n\nThe following types of data can be configured to be sent from Branch to mParticle\n\nAttribution\n\nBranch attribution events are mapped as follows:\n\nEvent Type = Custom Event\nCustom Event Type = attribution\nEvent Name = If this is an install attribution, the Event Name will be attribution. Other types of attribution events setup in Branch will be sent using Event Names of Purchase, Add to Cart, View Items, Product View, etc.\nBranch Event Mapping\n\nBranch attribution events are mapped as follows:\n\nGeneral Fields\nmParticle Field\tDescription\nevent_id\tUnique mParticle ID for the Event. Mapped to the Branch install event ID\ntimestamp_unix_ms\tUnix time in milliseconds\napplication_info\tmParticle Application info\ndevice_info\tmParticle Device info, including IDFA, IDFV or Android Advertising ID where available.\nip\tIP Address of the device\nCustom Attributes\n\nMost attribution events will include the following custom event attributes:\n\ncampaign\npublisher\nad_set_name\nchannel\nclickid\nfeature\ntags\n\nDepending on your Branch setup, Branch may also send other custom attributes. These attributes may be prefixed with symbols.\n\n~ indicates an analytics tag\n$ indicates a Branch configuration parameter\n+ indicates a parameter not instrumented by the user but added by Branch to enrich the event\nUser Identity Mapping\n\nThe Branch user_data_developer_identity can be sent to mPID, Customer ID, or any Other ID using the ID Mappings dropdown in Branch. Additionally, device identifiers associated with the act-as platform will also be provided by Branch.\n\nConfiguration\n\nIn mParticle, configure the Branch Input. Create a separate feed configuration for each platform (iOS, Android), and copy the Server Key and Secret. Follow Branch\u2019s instructions to use your Server Key and Secret to configure the postback in Branch.\n\nDOCS\nLast Updated:\n09/01/2025, 00:02:28\nCDP\nmParticle Docs\nHome\nGuides\nDevelopers\nIntegrations\nAnalytics\nAnalytics Docs\nOverview\nGetting Started\nDevelopers\nExpert Training\nPredictive AI\nCortex Docs\nOverview\nGetting Started\nDevelopers\nUsing Cortex\nLearn More\nBlog\nPodcast\nResources\nCommunity\nChange log\nGet in Touch\nHelp Center\nDemo\nBecome a Partner\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nNot Found\n\nYou hit a page that does not exist.\n\nReturn to the home page\nDOCS\nLast Updated:\n1/8/2025, 6:32:28 PM\nCDP\nmParticle Docs\nHome\nGuides\nDevelopers\nIntegrations\nAnalytics\nAnalytics Docs\nOverview\nGetting Started\nDevelopers\nExpert Training\nPredictive AI\nCortex Docs\nOverview\nGetting Started\nDevelopers\nUsing Cortex\nLearn More\nBlog\nPodcast\nResources\nCommunity\nChange log\nGet in Touch\nHelp Center\nDemo\nBecome a Partner\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nDocumentation\nDevelopers\nAPI References\nClient SDKs\nMedia SDKs\nServer SDKs\nQuickstart\nAndroid\n\nOverview\n\nStep 1. Create an input\n\nStep 2. Verify your input\n\nStep 3. Set up your output\n\nStep 4. Create a connection\n\nStep 5. Verify your connection\n\nStep 6. Track events\n\nStep 7. Track user data\n\nStep 8. Create a data plan\n\nStep 9. Test your local app\n\nHTTP Quick Start\niOS Quick Start\nJava Quick Start\nNode Quick Start\nPython Quick Start\nWeb\nTools\nGuides\n\nAPI Credential Management\n\nThe Developer's Guided Journey to mParticle\n\nGuides\nGetting Started\nPlatform Guide\nAnalytics\nIDSync\nData Master\nPersonalization\n\nWarehouse Sync\n\nData Privacy Controls\n\nData Subject Requests\n\nDefault Service Limits\n\nFeeds\n\nCross-Account Audience Sharing\n\nApproved Sub-Processors\n\nImport Data with CSV Files\n\nGlossary\n\nVideo Index\n\nAnalytics (Deprecated)\nOverview\n\nmParticle is a customer data platform that makes it easy to collect and organize data before sending it to product analytics, A/B testing, marketing automation, and data warehousing tools.\n\nmParticle provides several SDKs and APIs allowing you to collect data from a variety of sources, like a mobile app, web app, or data feeds from other SaaS providers.\n\nYou can collect two types of data with mParticle:\n\nEvent data: data that describes what your users are doing\nUser data: data that describes who your users are\nLearn how to integrate the Android SDK from start to finish\n\nTo demonstrate how mParticle works, you will learn how to track basic event data like page views and purchase events in a web app using the mParticle Android SDK. Then, you will learn how to send that data to a webhook. You will also learn how to manage your data quality by creating a data plan.\n\nmParticle is extremely flexible. There are thirteen SDKs for specific platforms like web, iOS, and Android, in addition to several APIs. mParticle also provides over 250 integrations with data warehouse, analytics, and marketing automation tools.\n\nTo keep your first steps with mParticle quick and easy, this tutorial uses the Android SDK and a sample ecommerce app called The Higgs Shop.\n\nThis is a technical tutorial for developers. If you aren\u2019t a developer, you can find a general introduction to mParticle in the Platform Guide.\nAbout the Android SDK\n\nmParticle\u2019s Android SDK supports all Android devices and tablets, including Amazon Fire TV.\n\nBy initializing the Android SDK in your app, you gain access to useful methods you can call in your app\u2019s code to send events and user data to mParticle.\n\nPrerequisites\nAccess to an mParticle instance\n\nIn order to begin sending data from your app to mParticle, you will need access to an mParticle account and an API key.\n\nDownload the mParticle sample app\n\nThis tutorial demonstrates how mParticle works using The Higgs Shop, a sample Android application written in the Kotlin programming language.\n\nOpen your terminal or command prompt, and clone the Android sample app repository on GitHub.\nOpen the sample app in your IDE and configure your virtual device\nIf you don\u2019t already have Android Studio installed on your computer, download and install the correction version for your OS now. You may use another IDE, but we recommend Android Studio.\nOpen your cloned directory for the Android sample app in Android Studio or your IDE of choice.\nIf you haven\u2019t configured a virtual device to emulate the sample app in, click the Device Manager button in the top tool bar.\nClick Create device.\nSelect a device category and name in the modal window. We recommend Phone and any version of Pixel. Click Next.\nSelect or download one of the recommended system images and click Next.\nAfter entering an AVD Name for your configuration, select the Portrait orientation (selected by default), and click Finish.\nWith your new virtual device configured, click Run in the top toolbar to ensure that you can successfully build the sample app.\nIt may take up to 2 minutes to launch the sample app.\nSince you haven\u2019t added your API key and secret to the sample app yet, you will receive a warning message. You will generate and add your API key and secret in the next step.\nFor now, ignore the API key warning that appears. In the next step, you will create and add your API key and secret before restarting the sample app.\n\nNext >> Create an input\n\nWas this page helpful?\n\nYes\nNo\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nDocumentation\nDevelopers\nAPI References\nClient SDKs\nMedia SDKs\nServer SDKs\nQuickstart\nTools\nGuides\n\nAPI Credential Management\n\nThe Developer's Guided Journey to mParticle\n\nGuides\nGetting Started\nPlatform Guide\nAnalytics\nIDSync\nData Master\nPersonalization\n\nWarehouse Sync\n\nData Privacy Controls\n\nData Subject Requests\n\nDefault Service Limits\n\nFeeds\n\nCross-Account Audience Sharing\n\nApproved Sub-Processors\n\nImport Data with CSV Files\n\nGlossary\n\nVideo Index\n\nAnalytics (Deprecated)\nIntegrations\n24i\nAarki\nABTasty\nAbakus\nActable\nAdChemix\nAdikteev\nAdjust\nAdMedia\nAdobe Marketing Cloud\nAdobe Audience Manager\nAdobe Campaign Manager\nAdobe Target\nAdPredictive\nAgilOne\nAlgolia\nAirship\nAlgoLift\nAlooma\nAmazon Kinesis\nAmazon Advertising\nAmazon Kinesis Firehose\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAmazon SNS\nAdobe Marketing Cloud\nAmobee\nAmpush\nAmplitude\nAnalytics\nAntavo\nAppLovin\nAnodot\nApptentive\nApptimize\nAttractor\nAttentive\nApteligent\nMicrosoft Azure Blob Storage\nBidease\nBing Ads\nBatch\nBluecore\nBluedot\nBranch\nBlueshift\nBugsnag\nBraze\nBranch S2S Event\nCadent\nButton\nCensus\nciValue\nCleverTap\ncomScore\nCortex\nConversant\nCordial\nCriteo\nCrossing Minds\nCustom Feed\nCustomerGlu\nDatabricks\nCustomer.io\nDatadog\nDidomi\nDynalyst\nEdge226\nDynamic Yield\nEmarsys\nEpsilon\nEverflow\nFacebook Offline Conversions\nFiksu\nFacebook\nFlurry\nGoogle Analytics for Firebase\nFlybits\nForeSee\nFormation\nFoursquare\nFreeWheel Data Suite\nGoogle Ad Manager\nGoogle Ads\nGoogle Analytics\nGoogle BigQuery\nGoogle Analytics 4\nGoogle Enhanced Conversions\nGoogle Cloud Storage\nGoogle Marketing Platform\nGoogle Marketing Platform Offline Conversions\nGoogle Pub/Sub\nHerow\nGoogle Tag Manager\nHeap\nHightouch\nIbotta\nHyperlocology\nIndicative\nImpact\nInMarket\nInMobi\nInspectlet\nInsider\niPost\nIntercom\nironSource\nIterable\nJampp\nKayzen\nKafka\nKlaviyo\nKochava\nKissmetrics\nKubit\nLaunchDarkly\nLeanplum\nLiftoff\nLifeStreet\nLiveLike\nLiveramp\nLocalytics\nMadHive\nmAdme Technologies\nMailchimp\nMarigold\nMediaMath\nMautic\nMediasmart\nMicrosoft Azure Event Hubs\nMintegral\nMixpanel\n\nAudience\n\nEvent\n\nForwarding Data Subject Requests\n\nMoEngage\nMovable Ink\nMoloco\nMonetate\nMovable Ink - V2\nmyTarget\nMultiplied\nNarrative\nNanigans\nNami ML\nNeura\nOneTrust\nNCR Aloha\nOptimizely\nOracle BlueKai\nOracle Responsys\nPersona.ly\nPaytronix\nPersonify XP\nPieEye\nPilgrim\nPinterest\nPlarin\nPostie\nPrimer\nPunchh\nPushwoosh\nQuantcast\nRadar\nReddit\nRegal\nRetina AI\nRemerge\nReveal Mobile\nRevenueCat\nRTB House\nSailthru\nRokt\nSalesforce Email\nSalesforce Mobile Push\nSamba TV\nScalarr\nSendGrid\nShareThis\nSignal\nSessionM\nShopify\nSimpleReach\nSingular-DEPRECATED\nSingular\nSkyhook\nSlack\nSmadex\nSmarterHQ\nSnapchat\nSnapchat Conversions\nSnowflake\nSnowplow\nSplunk MINT\nAppsFlyer\nSplit\nStartApp\nSprig\nStatsig\nStormly\nSwrve\nTalon.One\nTapad\nTapjoy\nTaptica\nTaplytics\nTeak\nThe Trade Desk\nTikTok Event\nTicketure\nTreasure Data\nQuadratic Labs\nTriton Digital\nTUNE\nTwitter\nValid\nVoucherify\nVkontakte\nWebtrends\nVungle\nWebhook\nWhite Label Loyalty\nWootric\nXandr\nYahoo (formerly Verizon Media)\nYouAppi\nYotpo\nZ2A Digital\nQualtrics\nZendesk\nEvent\n\nMixpanel\u2019s mission is to increase the rate of innovation. Companies use Mixpanel to analyze how & why users engage, convert, and retain in real-time on web, mobile, and IoT devices, and then use the data to improve their products.\n\nOverview & Prerequisites\n\nIf you are new to setting up Mixpanel\u2019s Mobile App Analytics, your best place to start is Mixpanel itself and the below are must-reads before proceeding:\n\nMobile App Analytics Setup Overview: https://mixpanel.com/help/reference\nBest Practices for setting up your events: https://mixpanel.com/help/reference/ios#sending-events\n\nWhen mParticle sends data to Mixpanel, Mixpanel\u2019s APIs are utilized. This allows mParticle to implement server side data forwarding and supports our value proposition to customers of not requiring that additional app SDK components be continually added and updated for integrations.\n\nIn order to enable mParticle\u2019s integration with Mixpanel, you will need an account with Mixpanel and have your Mixpanel Token for mParticle configuration. Your Mixpanel Token can be found at the Mixpanel topic, Find Project Token.\n\nIt is important to ensure that your mParticle data implementation plan captures the correct instrumentation scheme to map to your desired Mixpanel feature sets. In other words, depending upon what Mixpanel feature set you decide to implement, may drive some of how you structure your app with the mParticle SDK.\nData Processing Notes\nUser and Event attributes with string values of \"true\" or \"false\" (not case sensitive), will be converted to boolean values before being forwarded to Mixpanel.\nSupported Features\n\nmParticle\u2019s SDK supports nearly all of the Mixpanel SDK specific features natively. When you use the mParticle SDK, mParticle events will be transformed using Mixpanel-compliant naming conventions and activate the corresponding features automatically.\n\nFeature Name\tmParticle Support\tFeature Description\nFunnels\tYes\tAnalyze where users drop off.\nIdentity Management\tYes\tAliasing to merge identities.\nIn-app Notifications\tNo\tShowing your messages when app an opens.\nPeople Profiles\tYes\tGet to know your users, track their LTV.\nRetention\tYes\tAnalyze how many users come back to your apps, break down by cohorts\nSegmentation, now known as Insights\tYes\tSlice and dice data using all available dimensions (by events, event attributes, user attributes, etc.). For information about Insights replacing Segmentation see, Segmentation Retirement FAQ.\nSurvey\tNo\tAsk users what they think of your apps.\nUser Identification\n\nOne of the key features of Mixpanel is funnel tracking, this feature requires a consistent approach to identifying your users as they sign up, and progress from being only identifiable by their device, to having a unique \u201clogged in\u201d ID.\n\nmParticle manages this process using its IDSync feature. IDSync gives you granular control over how user profiles are managed. To support IDSync, mParticle maintains a hierarchy of different ID types.\n\nA traditional Mixpanel implementation, using the Mixpanel SDK, manages sign-up funnels by using the following process:\n\nWhen a user first downloads your app, the Distinct ID is set using a default anonymous device id (Apple Advertising ID for iOS, a random GUID for Android, a Cookie ID for Web).\nWhen you know the identity of the current user, typically after log-in or sign-up, you call Mixpanel\u2019s identify method. Mixpanel recommends against using identify for anonymous visitors to your site.\n\nIf your project has Mixpanel\u2019s ID Merge feature enabled, the call to identify will connect pre- and post-authentication events when appropriate.\n\nIf your project does not have ID Merge enabled, identify will change the user\u2019s local distinct_id to the unique ID you pass. Events tracked prior to authentication will not be connected to the same user identity. If ID Merge is disabled, Alias can be used to tie the original Distinct ID (an anonymous device ID) and the new Distinct ID (a unique User ID) together in Mixpanel.\n\nIf this process is not followed correctly, funnel tracking won\u2019t be possible, as Mixpanel will see the two Distinct IDs as two completely separate users.\n\nIf you wish to use Mixpanel\u2019s funnel tracking features, here is one option for implementing with mParticle:\n\nOption 1 - Use mParticle ID as the Distinct ID\n\nThis option is recommended for new implementations. This option lets your Mixpanel user profiles mirror those maintained by mParticle. This option lets your mParticle Identity strategy take care of aliasing for you, before your data ever reaches Mixpanel.\n\nFor this to work, you need to have selected an Identity Strategy that supports funnel tracking, such as the Profile Conversion Strategy. If you use the Profile Conversion Strategy and mParticle\u2019s Customer ID as your logged-in ID type, a sign-up flow works as follows:\n\nWhen a user first downloads your app, mParticle creates a new user profile, with a new, unique mParticle ID. mParticle immediately begins forwarding event data to Mixpanel, mapping the mParticle ID to Mixpanel\u2019s Distinct ID.\nWhen the user creates an account, and you add a Customer ID, mParticle associates the new Customer ID with the original user profile. Now you have a new way of identifying users in the mParticle SDK, the mParticle ID \u2014 mapped to Mixpanel as the Distinct ID \u2014 which never changes.\n\nTo use this option, set the External Identity Type to mParticle ID in the Configuration Settings.\n\nNote that this example is the minimum necessary to demonstrate the required sign-up flow and does not include additional features, such as completion handlers. Refer to the full Identity documentation:\n\niOS\nAndroid\nWeb\nSupported Feature Reference\n\nTo support each feature in the Supported Features table above, multiple methods will need to be implemented. The following table shows the mapping between each feature and SDK methods.\n\nMixpanel SDK Method\tMethod Description\tRelated Feature\tmParticle SDK Method\tNotes\nalias\tLinks two IDs as the same user.\tPeople Analytics\tNot Supported\tMixpanel no longer recommends using the alias method to merge identities.\ndeleteUser\tDelete current user\u2019s record from Mixpanel People.\tPeople Analytics\tNot Supported\t\nidentify\tSets the distinct ID of the current user.\tPeople Analytics\tSetUserIdentity\tBy default, device udid is used to identify a user. If the \u2018Use Mixpanel People\u2019 setting is enabled, and the \u2018Use Customer ID\u2019 setting is enabled, and a Customer Id is available, Customer Id is used.\nincrement\tIncrement the given numeric properties by the given values.\tPeople Analytics\tNot Supported\tFor revenue tracking, use logEvent with attributes and set up LTV tracking.\nregisterSuperProperties\tRegisters super properties, overwriting ones that have already been set.\tSegmentation, Funnels, Retention, People Analytics\tNot implemented. SetUserAttribute achieves the same effect.\tRecommendation is to use mParticle\u2019s SetUserAttribute method to set user attributes that could be added to every event if configured\nregisterSuperPropertiesOnce\tRegisters super properties without overwriting ones that have already been set.\tSegmentation, Funnels, Retention, People Analytics\tNot supported\tmParticle leaves this type of implementation to the developer.\nreset\tClears all stored properties and distinct IDs. Useful if your app\u2019s user logs out.\tPeople Analytics\tNot Supported\t\nset\tSet user properties\tSegmentation, People Analytics\tSetUserAttribute\tIf MessageType is AppEvent or ScreenView, user attributes will be sent if the \u2018Include User Attributes\u2019 setting is enabled\ntrack\ttracks an event with or without properties\tSegmentation, Funnels, Retention, People Analytics\tlogScreen / logEvent\t\ntrackCharge\tTrack money spent by the current user for revenue analytics\tPeople Analytics\tlogEvent. Also, the logged events need to be set up as LTV tracking event in mParticle\u2019s UI\t\nunion (Android only)\tadd an array of values to a user attribute key\tPeople Analytics\tNot supported\t\nunset (Android only)\tremove a property of the given name from a user profile\tPeople Analytics\tremoveUserAttribute\t\nEvent Tracking\n\nTracking standard events in the mParticle SDK is fairly straightforward. Events can be standalone or include event attributes. mParticle attributes are converted to Mixpanel properties automatically when forwarded.\n\nMixpanel\u2019s SDK Method\tmParticle\u2019s SDK Method\ntrack with properties\tlogEvent with event attributes or logEcommerceTransactionWithProduct\ntrack with no properties\tlogScreen or logEvent with no event attributes\nHistorical Event Tracking\n\nmParticle sends data to different Mixpanel endpoints depending on the age of the events:\n\nEvents that are less than or equal to 5 days old are sent to the track endpoint.\nEvents that are greater than 5 days old are sent to the import endpoint. In order for mParticle to send the historical data to Mixpanel, you must provide the API Secret. If not provided, mParticle will drop these events.\nSuper Property Tracking\n\nSuper properties allow certain properties that you want to include with each event you send. Generally, these are things you know about the user rather than about a specific event, for example, the user\u2019s age, gender, or source. These super properties will be automatically included with all tracked events. Super properties are saved to device storage, and will persist across invocations of your app.\n\nMixpanel\u2019s SDK Method\tmParticle\u2019s SDK Method\tDescription\nregisterSuperProperties\tSetUserAttribute\tSuper properties, once registered, are automatically sent for all even tracking calls.\nregisterSuperPropertiesOnce\tNot supported\t\nSetting User Properties and Attribute Mapping\n\nBoth Mixpanel and mParticle have the ability to set specific attributes for the user which will persist until overwritten.\n\nMixpanel\u2019s SDK Method\tmParticle\u2019s SDK Method\tDescription\nset\tSetUserAttribute\tSets a single property with the given name and value for this group.\n\nIf you have enabled the \u2018Include User Attributes\u2019 setting, then any messages with type ScreenView or AppEvent will include the email user identity (if available) and all user attributes. The SetUserAttribute method can be used to set user attributes. This method will overwrite the values of any existing user attributes.\n\nAttribute Mappings\n\nmParticle\u2019s attribute naming conventions closely resemble standard Mixpanel attributes, which a few exceptions:\n\nmParticle attribute\twill be changed to\n$FirstName\t$first_name\n$LastName\t$last_name\n$Mobile\t$phone\n\nThese mParticle attributes will just have the leading $ removed:\n\nmParticle attribute\twill be changed to\n$Gender\tGender\n$Age\tAge\n$Country\tCountry\n$Zip\tZip\n$City\tCity\n$State\tState\n$Address\tAddress\n\nIf these attributes are seen, they will be replaced with Mixpanel attributes:\n\nmParticle attribute\twill be changed to\ncreated\t$created\nemail\t$email\nlastSeen\t$last_seen\nname\t$name\nusername\t$username\n\nWith available user identity info and user attributes, standard people data being sent includes:\n\naction type: $set for user identification\ntoken: the application\u2019s Mixpanel token\ndistinct_id: device\u2019s UDID or user\u2019s customerId\nip: the IP address of the request or \u201c0\u201d\ntime: the message timestamp\ninsert_id: used to de-duplicate events - this is set to mParticle event_id.\n\nData being sent in the $set section:\n\nuser attributes: following the rules in Attribute mappings\nemail address: if it exists in the user identities\nRevenue Tracking and Commerce Events\n\nIn order to track revenue using mParticle and Mixpanel, you need to ensure that mParticle is forwarding on relevant data by enabling the Use Mixpanel People setting. If the mParticle SDK method has been called to log an event, the event and one event attribute have been set up for LTV tracking, and the event is not excluded by an account policy, a transaction message will be sent to Mixpanel.\n\nMixpanel\u2019s SDK Method\tmParticle\u2019s SDK Method\ntrackCharge\tlogEvent or logEcommerceTransactionWithProduct. Also, the logged events need to be set up as LTV tracking event in mParticle\u2019s UI.\n\nOnly specific data will be considered as part of the transactional funnel. Standard message data format is:\n\naction type: $transaction for a TrackCharge message\ntoken: the application\u2019s Mixpanel token\ndistinct_id: device\u2019s UDID or user\u2019s customerId\nip: the IP address of the request or \u201c0\u201d\ntime: the message timestamp\ninsert_id: used to de-duplicate events - this is set to mParticle event_id.\n\nData being sent in the transactions section:\n\n$amount: the total value of the event\n$time: the message timestamp in the format yyyy-MM-dd'T'HH:mm:ss\nevent attributes: follows the rules in Attribute Mappings\nWeb Attributes\n\nEvent batches sent to Mixpanel using the server-side web integration will also send the following data:\n\nbrowser: the user\u2019s web browser\nbrowser version: the version of the web browser\nos: the user\u2019s operating system\n\nNote that this data depends on the http_header_user_agent field so they will only be set if a value is included in the batch.\n\nEU Data Localization\n\nBy default, mParticle sends data to Mixpanel\u2019s US Servers, but offers an EU Data Localization option for their ingestion API, which allows data to be sent and stored in the Mixpanel EU Data Center (see Mixpanel\u2019s documentation on Storing Your Data in the European Union). This EU Residency is not automatically set within a Mixpanel project. You can find steps on how to set EU Data Localization for your Mixpanel project within the Mixpanel documentation. In mParticle, the Mixpanel Target Server Configuration Setting must be set to EU Residency Server.\n\nConfiguration Settings\nSetting Name\tData Type\tDefault Value\tDescription\nToken\tstring\t\tProject token, found by clicking the gear icon in your project.\nAPI Secret\tstring\t\tYour Mixpanel API Secret which can be found by clicking on your name in the upper right hand corner under Project Settings. This is required to forward Historical Data.\nExternal Identity Type\tstring\tCustomer ID\tThe mParticle User Identity type to forward as an External Id to Mixpanel.\nMixpanel Target Server\tstring\tStandard Server\tMixpanel Server where the data will be stored. You can set up EU Residency in your Mixpanel project settings.\nConnection Settings\nSetting Name\tData Type\tDefault Value\tPlatform\tDescription\nForward Session Start/End Messages\tbool\tTrue\tiOS, Android, tvOS, Roku, FireTV, Xbox\tIf enabled, all session start and session end messages will be forwarded to Mixpanel as separate events.\nSession Start Event Name\tstring\tsession-start\tiOS, Android, tvOS, Roku, FireTV, Xbox\tThe event name that will be forwarded to Mixpanel on a session start message. Only used if \u2018Forward Session Start/End Messages\u2019 is enabled.\nSession End Event Name\tstring\tsession-end\tiOS, Android, tvOS, Roku, FireTV, Xbox\tThe event name that will be forwarded to Mixpanel on a session end message. Only used if \u2018Forward Session Start/End Messages\u2019 is enabled.\nCreate Profile Only If Logged In\tbool\tFalse\tAll\tIf enabled, Mixpanel will only forward customer profile data if a customer ID is in the list of user\u2019s identities; if disabled, Mixpanel will always forward customer profile data.\nUse Mixpanel People\tbool\tTrue\tAll\tEnable this setting if you are using customer profiles in Mixpanel\nInclude User Attributes\tbool\tTrue\tAll\tIf enabled, all user attributes will be included when tracking events\nInclude Attribution Info\tbool\tFalse\tAll\tIf enabled, attribution info (publisher and campaign names) will be included when tracking events.\nInclude IP Address\tbool\tTrue\tAll\tIf enabled, IP Address will be sent with the event. This is used by Mixpanel to retrieve location data for the event.\nSend Event Attributes as Objects\tbool\tTrue\tAll\tIf enabled, mParticle will attempt to send event attributes as objects. Attributes should be string values containing serialized JSON objects.\nUpper Case Idfa and Idfv\tbool\tFalse\tAll\tWhether to upper case Idfa and Idfv as Mixpanel is case sensitive with device ids\nSimplified ID Merge\tbool\tFalse\tiOS, Android, tvOS, Roku, FireTV, Xbox\tIf enabled, mParticle will send $device_id and $user_id attributes for use with the MixPanel Simplified ID Merge API\nForward Web Requests Server Side\tbool\tFalse\tWeb\tIf enabled, requests will only be forwarded server-side.\nSuper Properties\tCustom Field\t\tAll\tMapped user attributes here will always be sent as event properties (regardless of the \u2018Include User Attributes\u2019 setting). Note they will also be excluded from people properties.\nDOCS\nLast Updated:\n1/8/2025, 6:32:28 PM\nCDP\nmParticle Docs\nHome\nGuides\nDevelopers\nIntegrations\nAnalytics\nAnalytics Docs\nOverview\nGetting Started\nDevelopers\nExpert Training\nPredictive AI\nCortex Docs\nOverview\nGetting Started\nDevelopers\nUsing Cortex\nLearn More\nBlog\nPodcast\nResources\nCommunity\nChange log\nGet in Touch\nHelp Center\nDemo\nBecome a Partner\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nDocumentation\nDevelopers\nAPI References\nClient SDKs\nMedia SDKs\nServer SDKs\nQuickstart\nTools\nGuides\n\nAPI Credential Management\n\nThe Developer's Guided Journey to mParticle\n\nGuides\nGetting Started\nPlatform Guide\nAnalytics\nIDSync\nData Master\nPersonalization\n\nWarehouse Sync\n\nData Privacy Controls\n\nData Subject Requests\n\nDefault Service Limits\n\nFeeds\n\nCross-Account Audience Sharing\n\nApproved Sub-Processors\n\nImport Data with CSV Files\n\nGlossary\n\nVideo Index\n\nAnalytics (Deprecated)\nIntegrations\n24i\nAarki\nABTasty\nAbakus\nActable\nAdChemix\nAdikteev\nAdjust\nAdMedia\nAdobe Marketing Cloud\nAdobe Audience Manager\nAdobe Campaign Manager\nAdobe Target\nAdPredictive\nAgilOne\nAlgolia\nAirship\nAlgoLift\nAlooma\nAmazon Kinesis\nAmazon Advertising\nAmazon Kinesis Firehose\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAmazon SNS\nAdobe Marketing Cloud\nAmobee\nAmpush\nAmplitude\nAnalytics\nAntavo\nAppLovin\nAnodot\nApptentive\nApptimize\nAttractor\nAttentive\nApteligent\nMicrosoft Azure Blob Storage\nBidease\nBing Ads\nBatch\nBluecore\nBluedot\nBranch\nBlueshift\nBugsnag\nBraze\nBranch S2S Event\nCadent\nButton\nCensus\nciValue\nCleverTap\ncomScore\nCortex\nConversant\nCordial\nCriteo\nCrossing Minds\nCustom Feed\nCustomerGlu\nDatabricks\nCustomer.io\nDatadog\nDidomi\nDynalyst\nEdge226\nDynamic Yield\nEmarsys\nEpsilon\nEverflow\nFacebook Offline Conversions\nFiksu\nFacebook\nFlurry\nGoogle Analytics for Firebase\nFlybits\nForeSee\nFormation\nFoursquare\nFreeWheel Data Suite\nGoogle Ad Manager\nGoogle Ads\nGoogle Analytics\nGoogle BigQuery\nGoogle Analytics 4\nGoogle Enhanced Conversions\nGoogle Cloud Storage\nGoogle Marketing Platform\nGoogle Marketing Platform Offline Conversions\nGoogle Pub/Sub\nHerow\nGoogle Tag Manager\nHeap\nHightouch\nIbotta\nHyperlocology\nIndicative\nImpact\nInMarket\nInMobi\nInspectlet\nInsider\niPost\nIntercom\nironSource\nIterable\nJampp\nKayzen\nKafka\nKlaviyo\nKochava\nKissmetrics\nKubit\nLaunchDarkly\nLeanplum\nLiftoff\nLifeStreet\nLiveLike\nLiveramp\nLocalytics\nMadHive\nmAdme Technologies\nMailchimp\nMarigold\nMediaMath\nMautic\nMediasmart\nMicrosoft Azure Event Hubs\nMintegral\nMixpanel\nMoEngage\nMovable Ink\nMoloco\nMonetate\nMovable Ink - V2\nmyTarget\nMultiplied\nNarrative\nNanigans\nNami ML\nNeura\nOneTrust\nNCR Aloha\nOptimizely\nOracle BlueKai\nOracle Responsys\nPersona.ly\nPaytronix\nPersonify XP\nPieEye\nPilgrim\nPinterest\n\nAudience\n\nEvent\n\nPlarin\nPostie\nPrimer\nPunchh\nPushwoosh\nQuantcast\nRadar\nReddit\nRegal\nRetina AI\nRemerge\nReveal Mobile\nRevenueCat\nRTB House\nSailthru\nRokt\nSalesforce Email\nSalesforce Mobile Push\nSamba TV\nScalarr\nSendGrid\nShareThis\nSignal\nSessionM\nShopify\nSimpleReach\nSingular-DEPRECATED\nSingular\nSkyhook\nSlack\nSmadex\nSmarterHQ\nSnapchat\nSnapchat Conversions\nSnowflake\nSnowplow\nSplunk MINT\nAppsFlyer\nSplit\nStartApp\nSprig\nStatsig\nStormly\nSwrve\nTalon.One\nTapad\nTapjoy\nTaptica\nTaplytics\nTeak\nThe Trade Desk\nTikTok Event\nTicketure\nTreasure Data\nQuadratic Labs\nTriton Digital\nTUNE\nTwitter\nValid\nVoucherify\nVkontakte\nWebtrends\nVungle\nWebhook\nWhite Label Loyalty\nWootric\nXandr\nYahoo (formerly Verizon Media)\nYouAppi\nYotpo\nZ2A Digital\nQualtrics\nZendesk\nAudience\n\nPinterest - Reach people who are using Pinterest to discover and plan the things they want to do in the future. Using the Ads API, businesses on Pinterest manage, scale and optimize their Promoted Pins.\n\nPrerequisites\n\nIn order to enable mParticle\u2019s integration with Pinterest, you need to have a business account with Pinterest, and the account credentials for the Pinterest account. The integration activation process in Audience Manager will prompt you to log into your Pinterest account, and once authorized, mParticle will automatically retrieve the credentials that it needs to forward audiences to Pinterest.\n\nIf you are not working with a Pinterest Marketing Developer Partner, contact advertiser-data-onboarders@pinterest.com and CC your Pinterest account team requesting access to the Pinterest API.\n\nData Processing Notes\nTiming - An audience named <Audience Name>-EMAIL and <Audience Name>-MAID will be available in Pinterest upon activating a subscription in mParticle, however it may take up to 24 hours before the size of the audience is shown in Pinterest.\nMinimum - An audience is selectable for targeting in Pinterest as long as it has a minimum of 100 users. If an audience is selected while in the processing state, and later is calculated to be less than 100 users, the campaign will not serve.\nMaximum - There is a limit of 300 million users in an audience. This is a count of the raw number of records that mParticle has sent to Pinterest. If an audience reaches this limit, mParticle will no longer send user updates to Pinterest.\nAdditional Notes - If an email or device ID is added to an audience, and is not currently a Pinterest user, the user will not be added to the audience even if the user later becomes a Pinterest user.\nUser Identity Mapping\n\nWhen forwarding audience data to Pinterest, mParticle will send SHA-1 hash of IDFAs, Google Advertising IDs and Emails based on the values of the Connection Settings.\n\nUpload Frequency\n\nThe Pinterest Audience Integration uses Bulk Forwarding. Bulk Forwarding means that, instead of uploading updates to an audience in real time, mParticle compiles updates into a queue until either a given amount of time has passed since the last upload, or until a certain number of updates are waiting to be sent.\n\nBy default, mParticle uploads to Pinterest whenever at least one of the following conditions is met:\n\n3 hours have passed since the last update.\nAt least 100000 messages are in the queue.\n\nUpload frequency can sometimes be adjusted. Reach out to your mParticle Customer Success Manager if you need to discuss upload frequency.\n\nDeleting an Audience\n\nmParticle doesn\u2019t delete the downstream audience when you delete an audience in mParticle.\n\nConnection Settings\nSetting Name\tData Type\tDefault Value\tDescription\nForward Email\tbool\ttrue\tIf enabled, and the user\u2019s e-mail address is available, e-mail address will be sent to Pinterest, and if matched to Pinterest users, will be in the audience \u201d<Audience Name>-EMAIL\u201d.\nForward Device ID\tbool\ttrue\tIf enabled, device IDs (IDFA for Apple OS, Google Advertising ID for Android) will be sent to Pinterest, and if matched to Pinterest users, will be in the audience \u201d<Audience Name>-MAID\u201d.\nDOCS\nLast Updated:\n1/8/2025, 6:32:28 PM\nCDP\nmParticle Docs\nHome\nGuides\nDevelopers\nIntegrations\nAnalytics\nAnalytics Docs\nOverview\nGetting Started\nDevelopers\nExpert Training\nPredictive AI\nCortex Docs\nOverview\nGetting Started\nDevelopers\nUsing Cortex\nLearn More\nBlog\nPodcast\nResources\nCommunity\nChange log\nGet in Touch\nHelp Center\nDemo\nBecome a Partner\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nDocumentation\nFilter by Category\nAll\nAdvertising\nData Onboarding\nRe-Targeting\nUser Acquisition\nAnalytics\nA/B Testing\nAttribution\nConversion Tracking\nCrash Reporting\nCustom Feeds\nFeature Flagging\nLocation\nMonitoring\nPredictive Analytics\nUser Analytics\nCustomer Service\nCustomer Support\nHelpdesk\nUser Feedback\nData Warehousing\nBusiness Intelligence\nDMP\nRaw Data Export\nFinance\nCommerce\nPayments\nMarketing\nAffiliate Marketing\nCRM\nCustomer Engagement\nData Enrichment\nDeep Linking\nLoyalty and Rewards\nPersonalization\nTag Management\nPrivacy\nConsent Management\nData Subject Request\nSecurity\nAccess Management\nFraud Monitoring\nIntegrations\n\nConnect your customer data to the leading marketing, analytics, and data warehousing solutions with just a few clicks.\n\nNote: New Partners, please register.\n\nFilter by integration type\nDOCS\nLast Updated:\n09/01/2025, 00:02:28\nCDP\nmParticle Docs\nHome\nGuides\nDevelopers\nIntegrations\nAnalytics\nAnalytics Docs\nOverview\nGetting Started\nDevelopers\nExpert Training\nPredictive AI\nCortex Docs\nOverview\nGetting Started\nDevelopers\nUsing Cortex\nLearn More\nBlog\nPodcast\nResources\nCommunity\nChange log\nGet in Touch\nHelp Center\nDemo\nBecome a Partner\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nDocumentation\nDevelopers\nAPI References\nClient SDKs\nMedia SDKs\nServer SDKs\nQuickstart\nTools\nGuides\n\nAPI Credential Management\n\nThe Developer's Guided Journey to mParticle\n\nGuides\nGetting Started\nPlatform Guide\nAnalytics\nIDSync\nData Master\nPersonalization\n\nWarehouse Sync\n\nData Privacy Controls\n\nData Subject Requests\n\nDefault Service Limits\n\nFeeds\n\nCross-Account Audience Sharing\n\nApproved Sub-Processors\n\nImport Data with CSV Files\n\nGlossary\n\nVideo Index\n\nAnalytics (Deprecated)\nIntegrations\n24i\nAarki\nABTasty\nAbakus\nActable\nAdChemix\nAdikteev\nAdjust\nAdMedia\nAdobe Marketing Cloud\nAdobe Audience Manager\nAdobe Campaign Manager\nAdobe Target\nAdPredictive\nAgilOne\nAlgolia\nAirship\nAlgoLift\nAlooma\nAmazon Kinesis\nAmazon Advertising\nAmazon Kinesis Firehose\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAmazon SNS\nAdobe Marketing Cloud\nAmobee\nAmpush\nAmplitude\nAnalytics\nAntavo\nAppLovin\nAnodot\nApptentive\nApptimize\nAttractor\nAttentive\nApteligent\nMicrosoft Azure Blob Storage\nBidease\nBing Ads\nBatch\nBluecore\nBluedot\nBranch\nBlueshift\nBugsnag\nBraze\n\nFeed\n\nAudience\n\nForwarding Data Subject Requests\n\nEvent\n\nBranch S2S Event\nCadent\nButton\nCensus\nciValue\nCleverTap\ncomScore\nCortex\nConversant\nCordial\nCriteo\nCrossing Minds\nCustom Feed\nCustomerGlu\nDatabricks\nCustomer.io\nDatadog\nDidomi\nDynalyst\nEdge226\nDynamic Yield\nEmarsys\nEpsilon\nEverflow\nFacebook Offline Conversions\nFiksu\nFacebook\nFlurry\nGoogle Analytics for Firebase\nFlybits\nForeSee\nFormation\nFoursquare\nFreeWheel Data Suite\nGoogle Ad Manager\nGoogle Ads\nGoogle Analytics\nGoogle BigQuery\nGoogle Analytics 4\nGoogle Enhanced Conversions\nGoogle Cloud Storage\nGoogle Marketing Platform\nGoogle Marketing Platform Offline Conversions\nGoogle Pub/Sub\nHerow\nGoogle Tag Manager\nHeap\nHightouch\nIbotta\nHyperlocology\nIndicative\nImpact\nInMarket\nInMobi\nInspectlet\nInsider\niPost\nIntercom\nironSource\nIterable\nJampp\nKayzen\nKafka\nKlaviyo\nKochava\nKissmetrics\nKubit\nLaunchDarkly\nLeanplum\nLiftoff\nLifeStreet\nLiveLike\nLiveramp\nLocalytics\nMadHive\nmAdme Technologies\nMailchimp\nMarigold\nMediaMath\nMautic\nMediasmart\nMicrosoft Azure Event Hubs\nMintegral\nMixpanel\nMoEngage\nMovable Ink\nMoloco\nMonetate\nMovable Ink - V2\nmyTarget\nMultiplied\nNarrative\nNanigans\nNami ML\nNeura\nOneTrust\nNCR Aloha\nOptimizely\nOracle BlueKai\nOracle Responsys\nPersona.ly\nPaytronix\nPersonify XP\nPieEye\nPilgrim\nPinterest\nPlarin\nPostie\nPrimer\nPunchh\nPushwoosh\nQuantcast\nRadar\nReddit\nRegal\nRetina AI\nRemerge\nReveal Mobile\nRevenueCat\nRTB House\nSailthru\nRokt\nSalesforce Email\nSalesforce Mobile Push\nSamba TV\nScalarr\nSendGrid\nShareThis\nSignal\nSessionM\nShopify\nSimpleReach\nSingular-DEPRECATED\nSingular\nSkyhook\nSlack\nSmadex\nSmarterHQ\nSnapchat\nSnapchat Conversions\nSnowflake\nSnowplow\nSplunk MINT\nAppsFlyer\nSplit\nStartApp\nSprig\nStatsig\nStormly\nSwrve\nTalon.One\nTapad\nTapjoy\nTaptica\nTaplytics\nTeak\nThe Trade Desk\nTikTok Event\nTicketure\nTreasure Data\nQuadratic Labs\nTriton Digital\nTUNE\nTwitter\nValid\nVoucherify\nVkontakte\nWebtrends\nVungle\nWebhook\nWhite Label Loyalty\nWootric\nXandr\nYahoo (formerly Verizon Media)\nYouAppi\nYotpo\nZ2A Digital\nQualtrics\nZendesk\nEvent\nmParticle supports Braze Web SDK v5 as of 10/30/2024\n\n\nYou can opt in to V5 of the Braze SDK by selecting the version in a connection setting in the mParticle UI.\n\nFor more details about upgrading to V5 of the Braze web SDK, follow these instructions, whether you self-host or load mParticle via snippet/CDN..\n\n\nIf you are using version 2 of the @mparticle/web-appboy-kit, you will need to transition to @mparticle/web-braze-kit per the instructions here before following the above instructions as well.\n\nBraze is a comprehensive customer engagement platform that powers relevant experiences between consumers and brands they love. Braze helps brands foster human connection through interactive conversations across channels.\n\nBraze offers a broad range of functionality via their solution and it is critically important that you work directly with your Braze representative to ensure that you are correctly planning and implementing their features. mParticle does not recommend enabling forwarding to Braze until you have completed the Braze planning process with your Braze team.\n\nBraze Documentation\nmParticle Braze Implementation Scenarios\n\nThe mParticle SDK allows you to include the Braze kit which allows Braze interface components (images, layout files, etc.) and as a result supports many Braze features, including:\n\nApp Analytics\nUser Segmentation\nPush Notifications\nEmail\nNews Feed\nIn-App Messaging\nFeedback\nGeolocation\n\nFeatures are supported by the mParticle SDK only after you install the mParticle Braze Kit (formerly Appboy), which then forwards data from your app to Braze.\n\nFeatures are supported in two ways:\n\nThe kit itself provides functionality directly without you having to call the third-party SDK. For example, most partners have a method called or equivalent to logEvent. When someone calls mParticle.logEvent, our kits map to the partner SDK logEvent method, in this case, Braze.logEvent, and automatically sends it to Braze. You don\u2019t have to call Braze.logEvent because mParticle does it for you after you call mParticle.logEvent.\nFor some features, for example some Braze banners or modals, you must call Braze.bannerMethod() or Braze.modalMethod(). Our kit loads Braze so that you can call any Braze method you need, even if our kit does not call it for you.\n\nThe mParticle S2S API allows you to send data server side (API reference). The S2S API supports iOS, Android and Web data. In this scenario, mParticle forwards data via Braze\u2019s REST API which supports a limited set of features.\n\nFor server-side data to be forwarded to Braze, it must include your selected External Identity Type. To relax this requirement, see the Disable External ID Constraint connection setting.\n\nThe following event types can be forwarded to Braze via S2S:\n\nCommerce Event\nCustom Event\nOpt Out\nPush Message\nPush Message Registration\nScreen View\nSession Start / End\nKit Integration\n\nThe Braze solution offers features that involve Braze-proprietary user interaction components including Newsfeed, In-App Messaging, and Feedback.\n\nTo use Braze with one of the mParticle mobile SDKs, please review the kit documentation for the Android SDK or the iOS SDK.\n\nYou must directly call the Braze kit to use the Newsfeed, In-App Messaging, and Feedback features. Examples of directly calling a kit are provided for both Android and iOS.\n\nBRAZE V9 IOS SDK UPGRADE NOTES\n\nTo comply with Apple\u2019s iOS 17 privacy manifest it is required to update to Braze SDK version 9.0.0 or later which is included by the mParticle Braze Kit version 8.60 or later. Note that it is also required to update to at least version 8.24.0 of the mParticle SDK if you are not creating your own Braze instance and passing it to the mParticle Braze Kit in order for this functionality to work.\n\nThere are also additional steps required to comply which are outlined in the Braze documentation here.\n\nIf you are creating your own Braze instance and passing it to the mParticle Kit as described in the iOS App Launch Tracking section below, you can simply follow instructions in the above Braze documentation directly.\n\nIf not, you will still need to follow the above Braze documentation, with the following exceptions:\n\nSet the trackingPropertyAllowList property on the mParticle Braze Kit before starting the mParticle SDK instead of setting it on the Braze instance directly.\nSwiftObjective-C\n// In this example, we're setting phone number and email as tracking data.\n// Replace the values in the allow list with the data you are tracking\nMPKitAppboy.setBrazeTrackingPropertyAllowList([.email, .dateOfBirth])\nIn your ATTrackingManager request callback, call the corresponding method on the mParticle SDK instead of setting it on the Braze instance directly.\nSwiftObjective-C\nATTrackingManager.requestTrackingAuthorization { status in\n    let mpStatus = MPATTAuthorizationStatus(rawValue: status.rawValue) ?? .notDetermined\n    let timestampMillis: NSNumber = (Date().timeIntervalSince1970 * 1000) as NSNumber\n    MParticle.sharedInstance().setATTStatus(mpStatus, withATTStatusTimestampMillis: timestampMillis)\n}\nPUSH NOTIFICATIONS\n\nPush notifications work differently for web and for mobile.\n\nWeb\n\nmParticle integrates with Braze to allow web push notifications to further engage your visitors. We integrated Braze\u2019s Soft Push Prompts, which allows you to ask your user if they\u2019d like to stay in touch before the browser alerts them to allow notifications. This is done since the browser throttles how often you can prompt the user for push notifications, and if the user denies permission, you can never ask them again. See below for directions on how to implement push notifications, which customizes Braze\u2019s implementation instructions to work with mParticle.\n\nConfigure your site\n\nCreate a service-worker.js file to your root directory. Inside your service-worker.js file, include\nself.importScripts('https://static.mparticle.com/sdk/js/braze/service-worker-3.5.0.js');\n\nmParticle hosts Braze\u2019s service worker in order to prevent unpredictable versioning issues. Do not use Braze\u2019s service-worker.js CDN.\n\nConfigure Safari Push\n\nGenerate a Safari Push Certificate following these \u201cRegistering with Apple\u201d Instructions\nIn the Braze dashboard, on the app settings page (where your API keys are located), select your Web app. Click \u201cConfigure Safari Push\u201d and follow the instructions, uploading the push certificate you just generated.\nIn your mParticle dashboard, open your Braze connection settings. Under Safari Website Push ID, type in your Website Push ID you used when generating your Safari Push Certificate (beginning with web) and click Save.\n\nCreate a \u201cPrime for Push\u201d in-app messaging Campaign on the Braze dashboard. Note that this is an In-App Messaging Campaign, and not a Push Notification messaging campaign.\n\nMake it a \u201cModal\u201d In-App Message. Give it whatever text and styling you wish to present to the user (\u201cCan we stay in touch?\u201d).\nGive the in-app message a Button 1 Text value of \u201cOK\u201d (or whatever affirmative text you wish), and set the On-Click Behavior to \u201cClose Message.\u201d\nUnder the gear composer section, add a key-value pair. Give it a key of msg-id and a value of push-primer.\nYou can create a prime-for-push custom event (or name it whatever you\u2019d like) from the Braze dashboard. While still in the Braze dashboard, create a trigger action of whatever your custom event is (ie, prime-for-push). In the mParticle Braze connection settings, fill in the \"Soft Push\" Custom Event Name with your custom event name (ie. prime-for-push). When this field is filled, users will be sent the Soft Push Prompt on session load.\n\nOptionally, you can change the name and location of service-worker.js. The following example will clarify the steps:\n\nLet\u2019s say that you want to rename your service-worker.js file to braze-push-worker.js and store it in inside a directory in your root folder called thirdParty/.\nIn your mParticle dashboard, open your Braze connection settings. Under Push Notification Service Worker File Location, type in /thirdParty/braze-push-worker.js and click Save.\nWarning - Setting a value here limits the scope of push notifications on your site. For instance, in the above example, because the service worker file is located within the /thirdParty/ directory, asking for push notifications MAY ONLY BE CALLED from web pages that start with http://your-site-name.com/thirdParty/.\nWeb Troubleshooting Tips\nFirefox - starting with version 72, Firefox requires user interaction before showing a full push permission dialogue box. See here for more details.\nEnsure that your OS-wide notifications for the browser you are testing are not disabled.\nIf you have previously allowed or rejected push requests while testing, you will need to clear local storage/cookies as well as the browser\u2019s notification preference for your development URL for optimal testing.\nMobile\n\nAs long as the Braze Kit is included in your app, mParticle will pass any Push Notifications from Braze to the kit for display. However, you will need to provide credentials in the Braze dashboard.\n\nSee the main iOS and Android Push Notification documentation for more detail.\n\niOS App Launch Tracking\n\nOn iOS, when a user launches your app by tapping on a push notification, you may see these launches recorded in the mParticle dashboard but not in Braze. This happens due to a race condition between the Braze SDK initializing and the app handling the push notification.\n\nTo work around this, you\u2019ll need to create your own instance of Braze in your applicationDidFinishLaunching method of your app delegate and pass it to the Braze kit before initializing the mParticle SDK. You\u2019ll also need to tell the kit not to process push notifications and instead handle them yourself. This ensures that Braze is initialized and ready to receive the push notifications immediately upon app launch.\n\nNote that by implementing your own instance of Braze, you will lose the ability to use the following Braze configuration options in the mParticle dashboard, as you\u2019ll be passing those options to Braze yourself on app launch. Only implement this if you need push notification app launches reported to Braze:\n\nBraze API Key\nBraze Endpoint\nBraze SDK Request Policy\nBraze SDK Flush Interval\nBraze SDK Session Timeout\nBraze SDK Minimum Time Interval Between Triggers\nBraze SDK Disable Automatic Location Tracking\n\nAll other Braze configuration options in the mParticle dashboard will continue to work as usual, and any of the above can be set in your app when you create the Braze instance by setting the relevant options on the Braze configuration object. See the example code for the didFinishLaunchingWithOptions method below.\n\nThe following is mostly following the instructions from Braze\u2019s \u201cManual push integration\u201d documentation, with the addition of some mParticle specific code in the didFinishLaunchingWithOptions method.\n\nPlease note that because we are following Braze\u2019s \u201cManual push integration\u201d instructions, you should not set the configuration option, configuration.push.automation = true, as described in Braze\u2019s Automatic push integration documentation as it will cause conflicts.\n\nMake sure to import the Braze SDK and mParticle Braze kit in your app delegate file.\nSwiftObjective-C\nimport BrazeKit\nimport mParticle_Appboy\nCreate and configure a Braze instance immediately upon app launch and pass it to the Braze kit.\nSwiftObjective-C\nvar braze: Braze? = nil\n\nfunc application(_ application: UIApplication, didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?) -> Bool {\n    // Register for remote notifications\n    application.registerForRemoteNotifications()\n    let center = UNUserNotificationCenter.current()\n    center.setNotificationCategories(Braze.Notifications.categories)\n    center.delegate = self\n    \n    // Request user authorization to send push notifications\n    var options: UNAuthorizationOptions = [.alert, .sound, .badge]\n    if #available(iOS 12.0, *) {\n        options = UNAuthorizationOptions(rawValue: options.rawValue | UNAuthorizationOptions.provisional.rawValue)\n    }\n    center.requestAuthorization(options: options) { granted, error in\n        print(\"Notification authorization, granted: \\(granted), error: \\(String(describing: error))\")\n    }\n    \n    // Create the Braze configuration object\n    // API key and endpoint were previously configured in the mParticle dashboard\n    let configuration = Braze.Configuration(apiKey: \"[YOUR_BRAZE_API_KEY]\", endpoint: \"[YOUR_BRAZE_ENDPOINT]\")\n    configuration.api.addSDKMetadata([.mparticle])\n    configuration.api.sdkFlavor = .mparticle\n    \n    // Default mParticle dashboard options\n    configuration.location.automaticLocationCollection = true\n    \n    // Optionally set any additional configuration options you would like here\n    \n    // Create a Braze instance with your chosen configuration and store the reference\n    let braze = Braze(configuration: configuration)\n    self.braze = braze\n    \n    // Pass the instance to the mParticle Braze kit so it can be used for other Braze functionality\n    MPKitAppboy.setBrazeInstance(braze)\n    \n    // Instruct the mParticle Braze kit to not handle push notifications, as they'll be handled in the app delegate\n    MPKitAppboy.setShouldDisableNotificationHandling(true)\n\n    // Place your existing application initialization code here\n}\nAdd the required push notification delegate methods\nSwiftObjective-C\nfunc application(_ application: UIApplication, didRegisterForRemoteNotificationsWithDeviceToken deviceToken: Data) {\n    self.braze?.notifications.register(deviceToken: deviceToken)\n}\n    \nfunc application(_ application: UIApplication, didReceiveRemoteNotification userInfo: [AnyHashable : Any], fetchCompletionHandler completionHandler: @escaping (UIBackgroundFetchResult) -> Void) {\n    if let braze = self.braze, braze.notifications.handleBackgroundNotification(userInfo: userInfo, fetchCompletionHandler: completionHandler) {\n        return\n    }\n    completionHandler(.noData)\n}\n\n//...//\n\nextension AppDelegate: UNUserNotificationCenterDelegate {\n    func userNotificationCenter(_ center: UNUserNotificationCenter, didReceive response: UNNotificationResponse, withCompletionHandler completionHandler: @escaping () -> Void) {\n        if let braze = self.braze, braze.notifications.handleUserNotification(response: response, withCompletionHandler: completionHandler) {\n            return\n        }\n        completionHandler()\n    }\n\n    func userNotificationCenter(_ center: UNUserNotificationCenter, willPresent notification: UNNotification, withCompletionHandler completionHandler: @escaping (UNNotificationPresentationOptions) -> Void) {\n        if #available(iOS 14, *) {\n            completionHandler([.list, .banner])\n        } else {\n            completionHandler(.alert)\n        }\n    }\n}\nLOCATION TRACKING\n\nThe Braze kits for iOS and Android support Braze\u2019s automatic location tracking features, provided that the appropriate app-level permissions are granted by the user.\n\nAndroid\n\nFor Android push notifications you will need to provide your Server Key in your app settings page under Push Notification Settings.\n\niOS\n\nFor iOS push notifications you will need to upload your APNs Push SSL certificate to Braze. See the Braze documentation for more.\n\nSPECIAL CONSIDERATIONS FOR MPARTICLE A/B TESTING WITH BRAZE AND THE MPARTICLE SDK\n\nmParticle supports the ability to conduct A/B testing with different integrations by sending a sample of users and their data to one integration and a different sample of users and their data to a different integration. If you are using the mParticle SDK for Braze deployment and calling Braze methods directly, when instrumenting with the mParticle SDK you must ensure that the Braze kit is active in the App before calling a Braze method. This is very important and ensures that you are not inadvertently calling Braze methods for apps/users that are not part of an Braze A/B sample.\n\nRoku\n\nmParticle supports the ability to forward server-side events for the Roku platform. Note that only data that includes your selected External Identity Type can be forwarded to Braze.\n\nData Processing Notes\n\nmParticle will always forward events if sent via the mParticle SDK, provided you have included the Braze kit, but will only forward events sent via the mParticle S2S API if the following conditions apply:\n\nThe App Group REST API Key setting is specified.\nEither your set External Identity Type, or a push token is specified.\nBraze has limits on the number of characters in a property key - they must be less than or equal to 255 characters, with no leading dollar signs. When forwarding via a client-side kit, mParticle will remove the dollar sign ($) when forwarding property keys for user attributes, custom, and e-commerce events. When forwarding server-to-server, mParticle will remove the dollar sign ($) from mParticle reserved attributes.\nBraze doesn't currently support timestamps before year 0 or after year 3000 in Time type custom attributes. Braze will ingest these values when they are sent by mParticle but the value will be stored as a String.\nBraze Instance\n\nBraze maintains several instances. As part of the Configuration Settings, you need to specify which one your data should be forwarded to. You can tell your Braze Instance from the URL of your Braze Dashboard.\n\nInstance\tDashboard URL\nUS 01 Cluster\thttps://dashboard-01.braze.com\nUS 02 Cluster\thttps://dashboard-02.braze.com\nUS 03 Cluster\thttps://dashboard-03.braze.com\nUS 04 Cluster\thttps://dashboard-04.braze.com\nUS 05 Cluster\thttps://dashboard-05.braze.com\nUS 06 Cluster\thttps://dashboard-06.braze.com\nUS 08 Cluster\thttps://dashboard-08.braze.com\nEU 01 Cluster\thttps://dashboard-01.braze.eu\nEU 02 Cluster\thttps://dashboard-02.braze.eu\n\nCheck with your Braze account manager if you are unsure which Braze instance you are using.\n\nThere is also the ability to specify a Custom instance, which allows you to specify separate endpoints for REST, SDK and Javascript.\n\nImportant: Your Custom Endpoint settings should be your URL's Authority. For example: sdk.iad-01.braze.com, not https://sdk.iad-01.braze.com.\n\nUsing https:// or a trailing / in your endpoint address will cause errors.\n\nPrerequisites\n\nIn order to activate the Braze integration, you will need your Braze App Identifier API key and your \u201cApp Group REST API Key\u201d if using the S2S API.\n\nSign into your Braze Account.\nClick on Developer Console in the left navigation, then API Settings, Identification, and choose the Identifier for the platform you\u2019re building to.\nIf you are sending data to mParticle via the S2S API, your \u201cApp Group REST API Key\u201d value is required. Click on the Developer Console in the left navigation to get this value.\n\nEvent Data Mapping\nCommerce Events\n\nDepending on the setting value for Bundle Commerce Event Data (see Connection Settings for more), Commerce Events may be expanded based on the number of products, promotions, or impressions, respectively. Ex:\n\nIf false, a single incoming event with 2 unique products/promotions/impressions would result in at least 2 outgoing Braze events.\nIn true, a single incoming event with 2 unique products/promotions/impressions would result in 1 outgoing event with a nested products[], promotions[] or impressions[] array, respectively.\nIf you are using all three platforms, make sure you determine whether or not you want products in Braze to be identified by their SKU or by their product name. Because of prior implementations, iOS and Android send the SKU by default while Web sends the product name. Check out the [Connection Settings](#connection-settings) 'Replace SKU as Braze Product Name' and 'Map Product Name to Product ID' settings if you'd like to ensure matching data from all platforms.\nPRODUCT-BASED COMMERCE EVENTS\nPurchase Events\n\nA purchase event is mapped and optionally-expanded to Braze purchase event(s) as described above.\n\nIn addition to the Common Commerce Fields, the following information is also captured, if defined:\n\nCommerce Event Field\tBraze Expanded Purchase Event Field\tBraze Purchase Event Nested Product Field\tData Type\tRequired\tDescription\tExample\nCurrencyCode\tcurrency\tcurrency\tstring\tNo\tCurrency Code. If not specified, this will default to \u201cUSD\u201d.\tUSD\nProductAction.Products[].Id\tproduct_id\tproperties.products[].Id\tstring\tNo\tThe ID associated with the given product.\t\u201c123456\u201d\nProductAction.Products[].Price\tprice\tproperties.products[].Price\tdouble\tNo\tThe price associated with 1 unit of the given product.\t1.99\nProductAction.Products[].Quantity\tquantity\tproperties.products[].Quantity\tint\tNo\tThe quantity associated with the given product.\t2\nProductAction.TotalAmount\tproperties[\u201cTotal Amount\u201d]\tproperties[\u201cTotal Amount\u201d]\tstring\tNo\tThe total amount associated with the given transaction.\t25.00\nProductAction.ShippingAmount\tproperties[\u201cShipping Amount\u201d]\tproperties[\u201cShipping Amount\u201d]\tstring\tNo\tThe shipping amount associated with the given transaction.\t2.99\nProductAction.TaxAmount\tproperties[\u201cTax Amount\u201d]\tproperties[\u201cTax Amount\u201d]\tstring\tNo\tThe tax amount associated with the given transaction.\t1.37\nOther Product-based Commerce Events\n\nAll other product-based commerce event types are mapped and optionally-expanded to Braze custom event(s) as described above. Note:\n\nIf expansion is active and it\u2019s a refund, an additional event representing the total is also generated.\n\nAs such, all relevant product and transaction information is conveyed via the properties field.\n\nIn addition to the Common Commerce Fields, the following information is also captured, if defined:\n\nCommerce Event Field\tBraze Expanded Custom Event Field\tBraze Custom Event Nested Product Field\tData Type\tRequired\tDescription\tExample\nProductAction.Products[].Id\tproperties[\u201cId\u201d]\tproperties.products[].Id\tstring\tNo\tThe ID associated with the given product.\t\u201c123456\u201d\nProductAction.Products[].Price\tproperties[\u201cPrice\u201d]\tproperties.products[].Price\tdouble\tNo\tThe price associated with 1 unit of the given product.\t1.99\nProductAction.Products[].Quantity\tproperties[\u201cQuantity\u201d]\tproperties.products[].Quantity\tint\tNo\tThe quantity associated with the given product.\t2\nCommon Product-based Commerce Fields\n\nAll product-based commerce events, regardless of type, capture these common fields in the properties dictionary in the following way:\n\nCommerce Event Field\tBraze Expanded Event Field\tBraze Event Nested Product Field\tData Type\tRequired\tDescription\tExample\nProductAction.TransactionId\tproperties[\u201cTransaction Id\u201d]\tproperties[\u201cTransaction Id\u201d]\tstring\tNo\tThe ID associated with the given transaction.\t\u201c123456\u201d\nProductAction.Products[].Name\tproperties[\u201cName\u201d]\tproperties.products[].Name\tstring\tNo\tThe name associated with the given product.\t\u201cMyProduct\u201d\nProductAction.Products[].Category\tproperties[\u201cCategory\u201d]\tproperties.products[].Category\tstring\tNo\tThe category associated with the given product.\t\u201cClothing\u201d\nProductAction.Products[].Brand\tproperties[\u201cBrand\u201d]\tproperties.products[].Brand\tstring\tNo\tThe brand associated with the given product.\t\u201cMyBrand\u201d\nProductAction.Products[].TotalProductAmount\tproperties[\u201cTotal Product Amount\u201d]\tproperties.products[][\"total product amount\"]\tstring\tNo\tThe total amount associated with the given product for the given transaction.\t\u201cMyBrand\u201d\nProductAction.Products[].Attributes[\u201cmyProductAttribute\u201d]\tproperties[\u201cmyProductAttribute\u201d]\tproperties.products[].Attributes[\u201cmyProductAttribute\u201d]\tstring\tNo\tA custom attribute associated with the given product.\t\u201cmyProductAttribute\u201d\n\nNote: For web kit data, in order to forward the TransactionId, you must include the transactionAttributes as the fifth argument below. See our API docs for how to build transaction attributes.\n\nmParticle.eCommerce.logProductAction(\n    mParticle.ProductActionType.AddToCart,\n    [product1, product2],\n    customAttributes,\n    customFlags, //can also be `null`, but can't be blank\n    transactionAttributes);\nPROMOTION-BASED COMMERCE EVENTS\n\nWhen promotions are expanded, each promotion is converted to its own outgoing Braze event. Alternatively, if expansion isn\u2019t toggled, promotions are included in a nested collection in a single outgoing Braze event.\n\nCommerce Event Field\tBraze Expanded Custom Event Field\tBraze Custom Event Nested Promotion Field\tData Type\tRequired\tDescription\tExample\nPromotionAction.Promotions[].Id\tproperties[\u201cId\u201d]\tproperties.promotions[].Id\tstring\tNo\tThe ID associated with the given promotion.\t\u201c123456\u201d\nPromotionAction.Promotions[].Name\tproperties[\u201cName\u201d]\tproperties.promotions[].Name\tstring\tNo\tThe name associated with the given promotion.\t\u201cpromo1\u201d\nPromotionAction.Promotions[].Creative\tproperties[\u201cCreative\u201d]\tproperties.promotions[].Creative\tstring\tNo\tThe creative associated with the given promotion.\t\u201ccreative1\u201d\nPromotionAction.Promotions[].Position\tproperties[\u201cPosition\u201d]\tproperties.promotions[].Position\tstring\tNo\tThe position associated with the given promotion.\t\u201cposition1\u201d\nIMPRESSION-BASED COMMERCE EVENTS\n\nWhen impressions are expanded, each product from each impression is converted to its own outgoing Braze event. Alternatively, if expansion isn\u2019t toggled, impressions and their associated products are included in a nested collection in a single outgoing Braze event.\n\nCommerce Event Field\tBraze Expanded Custom Event Field\tBraze Custom Event Nested Impression Field\tData Type\tRequired\nProductImpressions[].ProductImpressionList\tproperties[\u201cProduct Impression List\u201d]\tproperties.impressions[][\"product impression list\"]\tstring\tNo\nProductImpressions[].Products[].Id\tproperties[\u201cId\u201d]\tproperties.impressions[].products[].Id\tstring\tNo\nProductImpressions[].Products[].Name\tproperties[\u201cName\u201d]\tproperties.impressions[].products[].Name\tstring\tNo\nProductImpressions[].Products[].Category\tproperties[\u201cCategory\u201d]\tproperties.impressions[].products[].Category\tstring\tNo\nProductImpressions[].Products[].Brand\tproperties[\u201cBrand\u201d]\tproperties.impressions[].products[].Brand\tstring\tNo\nProductImpressions[].Products[].Price\tproperties[\u201cPrice\u201d]\tproperties.impressions[].products[].Price\tstring\tNo\nProductImpressions[].Products[].Quantity\tproperties[\u201cQuantity\u201d]\tproperties.impressions[].products[].Quantity\tstring\tNo\nProductImpressions[].Products[].TotalProductAmount\tproperties[\u201cTotal Product Amount\u201d]\tproperties.impressions[].products[][\"total product amount\"]\tstring\tNo\nProductImpressions[].Products[].Attributes[\u201cmyProductAttribute\u201d]\tproperties[\u201cmyProductAttribute\u201d]\tproperties.impressions[].products[].Attributes[\u201cmyProductAttribute\u201d]\tstring\tNo\nScreen Views\n\nYour screen view events will be passed to Braze using the screen name that you passed to our logScreen SDK method, as the event name.\n\nIf you are using automatic screen tracking in our Android SDK, the automatically-generated screen view events will be forwarded to Braze using the name of the associated Activity class.\n\nSession Start / End\n\nTo send session start and end events for S2S to Braze, enable the Forward Session Events connection setting. After enabling this setting, session start and end events are forwarded to Braze as custom events with the names Session Start and Session End. When available, session IDs are also sent in the session_id property on all session starts and ends, screen view, and custom events.\n\nHowever, note that mParticle SDK kits do not support session events, which are never forwarded with a kit, whether or not Forward Session Events is enabled.\n\nWhen creating segmentation filters within Braze, make sure to use the custom event filters for session data rather than the session filters.\nCustom Events\n\nAll custom events will be forwarded to Braze using the event name that you passed to your logEvent SDK method. All event attributes will be forwarded to Braze as Braze custom event properties using the attribute names you passed to your logEvent SDK method as well.\n\nNote: Braze documentation indicates that the following reserved keywords can\u2019t be used as custom event properties:\n\ntime\nevent_name\n\nTo learn more, visit the Braze documentation:\n\nAndroid and FireOS\niOS (legacy)\nSwift\nUser Attributes\n\nThe table below describes how the mParticle integration maps user attributes to Braze\u2019s profile attributes.\n\nmParticle Field\tBraze Profile Attribute\tDescription\nUser Identity of type CustomerId\texternal_id\t\nUser Attribute $FirstName\tfirst_name\t\nUser Attribute $LastName\tlast_name\t\nUser Identity of type Email\temail\t\nDerived from User Attribute $Age)\tdob\tmParticle estimates the user\u2019s date of birth by subtracting $Age from the current year, and using January 1st as the month and day. For example, if $Age is 10 and the current year is 2014, we\u2019ll forward the user\u2019s date of birth as 2004-01-01. If an exact birth date is desired, set a user attribute called dob with user\u2019s birth date. When both $Age and dob user attributes are sent, one value may override the other when mParticle forwards data to Braze. So it is recommended that one of them is toggled Off in mParticle\u2019s data filter for Braze.\nUser Attribute $Country\tcountry\t\nUser Attribute $City\thome_city\t\nUser Attribute $Gender\tgender\t\nUser Attribute $Mobile\tphone\t\nDerived from SDK opt-out status\temail_subscribe\tThis is based on calling the OptOut (setOptOut in Android) SDK methods. It will be set to opted_in when called with a value of true and will be set to unsubscribed when called with a value of false. Email subscription statuses can also be updated server side by setting the user attribute email_subscribe with a value of subscribed, unsubscribed, or opted_in, which correspond to the Global Subscription States in Braze. This will not be set as a custom attribute in Braze, it\u2019ll appear in Contact Settings.\nUser Attribute push_subscribe\tpush_subscribe\tPush subscription status can be updated server side by setting the user attribute push_subscribe with a value of subscribed, unsubscribed, or opted_in, which correspond to the Global Subscription States in Braze. This will not be set as a custom attribute in Braze, it\u2019ll appear in Contact Settings.\npush_tokens\tBecause Braze can only accept a single push token for each app/user pair, we will forward the most recently-registered push token to Braze per user and per app.\t\nUser Identity of type Facebook\tfacebook\t\nUser Identity of type Twitter\ttwitter\t\n\nBraze advises to coerce data types on user attributes shared with Braze before sending Production volume data. This ensures that the custom attributes received by Braze are of the expected data type. This can impact segment building and triggering campaigns. If incorrect data types are identified after data has been flowing, there can be extra work to true up the users with the legacy data type on those attributes. To give Braze\u2019s type detection a better opportunity to evaluate data types properly, you can enable the following 2 Connection Settings: Enable API Custom Attribute Type Detection and Enable Kit Custom Attribute Type Detection.\n\nEnriched Attributes and Identities\n\nBy default, mParticle forwards all available user attributes and user identities to Braze, including attributes added during profile enrichment. You can disable this behavior in the Connection Settings. Only data which is sent to Braze Server to Server can be enriched.\n\nSubscription Groups\n\nA User Attribute can be mapped to a Braze Subscription Group ID. Requests sent to mParticle containing the mapped User Attribute, with a valid true or false value, will determine if the user in the request should be subscribed (true) or unsubscribed (false) from the group. This allows for dynamic subscription management based on user attributes.\n\nTo configure Subscription Group Mapping:\n\nCreate the Subscription Group in Braze, if it doesn\u2019t already exist.\n\nMap the desired User Attribute to the Subscription Group ID in the mParticle dashboard.\n\nNote that the User Attribute must be sent to mParticle at least once before it is available for mapping.\n\nSend the mapped User Attribute on your event batch to mParticle with a boolean value.\n\ntrue: User will be subscribed to the Braze Subscription Group.\nfalse: User will be unsubscribed from the Braze Subscription Group.\nOther non-boolean values will be ignored.\nSUBSCRIPTION GROUP ERRORS\n\nA misconfigured Subscription Group mapping can affect how your data is forwarded to Braze. If you notice your Subscription Groups are not being updated, check the following:\n\nInvalid Subscription Group ID: Ensure the mapped Subscription Group IDs are correct in your Connection Settings.\nDeleted Subscription Group: If a Subscription Group is deleted in Braze, you must update or remove the mapping in the mParticle dashboard.\nInvalid User Attribute: Ensure the mapped User Attribute is being sent to mParticle with a boolean value.\n\nPlease note that Braze will only return an error in these cases when the Subscription Group attribute update is the only item in the request sent to Braze. If the request contains valid events and/or purchases, Braze will process these and skip the Subscription Group update. This also means that you will only see Subscription Group errors in the System Alerts page when the invalid Subscription Group update is the only item in the request.\n\nConsent\nTo adhere to the EU user consent policy, Braze is enforcing the consent field to be populated for EU users.\n\nBraze uses the $google_ad_user_data and $google_ad_personalization custom attribute keys to receive consent status for Google\u2019s EU User Consent Policy. The Boolean sent to these keys signify that a user has granted or denied consent to a marketer. As per their Collecting consent for EEA, UK, and Switzerland end users documentation, this value must be sent to Google as a Custom Attribute.\n\nUSER-SPECIFIED CONSENT\n\nTo configure user consent forwarding under this value, a mapping should be set-up leveraging mParticle\u2019s notion of Consent Purposes. To learn more about handling user consent within mParticle\u2019s platform, see the following docs: Data Privacy Controls.\n\nOnce a Consent Purpose is set up, user consent information can be associated with it in subsequent Event Batches. The Consent Purpose data mapping can then be configured for downstream forwarding via the Consent Mapping connection setting.\n\nConfiguration Settings\nSetting Name\tData Type\tDefault Value\tDescription\nApp Identifier API Key\tstring\t\tYour app\u2019s App Identifier API Key can be found in your Braze dashboard in Developer Console > API Settings > Identification > Identifier. This value is used for certain API calls to Braze (e.g. Push Token) and also used to initialize the Braze SDK via the client side kit\nExternal Identity Type\tenum\tCustomer ID\tThe mParticle User Identity Type to forward as an External ID to Braze.\nEmail Identity Type\tenum\tEmail\tThe mParticle User Identity Type to forward as the Email to Braze.\nBraze Instance\tenum\tUS 03 Cluster\tSpecify which cluster your Braze data will be forwarded to. Please ensure you are contractually authorized to use the EU cluster if you select that option. If you choose \u2018Custom\u2019, you will need to provide separate endpoints for your SDK, Server, and Web data.\nEnable Event Stream Forwarding\tbool\tFalse\tIf enabled, all events will be forwarded in real time. If not, all events will be forwarded in bulk. Real time forwarding has lower latency, but requires higher rate limits within Braze.\nConnection Settings\nSetting Name\tData Type\tDefault Value\tPlatform\tDescription\nApp Group REST API Key\tstring\t\tAll\tThe App Group REST API Key can be found in the developer console section of the Braze dashboard. This field is optional when sending in data via the SDK, but is required for using the S2S API.\nBraze SDK Session Timeout\tstring\t\tAll\tBraze SDK time interval for session time out in seconds.\nPush Enabled\tbool\tTrue\tiOS, tvOS, Android\tForward GCM registration IDs to the Braze SDK and enable Braze push notifications.\nForward Client-side Requests to Server-to-Server\tbool\tFalse\tiOS, tvOS, Android, FireTV, Web\tIf enabled, Client-side requests will only be forwarded server-side and will not be sent as a client-side kit.\nDisable External ID Constraint\tbool\tFalse\tS2S API\tThis allows relaxing the default External ID requirement for server-side forwarding. When unchecked, server-side data must include a value for the selected External Identity Type to be forwarded to Braze. If checked, it allows forwarding events to Braze even if the value is absent for the selected External Identity Type as long as there is a value for the selected Email Identity Type. Regardless of the setting, server-side data lacking both External ID and Email will not be forwarded, and the External ID is forwarded when available. Note: User profile creation and duplicate users are dependent on and handled by Braze.\nEvent Attributes that add to array\tCustom Field\t\tiOS, tvOS, Android, Roku\tSelect your mParticle event names and event attributes and enter the corresponding Braze custom attribute array name you want the event attribute ADDED to.\nEvent Attributes that remove from array\tCustom Field\t\tiOS, tvOS, Android, Roku\tSelect your mParticle event names and event attributes and enter the corresponding Braze custom attribute array name you want the event attribute REMOVED from.\nEvent Attributes that set to custom attribute value\tCustom Field\t\tiOS, tvOS, Android, Roku\tSelect your mParticle event names and event attributes and enter the corresponding Braze custom attribute you want the event attribute to map to. Note each time this event attribute is present, it will get sent to Braze and overwrite any previously sent value.\nSubscription Group Mappings\tCustom Field\t\tAll\tMaps User Attributes to Braze Subscription Group IDs. Note: Attributes mapped for Subscription Groups won\u2019t be sent to Braze as standard attributes.\nBraze SDK Flush Interval\tstring\t\tiOS, tvOS\tBraze SDK data flush internal in seconds (iOS only). Refer to Braze sdk doc for \u201cABKFlushIntervalOptionKey\u201d.\nConsent Mapping\tCustom Field\t\tiOS, tvOS, Android\tSelect which mParticle Consent Purposes you would like to map to the Google EU Consents supported by Braze. View Braze\u2019s Documentation to understand why this setting is necessary for EU customers. For more information on how to set consent state in mParticle, please view our iOS and Android documentation.\nBraze SDK Request Policy\tstring\t\tiOS, tvOS\tBraze SDK request policy at app start time (iOS only). Refer to Braze sdk doc for \u201cABKRequestProcessingPolicyOptionKey\u201d.\nBraze SDK Minimum Time Interval Between Triggers\tstring\t\tiOS, tvOS\tBraze SDK minimum time interval in seconds between triggers (iOS only). Refer to Braze sdk doc for \u201cABKMinimumTriggerTimeIntervalKey\u201d.\nUser Tags Value\tenum\t\u201ctrue\u201d\tAll but Web\tSelect the value to be sent to Braze for user tags. The possible values are null or \u201ctrue\u201d. When \u201ctrue\u201d, it will be affected by the parameter Enable type detection. When \u201cnull\u201d and set on a user attribute, the custom attribute (key and value) on the Braze user profile will be removed.\nBraze SDK Collect IDFA?\tbool\tFalse\tiOS, tvOS\tInforms the Braze Kit whether to collect IDFA.\nBraze SDK Disable Automatic Location Tracking\tbool\tFalse\tiOS, tvOS\tInforms the Braze Kit whether to disable automatic location tracking at app startup time\nMap Product Name to Product ID\tbool\tFalse\tiOS, tvOS, Android\tIf enabled, the product name is mapped to the product ID field in Braze. By default, iOS/Android maps the mParticle SKU (also known as product ID) field to Braze\u2019s product ID field.\nInclude Enriched User Attributes\tbool\tTrue\tAll\tIf enabled, mParticle will forward enriched user attributes from the existing user profile. Only data which is sent to Braze Server to Server can be enriched.\nInclude Enriched User Identities\tbool\tTrue\tAll\tIf enabled, mParticle will forward enriched user identities from the existing user profile. Only data which is sent to Braze Server to Server can be enriched.\nSkip Eventless Batches\tbool\tFalse\tAll\tIf enabled, batches containing no events will be skipped by the Braze forwarder.\nSend User Attribute Lists as Arrays\tbool\tFalse\tAll\tIf checked, mParticle will send each user attribute list server-side as an array, rather than a comma-separated string\nForward Screen View Messages\tbool\tFalse\tAll\tIf enabled, all screen view messages will be forwarded to Braze as separate events. Not supported for S2S requests.\nForward Session Events\tbool\tFalse\tAll\tIf enabled, S2S connections will forward session start and end events to Braze. Kits do not support forwarding session events. Session IDs will also be sent with events when populated.\nSoft Push Custom Event Name\tstring\t\tWeb\tThe custom event name that shows up in your Braze dashboard when priming your user for push notifications. Braze recommends \u201cprime-for-push\u201d. When filled in, users will be sent a Braze In-App message on session load\nPush Notification Service Worker File Location\tstring\t\tWeb\tOptional - If the \u201cservice worker.js\u201d file is not located in your root directory, then this field is the relative path, starting with \u201d/\u201d and including the filename.js. Please view integration docs for more information\nSafari Website Push ID\tstring\t\tWeb\tThe unique identifier for your Website Push ID, starting with the string \u201cweb\u201d, from the Apple Developer website\nAutomatically display new in-app messages\tbool\tTrue\tWeb\tAutomatically display new in-app messages when they come in from the server.\nForward Page Name as Braze Event Name\tbool\tFalse\tWeb\tIf enabled, the Page Name that is sent to Braze is the first argument in mParticle.logPageView(\u201cPageName\u201d). Otherwise the path will be used. For S2S requests, enabling this setting will send the event name as-is. Otherwise Viewed will be added to the event name.\nReplace SKU as Braze Product Name\tbool\tFalse\tWeb\tIf enabled, the SKU replaces Product Name when sent to Braze. By default, Web sends Product Name to Braze. New customers should check this to be consistent with iOS/Android which sends SKU by default.\nEnable HTML within in-app messages\tbool\tFalse\tWeb\tEnable HTML within in-app messages. This correlates to the enableHtmlInAppMessages setting of the Braze SDK.\nDo not Load FontAwesome\tbool\tFalse\tWeb\tDisable loading of FontAwesome from the FontAwesome CDN. Note that if you do this, you are responsible for ensuring that FontAwesome is loaded - otherwise in-app messages may not render correctly.\nEnable API Custom Attribute Type Detection\tenum\tNone\tS2S API\tSend custom attributes with parsed data types for User Attributes, Event Attributes, both, or neither. The following data types are evaluated, in this order, for a match to determine data type to include with the message to Braze: Boolean, Long (covers integer), Double (covers decimal), Date, String (if none else are true). This supports data point triggers in Braze that may require a specific data type. Data points can be automatically detected in Braze or forced. A list of the data types that Braze supports are here. The $Zip user attribute is exempt from type detection and will always be sent as a string.\nEnable Kit Custom Attribute Type Detection\tbool\tFalse\tAll kits but Web\tBy default, all kits (except for web) send custom attributes as strings unless they are special Braze reserved attributes. Checking this will force custom attributes to be sent as parsed data types where possible. Please note that Braze refers to these as Custom Attributes or Properties in the case of Events and Purchases. The $Zip user attribute is exempt from type detection and will always be sent as a string.\nBraze Web SDK Version\tstring\tVersion 3\tWeb\tUsers have the option to choose whether they want V3 or V4 of the Braze SDK. Due to significant changes that Braze has between V3 and V4, please first review our migration steps before opting in to V4.\nBundle Commerce Event Data\tbool\tFalse\tAll*\tIf checked, mParticle will not expand incoming commerce events, and will instead forward all product, promotion, and impression information in nested attributes in a single event to Braze. *For web, this feature is only available if you opt into version 4 of the Braze Web SDK (see above config setting).\nCalculate Price for Server-to-Server Data\tbool\tFalse\tS2S API\tIf enabled, the price for outgoing server-to-server purchase data will be calculated based on the sum of individual product cost (price*quantity). This setting is only applicable when Bundle Commerce Event Data is enabled.\nBraze Web Kit Critical Updates and Timelines\n\nBraze occasionally makes breaking changes to their SDK, so if you call Braze directly in your code, you will have to update your code to ensure your website performs as expected when updating versions of Braze.\n\nOpt In to Braze SDK Version 5\n\nWe recommend you use version 5 of the Braze SDK. You only need to make code changes if you directly call the Braze SDK from your code. Otherwise, you can simply select Version 5 from the mParticle connection settings.\n\nIf you are upgrading from V3, you should check the breaking changes from the Braze V4 Changelog as well as Braze V5 Changelog, in addition to the V4/5 migration guide to learn about the breaking changes. The most significant breaking changes are the replacement of the appboy class name with braze from V3 to V4, in addition to the removal and renaming of several APIs. If you are updating from V4 to V5, there are fewer changes, but you\u2019d follow the same defensive code recommendations below.\n\nYou can opt into the latest major version of the Braze Web SDK whether you implement mParticle\u2019s Web SDK using npm or our snippet/CDN.\n\nCustomers who self-host mParticle via npm - You should add @mparticle/web-braze-kit version 5.0.0 or greater in your package.json. You must also select Version 5 under Braze Web SDK Version in the Braze connection settings.\nCustomers who load mParticle via snippet/CDN - You must select Version 5 under Braze Web SDK Version in the Braze connection settings.\n\nNote that the following is only one example, and demonstrates an upgrade from V3 to V5. Everywhere you manually call appboy (or other deprecated API) needs to be updated similar to the below. If you are using NPM, you can skip to step 4. Please be sure to test your site fully in development prior to releasing.\n\nStep 1: Legacy code sample. Find all the places where your code references the appboy.display namespace. Braze has removed all instances of the display namespace:\nwindow.appboy.display.destroyFeed();\n\nStep 2: Roll out code changes to be used before you opt in to using Version 4 of the Braze Web SDK so that it works under both versions:\n\nif (window.appboy) {\n    window.appboy.display.destroyFeed();\n} else if (window.braze) {\n    window.braze.destroyFeed();\n}\n\nStep 3: Whether you are using the snippet or self-hosting, you need to navigate to your Braze connection settings and select Version 5 from the Braze Web SDK Version drop down.\n\nStep 4: After you opt in, you can simplify your code. We recommend testing and waiting at least 24 hours between opting in and removing previous instances of appboy and doing thorough testing of your application in a development environment to ensure everything is working:\n\nwindow.braze.destroyFeed();\n\nStep 4: Push Notifications via service-worker.js If you use Push Notifications, we have updated the service-worker.js file. In our testing, Braze\u2019s push notifications work as expected regardless of what version of the service-worker is used, but we recommend updating this file to ensure future compatibility. In your service-worker.js file, update the code to reference https://static.mparticle.com/sdk/js/braze/service-worker-5.5.0.js instead of https://static.mparticle.com/sdk/js/braze/service-worker-3.5.0.js. Your service-worker.js file should now contain:\n\nself.imports('https://static.mparticle.com/sdk/js/braze/service-worker-5.5.0.js')\nTransition from @mparticle/web-appboy-kit to @mparticle/web-braze-kit\n\nThe legacy @mparticle/web-appboy-kit from NPM includes version 2 of the Braze Web SDK. As part of this update, we\u2019ve created a new Braze web kit repo to replace our deprecated Appboy web kit repo. If you are still using @mparticle/web-appboy-kit, you will need to consider the breaking changes Braze made between V2 and V3 of the Braze SDK (found here) as well as the instructions above to get from V2 to V5 of the Braze SDK.\n\nDOCS\nLast Updated:\n09/01/2025, 00:02:28\nCDP\nmParticle Docs\nHome\nGuides\nDevelopers\nIntegrations\nAnalytics\nAnalytics Docs\nOverview\nGetting Started\nDevelopers\nExpert Training\nPredictive AI\nCortex Docs\nOverview\nGetting Started\nDevelopers\nUsing Cortex\nLearn More\nBlog\nPodcast\nResources\nCommunity\nChange log\nGet in Touch\nHelp Center\nDemo\nBecome a Partner\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nNot Found\n\nYou hit a page that does not exist.\n\nReturn to the home page\nDOCS\nLast Updated:\n1/8/2025, 6:32:28 PM\nCDP\nmParticle Docs\nHome\nGuides\nDevelopers\nIntegrations\nAnalytics\nAnalytics Docs\nOverview\nGetting Started\nDevelopers\nExpert Training\nPredictive AI\nCortex Docs\nOverview\nGetting Started\nDevelopers\nUsing Cortex\nLearn More\nBlog\nPodcast\nResources\nCommunity\nChange log\nGet in Touch\nHelp Center\nDemo\nBecome a Partner\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nDocumentation\nDevelopers\nAPI References\nClient SDKs\nMedia SDKs\nServer SDKs\nQuickstart\nTools\nGuides\n\nAPI Credential Management\n\nThe Developer's Guided Journey to mParticle\n\nGuides\nGetting Started\nPlatform Guide\nAnalytics\nIDSync\nData Master\nPersonalization\n\nWarehouse Sync\n\nData Privacy Controls\n\nData Subject Requests\n\nDefault Service Limits\n\nFeeds\n\nCross-Account Audience Sharing\n\nApproved Sub-Processors\n\nImport Data with CSV Files\n\nGlossary\n\nVideo Index\n\nAnalytics (Deprecated)\nIntegrations\n24i\nAarki\nABTasty\nAbakus\nActable\nAdChemix\nAdikteev\nAdjust\nAdMedia\nAdobe Marketing Cloud\nAdobe Audience Manager\nAdobe Campaign Manager\nAdobe Target\nAdPredictive\nAgilOne\nAlgolia\nAirship\nAlgoLift\nAlooma\nAmazon Kinesis\nAmazon Advertising\nAmazon Kinesis Firehose\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAmazon SNS\nAdobe Marketing Cloud\nAmobee\nAmpush\nAmplitude\nAnalytics\nAntavo\nAppLovin\nAnodot\nApptentive\nApptimize\nAttractor\nAttentive\nApteligent\nMicrosoft Azure Blob Storage\nBidease\nBing Ads\nBatch\nBluecore\nBluedot\nBranch\nBlueshift\nBugsnag\nBraze\n\nFeed\n\nAudience\n\nForwarding Data Subject Requests\n\nEvent\n\nBranch S2S Event\nCadent\nButton\nCensus\nciValue\nCleverTap\ncomScore\nCortex\nConversant\nCordial\nCriteo\nCrossing Minds\nCustom Feed\nCustomerGlu\nDatabricks\nCustomer.io\nDatadog\nDidomi\nDynalyst\nEdge226\nDynamic Yield\nEmarsys\nEpsilon\nEverflow\nFacebook Offline Conversions\nFiksu\nFacebook\nFlurry\nGoogle Analytics for Firebase\nFlybits\nForeSee\nFormation\nFoursquare\nFreeWheel Data Suite\nGoogle Ad Manager\nGoogle Ads\nGoogle Analytics\nGoogle BigQuery\nGoogle Analytics 4\nGoogle Enhanced Conversions\nGoogle Cloud Storage\nGoogle Marketing Platform\nGoogle Marketing Platform Offline Conversions\nGoogle Pub/Sub\nHerow\nGoogle Tag Manager\nHeap\nHightouch\nIbotta\nHyperlocology\nIndicative\nImpact\nInMarket\nInMobi\nInspectlet\nInsider\niPost\nIntercom\nironSource\nIterable\nJampp\nKayzen\nKafka\nKlaviyo\nKochava\nKissmetrics\nKubit\nLaunchDarkly\nLeanplum\nLiftoff\nLifeStreet\nLiveLike\nLiveramp\nLocalytics\nMadHive\nmAdme Technologies\nMailchimp\nMarigold\nMediaMath\nMautic\nMediasmart\nMicrosoft Azure Event Hubs\nMintegral\nMixpanel\nMoEngage\nMovable Ink\nMoloco\nMonetate\nMovable Ink - V2\nmyTarget\nMultiplied\nNarrative\nNanigans\nNami ML\nNeura\nOneTrust\nNCR Aloha\nOptimizely\nOracle BlueKai\nOracle Responsys\nPersona.ly\nPaytronix\nPersonify XP\nPieEye\nPilgrim\nPinterest\nPlarin\nPostie\nPrimer\nPunchh\nPushwoosh\nQuantcast\nRadar\nReddit\nRegal\nRetina AI\nRemerge\nReveal Mobile\nRevenueCat\nRTB House\nSailthru\nRokt\nSalesforce Email\nSalesforce Mobile Push\nSamba TV\nScalarr\nSendGrid\nShareThis\nSignal\nSessionM\nShopify\nSimpleReach\nSingular-DEPRECATED\nSingular\nSkyhook\nSlack\nSmadex\nSmarterHQ\nSnapchat\nSnapchat Conversions\nSnowflake\nSnowplow\nSplunk MINT\nAppsFlyer\nSplit\nStartApp\nSprig\nStatsig\nStormly\nSwrve\nTalon.One\nTapad\nTapjoy\nTaptica\nTaplytics\nTeak\nThe Trade Desk\nTikTok Event\nTicketure\nTreasure Data\nQuadratic Labs\nTriton Digital\nTUNE\nTwitter\nValid\nVoucherify\nVkontakte\nWebtrends\nVungle\nWebhook\nWhite Label Loyalty\nWootric\nXandr\nYahoo (formerly Verizon Media)\nYouAppi\nYotpo\nZ2A Digital\nQualtrics\nZendesk\nFeed\n\nBraze is a comprehensive customer engagement platform that powers relevant experiences between consumers and brands they love. Braze helps brands foster human connection through interactive conversations across channels.\n\nThe Braze Feed allows mParticle to receive Braze interaction data for the iOS, Android and Web platforms. It also supports an \u2018unbound\u2019 feed for events, such as emails, that are not connected to an app platform.\n\nEnable the Braze Feed\nLocate Braze in the mParticle Directory and add the Feed integration.\n\nThe Braze Feed Integration supports four separate feeds: iOS, Android, Web and Unbound. You will need to create an input for each feed. You can create additional inputs from Setup > Inputs, on the Feed Configurations tab.\n\nFor each feed, select an \u2018Act as Application\u2019 platform. For iOS, Android and Web, just select the appropriate option from the list. For your Unbound feed, leave this setting at \u2018Select Application\u2019.\n\nAs you create each input, mParticle will provide you with a Key and Secret. Copy these credentials, making sure to note which feed each pair of credentials is for. Provide the credentials to your Braze account manager and ask them to enable the mParticle feed.\nCommon Event Data\n\nAll events from the Braze Feed will include any available user/device identifiers. Platform feeds will include basic device information.\n\nUser Identifiers\nCustomer ID\nEmail\nApple Vendor ID (IDFV)\nDevice Info\nPlatform\nDevice Model\nEvents\n\nThe Braze Feed sends events for tracking campaign performance. Included as custom attributes for each event are Braze\u2019s IDs for the relevant Campaign, News Feed Card, Button, Canvas, etc. You can find the ID\u2019s in the relevant sections of the Braze Dashboard under API Identifier. See Braze\u2019s Identifier Types for more details.\n\nYou can also see a full list of your API Identifiers on Braze\u2019s Developer Console.\n\nA full list of events and their allowed attributes can be found in Braze\u2019s Documentation. For events with campaign_id, canvas_id, canvas_step_id, and canvas_variation_id listed note that an individual event may have a campaign_id, the canvas_* attributes, or neither.\n\nDOCS\nLast Updated:\n09/01/2025, 00:02:28\nCDP\nmParticle Docs\nHome\nGuides\nDevelopers\nIntegrations\nAnalytics\nAnalytics Docs\nOverview\nGetting Started\nDevelopers\nExpert Training\nPredictive AI\nCortex Docs\nOverview\nGetting Started\nDevelopers\nUsing Cortex\nLearn More\nBlog\nPodcast\nResources\nCommunity\nChange log\nGet in Touch\nHelp Center\nDemo\nBecome a Partner\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nDocumentation\nFilter by Category\nAll\nAdvertising\nData Onboarding\nRe-Targeting\nUser Acquisition\nAnalytics\nA/B Testing\nAttribution\nConversion Tracking\nCrash Reporting\nCustom Feeds\nFeature Flagging\nLocation\nMonitoring\nPredictive Analytics\nUser Analytics\nCustomer Service\nCustomer Support\nHelpdesk\nUser Feedback\nData Warehousing\nBusiness Intelligence\nDMP\nRaw Data Export\nFinance\nCommerce\nPayments\nMarketing\nAffiliate Marketing\nCRM\nCustomer Engagement\nData Enrichment\nDeep Linking\nLoyalty and Rewards\nPersonalization\nTag Management\nPrivacy\nConsent Management\nData Subject Request\nSecurity\nAccess Management\nFraud Monitoring\nIntegrations\n\nConnect your customer data to the leading marketing, analytics, and data warehousing solutions with just a few clicks.\n\nNote: New Partners, please register.\n\nFilter by integration type\nDOCS\nLast Updated:\n09/01/2025, 00:02:28\nCDP\nmParticle Docs\nHome\nGuides\nDevelopers\nIntegrations\nAnalytics\nAnalytics Docs\nOverview\nGetting Started\nDevelopers\nExpert Training\nPredictive AI\nCortex Docs\nOverview\nGetting Started\nDevelopers\nUsing Cortex\nLearn More\nBlog\nPodcast\nResources\nCommunity\nChange log\nGet in Touch\nHelp Center\nDemo\nBecome a Partner\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nDocumentation\nDevelopers\nAPI References\nClient SDKs\nMedia SDKs\nServer SDKs\nQuickstart\nTools\nGuides\n\nAPI Credential Management\n\nThe Developer's Guided Journey to mParticle\n\nGuides\nGetting Started\nPlatform Guide\nAnalytics\nIDSync\nData Master\nPersonalization\n\nWarehouse Sync\n\nData Privacy Controls\n\nData Subject Requests\n\nDefault Service Limits\n\nFeeds\n\nCross-Account Audience Sharing\n\nApproved Sub-Processors\n\nImport Data with CSV Files\n\nGlossary\n\nVideo Index\n\nAnalytics (Deprecated)\nIntegrations\n24i\nAarki\nABTasty\nAbakus\nActable\nAdChemix\nAdikteev\nAdjust\nAdMedia\nAdobe Marketing Cloud\nAdobe Audience Manager\nAdobe Campaign Manager\nAdobe Target\nAdPredictive\nAgilOne\nAlgolia\nAirship\nAlgoLift\nAlooma\nAmazon Kinesis\nAmazon Advertising\nAmazon Kinesis Firehose\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAmazon SNS\nAdobe Marketing Cloud\nAmobee\nAmpush\nAmplitude\nAnalytics\nAntavo\nAppLovin\nAnodot\nApptentive\nApptimize\nAttractor\nAttentive\nApteligent\nMicrosoft Azure Blob Storage\nBidease\nBing Ads\nBatch\nBluecore\nBluedot\nBranch\nBlueshift\nBugsnag\nBraze\nBranch S2S Event\nCadent\nButton\nCensus\nciValue\nCleverTap\ncomScore\nCortex\nConversant\nCordial\nCriteo\nCrossing Minds\nCustom Feed\nCustomerGlu\nDatabricks\nCustomer.io\nDatadog\nDidomi\nDynalyst\nEdge226\nDynamic Yield\nEmarsys\nEpsilon\nEverflow\nFacebook Offline Conversions\nFiksu\nFacebook\nFlurry\nGoogle Analytics for Firebase\nFlybits\nForeSee\nFormation\nFoursquare\nFreeWheel Data Suite\nGoogle Ad Manager\nGoogle Ads\nGoogle Analytics\nGoogle BigQuery\nGoogle Analytics 4\nGoogle Enhanced Conversions\nGoogle Cloud Storage\nGoogle Marketing Platform\nGoogle Marketing Platform Offline Conversions\nGoogle Pub/Sub\nHerow\nGoogle Tag Manager\nHeap\nHightouch\nIbotta\nHyperlocology\nIndicative\nImpact\nInMarket\nInMobi\nInspectlet\nInsider\niPost\nIntercom\nironSource\nIterable\nJampp\nKayzen\nKafka\nKlaviyo\nKochava\nKissmetrics\nKubit\nLaunchDarkly\nLeanplum\nLiftoff\nLifeStreet\nLiveLike\nLiveramp\nLocalytics\nMadHive\nmAdme Technologies\nMailchimp\nMarigold\nMediaMath\nMautic\nMediasmart\nMicrosoft Azure Event Hubs\nMintegral\nMixpanel\nMoEngage\nMovable Ink\nMoloco\nMonetate\nMovable Ink - V2\nmyTarget\nMultiplied\nNarrative\nNanigans\nNami ML\nNeura\nOneTrust\nNCR Aloha\nOptimizely\nOracle BlueKai\nOracle Responsys\nPersona.ly\nPaytronix\nPersonify XP\nPieEye\nPilgrim\nPinterest\nPlarin\nPostie\nPrimer\nPunchh\nPushwoosh\nQuantcast\nRadar\nReddit\nRegal\nRetina AI\nRemerge\nReveal Mobile\nRevenueCat\nRTB House\nSailthru\nRokt\nSalesforce Email\nSalesforce Mobile Push\nSamba TV\nScalarr\nSendGrid\nShareThis\nSignal\nSessionM\nShopify\nSimpleReach\nSingular-DEPRECATED\nSingular\nSkyhook\nSlack\nSmadex\nSmarterHQ\nSnapchat\nSnapchat Conversions\nSnowflake\nSnowplow\nSplunk MINT\nAppsFlyer\n\nFeed\n\nForwarding Data Subject Requests\n\nEvent\n\nSplit\nStartApp\nSprig\nStatsig\nStormly\nSwrve\nTalon.One\nTapad\nTapjoy\nTaptica\nTaplytics\nTeak\nThe Trade Desk\nTikTok Event\nTicketure\nTreasure Data\nQuadratic Labs\nTriton Digital\nTUNE\nTwitter\nValid\nVoucherify\nVkontakte\nWebtrends\nVungle\nWebhook\nWhite Label Loyalty\nWootric\nXandr\nYahoo (formerly Verizon Media)\nYouAppi\nYotpo\nZ2A Digital\nQualtrics\nZendesk\nFeed\n\nAppsFlyer is a leading Mobile Attribution & Marketing Analytics platform that allows app marketers to easily measure the performance of all their marketing channels - paid, organic and social - from a single real-time dashboard.\n\nInput Data Details\n\nThe following types of data can be configured to be sent from AppsFlyer to mParticle\n\nAttribution\nIn-App\nUninstall\n\nAppsFlyer attribution events are mapped as follows:\n\nEvent Type = Custom Event\nCustom Event Type = attribution\nEvent Name = attribution\nAppsFlyer Event Mapping\n\nAppsFlyer attribution events are mapped as follows:\n\nAttribution Field\tmParticle Mapping\tAppsFlyer Macro Used\nios_idfa\tIDFA Device ID\t(advertiserId)\nios_idfv\tIDFV Device ID\t(vendorId)\nandroid_id\tAndroid ID Device ID\t(android_id)\nandroid_advertising_id\tAndroid Advertising ID Device ID\t(advertiserId)\ncustomer_id\tCustomer ID User Identity\t(custom-user-id)\npublisher\tPublisher custom event attribute\t(promoter-id)\ncampaign\tCampaign custom event attribute\t(campaign)\nts\tUnix time in milliseconds\t(install-unix-ts-ms)\naf_c_id\taf_c_id custom event attribute\t{af_c_id}\nConfiguration\n\nConfigure the AppsFlyer Input:\n\nSelect Directory, and click the AppsFlyer tile\nClick Add AppsFlyer to Setup\nSelect the Input Feed Integration Type and click Add to Setup\nSelect the AppsFlyer input configuration group to specify the configuration parameters:\nConfiguration Name\nAct as Application\nClick Create\nCopy the Token\n\nFollow these instructions to configure the postback in AppsFlyer.\n\nSelect Configuration > Integrated Partners.\nSearch for and select mParticle.\nIn the Integrations tab, click Activate Partner.\nEnter the mParticle Token, copied from above.\nClick Save & Close.\nDOCS\nLast Updated:\n1/8/2025, 6:32:28 PM\nCDP\nmParticle Docs\nHome\nGuides\nDevelopers\nIntegrations\nAnalytics\nAnalytics Docs\nOverview\nGetting Started\nDevelopers\nExpert Training\nPredictive AI\nCortex Docs\nOverview\nGetting Started\nDevelopers\nUsing Cortex\nLearn More\nBlog\nPodcast\nResources\nCommunity\nChange log\nGet in Touch\nHelp Center\nDemo\nBecome a Partner\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data",
        "DOCS\nHome\nGuides\nDevelopers\nIntegrations\nChangelog\nSign Up\nDocumentation\nFilter by Category\nAll\nAdvertising\nData Onboarding\nRe-Targeting\nUser Acquisition\nAnalytics\nA/B Testing\nAttribution\nConversion Tracking\nCrash Reporting\nCustom Feeds\nFeature Flagging\nLocation\nMonitoring\nPredictive Analytics\nUser Analytics\nCustomer Service\nCustomer Support\nHelpdesk\nUser Feedback\nData Warehousing\nBusiness Intelligence\nDMP\nRaw Data Export\nFinance\nCommerce\nPayments\nMarketing\nAffiliate Marketing\nCRM\nCustomer Engagement\nData Enrichment\nDeep Linking\nLoyalty and Rewards\nPersonalization\nTag Management\nPrivacy\nConsent Management\nData Subject Request\nSecurity\nAccess Management\nFraud Monitoring\nIntegrations\n\nConnect your customer data to the leading marketing, analytics, and data warehousing solutions with just a few clicks.\n\nNote: New Partners, please register.\n\nFilter by integration type\nDOCS\nLast Updated:\n09/01/2025, 00:02:28\nCDP\nmParticle Docs\nHome\nGuides\nDevelopers\nIntegrations\nAnalytics\nAnalytics Docs\nOverview\nGetting Started\nDevelopers\nExpert Training\nPredictive AI\nCortex Docs\nOverview\nGetting Started\nDevelopers\nUsing Cortex\nLearn More\nBlog\nPodcast\nResources\nCommunity\nChange log\nGet in Touch\nHelp Center\nDemo\nBecome a Partner\n\u00a9 2025 mParticle, Inc. All rights reserved.\nmParticle.com\nPrivacy Policy\nCookie Policy\nDo Not Sell or Share My Personal Data"
    ],
    "lytics": [
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nDeveloper Quickstart\nSuggest Edits\nIntroduction\n\nWelcome to the Lytics developer tier! This guide will walk you through the steps to get started with Lytics and leverage its powerful personalization capabilities for your website.\n\nBefore You Begin\n\nBefore diving into the setup process, make sure you have the following:\n\n Site Access / Management Permission: To install Lytics, you need permission to install JavaScript either via a tag manager or directly onto your website. Alternatively, you can install Lytics via a Drupal module.\n Active Lytics Account: Verify that you can access an active Lytics account. If you don't have one yet, you can claim your free developer account.\n Lytics Dev Tools Chrome Extension: Install our developer tools Chrome extension to streamline the development and installation process.\nGetting Started Checklist\n\nGetting started with Lytics is quick and easy! In just a few minutes, you'll be able to set up Lytics and start personalizing your website. We've focused this guide on the 3 essential steps to ensure a positive experience for you and your customers:\n\n 1. Install the Lytics tag on your site.\n 2. Ensuring site content and Lytics are syncing.\n 3. Create your first personalized message.\nDigging Deeper\n\nAfter completing the initial checklist outlined above, it's time to explore further avenues for enhancing and utilizing your profiles to their fullest potential. We've broken additional guides into two core focuses:\n\nBuilding Profiles\n\nHere, we'll gain a comprehensive understanding of all available out-of-the-box attributes. Discover how to tag your site and integrate other sources to create robust and comprehensive profiles. This section is divided into:\n\nDefault Attributes & Segments:\nProfile Attributes\nAudience Segments\nContent Collections\nSite Activity & Conversion Tagging:\nCapturing Website Activity (coming soon)\nCapture Website Conversion Activity (coming soon)\nUsing Profiles\n\nHere, we'll explore leveraging out-of-the-box personalization SDKs and APIs to deliver optimal user experiences. Discover how to harness Lytics' tools and integrations to create tailored experiences that resonate with your audience. This section covers:\n\nGuides & Inspiration\nSurface a lead capture form only to unknown visitors.\nSurface content recommendations based on interests.\nSurface a promotional message to high-momentum visitors. (coming soon)\nSync profiles & audiences to GA4 or meta. (coming soon)\nPersonalize your site based on behaviors and stored attributes. (coming soon)\nSDK Documentation\nWeb\nJavaScript SDK\nPersonalization SDK\nMobile\niOS SDK\nAndroid SDK\nReact Native SDK\n\nUpdated 5 months ago\n\nWHAT\u2019S NEXT\n1. Account Setup\nTABLE OF CONTENTS\nIntroduction\nBefore You Begin\nGetting Started Checklist\nDigging Deeper\nBuilding Profiles\nUsing Profiles",
        "",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nImporting External Experiences\nSuggest Edits\nImporting External Experiences\n\nYou can import existing marketing initiatives currently managed by your channel providers, which enables you to quickly gain insights about the campaigns you\u2019re already running. You can then take action by connecting these Experiences to your Lytics audiences.\n\nWhile you can monitor Experiences and activate audiences for them within Lytics, the management of external Experiences still happens inside your channel tools. If you want to stop, delete, or edit the execution of an Experience, you need to do so within the provider. If you choose to delete an Experience in Lytics that still has a matching campaign in your channel tool, it will show up on the import list and can be re-imported.\n\nCentralize cross-channel reporting\n\nLytics serves as a central hub to monitor your cross-channel marketing, making it easier to understand and improve the performance of your campaigns. For example, instead of logging into Facebook to check your ad campaign metrics and Iterable to check your email newsletter open rates, you can monitor the performance of both tools within your Lytics dashboard.\n\nTo start monitoring external Experiences on Lytics, follow the batch import steps.\n\nMonitor Experiences via batch import\n\nOn your Lytics dashboard, navigate to Experiences, and select Add Experiences > Import.\n\n\nSelect your provider\n\nThe Lytics Canvas connects out-of-the-box with select channel providers. Over time, more providers will be added to this list. If you want to use a provider that is not currently available, you can use the \"Generic Experience\" workflow to export Lytics audiences to any of your integrated providers.\n\nAuthorize\n\nNext, you will be prompted to select an authorization for your chosen provider. Existing authorizations will be shown. If you have multiple accounts within a particular channel such as Facebook, you will choose which account to import from. If you don\u2019t have an authorization, you will be prompted to add one before continuing. For Facebook, there is a second step for authorizing Ad Set ID after you\u2019ve selected your account ID.\n\nImport Experiences\n\nFinally, select the Experiences you want to import into Lytics (up to 10 at a time). The total number of Experiences you can bring into Lytics is only limited by the amount in your connected provider. However, to have meaningful and accurate reporting, it's recommended to only import Experiences that will add value to your use cases.\n\nCompleted experiences, such as an Ad Set that previously \"ended\", will not show on the list of importable Experiences. But if you have Ad Sets in Facebook that are saved as drafts, paused, or currently running, those will be available to import.\n\nConnect Experiences with Lytics audiences\n\nOnce you start monitoring external Experiences, you can activate them by adding a Lytics audience. This lets you enrich your existing campaigns with Lytics behavioral audiences, content affinities, and delivery optimization.\nAn example use case is to conserve Facebook ad spend by only targeting \u201cCurrently Engaged\u201d users who have a high affinity for the content of a particular campaign. Leveraging data science under the hood, Lytics audiences can bring immediate value to your existing marketing campaigns.\n\nIf you have already imported an external Experience, you can activate it from the Experience summary view by clicking Edit in the top right.\n\nIf you want to activate an Experience that you haven't imported yet, follow the single import steps below, which will guide you through the Experience editor.\n\nActivate Experiences via single import\n\nTo activate an Experience that you haven't imported yet, you will follow the workflow of creating a new Experience in Lytics. From the Experiences list view, click Add Experiences > New.\n\n\nSelect your chosen provider and then Import the campaign, ad set, or journey. This example will continue with importing a SendGrid campaign.\n\nNext you will select an Authorization for your chosen provider as described above.\n\nThen you will select the campaign or ad set to import. Note that here you can only import one Experience at a time. Click Import 1 Experience to continue.\n\n\nComplete the Experience Editor steps\n\nFinally, you will complete the following Experience Editor steps:\n\nTarget: configure your target audience for this Experience.\nConfigure provider: choose how the audience for your Experience will be exported to the third-party tool.\nConfigure delivery: option to automatically determine when to deliver messages to individual users on the third-party tool.\n\nUpon completion, your Experience will be ready to publish.\n\nUpdated over 1 year ago\n\nTABLE OF CONTENTS\nImporting External Experiences\nCentralize cross-channel reporting\nMonitor Experiences via batch import\nConnect Experiences with Lytics audiences\nActivate Experiences via single import",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nWhat is Vault?\nSuggest Edits\nIntroduction\n\nVault is a centralized hub for managing security and other administrative features in Lytics. It provides a single place for Lytics admins to access and change their key security features and user permissions, regardless of product. This means that the controls set in Vault apply to Conductor, Decision Engine, Cloud Connect, and any other products associated with your Lytics account.\n\nTake a quick tour of Lytics Vault.\n\nNavigating Vault\n\nVault is comprised of the following sections:\n\nAccount Usage - an overview of your general account quota usage related to event consumption.\nAccount Settings - view and change necessary administrative settings and details of your account. These are sectioned into the following categories.\nAccount Details\nJavaScript Tag\nLytics API\nContent\nSecurity\nSchema\nUsers - view a list of users accessing your Lytics account, change their details or permissions, remove a user, or invite a new user to the account.\nSecurity\nAccess Tokens - view and manage a list of Lytics API access tokens. New tokens can be provisioned with specific permissions.\nAuthorizations - create, edit, view, and delete authorizations within Lytics. Authorizations are credentials to third parties that enable the necessary scopes and permissions for data import and export jobs to run.\nAccount Setup\nJavaScript Tag - learn how to install the Lytics JavaScript tag and validate the installation.\nWho can access Vault?\n\nVault is focused primarily on account admins or those that have permission to administer settings, user access, etc. Based upon these permissions, your experience in Vault may vary, and all sections outlined above may not be available.\n\n\ud83d\udcd8\n\nIf you are unable to access a section please reach out to your account administrator to request those adjusted permissions.\n\nIn general, Admins will be able to access all areas. Data Managers, Campaign Managers, Experience Managers, and Goal Managers can access the Authorizations section to manage the credentials for their jobs and experiences. Finally, all Lytics users, regardless of role, should have access to view the JavaScript tag installation page under Account Setup, and they should be able to view their user profile where they can change details such as their Name, Email Address, Phone Number (for two-factor authentication) and change their password. Your user profile will be accessible under the main Lytics navigation under \"Manage My Profile.\"\n\nNotable Changes\n\nFor existing customers, slight changes will impact your day-to-day management activities.\n\nProduct Switcher\n\nVault will be available directly from the primary product switcher at the top left of your Lytics interface. This will be your primary access point for account management from now on.\n\nAccount Usage\n\nOur account usage data and quota meters received a much-needed facelift. The usage metrics act as the Vault \"dashboard\" for admin users.\n\nAccount Settings\n\nAccount setting sections are now accessible through the main navigation. These settings have received a minor facelift update. The form controls for multi-text fields have been slightly updated for a more standard user experience. In addition, users are now prompted to save or discard their changes when navigating away from these pages with unsaved settings.\n\nUsers\n\nThe user list is now sortable and filterable based on name, email, who invited them, and how long they've been a Lytics user. Each user has a page to view their details and roles. In Vault, in addition to assigning new roles, an administrator can edit any user's name and email address.\n\nThe user invite form is now on its page to improve the flow and experience of inviting users. Roles are now sorted into two categories - Admin or Custom Roles. An admin inherently has access to everything, while custom roles give a finer-grain definition of what the user can access. The details on each role are shown in a tooltip when you hover over the role name.\n\nAccess Tokens\n\nThe access tokens list is now sortable and filterable based on the name, description, the creator of the token, or when/if it has expired. You can now view additional details about the access tokens you've already created, such as the roles assigned, when it was made, and when it will expire. In addition, new access tokens can now be assigned as an admin or with any combination of more granular access roles.\n\nAuthorizations\n\nAuthorizations are now to be created and updated only in Vault. You will still be able to utilize the auths you create in Vault in other products, but for example, when creating a job in Conductor or Decision Engine, if you don't see an authorization you want to use for that job, you will be linked into the authorizations wizard within Vault to create the auth.\n\nJavaScript Tag Installation\n\nThis page remains unchanged, with simple styling changes and updated links to our documentation for troubleshooting.\n\nUpdated over 1 year ago\n\nTABLE OF CONTENTS\nIntroduction\nNavigating Vault\nWho can access Vault?\nNotable Changes\nProduct Switcher\nAccount Usage\nAccount Settings\nUsers\nAccess Tokens\nAuthorizations\nJavaScript Tag Installation",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nData Collection & Onboarding\nSuggest Edits\n\nWelcome to the Data Collection and Onboarding section, where we will explore the process of collecting and onboarding data into the Lytics platform. This section covers how to ensure data quality and accuracy, including how to integrate with various data sources and best practices for data management. Whether you are a developer, system administrator, or data analyst, this section will provide you with the essential knowledge needed to gain deeper insights into customer behavior and preferences within the Lytics platform.\n\nUpdated over 1 year ago",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nChrome Extension\nSuggest Edits\nLytics Dev Tools Chrome Extension\nOverview\n\nThe Lytics Dev Tools Chrome extension is a comprehensive tool designed to simplify validation, debugging, and exploration of Lytics' client-side capabilities as facilitated by our core and personalization SDKs. With this extension, users can easily:\n\nValidate that Lytics is connected to your site.\nDebug and validate that the data collected by Lytics, be it automatic or custom, is behaving as expected.\nExplore the current visitor's full profile to validate:\nIf the profile is available for personalization.\nThe events collected are having the expected impact on attributes and segment membership.\nExplore \"Experiences\" currently active on your site and could be surfaced to visitors.\nInstallation\nInstall Now\n\nInstall Extension: This extension is installed directly into your Chrome browser. To install, visit the primary extension page and follow the instructions.\n\nPin Extension to Nav Bar: Once installed, we recommend pining the extension to your top bar for easy access.\n\nGetting Started\n\nOnce the extension is installed, you can visit any website where Lytics is or should be installed and begin debugging.\n\nEnable the extension: To activate the Lytics Dev Tools extension, open the extension and toggle the slider at the top right of the extension to either \"Enable\" or \"Disable.\"\nVerify Installation: The extension will verify that the Lytics tag has been installed successfully before further debugging. This is confirmed by the \"Lytics JavaScript SDK Installed\" alert displayed on the dashboard of the extension.\nNavigation: Once enabled, you'll access three key sections: Debugger, Profile, and Personalization, each offering specific functionalities tailored to streamline your debugging and exploration process.\nDebugger\n\nThe debugger section of the extension serves multiple purposes. Firstly, as we did previously, it allows you to validate installation, ensuring that the Lytics tag is successfully implemented. Additionally, it provides access to the full active configuration of the JavaScript SDK through the configuration tab. Moreover, it includes a live event debugger that monitors and displays comprehensive details of any calls made to the Lytics APIs in real time. This feature is particularly valuable for reviewing data sent via jstag.send calls, offering insights into the interaction between your website and Lytics APIs.\n\nProfile\n\nIn the \"Profile\" section of the extension, users gain a live look at the current visitor's profile, providing valuable insights into their browsing behavior and demographic information. This section offers a snapshot of key details, such as the visitor's unique identifier and behavioral scores, allowing for a quick assessment of their engagement level. Additionally, users can access a detailed view of all information available to the browser, including demographic data, past interactions, and any custom attributes stored in the visitor's profile. This comprehensive overview enables users to tailor their strategies and personalize experiences based on individual visitors' specific characteristics and preferences.\n\nPersonalization\n\nIn the \"Personalization\" section of the extension, users gain access to a log of all active Lytics experiences and campaigns, along with their associated details. This feature proves invaluable when debugging the configuration of experiences or ensuring that overrides have produced the expected results. Users can easily track the performance and behavior of each personalized element on their website by providing a comprehensive overview of active campaigns and experiences, including their parameters and settings. This functionality streamlines debugging and empowers users to fine-tune their personalization strategies for optimal effectiveness.\n\nUpdated 11 months ago\n\nTABLE OF CONTENTS\nLytics Dev Tools Chrome Extension\nOverview\nInstallation\nGetting Started\nDebugger\nProfile\nPersonalization",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nAudiences\nSuggest Edits\nIntroduction\n\nAudiences are central to creating personalized marketing using Lytics. It is how groups of users get targeted and defines how cross-channel user data is used in campaigns. Lytics offers audiences as a tool that operates on user fields as well as other audiences. This provides the ability to craft audiences that target one user, every user, or any number in between.\n\nLytics audiences are automatically kept up to date. As time passes, an audience that once targeted 500 users may grow to target 10,000. This is great if your audience is targeting engaged users who have shown interest in your upcoming product, but not so great if your audience targets users with outstanding support tickets. Lytics makes it easy to find these insights so your campaign strategies can be adjusted accordingly.\n\nAudiences can be used in Lytics Experiences, third-party ad campaigns, and tools like Google BigQuery for advanced reporting. When all of that isn\u2019t enough, the Lytics APIs provide deep integrations with the Lytics platform.\n\n\ud83d\udcd8\n\nAccess the audience builder from Lytics navigation menu by expanding the Audiences tab and clicking Audiences. Once on the Audiences page, click on the + Create New Audience button.\n\nCreating Audiences\n\nBuilding new audiences allows you to target specific groups of your users. Why send the same email to your entire audience? Instead, send emails to distinct audiences with content that you know those users enjoy. Lytics gives you the ability to create audiences based on cross channel user data, behaviors and interests.\n\nAudiences can be built using existing audiences, user content affinity, your personalization campaigns, or by custom rules utilizing any user fields in your Lytics account. Create rulesets by combining multiple audiences and boolean logic to target users with as much or as little granularity as you require.\n\nLytics updates your audiences automatically. Once an audience is built, users will join or leave whenever they match (or fail to match) the rules you have set. Export your audiences, often in real-time, to your integrated marketing tools giving you the power of Lytics however you engage with your users.\n\n\ud83d\udcd8\n\nAccess the audience builder from Lytics navigation menu by expanding the Audiences tab and clicking Audiences. Once on the Audiences page, click on the + Create New Audience button.\n\nBuilding Audiences: Existing Audiences\n\nThe Existing Audiences tab in the audience builder contains a list of all existing audiences and their size categorized by audience type such as characteristic. Characteristics are pre-built audiences pertaining to single user attributes such as location or behavior.\n\nKeep in mind that the audience sizes you see in this list are not real-time. They are updated every few hours for performance reasons. When creating or editing an audience, however, you will always see real-time numbers so that you can make precise decisions based on accurate data.\n\nYou can use the Filter by Audience Type drop-down to filter the list or use the Search box to quickly find an audience by name.\n\nSelect an audience or characteristic to add it to the definition of the audience you are building or editing.\n\nBuilding Audiences: Content Affinity\n\nThe Content Affinity tab in the audience builder is used to add Affinity-based rules to your audience. Since Affinities are a group of related Topics, this allows you to effectively target users interested in several subjects or products.\n\nIn the example above, we created a \"Lavender\" Affinity for an online beauty store, which includes 6 Topics such as \"lavender\", \"lavender lotion\", and \"lavender luxury.\" Instead of creating an audience with 6 rules (one per Topic), we just select our Lavender Affinity that contains those 6 Topics.\n\nOnce you choose an Affinity for your audience, you will be able to select the range of affinity values. The histogram displays the distribution of user interest for the selected Affinity and is helpful to build an audience that has the desired size. By default, users with Any Affinity are selected. You can choose a built-in range by clicking any of the Affinity buttons or, to set a custom range, use the slider to refine the range of users to target. As you change the range, the total number of users that fit the range is shown in the right column.\n\nAs soon as you create an Affinity, Lytics start scoring users against it as they interact with your content or products. For active users, you can expect this number to start populating within a few days. For inactive users, it can take up to 2 weeks for them to be scored against an Affinity. So it's recommended that you create Affinities in advance of the audience needed for an Affinity-based campaign.\n\nTopic-Based Audiences\n\nUnder the Custom Rule tab in the Audience Builder, you can find the existing audiences based on Topics in your content taxonomy. If you want to create new audiences based on a single Topic, you can do so as follows:\n\nSearch for \"Topics\" in the Custom Rule tab of the Audience Builder\nSelect the Topic of interest.\nSet the threshold according to the desired users' level of interest in that Topic.\n\nOnce you hit \"Add Condition\", the custom rule for a Topic will look like this:\n\n\nAlthough Topics are still supported in the Audience Builder, we strongly recommend you use Affinities to build audiences for a more efficient workflow and more effective targeting.\n\nInferred Content Affinity\n\nSome users may find a field named Inferred Content Affinity when trying to build affinity or topic based audiences. This field contains topics that a user could be interested in even though they may not have interacted with the topics directly. To learn more about how these inferrences are made, you can read about our content taxonomy process. Note that topic affinity already includes this information, but this field contains only inferred affinity. We would also recommend not using this field to build audiences as it is not a wholistic view of a user's affinity.\n\nBuilding Audiences: Custom Rules\n\nThe Custom Rules tab in the audience builder is used to add any rule based on a user field to the audience being built or edited.\n\nCustom rules allow for ultimate access of all data that is aggregated in user fields. Custom rules are simple statements that can be combined to create very precise audiences. Using them correctly requires a thorough understanding of the data being used, but there is no replacement for this level of segmentation.\n\n\ud83d\udea7\n\nAudiences Field Value Limits\n\nAudiences can have a maximum of 1,000 field values. \"Values\" in this context apply to a single evaluation against a user field.\n\nWhen exceeded, the error message \"Audience too large! Remove values to save this audience\" will be displayed. For example, the following scenarios would trigger the alert:\n\nAn audience with a single rule that supports multiple values ( e.g. contains one of) with 1001+ values.\nAn audience with 1001+ user field rules each with a single evaluation (e.g. exists).\nAny variation in between such as two rules each containing over 500 evaluations, etc.\n\nThe user field list displays all the fields in your account. The Source drop-down can be used to filter on the data stream the user field came from. The Includes drop-down can be used to filter user fields on the percentage of users that have a field. Use the Search input to search for a user field by name.\n\nGenerally, to add a Custom Rule to an audience definition:\n\nChoose a User Field\nChoose an operator (e.g., text contains, value is less than)\nChoose a value\n\nThere are variations to this general procedure based on the type of the user field and the operator chosen.\n\nUser Field Types\n\nHow you work with user fields in the Custom Rules tab will depend on way the field is mapped in on the user profile.\n\nNumeric User Fields\nText User Fields\nDate User Fields\nSet User Fields\nMap/Nested User Fields\nOut of the Box Custom Rules\n\nSome user fields are mapped by default and populated through the use of the Lytics Javascript tag and Experiences. These out of the box fields can be incredibly useful when building audiences using Custom Rules.\n\nLytics Javascript Tag\nExperiences\nBuilding Audiences: Multiple Rules and Rule Sets\n\nAll audience definitions are comprised of rules based on existing audiences, content affinity, campaigns, and custom rules. You may have noticed that after you add your first rule in the audience builder you have the option to Add New Rule or Add New Rule Set. Adding new rules and rulesets, and setting the conditions for their interaction, gives you the ability to refine which users your audiences target.\n\nWorking With Multiple Rules\n\nThis audience has a single rule: included all users with the characteristic Behavior: Perusers. This will select all users with an intensity score of 20 or less but with a frequency score of 20 or greater. These are users who interact often but not on a deep level. You can refine this rule further by clicking Add New Rule.\n\nThis audience now also includes a rule for users who have a low content affinity for Technology content. Now that it has been added, there are three numbers of interest. The first number, 87,650, is the number of users with the characteristic Behavior: Perusers. The second number, 466,730, is the number of users who have low affinity for Technology content. The third number, 11,514, are the users who meet both of these conditions.\n\nClicking the And/Or toggle, will switch the way the two rules intereact. The audience now includes all 542,921 users who either have the characteristic Behavior: Perusers or have low afffinity for Technology content.\n\nExcluding Rules\n\nIn addition to the And/Or toggle, there is the Included/Excluded toggle. This toggle inverts a condition.\n\nThis audience contains a rule that targets the 159,084 users with the characteristic Browser / OS: Mobile or users who have used a mobile devices.\n\nClicking the Include/Exclude toggle the audience will now target the 705,166 users who do not have the characteristic Browser / OS: Mobile.\n\nThe sum of the two numbers will be the total audience size that meets those rule(s).\n\nThis is especially apparent when using a rule such as Created Timestamp Exists. Every user in Lytics will have a timestamp of when they were created. Therefore, the number of users who do not meet the condition of having a created timestamp will always be 0.\n\nWorking With Rule Sets\n\nWhen building elaborate audiences, having a single set of rules that are evaluated using either an AND or an OR is not enough. A ruleset is a set of rules that itself can be used as a rule for an even larger set of rules that is again evaluated using an AND or an OR.\n\nThis audience has two rulesets. The first ruleset has one rule: all users who have the characteristic Email Capture Status: Known Email. The second ruleset has two rules: all users have some affinity for Marketing OR Advertising content. Combining these two rulesets with the AND operator produces an audience that targets users with an email address that have some interest in either marketing or advertising. This audience could be used to email a special newsletter or promotion about a product or trending topic that is related to marketing or advertising.\n\nExcluding Rule Sets\n\nRulesets, just like rules, can be excluded. Clicking the Include/Exclude toogle on the second ruleset will result in the audience targeting all users with an email address who do not have some interest in either marketing or advertising. It could be used to send a different email to the remainder of your users with email addresses.\n\nBuilding Audiences: Configuration Options\n\nConfigure your Lytics audiences with the following options to execute your use cases.\n\nEnabling API Access\n\nBy default, newly created Lytics audiences are not enabled for API access and thus will not be available to public APIs. Enabling API access is necessary when integrating with other client-side tags/ pixels, Pathfora, or the Lytics personalization API endpoints.\n\nSelect the API Accessible checkbox to enable access.\n\nGenerate Insights\n\nLytics Insights provide visibility into the performance of your audiences and campaigns as users interact with your brand. Insights are generated out-of-the-box for Lytics Behavioral and Engagement audiences. You can also request up to 10 custom audiences to be prioritized as candidates for generating Insights.\n\nWhen creating a new audience or editing an existing one, select the Generate Insights checkbox to make that audience a candidate for Insights (shown in the screenshot above).\n\nLytics prioritizes Insights based on the statistical significance of the data and the most frequently used audiences for your account. Therefore, even if you select the \"Generate Insights\" option for a particular audience, it is not guaranteed to show up in your Insights drawer.\n\nManaging Audiences\n\nManaging audiences becomes important as an account develops and your user base grows. Audiences are critical to both targeted campaigns as well as discovering insights about your users.\n\nThe Audiences section of Lytics houses all of your account\u2019s audiences. You can view, create, and manage audiences from this section.\n\nAudiences are listed alphabetically by default. Click any column header to change the sort property. Common sort options are Last Modified and Size (the number of users in the audience).\n\nNaming Your Audiences\n\nFor better targeting, Lytics encourages the creation of many audiences. They\u2019re easy and free to make. When working with a large library of audiences, naming conventions become important.\n\nName your audiences by purpose and include any categorization in the name. Giving them a common prefix (e.g., Holiday Promo: Tech lovers, Holiday Promo: Loyal Shoppers) will help you to keep everything related next to each other in the user interface.\n\nAudiences appear throughout the Lytics platform. Make sure your naming convention works in any context to avoid confusion.\n\nFinding Audiences\n\nThe easiest way to find an audience is to use the Search box. As you enter your search term the audience list will display any matching audiences.\n\n\ud83d\udcd8\n\nFuzzy matching is used to search for audiences so the results will not be an exact match to the search term.\n\nEditing Audiences\n\nOccasionally you will want to return to previously created audiences and update the logic used to create the segment.\n\nFind the audience you would like to edit\nClick on the audience to navigate to the overview of the audience\nClick the Edit button to enter the audience builder\nMake your desired changes within the audience builder\nClick Save\n\ud83d\udea7\n\nWhen editing audiences that are in an active triggered workflow, it is recommended that you reconfigure the job after the audience changes have been made.\n\nDuplicating Audiences\nFind the audience you would like to duplicate\nClick on the audience to navigate to the overview of the audience\nClick the ... button to enter the audience builder. The audience builder will be pre-populated with all of the criteria of the audience you duplicated.\nMake any desired changes\nClick Save\n\ud83d\udea7\n\nAudiences created by duplication will retain the name and slug of the duplicated audience, but with have \"duplicate\" appended at the end. It is recommended that you update both the name and manually input a unique slug name for clarity sake.\n\nDeleting Audiences\n\nDeleting audiences is an integral part to keeping a tidy workspace in Lytics. Audiences can be deleted in one of two ways: individually or in batch.\n\nNote: Deleting an audience does not delete the users associated with that audience.\n\nDeleting an Individual Audience\nFrom the Browse Audiences list, select the name of the audience you wish to delete.\n\nFrom the options menu, select Delete.\n\nClick Delete.\n\nClick Accept to confirm.\n\nDeleting Multiple Audiences\nFrom the Browse Audiences list, select the checkboxes of the audiences you wish to delete.\nNOTE: Audience selection persists when changing pages allowing you to select audiences from multiple pages.\nClick Delete Selected Audiences.\n\nClick Delete.\nNOTE: Lytics will check to see if the audiences you selected are safe to delete. Audiences are considered safe if they are not used in an ongoing export, campaign, or another audience's definition. You will be prompted to delete dependant audiences, click Yes to delete the selected audience and any dependants listed or Skip cancel deletion of the selected audience.\n\nClick Accept to confirm.\n\nMaintaining Audiences\n\ud83d\udcd8\n\nFrom the Audiences page, click any audience in the list to view all of its details.\n\nAn audience's landing page contains a lot of valuable information about your users and the audience itself. At the top of the audience page, you will find the following information...\n\nAudience name\nID\nSlug\nCreated by\nCreated on date\nLast updated date\nAPI access configuration\n\nBelow this information you will find multiple tabs where you can access additional information about the audience.\n\nAudience Summary\n\nThe audience summary page is designed to provide insights into the current state of an audience as well as how the audience has changed over time. When first navigating to an audience landing page, this tab will be selected by default.\n\nThe Summary tab includes the following sections:\n\nGeneral Metrics\nThe total number of users in this audience\nThe total percentage of this audience as compared to the total number of profiles in your account\nThe percentage change in audience size over the last 7 and last 30 days\nActivity trend (graph of audience size over the defined date range)\nActivity Graph\n\nAfter creating a new audience, the audience size graph will begin to populate within a day. Depending on the date range you specify, the activity graph can be viewed using following intervals:\n\nHourly\nDaily\nWeekly\nMonthly\n\nAudience Size Trends\n\n\ud83d\udcd8\n\nThe statistics for audience size start from the creation date of the audience.\n\nKeep in mind that audiences are real-time. This makes them quite different from the user lists in other marketing tools which are frozen in time and only change when you update the list. Audiences are filters applied to the total audience and will change whenever a user's behavior changes in a way that will add or remove them from the audience definition. In this way, tracking audience size over time is a powerful reporting tool.\n\nFor example, you create an audience that contains users who have visited the international news section of your website. Upon creation the audience contains 120,000 users. You then run a marketing campaign promoting your international news section. After one week, the audience has grown from 120,000 to 400,000 users with most of the growth occurring in the 24 hours after the campaign was launched. This is clear evidence that the campaign was effective, and most effective in the first 24 hours, at driving users to your international news section.s\n\nAudience Details\n\nSelect the Details tab to view the following sections:\n\nExports: This section outlines what exports (if any) are using this audience\nRecent Users: This section allows you to view a sample of users in this audience who have been active recently (by clicking the View button)\nAudience Characteristics\n\nSelect the Characteristics tab to view the characteristics of users in this audience. Characteristics include:\n\nBehavior\nWeb Activity\nContent Affinity\nCampaign Referral Interactions\nAudience Logs\n\nSelect the Logs tab to view recent events relating to this audience. Activity such as creation, updating and syncing can all be found here.\n\nActivating Audiences\nActivating Audiences in Experiences\n\nCreating a Lytics Experience is one of the easiest and most common ways to activate Lytics Audiences. Learn more about Lytics Experiences to find out how you can engage your audiences on site.\n\nAccessing Audiences Using the Public Entity API\n\nThe Lytics Experience Editor is a great way to get started with website personalization, but through the use of the Lytics API, there is much more that can be done.\n\nThe Lytics Experience editor is powered by the Pathfora SDK which is in turn powered by the Lytics Entity API. The Lytics Entity API is how to find out what audiences the current user belongs to as well as what traits the user has. Using the Lytics Entity API it's possible to create completely custom personalized experiences on your site.\n\n\ud83d\udcd8\n\nPersonalizing a site using the Public Entity API requires making code changes to the host website.\n\nTo read more about the Entity API, check out the API documentation.\n\nExporting Audiences to Other Tools\n\nA core benefit of Lytics is exporting your smart audiences to the other tools in your marketing tech stack. That way, the campaigns run on your channels are enriched with the behavioral data and content affinities powered by data science on Lytics.\n\nFrom the audience overview page, click the Export button to create a new export job . The Export menu lists built in exports such as Email CSV and any out of the box Integrations. Specific instructions for exporting can be found in the Integrations section (under the desired tool) of our documentation.\n\nUpdated 4 months ago\n\nTABLE OF CONTENTS\nIntroduction\nCreating Audiences\nBuilding Audiences: Existing Audiences\nBuilding Audiences: Content Affinity\nBuilding Audiences: Custom Rules\nBuilding Audiences: Multiple Rules and Rule Sets\nBuilding Audiences: Configuration Options\nManaging Audiences\nNaming Your Audiences\nFinding Audiences\nEditing Audiences\nDuplicating Audiences\nDeleting Audiences\nMaintaining Audiences\nAudience Summary\nAudience Details\nAudience Characteristics\nAudience Logs\nActivating Audiences\nActivating Audiences in Experiences\nAccessing Audiences Using the Public Entity API\nExporting Audiences to Other Tools",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nConsent & Privacy\n\nFor more information on Lytics' security and privacy program, please visit Lytics' trust center at https://trust.lytics.com/\n\nSuggest Edits\nIntroduction\n\nAs businesses continue to collect and utilize customer data to power their marketing efforts, obtaining, managing, and enforcing proper consent is crucial. This is important for complying with various privacy laws and regulations and maintaining trust with customers. This document will explore the importance of granularity in a consent strategy and how Lytics can assist in the consent management process. We will cover best practices to consider and provide guidance on where to get started. The following four steps will be outlined:\n\nData Collection: How to collect consent adequately from disparate sources consistently.\nProfile Materialization: How to flexibly surface the state of consent to individual user profiles.\nSegmentation: How can the required consent rules be enforced when driving downstream campaigns?\nMaintenance: How to ensure that your consent strategy scales and flexes to your ongoing market conditions without fail?\n\nBy following these steps, businesses can establish a solid and effective consent management strategy that respects customer privacy and builds trust.\n\nData Collection\n\nTo maintain compliance with data privacy regulations, businesses need a clear and flexible path for collecting consent data. Lytics offers a range of SDKs and data collection APIs to support this effort. When approaching consent data collection, it is essential to consider the following:\n\nThe source of the data and any inherent limitations. Can it be real-time, or does it need to be batch?\nThe granularity of the consent. Is this a simple boolean, on or off, or is this something more granular, like opting into a particular communication method?\nAdditional context related to the consent that may be important for your needs. Are there specific forms that are being consented to or a unique source?\n\nTo address these considerations, Lytics has developed a suggested schema to get you started. Of course, this can always be customized via Conductor to meet your specific needs.\n\nData Structure\n\nEffective consent management requires a comprehensive set of properties that can be deployed to ensure compliance and maintain customer trust. These properties should cover various aspects of data collection, storage, and usage and provide businesses with the tools to manage consent-related data effectively.\n\nSchema Property\tDescription\nEvent Name\tUsing a standardized event name for consent collection provides crucial context that can be utilized through if/else rules for individual mappings. By leveraging different event names, such as \"consent-type-1\" versus \"consent-type-2,\" it becomes easier to differentiate between multiple types of consent on a single profile as events stream in and is subsequently mapped. This approach enables businesses to maintain a more granular and organized record of consent-related data.\nConsented\tA clear and concise confirmation, or lack thereof, reduces the possibility of accidentally mapping consent when consent has been denied or not yet collected.\nLocation\tAn optional parameter to build context around a user's location that may become important. Are they consenting to something location specific?\nDate\tAn optional parameter to build context around when consent was most recently given. Date may be important if consent is valid for a specified length of time and may help support efforts to gather consent in the event that the consent timeframe has lapsed.\nDocuments\tThe \"documents\" array within the default schema represents additional context that can be used to specify the source of consent. For instance, this field can indicate whether the consent was given via a global site-wide agreement or a specific call to action during a purchase. While the data manager determines the values and format of this field, including this information helps ensure that critical context is not lost during execution.\nExamples\n\ud83d\udcd8\n\nWe are here to help!\n\nConsent is not a one size fits all scenario. Each customers data and business goals are unique. Before deploying any of the following examples it is always recommended to consult with your Technical Account Manager or a Solutions Architect.\n\nLytics deploys a flexible data model, and though the following is not representative of every method of collection, we've highlighted two working examples leveraging our available SDKs as a baseline:\n\nCollecting Consent via the Web: Lytics JavaScriptTag\nJavaScript\njstag.send({\n  \"event\": \"form-submit\",\t\n  \"consent\": {\n    \"purpose\": \"global\",\n    \"location\": \"Portland, OR\",\n    \"documents\": [\"generic-submit-form\"],\n    \"consented\": true\n  }\n})\n\nCollecting Consent via Mobile: Lytics iOS SDK\nLytics.shared.consent(\n  name: \"consent-1\",\n  identifiers: AnyCodable([\n    \"userid\": \"my-fake-userid-1234\"\n  ]),\n  attributes: AnyCodable([\n    \"firstName\": \"Kevin\",\n    \"lastName\": \"McCalister\"\n  ]),\n  consent: AnyCodable([\n    \"documents\": [\n      \"terms_jan_2023\",\n      \"sharing_policy_jan_2023\"\n    ],\n    \"location\": \"Chicago, IL\",\n    \"consented\": true\n  ])\n)\n\n\ud83d\udcd8\n\nWhen using Lytics SDKs, you do not have to include date within the data passed in the consent event. The timestamp of the consent event can be used to create the field and mapping for the date of consent.\n\nProfile Materialization\n\nBuilding from the collection strategy outlined in Collecting Consent Data, we must determine how to materialize the consent-related data we've just collected to user profiles for segmentation. This can be achieved in a variety of ways. Regardless of the approach, however, it is essential to consider the level of granularity of the consent.\n\nGranularity\n\nWhen collecting consent from users, obtaining granular consent for each specific use of a customer's personal data is an important consideration. Granular consent means that customers are provided with a clear understanding of the exact purposes for which their personal data will be used and can choose to consent or withhold consent for each specific use case.\n\nFor example, a business may seek granular consent from a customer to collect their email address and use it to send them promotional emails but not to share the email address with third-party partners for advertising purposes. This approach allows customers to make more informed decisions about how their personal data is used and provides greater control over their privacy.\n\nObtaining granular consent not only requires careful planning and clear communication with customers about the specific use cases for their personal data but also a rock-solid means for enforcing an individual's consent wishes across all future touchpoints.\n\nSchema\n\nEnforcement begins by ensuring consent wishes are accurately materialized on each user's profile. This is done by first defining consent-related attributes to be used in segmentation.\n\nField Definition\n\nWhen defining profile fields, there are two primary considerations. What type of field should be used, such as a string or a map and how do you want to handle data merging when profiles are stitched?\n\nField Type\nType\tBenefits\tDrawbacks\nMap (Recommended)\tA map is the most straightforward approach, allowing you to add additional context with minimum overhead. You create one field and map multiple data points to that field.\tDuring segmentation, maps may be slightly more challenging to navigate. Because the keys that are nested under the map are not part of the top-level schema, you'll need to drill into the field to build segments vs. searching at the top level.\nString\tWhen building segments, because all consent fields would be at the top level of the schema, it may be easier to find and select the contextual data.\tWhen using a string, you can only store a single value. This represents a heavier lift during the initial setup, as you'll need to create many fields to facilitate a single consent use case.\nMerge Operator\n\nType String\nBecause consent is something that may change for a consumer over time, the values must represent the most recent data. As such, the latest merge operator is always recommended. This means that as additional data is stitched to a user profile, the most recent events that are mapped to a field will win.\n\nExample String\n\nType Map\nFor a more complex or granular consent strategy, the map field type can be very helpful in accurately managing consent. As such, the merge merge operator is always recommended when trying to keep the key-value pairs up to date to the most recent consent state. This means that as additional data is stitched to a user profile, the most recent events that are mapped to a key-value pair will win.\n\nExample Map\n\nMapping Definition\n\nMappings are then leveraged to determine how data from any number of streams map to the defined field. In the example below, we take a simple approach to map the boolean value of true or false to the consent field if the consent is related to marketing-consent.\n\nThough this is one elementary example, the same practice can be replicated to ensure a consent attribute has the status, timestamp, and context of any important policies or sources for segmentation.\n\nExample\n\nSegmentation\n\nBusinesses can better understand their customers and create more effective campaigns by breaking down audiences into smaller, more targeted segments. Here we will discuss best practices for constructing \"building block\" audiences related to consent that can then be leveraged in campaign audiences. This progressive approach ensures consent rules are enforced while minimizing ongoing maintenance overhead.\n\nBuilding Blocks\n\nBuilding Block Audiences provide the perfect means to maintain consent-related rules and extend the ruleset to all campaign audiences. The number and these audiences will depend on the granularity of your consent strategy. Still, as a basic example, we recommend creating both a \"has consented\" and \"has not consented\" counterpart for each level of consent.\n\nThis can be done simply through the GUI for our powerful segmentation engine, as shown below:\n\nCampaign Segments\n\nOnce you have the necessary building blocks constructed, you can quickly integrate that rule set into your campaign audiences again through the GUI for our segmentation engine. The example below outlines a use case where you want to target high-value users who have opted in.\n\nGlobal Job Segment Filter (Consent)\n\nConfigure a segment to prevent profiles from being sent to downstream destinations based on consent status or another relevant filter. This ensures that individuals who should be excluded will not be activated downstream.\n\nAPI documentation can be found here.\n\nUpdated 5 months ago\n\nTABLE OF CONTENTS\nIntroduction\nData Collection\nData Structure\nExamples\nProfile Materialization\nGranularity\nSchema\nSegmentation\nBuilding Blocks\nCampaign Segments\nGlobal Job Segment Filter (Consent)",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nIdentity Resolution\nSuggest Edits\nIntroduction\n\nIdentity Resolution is a crucial component of customer communication. Establishing and maintaining a well-defined identity resolution strategy is essential, but it can be overwhelming to get started. Several challenges may arise, such as limited and outdated customer data, data silos, technical difficulties, in-flexibility, consistency, compliance, and maintenance.\n\nLytics offers a solution that powers brands' identity resolution strategies by aggregating data from disparate sources, defining a standardized customer schema, unifying disparate sources, surfacing individual consumer profiles, enabling better understanding, segmentation, and activation across channel tools, ensuring control, visibility, flexibility, security, privacy, and compliance, and maximizing match rates and accuracy across activation channels.\n\nThe goal is to construct complex consumer profiles and maintain accurate and compliant user profiles as your brand evolves. Defining the relationship between consumer identifiers is a necessary first step in surfacing unified user profiles that enable your brand to create the best consumer engagements while meeting evolving compliance requirements. To achieve this, we've broken our approach into three components:\n\nOur Approach\nDefine: Relationships Between Identifiers\n\nProfile definition is the first step in surfacing unified user profiles that enable your brand to create the best consumer engagements while meeting evolving compliance requirements.\n\nTo build a strong identity resolution strategy, three key questions must be answered:\n\nHow important is accuracy?\nHow do you define and manage the strength of each identifier to ensure profiles are materialized properly?\nHow can the materialized profiles be analyzed and delivered to other tools?\nConstruct: Complex Profiles\n\nData is messy. Lytics' Profile Materialization provides a necessary cleansing & polishing layer atop the defined resolution strategy. This ensures what is surfaced is accurate and accessible and enables your team to accelerate impact.\n\nAnswering the following two key questions guarantee profiles represent the ideal marketable entity:\n\nWhat attributes should be surfaced on profiles for segmentation, and does the data need to be normalized at all?\nWhat rules should be used to maintain profile integrity?\nMaintain: Accurate & Compliant Profiles\n\nOver time profiles will bloat, attributes will become stale, and use cases become more complex. If unmanaged, a quality identity strategy will begin breaking down the moment it is implemented.\n\nLytics prevents your identity strategy from breakdown through a set of powerful tools focused on ensuring the following profile health-related questions have concrete answers:\n\nHow can you validate the health of your ID resolution strategy over time?\nWhat is the process for managing consent?\nHow long is a profile relevant if it no longer has a means of being updated?\nWhat is the life expectancy of expirable IDs, such as browser cookies?\nAdvanced Concepts\nThe Lytics Identity Graph\n\nBehind each Lytics profile is an identity graph. This graph represents connections between pieces of data observed across multiple sources or even within a single source.\n\nTo create (or update) profiles from a data stream, the stream must contain one or more identity keys that identify distinct users with which to associate the data. When data is observed for a given identity key, it stores the relevant profile metadata in an object called an identity fragment. When there's evidence on a data stream that two keys or fragments should be connected\n\nWhen evidence on a data stream shows that two keys or fragments identify the same real-world entity, those fragments become connected in the same identity graph. Some identity graphs are significant and represent complex relationships in the data. In contrast, other identity graphs are small and describe a small interaction, like an anonymous cookie from a single-visit, incognito browser.\n\nGraph Mechanics\n\nAs you learned from the identity resolution overview, a profile comprises one or more identity fragments. Many profiles start as singletons \u2013 new data is observed on a data stream. That event's identifier keys create any necessary identity fragments and store the event's associated data on that fragment.\n\nHowever, we're not satisfied with several singletons \u2013 our objective is to stitch data sources together by linking the appropriate underlying fragments. Stitching occurs when we observe two identifiers in a single event. A common stitching event is a newsletter signup, where the email address from the newsletter form is linked to the cookie from their web activity and creates a link between activity from the browser on the device and any activity associated with the email address, which could eventually encompass purchases, support tickets, CRM data, etc.\n\nIn graph terms, stitching creates an edge between two nodes representing two identity fragments. If we wanted to retrieve a profile associated with an email address, we would retrieve all of the fragments with edges or connections to other fragments. From there, we'd want to find all of the connections to those other fragments, and so on, until there are no more connections to follow. Following one fragment to another is called traversal. The full set of fragments that are found to have connections to the initial fragment are called neighbors.\n\nIn Lytics, default graph limits cap the number of traversals allowed for an individual profile at 50 and cap the number of neighbors allowed at 50. Changing these values can create different sets of user profiles over the same data set and should not be adjusted lightly. To change these values for your data, please contact Lytics support.\n\nIdentity Keys\n\nAs we traverse identity graphs, we'll quickly find that identity fragments and their corresponding identity keys are not all created equal. An identifier's strength must contribute proportionally to its influence on identity resolution.\n\nFor example, you have email addresses and cookies as identity keys. Generally, a user identified with an email address can have multiple cookie values (from different devices, browsers, periods, etc.). Imagine hosting a promotional, in-person event and having multiple tablets collecting participants' email addresses. Depending on how those email addresses are collected (most likely through an online form), you'll likely have one cookie associated with many email addresses.\n\nField Types\n\nField types for Identity Keys can be either a string or a string set. String sets are a common field type for cookies since one profile is expected to have many cookie values over time. Email addresses are not so cut-and-dry. Some organizations will constrain profiles to have one email address, while others will allow profiles to have multiple (personal, work, etc.). In our example of email collection via physical tablet, if the email address is a single-valued string type, we won't end up with an over-merged hairball.\n\nUsing an identity key that allows for a set of values is usually a good idea to have a sensical capacity cap on the field type. A set of cookies, for example, might have a capacity limit of 50 values. On the other hand, a set of emails might have a capacity limit of 5 values.\n\nIdentity Key Rankings\n\nThe ranking of your identity keys should reflect their reliability and their relative importance in the strategy. In the event of a conflict in stitching and merging, higher-ranked identity keys will win. Typically, most Lytics users configure email identifiers to be ranked higher than cookie identifiers.\n\nImagine a scenario where email A is connected to cookies X and Y, while email B is connected to cookie Z. If new data is observed that connects email B with cookie Y, we have a conflict, meaning that a resulting stitching between the two fragments would yield a profile with two different email fragments and violates our merge rules.\n\nThe ranks of identity keys would dictate that, for the new event, the email address it contains is of a higher priority than the cookie value that it contains and would consequently update the profile for Email A and not the profile for Email B.\n\nGraph Compaction\n\nWe mentioned that a critical tenet of bulletproof identity resolution is that profile complexity remains stable over time. That is, we need a way to ensure that a relatively greedy algorithm doesn't result in profiles becoming more fragile and susceptible to conflicts.\n\nIn Lytics, that is accomplished via graph compaction, a process by which data from multiple fragments is combined into a single fragment. Doing so allows well-established relationships in the graph to be solidified while making room for new relationships within the profile. It functions more as a type of profile housekeeping to keep profile fragments tidy.\n\nCompaction in identity graphs can take on a few forms.\n\nRank-based Compaction\n\nLet's go back to our example with Email A and Email B. The point of identity resolution within a customer data platform is to enable long and rich relationships with customers. The longer that Email A and Email B represent profiles in the platform, the more cookies with which they'll eventually become connected. Each identity key's ranking allows an identity graph to compact by size or time.\n\nSize compaction: Identity key sets can be compacted after they reach a configurable size. If size compaction is enabled to compact a set after 30 values, then the data from the oldest 30 fragments would be combined into a single fragment and would be further compacted with new data as new values are observed.\nTime compaction: These sets would be compacted after a configurable time threshold. If time compaction is enabled to compact a set after 14 days, then the data from fragments older than 14 days would be compacted into a single fragment.\n\nUpdated over 1 year ago\n\nTABLE OF CONTENTS\nIntroduction\nOur Approach\nDefine: Relationships Between Identifiers\nConstruct: Complex Profiles\nMaintain: Accurate & Compliant Profiles\nAdvanced Concepts\nThe Lytics Identity Graph\nIdentity Keys\nGraph Compaction",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nIDP-initiated SSO (legacy)\nSuggest Edits\nIDP-initiated SSO (Legacy)\n\ud83d\udea7\n\nThis document describes a legacy implementation that Lytics is actively deprecating. If you're looking for details on the current SSO implementation, check out the new documentation.\n\nLytics supports enterprise Single Sign-On (SSO) by using Auth0 as a service provider using SAML protocol. Lytics integrates with Identity Providers (IdPs) in such a way that the IdP initiates SSO. That is, when your end user logs in to their IdP, they will use a global portal for your organization. Users can then click a link or button that will log them into Lytics seamlessly.\n\nBehind the scenes, the IdP will be contacting the Lytics Auth0 service provider to verify the user and redirect them to a logged-in instance of the Lytics app. This document describes the process for integrating with a new IdP that uses SAML.\n\nService Provider Configuration\n\nTo configure SAML for the Lytics service provider some information is required about your IdP. If you have a metadata file that contains SAML provider information this may be appropriate, just make sure that the following information is provided to Lytics Support:\n\nSign In URL\nX509 Signing Certificate (The identity provider public key - if possible encoded in a separate PEM or CER formatted file)\nSign Out URL\n\nFurther configuration details such as mappings may need to be provided. Once this information has been received, Lytics can configure the SAML connection in Auth0.\n\nIdP Configuration\n\nAfter Lytics configures the SAML connection on the service provider, Lytics Support will provide a metadata XML file with the following information in order to complete the IdP configuration. The key fields in this metadata are:\n\nConnection Name\nAssertion Consumer Service (ACS) URL (aka post-back or callback URL)\nEntity ID of the Service Provider\n\nWith this information, your IdP connection can be configured to complete the SSO integration.\n\nTesting SSO\n\nOnce all the information has been configured in both the IdP and the Lytics service provider, you can test and verify that the SSO implementation works as expected. If you are using SSO as your only sign-in method, please disable any password restriction or expiration settings that may have been enabled in the UI.\n\nDuring the testing process, Lytics can be configured to allow both SSO logins and regular username and password (or Google OAuth) login through the app. This allows users to test SSO without disrupting the day-to-day usage of the app.\n\nIf requested, once the SSO implementation has been tested and verified, Lytics can disable the use of other logins for an account.\n\nTroubleshooting SSO\n\nIf it's known that SSO is going to be added to an account, then the user email addresses added to the account should match the email address present within the IdP. If the email address doesn\u2019t match, then the login will fail as Lytics will not be able to verify that there is a user with that email address.\n\nFor instance, if the email listed in the IdP is abc@123, and within Lytics it\u2019s def@456, then the user will have successfully been verified by their IdP, but the login within Lytics would fail as the authenticated user email would not match the user within Lytics. You would need to create a new user within Lytics with the email address abc@123 for the SSO login to be successful.\n\nIt should also be noted that primary accounts are decided as the first account that a user was added to. Due to this, users from a single group/organization will often have different primary accounts. This is important for SSO as it will also be the account that the user is logged into at the start of their session. If SSO is enabled as the only means of login on one account and a user attempts to log-in using Google OAuth or their username and password, the login session will fail. The following options are possible solutions:\n\nAdd that user to your IdP.\nOther methods of logging in would need to be added (Google OAuth, username/password).\nRemove user from all accounts and then add them back with the first account being the account that you want to be their primary account.\n\nLytics Support can assist in the troubleshooting process. When testing for the first time Lytics can enable logging to help troubleshoot any issues you may encounter. If you are receiving the error message shown below after being redirected to the app, that means there is an issue on the application side.\n\nThe Lytics Support team can help debug this if you provide information on the login attempt such as the time of the login and the user and account.\n\nIf you\u2019re encountering an error earlier in the login process, such as before you are redirected to the Lytics app, this may be an issue with the SAML configuration either on the IdP or Service provider side. Lytics\u2019 support team can review these issues and coordinate a fix, just contact us with the details of the issue.\n\nUpdated over 1 year ago\n\nTABLE OF CONTENTS\nIDP-initiated SSO (Legacy)\nService Provider Configuration\nIdP Configuration\nTesting SSO\nTroubleshooting SSO",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nData Pipeline\nSuggest Edits\n\nWelcome to the Pipeline & Profiles section, where we will explore data management and analysis fundamentals. This section will delve into the concepts of data streams, data sources, and queries/LQL.\n\nData Sources refers to the origin of the data and how you can leverage hundreds of different out-of-the-box integrations or APIs to aggregate your data from disparate channels.\n\nData Streams outline how to control and monitor data that is being streamed into the Lytics platform.\n\nQueries & LQL breaks down our core query language and capabilities for transforming and unifying your data.\n\nUpdated over 1 year ago",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nImproved\nInspiration Filters, Adobe, OneTrust, Classification\nabout 2 months ago by Mark Hayden\n\nResolved internal link issues, enhanced Adobe and OneTrust integrations, streamlined onboarding alerts, and added filtering for home Inspiration cards.\n\nAdded\nAcademy, Inspirations and an all new Dashboard!\nabout 2 months ago by Mark Hayden\n\nWe've officially released the new and improved application home page! Here is an overview of what you'll get with this new experience.\n\nFixed\nContent Collection List View Improvements\nabout 2 months ago by Mark Hayden\n\nContent Collection list view bug fixes and metric freshness timestamp.\n\nAdded\nHelp Center, Assertions and Credit Usage\nabout 2 months ago by Mark Hayden\n\nWe've fully launched the new and improved Help Center, better credit usage information for Developer and Growth accounts along with assertion management within Schema Studio!\n\nAdded\nNew billing and package management\n2 months ago by Mark Hayden\n\nWe've released the new package and billing portal to all Developer and Growth tier accounts. Now you can upgrade directly from the interface. In addition, we've added an improved interface for \"active\" Lookalike models and more.\n\nImproved\nImproved Job status, metric freshness timestamp, Chrome extension and more.\n2 months ago by Mark Hayden\n\nUpdates include refining job statuses to show as Running, a timestamp to indicate when metrics were last updated, instructions for setting up the Chrome Extension, and improved URL matching for easier classification.\n\nImproved\nLookalike dashboard, onboarding wizard, general UI\n3 months ago by Mark Hayden\n\nWe've launched the new.home dashboard in beta, fixed issues with report overlaps and onboarding channel selection, and removed distracting icons for a cleaner interface. A bug allowing empty segment selections in job configurations, which led to failures, has been corrected.\n\nAdded\nWordPress plugin now available\n3 months ago by Mark Hayden\n\nWe've officially launched our WordPress plugin, which brings many of the core personalization and content-related features from Lytics directly to your favorite content management system.\n\nImproved\nClassification URL validation and skipping auth setup\n3 months ago by Mark Hayden\n\nWe've reworked the content classification dashboard. This page now includes much more information related to the status of your classification, insights into potential issues, and more.\n\nAdded\nOnboarding wizard, UI enhancements and bug fixes\n3 months ago by Mark Hayden\n\nThe content classification dashboard now offers enhanced status insights and issue detection. We\u2019ve set default sorting for job types and providers by name, improved login validation for de-authenticated users, refined the onboarding wizard experience and made typography globally consistent for better readability.\n\n1 of 11",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nConnecting Warehouses\nSuggest Edits\nAccess\n\nCloud Connect tabs are found under Data Pipeline > Cloud Connect. Connections configure the access to your data warehouses, and Data Models configure the SQL queries that are run to connect audience membership and profile attributes.\n\nCreating a Connection\n\nClick + Create New Connection from the Connections Dashboard at the top right and complete the following steps.\n\nChoose the provider.\nChoose the Connection type.\nSelect an existing Authorization or create a new one by following the Authorization instructions.\nAdd a name (label), description, and complete the configuration options. These will vary slightly between providers.\n\nAuthorization & Security\n\nThe authorization selected for your Connection will control your Lytics account users' access to your data warehouse. You can control whether a user has read access to the entire dataset or individual tables, maintaining your security and governance practices within your data warehouse.\n\nSupported Data Warehouses\n\nCloud Connect currently supports a number of popular data warehouses:\n\nAmazon Redshift\nDatabricks\nGoogle BigQuery\nMicrosoft Azure SQL Database\nSnowflake\nManaging Connections\n\nOnce you have created a Connection, you can access a summary page showing how data from your data warehouse is being leveraged in Lytics. At the top of the page, you\u2019ll see the following information:\n\nProvider: Data warehouse that you are connecting with Lytics.\nAuthorization: Name of the authorization, such as \u201cCloud Connect JWT.\u201d Note: Lytics users can access any data tables that the Connection Authorization has read access to.\nType: Indicates the type based on your provider.\nCreated By: Lytics user who created the Connection.\nCreated On: Date the Connection was initially created.\nLast Updated: Date the Connection was most recently edited.\n\nThe rest of the Summary tab shows how many active and inactive data models are built using this Connection as a data source and how many tables are accessible from this dataset. The Activity chart displays how many rows are being queried, which can have cost implications based on your data warehouse usage.\n\nExplore\n\nThe Explore tab provides a simple Schema Explorer to validate that the data shown is as you would expect to see in your data warehouse. In the example below, we only connected an individual table, but here you will see as many tables as the authorization has read access to.\n\nDetails\n\nThe Details section displays all the information about setting up your Connection, including the authorization and configuration settings.\n\nLogs\n\nThe Logs section records the history of events for this Connection, which are helpful to ensure your connection is working as expected. Below are the connection event types you may see.\n\nConnection Events\tDescription\nCreated\tFirst event indicating the Connection is active.\nUpdated\tConnection was updated by a Lytics user.\nDeleted\tConnection was removed and any data models built on this Connection will no longer be updated.\n\nUpdated 3 months ago\n\nTABLE OF CONTENTS\nAccess\nCreating a Connection\nAuthorization & Security\nSupported Data Warehouses\nManaging Connections\nExplore\nDetails\nLogs",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nMonitoring Metrics and Alerts\n\nHow to monitor Lytics, and alert when help is needed.\n\nSuggest Edits\nMonitoring\n\nMonitoring and alerting is available on every job and every authorization within Lytics.\n\nTo set up alerting on your jobs or authorizations, you can set up a monitoring job from either the Job API or the Lytics UI for alerting to Slack, Microsoft Teams, or directly to email.\n\nIf a source or destination job has failed, Lytics will show the latest error message on the Conductor Diagnostics Dashboard and on the Logs tab of the Source/Destination Job Summary interface, and allow the job to be restarted if needed. The most detailed information for troubleshooting can be accessed from the Job Logs API\n\nAdditional generic monitoring on the Lytics system is available on our status page at lytics.statuspage.io.\n\nUpdated 10 months ago\n\nTABLE OF CONTENTS\nMonitoring",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nData Models & Queries\nSuggest Edits\nCloud Connect Data Models\n\nA Data Model is used by Cloud Connect to link or \"connect\" a user's external data warehouse to Lytics profiles. Each Data Model represents a set of records defined by a SQL query. Each Data Model will also configure a join key which defines how the Data Model is joined to Lytics Profiles.\n\nOnce a Connection to your data warehouse has been created, Data Models can be created via the Data Models tab within the navigation bar:\n\nCreating a Data Model\n\nClick + Create New from the Data Models dashboard and complete the following steps.\n\nSelect the Connection.\n\nWrite the SQL query.\n\n\n\n\n\nQuery Editor: Write and test standard SQL queries directly in Lytics. Alternatively, copy and paste queries you've tested in your BigQuery or Snowflake instance. When you click Test Query, Lytics will return 10 sample records.\n\nTest the query and validate the results.\n\n\n\nConfigure the Data Model.\n\n\n\nName: Data Model name\nDescription: The Description of the Data Model.\nSlug: The Data Model name that will be used in the membership and activation fields in the audience builder. It is important that the name chosen here makes sense for those building audiences in the tool. If no slug is selected, it will auto-populate using the model name.\n\n\n\nPrimary Key: Select the data warehouse ID that will be used to match against profiles in Lytics.\nExternal Lytics Key: Select a Lytics user profile field that is a unique identifier to map to an identifier in your incoming data source.\nActivated Fields:Select the fields you want to bring into Lytics profiles for activation. Note that each data model can only include 25 columns as activated fields.\nSync Frequency: Select a time interval to run the query on - this is also the frequency that your Lytics profiles will be updated. The beta options are currently every 1 hour to every 3 weeks.\nCreate new Lytics profile: Check this box if you want to create new profiles in Lytics if a profile with a matching ID does not already exist in Lytics. If you do not select this check box, only users with matching IDs to the Data Model IDs will be updated.\n\n\nActivate the Model. Until the Data Model is activated, its SQL query will not be run against your data warehouse and Lytics profiles will not be updated with the selected fields.\n\ud83d\udcd8\n\nWhen building multiple Cloud Connect Data Models with the same primary key, you must select the same External Lytics Key mapping. Selecting a different Lytics key will result in an error when you try to save the new Data Model.\n\nExample Use Case\n\nConsider this scenario to demonstrate why you'd want to create a Cloud Connect Data Model driven audience instead of a standard Lytics audience. Your company sells e-bikes and wants to run a holiday campaign that sends a promotion to any customer who purchased an e-bike in November or December last year. Perhaps you also want to refine your audience to those who are interested in particular bike brands. All of this purchase and product data already exists in your Google BigQuery instance (as shown below).\n\nInstead of directly importing all that purchase history data into Lytics, you can write a SQL query to find which customers meet this audience criteria. By copying this query directly into the Lytics Model Builder (as shown in the generate a query screenshot above), you'll create a new audience in Lytics that will be updated on the frequency interval you decide.\n\nOnce the Data Model is activated, profiles will be updated with a membership field that can be used to create an audience. Activated fields can be added to the Data Model that can then be used for more dynamic audiences.\n\nSQL Translator\n\nIf you would like to skip having to write a SQL query, simply describe the Data Model of what you wish to fetch and using GenAI, Lytics will translate the description to a SQL query for you. Lytics makes queries to your database to ensure that the data is up to date and accurate when creating the query.\n\nUpdated 19 days ago\n\nTABLE OF CONTENTS\nCloud Connect Data Models\nCreating a Data Model\nExample Use Case\nSQL Translator",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nInsights\nSuggest Edits\nIntroduction\n\nLytics Insights provide visibility into how your audiences and campaigns perform and recommend how you may improve engagement, drive conversions, and increase marketing ROI.\n\nYou can think of Lytics Insights as a way to connect the data you have with the decisions you are making for marketing campaigns. Defined in a few words, Lytics Insights are:\n\nActionable: Direct knowledge to your marketing initiatives.\nContextual: Provide actions that are relevant to the Lytics user consuming them.\nExplanatory: Describe phenomena in terms of your business processes or outcomes.\nNovel: Offer a new understanding of your customers and campaigns.\n\nInsights serve as a key ingredient to achieve marketing personalization. To effectively leverage customer data, you must apply it throughout your campaign planning and execution process. Unlike metrics that remain on a BI dashboard or report, Lytics Insights are designed be immediately activated in your segmentation and campaign strategies.\n\nIf your account package includes Insights, you can access Insights from anywhere in the Lytics UI.\n\nHow Insights Work\n\nInsights surface meaningful data and recommended action, enabling you to make better, data-driven decisions during your campaign planning and execution. The core idea behind Insights is simple. Insights are a combination of facts and actions:\n\nFact: Observable, data-driven information providing demonstrable value. This is a unique set of raw metrics, such as the number of users or the amount of lift.\nAction: Suggested behavior based on user role, data source, activation channels, industry, etc. This is the contextualization of a fact into something to be done.\n\nInsights are made possible thanks to the data science capabilities built into Lytics. The heavy lift of processing and interpreting data is automatically handled by machine learning models under-the-hood.\n\nThis is a prime example of how machine learning can augment (not replace) a marketer\u2019s role. Machines efficiently and accurately sift through lots of data, allowing people to then make use of the important information in the most relevant context. For example, a model can predict which users are most likely to churn, and a marketer can then target those at-risk users with a win-back campaign on Facebook.\n\nInsight Cards\n\nInsights are presented as Insight Cards that are available throughout the entire app.\nSimply navigate to the left-hand side of the Lytics UI to open the Insights Drawer, which contains a list of Insight Cards unique to your account.\n\nBelow is an example of an Insight card and what it's comprised of:\n\nCreation date: When the Insight was generated.\nExpiration date: When the Insight will be removed from the drawer. Insights expire after 2 weeks.\nInsight statement: The explanation or comparison of noteworthy data will depend on the type of Insight. You can click on an audience name to view its summary report.\nRecommended action: Lytics will suggest how you can apply this Insight to your targeting along with a related use case.\nExpanded definition: View more details by clicking the arrow. This example explains the Lytics score or other data field being calculated.\n\nInsights are generated and refreshed on a weekly basis to ensure you are viewing the most recent user data. Insights expire after 2 weeks, so you only target and activate an audience and campaigns during a relevant time frame.\n\nThe Insights Drawer will display a maximum of 25 cards at a time. Insights will be created for up to 10 custom audiences, in addition to Lytic's out-of-the-box engagement audiences. Lytics will prioritize Insights based on the statistical significance of the data and the most frequently used audiences.\n\nCompositional Insights\n\nCompositional Insights help you understand the makeup of your audiences. This information can answer, \u201cwhat makes an audience unique?\u201d and \u201cHow should I target these users differently?\u201d\n\nCompositional Insights make it easier for you to create effective segmentation strategies. They compare data fields within audiences to surface which attributes drive user behavior. Compositional Insights currently include the following:\n\nAudience Pairs\n\nThis Insight compares high-value audiences on their levels of engagement. This can take the form of comparing Lytic's out-of-the-box audiences on a particular behavioral score or a custom audience.\n\nAs a bite-size piece of information, this Insight Card has provided:\n\nAn easy way to leverage Lytics audiences and scores that are derived from data science.\nA suggested next step of what to do with this information. For example, you may target these at-risk users with a win-back campaign.\nField Candidates\n\nThis Insight compares the prevalence of specific data fields within two audiences. Candidates included are fields used in audience definitions and fields from the Promoted Schema Fields in your Account Settings.\n\nHaving a granular understanding of your audiences allows you to make more strategic targeting decisions. For example, there may be a data field that your organization is paying to collect. If you discover through an Insight Card that this field is not performing as expected in your campaigns, you could save marketing spend by eliminating that field and using others that are shown to be more predictive of user behavior.\n\nExperience Insights\n\nExperience Insights help you track and understand campaign performance. This information can answer, \u201cWhich campaigns are resonating with my users?\u201d and \u201cAre my marketing tactics driving engagement or conversions?\u201d\n\nExperience Insights allow you to refine campaigns based on current metrics proactively. Since Lytics Insights are generated and refreshed every week, you can take action to improve a campaign while it\u2019s running.\n\nExperience Insights include the following types:\n\nExperience Management\n\nThis Insight will surface when an experience is missing UTM parameters, which enable Lytics to track conversions from downstream tools such as Facebook.\n\nWhile this Insight is more straightforward than the other types Lytics offers, it helps you quickly identify when an Experience configuration is incomplete, affecting your ability to track performance and target users who have previously engaged in future campaigns.\n\nExperience Performance\n\nThis Insight appears when a significant spike up or down in campaign performance exists. Awareness of such incidents allows you to make the necessary changes to improve campaign ROI.\n\nFor example, if conversion rates have rapidly declined for a particular campaign in the last week, you may need to reevaluate and consider changes such as refining your audience, messaging, or campaign tactics.\n\nActivating Insights\n\nTaking action based on your Insights is the most important part of the process. As the introduction explains, Lytics Insights are designed to be actionable. By understanding how users interact with your brand at a granular level, you can iterate campaigns to improve their performance and deliver better user experiences.\n\nActivating against Lytics Behavioral Scores\n\nUser engagement is a crucial component of personalization. Each Lytics Score (quantity, frequency, recency, intensity, momentum, propensity) indicates a different aspect of user behavior, but overall, higher scores indicate high engagement, while lower scores indicate low engagement.\n\nBelow are some everyday use cases you might consider, depending on whether the target audience has higher behavioral scores (anything above 50) or lower behavioral scores (anything below 50).\n\nTargeting Users with Low Engagement\n\nThere are two main approaches to reaching unengaged users. The first is to drive engagement using a variety of tactics, such as messaging on different channels, targeting based on content affinities, etc. The second approach is to increase marketing efficiency by suppressing these users, thus improving your conversions.\n\nTargeting Users with High Engagement\n\nWhen creating strategies around your most active users, you will want to keep them engaged by delivering relevant content and establishing a first-party relationship that increases their Lifetime Value (LTV). You can find more users who are similar to your best customers using lookalike audiences.\n\nYou can also learn more about what makes these engaged users different, which can inform your overall targeting strategies.\n\nView the Audience summary by clicking the audience name on your Insight Card to see the history of this audience and its characteristics.\nActivating against Lytics Content Affinity\n\nGiving your users more of what they love is a great tactic to consider to either drive engagement for less active users or to keep currently active users engaged.\n\nUpdated over 1 year ago\n\nTABLE OF CONTENTS\nIntroduction\nHow Insights Work\nCompositional Insights\nAudience Pairs\nField Candidates\nExperience Insights\nExperience Management\nExperience Performance\nActivating Insights\nActivating against Lytics Behavioral Scores\nActivating against Lytics Content Affinity",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nJUMP TO\nLYTICS API\nJobs\nAuths\nProfiles\nSchema\nStreams\nSystem\nReports\nAccount\nConnections\nDataModels\nML Models\nSegments\nTemplates\nUser\nV1 LYTICS API\nDataUpload\nSegment\nSegmentCollection\nSubscription\nPersonalization\nContent\nSystem Events\nCatalog\nMetric\nAccount\nAccount Settings\nUser\nWork\nAuth\nProvider\nWorkflow\nQuery\nExperience\nAdrollSync: Create job\nPOST\nhttps://api.lytics.io/v2/job/adroll-sync\n\nCreate a AdrollSync job\n\nLANGUAGE\nShell\nNode\nRuby\nPHP\nPython\nCREDENTIALS\nHEADER\nHeader\nRESPONSE\nClick Try It! to start a request and see the response here!",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nWeb\nSuggest Edits\n\nWelcome to the Lytics JavaScript SDK documentation, where we will explore the capabilities and features of the Lytics JavaScript SDK. This SDK enables website owners and developers to integrate Lytics' powerful customer data platform directly into their websites and web applications.\n\nWith Lytics JavaScript SDK, you can easily collect data from website visitors and enrich their profiles with valuable insights, including behavior, preferences, and interests. By leveraging the Lytics JavaScript SDK, you can better understand your customers and deliver personalized experiences that drive engagement, retention, and revenue.\n\nIn this documentation, we will cover how to install and configure the Lytics JavaScript SDK and how to use it to identify users, manage consent, track events, and personalize experiences. We will also examine integrating Lytics JavaScript SDK with other third-party analytics and marketing platforms to create a complete web data strategy.\n\nWhether you are a website owner, developer, or marketer, this documentation will provide you with the essential knowledge needed to leverage the Lytics JavaScript SDK to build better web experiences and gain deeper insights into your customers' behaviors and preferences.\n\nUpdated over 1 year ago",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nCommunity\nAsk a Question\n0\n0\nJS event or function when campaign loads\n\nIs there a way for me to know that the campaign has loaded on my site? Is there any hook to tie into?\n\nPosted by null about 15 hours ago\n0\n0\nhow to post json data by using this API call\n\nwhat are the changes of API call\nhttps://api.lytics.io/collect/json/stream?dryrun=false and how to post json data by using this API call?\n\nPosted by Sandya about 1 month ago\n0\n2\nANSWERED\nDoes Lytics support PDPA compliance?\n\nHi Team,\n\nPosted by Raghavendra kumar about 2 months ago\n0\n2\nANSWERED\nI am not seeing my live experience\n\nHello,\n\nPosted by ATest about 2 months ago\n0\n4\nANSWERED\nInquiry on Lytics Solution Accelerator for Identity Resolution\n\nHi Team\n\nPosted by Bharadwaja Raghavendra Kumar 2 months ago\n1\n2\nANSWERED\nStreams\n\nHow to hide the streams and what is the retention period if the stream deleted?\n\nPosted by Darshan L C 2 months ago\n0\n1\nANSWERED\nExperience Best Practices for Improved Engagement\n\nHello. I have a modal/Experiences question. If you have an Experience that is stand-alone - not in a goal, how would that Experience get prioritized for being shown? I am trying to troubleshoot why a modal is getting very low engagement and want to be sure I consider everything that might impact the serving of the modal.\n\nexperiencesbest practices\nPosted by Sean McMahon 2 months ago\n0\n1\nANSWERED\nOur site is built using a framework in .net. Can I still use the free developer tier?\n\nYour free tier mentions Drupal and WordPress but we are curious if there are capabilities outside of those two CMS platforms?\n\ndeveloper tierpersonalization engine\nPosted by -- 3 months ago\n0\n2\nANSWERED\nPersonalization Engine - Anti-Flicker CSS?\n\nFrom an agency:\n\ncssflicker\nPosted by James McDermott 3 months ago\n0\n1\nPersonalization Engine - Tag Implementation - CMS Plugin or Google Tag Manager?\n\nFrom an agency partner:\n\nPosted by James McDermott 3 months ago\nFAQs\nRecent\nUnanswered\naudiencesbest practicescompetitivecompliancecssdata exportdata sciencedeploymentdeveloperdeveloper tierexperiencesflickerga4graphical databaseinternationalizationloginmergeoppersonalization enginepersonalization sdkprivacyprivate instanceregion controlrollupschemasdksecurity\n1 of 16",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nWhat is Cloud Connect?\nSuggest Edits\nIntroduction\n\nCloud Connect allows you to run complex SQL queries directly against your data warehouse and translate the resulting records into profile attributes and audiences.\n\nCloud Connect in Action\n\ud83d\udcd8\n\nCloud Connect in Action\n\nCheck out the solution architecture developed with Google which leverages BigQuery and Analytics Hub in addition to Cloud Connect to create a rich pattern for secure data collaboration and activation.\n\nGetting Started\nAccessing Cloud Connect\n\nCloud Connect is available to all Lytics customers. If you don't currently have access via the product switcher at the top of the primary navigation, contact support or your account management team.\n\nCommon Use-cases\n\nCloud Connect unlocks many use cases enabling marketers to segment their customers based on any attributes stored in their database. A few examples include:\n\nTime Window: All users who did not log in last month.\nJoins (B2B): All users associated with accounts without feature \"x.\"\nLifetime Value (LTV) or Rollup: All users with a premium subscription have purchased at least two products.\n\ud83d\udcd8\n\nNeed a warehouse?\n\nDon't currently have access to a data warehouse? Lytics has you covered with Lytics Warehouse. We provide a simple data warehouse on top of Google BigQuery to all of our customers as part of Conductor. Simply reach out to account manager or support for access and information.\n\nHow it Works\n\nCloud Connect augments the rest of the Lytics CDP suite by providing a secure and flexible entry point into your data warehouse. Users can quickly write a standard SQL query to perform a number of complex data manipulations or aggregations and stream the results directly to materialized user profiles. From there, generate insight-heavy reports, enrich targeted segments, and activate across hundreds of marketing channels in just a few clicks.\n\nBelow you'll find an overview of the general architecture, which can be summarized into these key value drivers:\n\nDirect access to existing customer data in your warehouse.\nNo need to rip and replace or replicate consumer data in order to activate.\nLeverage standard SQL to extract only what is relevant to your business goals.\nMaintain your warehouse as the \"source of truth.\"\nRequires very little to implement and begin generating ROI.\n\nUpdated about 1 year ago\n\nTABLE OF CONTENTS\nIntroduction\nCloud Connect in Action\nGetting Started\nAccessing Cloud Connect\nCommon Use-cases\nHow it Works",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nLytics Integration Options\nSuggest Edits\nLytics Integration Options\n\nIf you don\u2019t see your ads or marketing tool in our integrations list we offer the following options to connect with other platforms. With our Integration Options we enable flexibility across the various channels where your data exists and tools to build customized integrations. Lytics Integration Options include:\n\nWebhooks\nLytics File Service integrations for server-to-server file transfer\nJavascript Tag\nMobile SDK\nData Warehouse Integrations\n\nRead on for details on each option. If you have questions about any of these options or would like to further explore an integration with a specific platform reach out to your Lytics contact.\n\nWebhooks\n\nWebhooks offer one of the simplest and most flexible options for enabling applications to share data. They\u2019re broadly used in the cloud computing world for integrations that can be stood up quickly with a minimal amount of code. The Lytics Webhook integration supports triggered delivery of audience membership (enters and exits) to a destination url where another application can pick it up. See our Webhook documentation for full details.\n\nInterested to understand how a Webhook integration might be applied? Look at this Mobile Messaging use case for a simple Lytics Webhook integration to support one-to-one personalized messaging with Twilio.\n\nServer-to-Server File Transfer\n\nServer-to-server file transfer is a very common mechanism for data sharing. Lytics File Service options enable Lytics to pull or push user data from an SFTP (secure file transfer protocol) using a CSV (comma-separated values) or JSON file. If you have another platform that also enables integrations via SFTP, this integration path helps to automate the process of passing data between your system and Lytics. Check out the Lytics File Service documentation to learn about setting up the file transfer job.\n\nIn addition to SFTP, Amazon S3 is a very commonly used medium for server-to-server file transfer. Refer to our Amazon S3 documentation for data import and export options.\n\nJavascript Layer/JS Tag\n\nThe Lytics JS Tag is the default option for client-side (website) data sharing with Lytics. The JS Tag supports the delivery of user behavior attributes from your website into Lytics and can also return back to the website user profile data such as audience members or content recommendations from Lytics for real time personalization. Refer to our JS Tag Documentation for full details.\n\nMobile SDK\n\nThe Lytics Mobile SDK facilitates in-app data sharing with iOS and Android. Similar to the JS Tag, the Mobile SDK supports user behavior-based data delivery to Lytics and profile data to the app for real-time personalization. See our Mobile SDK Documentation for more details.\n\nUsing Your Data Warehouse\n\nIf you already use a data warehouse to manage your data this is a great option for data import and export with Lytics. Lytics integrates with\n\nGoogle Big Query\nAmazon Redshift\nMicrosoft Azure\nSnowflake\nDatabricks\n\nEach of these providers offers options for importing data into databases which can then be imported into Lytics using Cloud Connect or our Data Import integrations. Lytics can also export data into each of these data warehouses.\n\nUpdated 9 months ago\n\nTABLE OF CONTENTS\nLytics Integration Options\nWebhooks\nServer-to-Server File Transfer\nJavascript Layer/JS Tag\nMobile SDK\nUsing Your Data Warehouse",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nAccessing Accounts\nSuggest Edits\nIntroduction\n\nUnder the account menu in the main navigation, find the Account Settings option.\n\nYou should see your account information, such as the name of your account, domain, the account owner's email, and more.\n\nYou can edit your account name, domain, and primary contact email from this page anytime. Once you change one of these values, you will be prompted to save or undo the changes.\n\nAccount ID and account number are never editable. They are assigned at account creation and are permanent identifiers.\n\nNavigating Accounts\n\nFor users who have access to multiple accounts, you can quickly and securely navigate between Accounts using our account switcher at the bottom right of the primary navigation:\n\n1. Click \"Switch Account\"\n2. Select Account\n\nA menu will appear outlining the accounts you have access to. Simply select an account and a new tab will open with this accounts dashboard.\n\nUpdated over 1 year ago\n\nTABLE OF CONTENTS\nIntroduction\nNavigating Accounts",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nClient & Server Side Cookies\n\nEverything you wanted to know about cookies from client side to server side.\n\nSuggest Edits\nWhat is a Cookie\n\nAn HTTP cookie (web cookie, browser cookie) is a small piece of data that a server sends to a user's web browser. The browser may store the cookie and send it back to the same server with later requests. Typically, an HTTP cookie is used to tell if two requests come from the same browser\u2014keeping a user logged in, for example. It remembers stateful information for the stateless HTTP protocol.\n\nCookie Flavors\nFirst-party\n\nFirst-party cookies are created by the host domain \u2013 the domain the user is visiting. These cookies are generally considered good; they help provide a better user experience and keep the session open. This means the browser can remember key information, such as items you add to shopping carts, username, and language preferences.\n\nThird-party\n\nThird-party cookies are those created by domains other than the one the user is visiting at the time and are mainly used for tracking and online-advertising purposes. They also allow website owners to provide certain services, such as live chats.\n\nCommon Cookie Use Cases\nSession management\n\nLogins, shopping carts, game scores, or anything else the server should remember.\n\nPersonalization\n\nUser preferences, themes, and other settings.\n\nTracking\n\nRecording and analyzing user behavior.\n\nHow does Lytics use cookies?\n\nCookies are one of the preferred methods for maintaining a user identifier in the browser, both known and anonymous. This gets associated with all inbound events captured by our Javascript tag as they interact with customer websites. Specifically, our Javascript tag stores a string of digits and characters that are used as a unique ID commonly referred to as our _uid or, in some cases seerid as a first-party cookie.\n\nCreating & Managing Cookies\nWhere do cookies come from?\nClient-Side\n\nIn web development, the client-side refers to everything in a web application displayed on the client (end-user device). This includes what the user sees, such as text, images, and the rest of the UI, along with any actions an application performs within the user's browser.\n\nWhen someone refers to a \u201cclient-side cookie,\u201d they are generally referring to a cookie that is created and/or managed via a common client-side programming language such as Javascript.\n\nServer Side\n\nLike with client-side, server-side means everything that happens on the server instead of on the client. In the past, nearly all business logic ran on the server-side, including rendering dynamic webpages, interacting with databases, identity authentication, and push notifications.\n\nWhen someone refers to a \u201cserver-side cookie,\u201d they are generally referring to a cookie that is created and managed using one of the many common server-side programming languages such as NodeJS, PHP, Python, etc.\n\nCookie Ingredients\nName\n\nDefines the cookie name. Generally, the cookie name is the primary way of retrieving a cookie and its associated value and attributes.\n\nValue\n\nThe stored value for the cookie. This can include any US-ASCII character excluding a control character, whitespace, double quotes, comma, semicolon, and backslash.\n\nExpires\n\nIndicates the maximum lifetime of the cookie as an HTTP-date timestamp. See Date for the required formatting.\n\nMax-Age\n\nIndicates the number of seconds until the cookie expires. A zero or negative number will expire the cookie immediately.\n\nDomain\n\nDefines the host to which the cookie will be sent.\n\nPath\n\nIndicates the path that must exist in the requested URL for the browser to send the Cookie header.\n\nSecure\n\nThis indicates that the cookie is sent to the server only when a request is made with the HTTPS: scheme (except on localhost) and, therefore, is more resistant to man-in-the-middle attacks.\n\nHttpOnly\n\nForbids JavaScript from accessing the cookie, for example, through the Document.cookie property. Note that a cookie created with HttpOnly will still be sent with JavaScript-initiated requests, for example, when calling XMLHttpRequest.send() or fetch(). This mitigates attacks against cross-site scripting (XSS).\n\nSameSite\n\nControls whether or not a cookie is sent with cross-origin requests, providing some protection against cross-site request forgery attacks (CSRF).\n\nWho\u2019s taking away my cookies?!?\n\nIn June 2017, Apple introduced a new privacy feature called Intelligent Tracking Prevention (ITP). This same feature was officially released in September 2017 with Safari 12 and iOS 11. Since then, the ITP has evolved and introduced several subsequent versions leading us to the current state (as of July 2022), which has many impacts on marketing but most notably:\n\nITP blocks all third-party cookies by default. (ITP 1.0/1.1)\nITP can grant exceptions to third-party cookies with Storage API. (ITP 2.0)\nITP caps all first-party cookies set with JavaScript to 7 days or 24 hours. (ITP 2.1/2.2)\nITP caps first-party cookies set by the server using CNAME cloaking to 7 days. (ITP 2.3)\n\nAs a result of the privacy and security efforts in general other browsers such as Mozilla\u2019s Firefox(ETP) and Google Chrome have followed suit in announcing and/or implementing their security and tracking protocols, which continue to impact tools such as third and first-party cookies that have long been a staple in providing the data necessary for marketers to personalize their communications effectively.\n\nWhat can I do to replace my cookies?\n\nWith each iteration on more stringent privacy-related changes comes a wave of workarounds or alternate approaches to maintaining access to behavioral data essential to marketing:\n\n1. Deploy a strong first-party identification strategy.\n\nThere is no replacement then a strong identification strategy. Creating a relationship in which a user will openly share their identity through a login or some other authenticated means will always result in the highest level of certainty on identity, which leads to the best level of personalization. However, many use cases focus on anonymous users or users who have not built up the relationship necessary to unlock this level of authentication.\n\n2. First-party client-side cookies have changed but are still a viable solution.\n\nThough third-party cookies are effectively dead, first-party cookies are still viable for many use cases. Not only do they offer a very simple off-the-shelf type of implementation for leveraging, they also have a long shelf life, assuming that an anonymous user interacts frequently enough to overcome the 7-day expiration window.\n\n3. First-party server-side cookies offer an extended expiration window.\n\nOver the past 12 months, there has been a surge of interest in server-side cookies. This method for setting cookies currently is not affected by the ITP changes that impact client-side cookies, most notably the automatic expiration at seven days. Rather, server-side cookies can live for long periods, leading to a higher quality identification for anonymous users. The downside, however, is they are far more difficult to leverage than the client-side. They require a much more technical integration with whatever server-side technology is used to power the web asset and may not be accessible in the same manner as client-side cookies.\n\nGetting Technical with Server Side Cookies\n\nIn general, regardless of the specific attribute settings used when leveraging server-side cookies, they currently are not impacted by the 7-day expiration window that client-side cookies fall victim to. However, Apple has made it clear that they have additional plans to extend some of the client-side cookie limitations to the server-side, and the most important attribute in that discussion is the HttpOnly attribute.\n\n\nhttps://webkit.org/blog/9521/intelligent-tracking-prevention-2-3/\n\nAs a CDP and technology leader, we always aim to help our customers future-proof their implementations. As such, even though a non-HttpOnly server-side cookie offers an easier means to bypass current client-side restrictions, it is our recommendation to consider investing to leverage HttpOnly cookies set by the server-side to prevent any potential impacts of the next few iterations of ITP. Below we\u2019ll explore the two options and demonstrate the key differences.\n\nServer Side Cookie without HttpOnly\n\nThe following example in Node.js demonstrates a sample snippet for setting the cookie server side. Most server-side languages have existing methods to make this very easy. In the case of creating a cookie for Lytics to leverage, you will also need to generate a unique ID which can be done in a variety of ways. In the case of Node, you may consider using the randomUUID() method of the Crypto interface. A simple Google search can lead you or your developer down the road of generating a unique ID that best fits your use case.\n\nOnce you have that unique ID, you simply set the cookie using the pre-defined Lytics name our Javascript tag is looking for. Alternatively, the tag can be configured to use any custom name.\n\n// THE FOLLOWING REPRESENTS A UNTESTED AND NON-PRODUCTION LEVEL\n// EXAMPLE OF HOW TO SET A COOKIE SERVER SIDE WITH NODE.JS AND\n// ACCESS THE DOCUMENT.COOKIES FROM THE CLIENT SIDE\n\nvar express = require('express');\nvar app = express();\nvar cookieParser = require('cookie-parser');\nvar crypto = require('crypto');\nvar dayjs = require('dayjs');\n\n// cookieParser middleware\napp.use(cookieParser());\n\n// route for setting the cookies\napp.get('/setcookie', function (req, res) {\n  // setting a server side cookie without httponly\n  res.cookie(\"seerid\", crypto.randomUUID(), {\n    httpOnly: false,\n    expires: dayjs().add(30, \"days\").toDate(),\n  });\n\n  res.send(`<html><body><p>A server side cookie has been set.</p></body></html>`);\n})\n\n// Route for getting all the cookies\napp.get('/getcookie', function (req, res) {\n    res.send(`\n        <html>\n            <head>\n                <script>\n                    alert(document.cookie);\n                </script>\n            </head>\n            <body>\n                <p>This is an example of how to get the cookies client side.</p>\n            </body>\n        </html>`\n    );\n})\n\napp.listen(3000, (err) => {\n    if (err) throw err;\n    console.log('server running on port 3000');\n});\n\n\nSince non-HttpOnly cookies are accessible out of the box client-side there is no additional lift necessary. Our tag will pick up the cookie and use that ID as the identifier. It is important to note that any UID changes must also be managed server-side as setting the cookie client-side will engage the 7-day expiration max. In this example, we simply raise an alert with the cookie string and do not show the actual implementation of the Lytics tag.\n\nServer Side Cookie with HttpOnly\n\nSetting HttpOnly to true comes with an additional level of complexity but benefits from following the stringent and recommended guidelines of Apple ITP, which in theory will go farther in the way of future-proofing.\n\nMuch like the above example, we\u2019ll set a cookie, in this case using Node, in the same way. The only difference here is setting httpOnly to true. This means that the cookie is secure but will no longer be accessible out of the box by Javascript. Rather, you\u2019ll have to implement an alternative method for surfacing that ID to Javascript so that it can be passed to the Lytics Javascript tag and used during collection/resolution.\n\n// THE FOLLOWING REPRESENTS A UNTESTED AND NON-PRODUCTION LEVEL\n// EXAMPLE OF HOW TO SET A COOKIE SERVER SIDE WITH NODE.JS AND\n// ACCESS THE DOCUMENT.COOKIES FROM THE CLIENT SIDE WHEN USING HTTPONLY\n\nvar express = require('express');\nvar app = express();\nvar cookieParser = require('cookie-parser');\nvar crypto = require('crypto');\nvar dayjs = require('dayjs');\n\n// cookieParser middleware\napp.use(cookieParser());\n\n// route for setting the cookies\napp.get('/setcookie', function (req, res) {\n  // setting a server side cookie with httponly\n  res.cookie(\"uuid\", crypto.randomUUID(), {\n    httpOnly: true,\n    expires: dayjs().add(30, \"days\").toDate(),\n  });\n\n  res.send(`<html><body><p>A server side cookie has been set using httpOnly.</p></body></html>`);\n})\n\n// Route for getting all the cookies\napp.get('/getcookie', function (req, res) {\n    res.send(`\n        <html>\n            <head>\n                <script>\n                    var uuid = '${req.cookies.uuid}';\n                    alert(uuid);\n                </script>\n            </head>\n            <body>\n                <p>This is an example of how to get the cookies client side when cookie is httpOnly.</p>\n            </body>\n        </html>`\n    );\n})\n\napp.listen(3000, (err) => {\n    if (err) throw err;\n    console.log('server running on port 3000');\n});\n\n\nAll code examples in this document are purely for demonstration. Any customer facing implementation should follow the guidance of our customer facing technical teams and the technical experts on our customer\u2019s end. These examples in production represent HIGH risk as documented.\n\nUpdated 12 months ago\n\nTABLE OF CONTENTS\nWhat is a Cookie\nCookie Flavors\nCommon Cookie Use Cases\nHow does Lytics use cookies?\nCreating & Managing Cookies\nWho\u2019s taking away my cookies?!?\nWhat can I do to replace my cookies?\nGetting Technical with Server Side Cookies",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nDeveloper Quickstart\nSuggest Edits\nIntroduction\n\nWelcome to the Lytics developer tier! This guide will walk you through the steps to get started with Lytics and leverage its powerful personalization capabilities for your website.\n\nBefore You Begin\n\nBefore diving into the setup process, make sure you have the following:\n\n Site Access / Management Permission: To install Lytics, you need permission to install JavaScript either via a tag manager or directly onto your website. Alternatively, you can install Lytics via a Drupal module.\n Active Lytics Account: Verify that you can access an active Lytics account. If you don't have one yet, you can claim your free developer account.\n Lytics Dev Tools Chrome Extension: Install our developer tools Chrome extension to streamline the development and installation process.\nGetting Started Checklist\n\nGetting started with Lytics is quick and easy! In just a few minutes, you'll be able to set up Lytics and start personalizing your website. We've focused this guide on the 3 essential steps to ensure a positive experience for you and your customers:\n\n 1. Install the Lytics tag on your site.\n 2. Ensuring site content and Lytics are syncing.\n 3. Create your first personalized message.\nDigging Deeper\n\nAfter completing the initial checklist outlined above, it's time to explore further avenues for enhancing and utilizing your profiles to their fullest potential. We've broken additional guides into two core focuses:\n\nBuilding Profiles\n\nHere, we'll gain a comprehensive understanding of all available out-of-the-box attributes. Discover how to tag your site and integrate other sources to create robust and comprehensive profiles. This section is divided into:\n\nDefault Attributes & Segments:\nProfile Attributes\nAudience Segments\nContent Collections\nSite Activity & Conversion Tagging:\nCapturing Website Activity (coming soon)\nCapture Website Conversion Activity (coming soon)\nUsing Profiles\n\nHere, we'll explore leveraging out-of-the-box personalization SDKs and APIs to deliver optimal user experiences. Discover how to harness Lytics' tools and integrations to create tailored experiences that resonate with your audience. This section covers:\n\nGuides & Inspiration\nSurface a lead capture form only to unknown visitors.\nSurface content recommendations based on interests.\nSurface a promotional message to high-momentum visitors. (coming soon)\nSync profiles & audiences to GA4 or meta. (coming soon)\nPersonalize your site based on behaviors and stored attributes. (coming soon)\nSDK Documentation\nWeb\nJavaScript SDK\nPersonalization SDK\nMobile\niOS SDK\nAndroid SDK\nReact Native SDK\n\nUpdated 5 months ago\n\nWHAT\u2019S NEXT\n1. Account Setup\nTABLE OF CONTENTS\nIntroduction\nBefore You Begin\nGetting Started Checklist\nDigging Deeper\nBuilding Profiles\nUsing Profiles",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nReports\n\nLearn about Lytics' Audience Reporting\n\nSuggest Edits\n\nWith Lytics' Audience Reporting, you can quickly create powerful dashboards with your customer data. This documentation explores the capabilities of Lytics' reporting system and how it can be customized to meet your unique business needs. The following links provide technical details for creating custom reports and how they can be used to gain deeper insights into your customer data.\n\nWhat are Reports?\n\nComponents\n\nManaging Reports\n\nDashboard Report\n\nUpdated over 1 year ago",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nLytics Communication\nSuggest Edits\n\nThere are many reasons why Lytics and partners need to communicate across different tools, here are some best practices on how to get the most out of your communication with Lytics staff. Please note, that this is not an exhaustive list of scenarios, the following should be considered a guideline.\n\nCommunity - Lytics has a community section on their docs site where users of Lytics can ask general questions about Lytics, how to use Lytics, and what\u2019s possible with Lytics. Think of this community area as your Lytics Stackoverflow. The Lytics Community can be found here.\nSome examples of community-type questions are:\n\n\u201cRegarding the NLP engine, are you able to scrape all the contents of, apply topics to, and associate topic scores to users on PDFs?\u201d\n\u201cCan one set up alerts for jobs failing and/or being deleted like you can set up alerts for event overages?\u201d\n\u201cIs there an endpoint I can hit to see hidden streams?\u201d\n\nCustomer-specific questions or questions that regard Solution Architecture (complex) for a customer are not suitable for the Community section and should be asked in Zendesk.\n\nZendesk - All customers and partners working on behalf of customers can use the Lytics Support Portal for the following:\n\nReport technical issues with the Lytics platform, e.g. Bug reports\nFeature Requests\nSolution architecture questions that require discretion with customer needs\nAssistance with features that may not be accessible by a partner or customer\n\nPartners can contact the Lytics support team through support.lytics.com or support@lytics.com, which will create a Zendesk ticket.\n\nSlack & Email - Partners are encouraged to communicate directly with the Lytics team for other partnership needs. Please prioritize Community and Zendesk for any customer CDP-related questions.\n\nSome examples of Email and Slack communications are:\n\nContract questions, billing invoices, or other administrative needs\nPartnership questions\nLytics Training\nCustomer status check-ins\nMeeting coordination\nCustomer-specific issues that are not appropriate in the Lytics public Community forum\n\nUpdated 2 months ago",
        "Jump to Content\nLog In\nAll of your CDP answers, in one place\n\nIf you have questions about Lytics \u2014 whether you're a marketer, an engineer, an analyst, or an intern \u2014 we've got you covered.\n\nGet Started\nAPI Reference\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQuick Start\nDeveloper Quickstart\nAccount Management\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nView More\u2026\nKey Concepts\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nView More\u2026\nPipeline & Profiles\nData Pipeline\nSchema Management\nIdentity\nWarehouse Access\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nView More\u2026\nAudiences & Activation\nUser Profiles\nAudiences\nReports\nView More\u2026\nTutorials\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nView More\u2026\nSDKs & Tools\nWeb\nMobile\nChrome Extension\nIntegrations\nLytics Integration Options\nAcoustic\nAdobe\nView More\u2026\nPartners\nPartner Types\nLytics Communication\nLegacy\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nView More\u2026",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nMobile\nSuggest Edits\n\nWelcome to the Lytics Mobile SDK documentation, where we will explore the native mobile SDKs for both Android and iOS platforms. In this section, we will delve into the capabilities and features of these SDKs, which enable app developers to integrate Lytics' powerful customer data platform directly into their mobile applications.\n\nLytics Mobile SDKs provide a simple way to collect data from mobile apps and enrich user profiles with valuable insights, including user behavior, preferences, and interests. By integrating Lytics Mobile SDKs into your mobile app, you can better understand your customers and deliver personalized experiences that drive engagement, retention, and revenue.\n\nIn this documentation, we will cover how to install and configure Lytics Mobile SDKs and use them to identify users, manage consent, track events, and personalize experiences.\n\nWhether you are a mobile app developer or a marketer, this documentation will provide you with the essential knowledge needed to leverage Lytics Mobile SDKs to build better mobile experiences and gain deeper insights into your customers' behaviors and preferences.\n\nUpdated over 1 year ago",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nSchema Management\nSuggest Edits\n\nWelcome to the Schema Management section, where we will explore how to define and manage data structures within the Lytics platform. This section will cover how to ensure data quality and consistency, including defining data types, creating custom fields, and enforcing validation rules. Whether you are a data analyst, developer, or system administrator, this section will provide you with the essential knowledge needed to understand schema management within the context of the Lytics platform.\n\nUpdated over 1 year ago",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nAdobe\nSuggest Edits\nOverview\n\nThis guide offers an overview of options for leveraging Lytics standard integration capabilities to connect with Adobe\u2019s ads products.\n\nThe decisioning capabilities of Lytics can be combined with the personalized activation proficiency of the Adobe products:\n\nAdobe Campaign\nAdobe Analytics\nAdobe Target\nAdobe Ad Cloud\nAdobe Audience Manager\nOptions for sending data from Lytics to Adobe:\nSending file based data directly to Adobe using a Lytics File Servce\nSending data in near real time from Lytics to one of our supported data warehouse integrations (Google BigQuery, Microsoft Azure, Snowflake, Amazon Web Services (AWS) Redshift).\nThen from the data warehouse, send the data to Adobe Cloud\nClient Side - Lytics surfaces data in Web Layer via the Lytics JavaScript Tag. Learn how you can via the Lytics tag here.\nOptions for sending data from Adobe to Lytics:\nFrom Adobe send data to a Lytics Supported data warehouse (Google BigQuery, Microsoft Azure, Snowflake, Amazon Web Services (AWS) Redshift). The data from the data warehouse can be imported into Lytics. In many cases this is the recommended approach to avoid duplication of processes and data storage.\nSending file based data from Adobe to Lytics using a Lytics File Service\nClient Side - Using the Lytics JavaScript Tag [link] to pass Information to Lytics from the website / data layer\nFurther details on integrating with Adobe products\n\nAdobe Campaign\n\nLytics can export file based data to Adobe Campaign via hourly/daily CSV file export to an SFTP location. Adobe Campaign can import CSV files from its own secure location. This is more of a limitation on the Adobe Campaign side regarding limited API access.\n\nAdobe Analytics\n\nThere are many ways to get data into Lytics from Adobe Analytics. One common method is to use the Lytics Javascript tag (jstag.send) function to send eVars directly from the data layer into Lytics on page load. This method will simplify the onboarding process and minimize the need to do a data transfer from Adobe Analytics on a regular basis.\n\nAs part of onboarding, we can help identify which data you would like to pull in for activation and identify the right mechanisms for ingestion. Lytics provides ways to ingest this data from scheduled SFTP pickups to Bulk API imports or transactional APIs.\n\nAdobe Target\n\nOur Javascript Tag will communicate user audience membership to Adobe Target, which will respond with the appropriate site personalizations. This Lytics data will be returned from our platform on page load and the Users Profile, segmentation information and content recommendations will be placed in the web page data layer.\n\nYou can pass this information simply by leveraging the lio.data.segments object that is loaded onto every page where the Lytics JS tag is deployed.\n\nAdobe Ad Cloud\n\nSimilar to other Adobe products, Adobe Ad Cloud supports the consumption of user audiences via SFTP, Json file upload, API (Lytics can create a webhook to stream data - but more conversation with Adobe will be needed) as well as direct from other Adobe products like Adobe Audience Manager.\n\nLytics can also directly send audiences from Lytics into ad platforms (e.g. Facebook, Google, Linkedin, Twitter, Snapchat, Instagram, LiveRamp. With our Google partnership, Lytics has early access to Google\u2019s API-based integration for custom audiences within DV360.\n\nAdobe Audience Manager\n\nWe can deliver audiences into Audience Manager via SFTP or Data Warehouse integration, similar to how the above integrations have been documented.\n\nUpdated 25 days ago\n\nTABLE OF CONTENTS\nOverview\nOptions for sending data from Lytics to Adobe:\nOptions for sending data from Adobe to Lytics:\nFurther details on integrating with Adobe products",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nUser Profiles\nSuggest Edits\nIntroduction\n\nUser profiles provide relevant insights about individual customers in relation to your entire customer base. Referencing a user profile helps answer questions about who your customers are and how they are engaging with your brand.\n\nInitially, the user profile offers a way to test your Lytics implementation and validate the outcomes of your marketing tactics. For example, you can check if campaigns are working as expected by using a \u201ctest\u201d persona and seeing how the user profile data gets updated.\n\nAs you get deeper into building campaigns, the user profile provides insight into how your marketing influences user behaviors. For example, if you run a mobile campaign, you can drill down to see if customers are engaging on mobile devices, what days and times they are most active, and what types of content they are affinity for. User profiles are updated in real-time, but there can be a delay while the data is being processed and scores are being calculated for behavior and content affinity.\n\nTo provide quick access to customer data, the user profile is divided into three tabs: Intelligence, Audiences, and Details.\n\nAnatomy of a User Profile\nIntelligence\n\nThe intelligence tab gives a snapshot of key information about an individual user. At the top, a bar is displayed to help you quickly understand user engagement across all channels: Likelihood to re-engage, Current engagement level, and Frequency of user interactions.\n\nEach of these boxes summarizes the data science-powered behavioral scores into practical terms. The individual behavior scores and the user\u2019s content affinity are displayed below. This rich data based on a user\u2019s actual behavior is available thanks to the predictive analytics built into Lytics.\n\nRecent activity by channel\n\nThis module gives an overview of which channels (email, web, mobile, etc.) an individual user has been active in. When appropriately mapped, it will also provide information on when users were active on each channel (within the last day or within the last 30 days).\n\n\nNOTE: If this module is empty, please see below for details on how to configure the user fields. As with all data mapping exercises, it is best to include your Customer Success representatives for optimal results.\n\nHow the recent activity by channel is managed\n\nIf you are customizing LQL or handling custom streams, it is important to understand how this module should be configured. For all our built-in integrations, Lytics includes this data mapping by default.\n\nAt its core, the \"Recent activity by channel\" module is built from a single user field. This field will not exist in legacy accounts and must be added to the existing LQL manually. For all new accounts that get the latest default LQL files, the baseline should be included.\n\nIn order to handle either custom data mapping for an account or backfill an old account, the user field last_channel_activities must be defined in a streams LQL file. This field is defined as a map[string]time and should not be altered.\n\nIn each stream you'll simply map a string, such as \"web\" or \"email\" to a timestamp such as epochms() in order to update the field. A more lengthy example for an email LQL might look something like:\n\n-- Profile Support\nmap(\"email\", epochms())    AS last_channel_activities    IF eq(tolower(action), \"open\") OR eq(tolower(action), \"click\")    SHORTDESC \"Last Activity By Channel\"    KIND map[string]time\n\n\nThe example above is for when you only want to map the activity event in the case of a click or open. This definition will vary from stream to stream.\n\nWhen it comes to which keys to use, we recommend following these standards for common channels:\n\nemail\nad\nweb\nmobile\nsupport\nUnique identifiers\n\nHighlights the identifiers that have been used to materialize this particular user's profile.\n\n\nEvent chart\n\nThe event chart shows a user\u2019s activity by hour and day of the week. The top right of the chart highlights when a user is most active, for example Tuesdays at 10am.\n\n\nAudience Membership\n\nThis tab displays a list of all the current Audiences a user belongs to and the total size of each audience. Audiences include factual information such as \u201cHas Email Address\u201d and \u201cKnown Location\u201d along with Lytics score-driven characteristics such as \u201cDeeply Engaged\u201d or \u201cBinge User\u201d.\n\nProfile Details\n\nThe details tab gives an under-the-hood display of the exact user fields contributing to a user\u2019s behavioral data and their audiences. Unique identifiers are shown first, which are essential for enabling cross-channel mapping. For all user fields, the associated stream and value is provided.\n\nProfile Picture\n\nSome profiles may include a profile picture when viewed in the user interface. This image comes from Gravatar. If the profile in question has an email address, Lytics converts it to an MD5 hash. That hash is then used to look up the corresponding image on Gravatar. This image is only displayed in the Lytics UI and is not part of the profile itself.\n\nUpdated over 1 year ago\n\nTABLE OF CONTENTS\nIntroduction\nAnatomy of a User Profile\nIntelligence\nAudience Membership\nProfile Details\nProfile Picture",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nAcoustic\nSuggest Edits\nOverview\n\nAcoustic (previously known as Silverpop) is a marketing platform that enables email, SMS, and mobile campaigns, among other marketing analytics and automation solutions.\n\nIntegrating Lytics with Acoustic allows you to import users and their activity data to build behavioral audiences and gain Insights in Lytics. You can then export Lytics audiences back to Acoustic to refine your targeting and deliver personalized messaging.\n\nAuthorization\n\nIf you have not done so already, you will need to set up an Acoustic account before you begin the process described below. Your Acoustic account user must have permission to create Application Access via the API.\n\nSelect Acoustic from the list of providers.\nSelect the Full Auth method for authorization.\nEnter a Label to identify your authorization.\n(Optional) Enter a Description for further context on your authorization.\nEnter your Client ID for the application that you have created in your Acoustic account. For more information, please refer to Acoustic Documentation.\nEnter the Client Secret for your entered Client ID.\nEnter the Pod number of your Acoustic Campaign. You can find the Pod information in Account Setting page of your Acoustic account. For example, if it is Pod 2 then just enter 2.\nEnter Refresh Token that you have created to grant Lytics access. For more information, please refer to Acoustic Documentation.\nEnter your SFTP Username for the Acoustic Account.\nEnter your SFTP Password for the above SFTP User. You can use your own credentials here, or create a new user for this integration.\nClick Save Authorization.\n\ud83d\udcd8\n\nFor this integration, it is recommended to create a new Acoustic User specific for Lytics with the option Do not enforce password expiration policies for this user selected so the authentication is long lasting, and can be revoked on an account by account basis.\n\nImport Audiences & Activity\n\nImport Acoustic users and their activity information into Lytics so you can leverage that data to build behavioral audiences and gain Insights powered by Lytics data science.\n\nIntegration Details\nImplementation Type: Server-side Integration.\nImplementation Technique: XML API Integration to download CSV formatted data.\nFrequency: Data is imported as a Batch Integration; imported once, or every 8 hours on a continuous basis.\nResulting Data: Full Lytics user profiles for all Acoustic users complete with activity data.\n\nThis integration utilizes the Acoustic XML API to import Acoustic users and their activity to Lytics. Each run of the job will proceed as follows:\n\nExports users from the selected Acoustic Database as a CSV file to an Acoustic-manged SFTP server. The CSV files are then read and imported to the silverpop_users stream in Lytics.\nExports email activity from users in the selected Acoustic Database as a CSV file to an Acoustic-manged SFTP server. The CSV files are then read and imported to the silverpop_activity stream in Lytics.\nAfter successful completion, the job will import new/updated users with their updated activity every 8 hours if configured to run continuously.\nFields\n\nThe following fields are included in the default mapping for the silverpop_users stream. Note this integration was formerly named Silverpop, hence the data stream name, but this job will receive your current Acoustic data.\n\nSource Field\tLytics User Field\tDescription\tType\nemail\temail unique_id\tEmail Address\tstring\nFirst Name\tfirst_name\tFirst Name\tstring\nLast Name\tlast_name\tLast Name\tstring\nEmail Type\tsp_email_type\tAcoustic Email Type\tstring\nmap(list, Opted Out)\tsp_list_optout_status\tAcoustic Optout Status by List\tmap[string]string\nmap(list, todate(Opted Out Date))\tsp_list_opt_out_date\tAcoustic Opt Out Date by List\tmap[string]time\nmap(list, todate(Opt In Date))\tsp_list_opt_in_date\tAcoustic Opt In Date by List\tmap[string]time\nmap(list, Opt In Details)\tsp_list_opt_in_details\tAcoustic Opt In Details\tmap[string]string\nemaildomain(email)\temaildomain\tEmail Domain\tstring\n\n\n\nSimilarly, the following fields are included in the default mapping for the silverpop_activity stream:\n\nSource Field\tLytics User Field\tDescription\tType\nEmail\temail unique id\t\tstring\nemail\temaildomain\tEmail Domain\tstring\nUrl\thashedurls\tHashed Urls Visited\tmap[string]intsum\nmax(epochms())\tlast_active_ts\tLast Active on Any Channel\tdate\nCampaign Id\tsv_campaign_ids\tAcoustic Campaign the send originated from\t[]string\nevent\tsv_clickct\tAcoustic Click count\tint\nEvent Type\tsv_email_event\tAcoustic Events\tmap[string]intsum\nUrl\tsv_email_urls\tAcoustic URLs\t[]string\nmin(epochms())\tsv_firstclick_ts\tAcoustic First Click\tdate\nmin(epochms())\tsv_firstopen_ts\tAcoustic First Open\tdate\nmin(epochms())\tsv_firstsent_ts\tAcoustic First Sent\tnumber\nhourofday()\tsv_hourlyopen\tAcoustic Hourly Events\tmap[string]intsum\nhourofweek()\tsv_hourofweek\tAcoustic Hour of Week Events\tmap[string]intsum\nmax(epochms())\tsv_lastclick_ts\tAcoustic Last Click\tdate\nmax(epochms())\tsv_lastopen_ts\tAcoustic Last Open\tdate\nmax(epochms())\tsv_lastsent_ts\tAcoustic Last Sent\tdate\nlist_id\tsv_list_ids\tAcoustic List the send originated from\t[]string\nMailing Id\tsv_mailing_ids\tAcoustic Mailing the send originated from\t[]string\nMailing Name\tsv_mailing_names\tAcoustic Mailing Names\t[]string\nMailing Name\tsv_mailing_names_click_count\tAcoustic Mailing Names Click Count\tmap[string]intsum\nMailing Name\tsv_mailing_names_clicked\tAcoustic Mailing Names Clicked\t[]string\nMailing Name\tsv_mailing_names_open_count\tAcoustic Mailing Names Open Count\tmap[string]intsum\nMailing Name\tsv_mailing_names_opened\tAcoustic Mailing Names Opened\t[]string\nMailing Name\tsv_mailing_names_sent\tAcoustic Mailing Names Sent\t[]string\nMailing Name\tsv_mailing_names_sent_count\tAcoustic Mailing Name Sent Count\tmap[string]intsum\nyymm()\tsv_monthly\tAcoustic Opens By Month\tmap[string]intsum\nevent\tsv_openct\tAcoustic Open count\tint\nProgram Id\tsv_program_ids\tAcoustic Program Ids\t[]string\nmin(epochms())\tsv_subscribe_ts\tAcoustic Subscribe Time\tdate\nmin(epochms())\tsv_unsubscribe_ts\tAcoustic Unsubscribe Time\tdate\nConfiguration\n\nFollow these steps to set up an Import Audiences and Activity Data job for Acoustic.\n\nSelect Acoustic from the list of providers.\nSelect the Import Audiences and Activity Data job type from the list.\nSelect the Authorization you would like to use or create a new one.\nEnter a Label to identify this job you are creating in Lytics.\n(Optional) Enter a Description for further context on your job.\nFrom the Type dropdown, select the Acoustic source type: database, list, or query.\nFrom the Source dropdown, select the Acoustic source to import user data from. \nClick on the Show Advanced Options tab to expand the advanced configuration.\nCheck the Keep Updated checkbox to update the list every 8 hours.\nIn the Activity Start field, enter the date to import activity from. The default is to retrieve one year of activity.\nClick Start Import.\n\nExport Audiences\n\nSend Lytics user profiles and audience membership to your Acoustic Database to refine your targeting and deliver personalized messaging across channels. All existing users and new users of the selected Lytics audiences are exported.\n\nIntegration Details\nImplementation Type: Server-side Integration.\nImplementation Technique: API Integration , Audience Trigger Integration .\nFrequency: Real-time Integration.\nResulting data: Lytics users are exported to Acoustic Database.\n\nThis integration utilizes the Acoustic Import List API to export user data from Lytics to the Acoustic Database. Once the export is started, the job:\n\nCreates the column mapping file according to the fields selected during configuration and uploads to the Acoustic Campaign SFTP server.\nWrites the users that are part of selected Lytics audiences to the CSV file and initiates the CSV Import job in Acoustic.\nThe updates will be sent to Acoustic every minute or a batch of 10000 users, whichever occurs first.\nFields\n\nYou can export any Lytics user fields to Acoustic Database columns that are present in the selected Acoustic Database. Lytics allows you to map user fields with the corresponding Acoustic Database columns as part of the job configuration described below.\n\nTo export using the Acoustic Recipient ID as the sync field use the Acoustic Export Audience with Recipient ID export.\n\nConfiguration\n\nFollow these steps to set up an Export Audiences job for Acoustic.\n\nSelect Acoustic from the list of providers.\nSelect the Export Audiences job type from the list.\nSelect the Authorization you would like to use or create a new one create a new one.\nEnter a Label to identify this job you are creating in Lytics.\n(Optional) Enter a Description for further context on your job.\nFrom the Acoustic Database dropdown, select the Acoustic database to populate with Lytics users.\nUsing the Audience list, select the Lytics audiences to export. As users enter or exit the selected audience(s) their record will be sent to Acoustic.\nFrom the Sync Fields mapping, choose the sync fields that are used to match and identify users like unique ID or email. At least one sync field must be specified.\nFrom the Additional Map Fields, map additional fields that you would like to send from Lytics to Acoustic by selecting the Lytics field on the left, and its Acoustic destination on the right.\nFrom Fields to Trigger, select up to 75 user fields to trigger user change events. For any user in the exported audience, if any of the selected field values change, then the user will be updated in Acoustic.\nCheck the Create New Contacts checkbox to create new contacts in Acoustic if they do not already exist.\nCheck the Update Contacts checkbox to update fields of existing contacts in Acoustic.\nIn the Single Audience Field text field, select a field or enter a name to create a field to write the additional Lytics audiences to. If the field is left empty, a Yes/No field is created for each selected Lytics audience.\n(Optional) In the Single Audience File Empty String text field, specify this to a value like \"null\" to represent an empty audience. Otherwise, an empty string is sent by default.\nCheck the Keep Updated checkbox to continuously export users as they enter the audience.\nFrom the Time of Day dropdown, select a time of day to complete export each day. Export will sync every hour if left empty.\nFrom the Timezone dropdown, select the timezone for time of day specified above.\nClick Start Export.\nExport Audiences with Recipient ID\n\nSend Lytics user profiles and audience membership to your Acoustic Database to refine your targeting and deliver personalized messaging across channels. All existing users and new users of the selected Lytics audiences are exported.\n\n\ud83d\udcd8\n\nThis export is specifically used for exporting using the Acoustic Recipient ID as the sync field. It will only update users already in Acoustic with a Recipient ID. In order to add users to Acoustic or if you would like to use other sync fields use the standard Acoustic Export Audience export.\n\nIntegration Details\nImplementation Type: Server-side Integration.\nImplementation Technique: API Integration , Audience Trigger Integration .\nFrequency: Real-time Integration.\nResulting data: Lytics users are exported to Acoustic Database.\n\nThis integration utilizes the Acoustic Import List API to export user data from Lytics to the Acoustic Database. Once the export is started, the job:\n\nCreates the column mapping file according to the fields selected during configuration and uploads to the Acoustic Campaign SFTP server.\nWrites the users that are part of selected Lytics audiences to the CSV file and initiates the CSV Import job in Acoustic.\nThe updates will be sent to Acoustic every 5 minutes or a batch of 10000 users, whichever occurs first.\nFields\n\nYou can export any Lytics user fields to Acoustic Database columns that are present in the selected Acoustic Database. Lytics allows you to map user fields with the corresponding Acoustic Database columns as part of the job configuration described below.\n\nConfiguration\n\nFollow these steps to set up an Export Audiences job for Acoustic.\n\nSelect Acoustic from the list of providers.\nSelect the Export Audiences job type from the list.\nSelect the Authorization you would like to use or create a new one.\nEnter a Label to identify this job you are creating in Lytics.\n(Optional) Enter a Description for further context on your job.\nFrom the Acoustic Database dropdown, select the Acoustic database to populate with Lytics users.\nUsing the Audience list, select the Lytics audiences to export. As users enter or exit the selected audience(s) their record will be sent to Acoustic.\nFrom the Recipient ID field, choose the field that contains the Acoustic Recipient ID in Lytics.\nFrom the Additional Map Fields, map additional fields that you would like to send from Lytics to Acoustic by selecting the Lytics field on the left, and its Acoustic destination on the right.\nFrom Fields to Trigger, select up to 75 user fields to trigger user change events. For any user in the exported audience, if any of the selected field values change, then the user will be updated in Acoustic.\nIn the Single Audience Field text field, select a field or enter a name to create a field to write the additional Lytics audiences to. If the field is left empty, a Yes/No field is created for each selected Lytics audience.\n(Optional) In the Single Audience File Empty String text field, specify this to a value like \"null\" to represent an empty audience. Otherwise, an empty string is sent by default.\nCheck the Keep Updated checkbox to continuously export users as they enter the audience.\nFrom the Time of Day dropdown, select a time of day to complete export each day. Export will sync every hour if left empty.\nFrom the Timezone dropdown, select the timezone for time of day specified above.\nClick Start Export.\nExport Audiences to Relational Tables\n\nSend Lytics user profile data and audience membership to your Acoustic Relational tables. All existing and new users of the selected audience are exported in real-time.\n\nIntegration Details\nImplementation Type: Server-side Integration.\nImplementation Technique: API Integration , Audience Trigger Integration .\nFrequency: Real-time Integration.\nResulting data: Lytics users are exported to Acoustic Relational Table.\n\nThis integration utilizes the Acoustic Relational Table Management api to export user data from Lytics to the Acoustic Relational Table. Once the export is started, the job:\n\nCreates or writes to the following tables depending on the configuration of the job (see the example below for a visual example),\nUser table - the user identifier plus all scalar fields will be written to this table.\nAudience table - the user identifier plus a boolean (Yes/No) column for each Lytics audience exported. Users that belong to the audience will have true as value in the respective audience column, and similarly, will have false if the user does not belong to the audience.\nNon-scalar tables - for each non-scalar field exported a separate table will be created.\nFor sets, the table will have a column for the user identifier plus a column called lytics_value with a row written for each value in the set.\nFor maps, the table will have a column for the user identifier plus two columns, lytics_key and lytics_value with a row written for each key/value pair in the map.\nAs users enter or exit the audience, rows will added or updated in the tables above\nThe updates will be sent to Acoustic every 5 minutes or a batch of 1000 users, whichever occurs first.\nFields\n\nYou can export any Lytics user fields to Acoustic Relational Table. Lytics allows you to map user fields with the corresponding table columns as part of job configuration. The job also has option to create column in Acoustic table for Lytics user field.\n\nConfiguration\n\nFollow these steps to set up an Export Audiences job for Acoustic.\n\nSelect Acoustic from the list of providers.\nSelect the Export Audience to Acoustic Relational Table job type from the list.\nSelect the Authorization you would like to use or create a new one.\nEnter a Label to identify this job you are creating in Lytics.\n(Optional) Enter a Description for further context on your job.\nUsing the Audiences list, select the Lytics audiences to export. As users enter or exit the selected audience(s) their record will be sent to Acoustic.\nFrom the Acoustic User Table dropdown, select the Acoustic relational table to populate with user profile data. Select Create New Table to create new user table with name lytics_user_table_{timestamp}. {timestamp} is replaced with current timestamp in format YYYYMMDDHHmmss.\nIf Create New Table is selected, from the User Table Key Field dropdown, select the Lytics user field to be used as primary key for new user table. Should be left blank if writing to an existing table.\nIf writing to an existing table, from the Acoustic User Table Mapping, map Lytics user fields that you would like to send from Lytics to corresponding Acoustic field. NOTE: All the key column for the selected Acoustic user table must be mapped.\nFor either exporting to a new table or writing an existing table, from the Extra Fields list, select the additional Lytics user fields to export. These are extra scalar fields to be sent to Acoustic user table. Each field will be added as a separate column in the table.\nFrom the Acoustic Audience Membership Table dropdown, either select the Acoustic relational table to populate with audience information or select Create New Table. If the creation of a new audience table is selected, a table will be created with the name lytics_audiences_table_{timestamp}. {timestamp} is replaced with current timestamp in format YYYYMMDDHHmmss.\nIf Create New Table is selected for the Acoustic audience membership table, in the Audience Membership Table Key Field dropdown, select the Lytics user field to be used as primary key for new audience membership table. Should be left blank if writing to an existing table.\nIf writing to an existing audience table, from the Acoustic Audience Membership Table Key Mapping, map the keys from Lytics to the key columns in Acoustic. The primary key in Acoustic must be mapped.\nUsing the Non-Scalar User Fields list, select any Lytics non-scalar user field to export. An Acoustic relational table with name lytics_{field-name}_table_{timestamp} will be created for each selected non-scalar field.\nIf exporting non-scalar fields, from the Non-Scalar Field Table Key Field dropdown, select Lytics user field to be used as primary key for all new non-scalar field table.\nSelect Existing Users to send users currently in the Lytics audience to Acoustic.\nClick Complete to create the job.\n\n\nExample\n\nBelow is an example of a user exported and how they may look in Acoustic.\n\nText\nSample User\n{\n  \"email\": \"james@lytics.com\",\n  \"first_name\": \"James\",\n  \"last_name\": \"McDermott\",\n  \"channels\": [ \"web\", \"email\" ],\n  \"hourly\": {\n    \"17\": 1,\n    \"21\": 1,\n    \"23\": 2\n  },\n  \"segments\": {\n    \"all\",\n    \"highly_engaged\"\n  }\n}\n\n\nUser Table: lytics_user_table_20240408220715\n\nemail\tfirst_name\tlast_name\njames@lytics.com\tJames\tMcDermott\n\nAudience Table: lytics_audiences_table_20240408220715\n\nemail\tall\thighly_engaged\tlow_engagement\njames@lytics.com\ttrue\ttrue\tfalse\n\nNon-scalar table for channels: lytics_channels_table_20240408220715\n\nemail\tlytics_value\njames@lytics.com\tweb\njames@lytics.com\temail\n\nNon-scalar table for hourly: lytics_hourly_table_20240408220715\n\nemail\tlytics_key\tlytics_value\njames@lytics.com\t17\t1\njames@lytics.com\t21\t1\njames@lytics.com\t23\t2\n\nUpdated 5 months ago\n\nTABLE OF CONTENTS\nOverview\nAuthorization\nImport Audiences & Activity\nIntegration Details\nFields\nConfiguration\nExport Audiences\nIntegration Details\nFields\nConfiguration\nExport Audiences with Recipient ID\nIntegration Details\nFields\nConfiguration\nExport Audiences to Relational Tables\nIntegration Details\nFields\nConfiguration\nExample",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nPartner Types\nSuggest Edits\n\nOur partners are trusted collaborators who work directly with our customers to ensure successful implementation and ongoing success. They work directly with Lytics customers on implementations and provide post-implementation technical and strategic support. Through their expertise and commitment, our partners help to streamline processes, enhance customer satisfaction, and drive the effective utilization of our solutions.\n\nTypes of partners:\nSubcontractor Implementations - A partner who implements a Lytics customer that has been acquired directly by Lytics.\nSubcontract Technical Account Management - A partner who helps aid a Lytics-acquired customer with technical management and strategy components as an extension of professional services.\nReferral Partner (Enterprise or Mid-Market) - Under a reseller agreement partners can bring their customers to Lytics to be supported by licensing and provide implementation or technical account management services at their discretion. Partners interested in becoming resellers and who do not have an agreement with Lytics can contact the Lytics commercial team to create one.\n\nUpdated 2 months ago\n\nTABLE OF CONTENTS\nTypes of partners:",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nLeveraging User Profiles\nSuggest Edits\n\nWelcome to the Leveraging User Profiles section, where we will explore how to use Lytics user profiles to gain a deeper understanding of your customers and deliver personalized experiences. This section will cover how to define and segment audiences, personalize experiences, and track user behavior using user profiles. Whether you are a marketer, developer, or data analyst, this section will provide you with the essential knowledge needed to leverage Lytics user profiles and gain insights into customer behaviors and preferences.\n\nUpdated over 1 year ago",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nData Management\nSuggest Edits\n\nWelcome to the Data Management section, where we will explore the various tools and techniques available within the Lytics platform for managing and organizing your customer data. This section covers how to ensure data accuracy and completeness, including data validation, deduplication, and transformation. Whether you are a data analyst, developer, or system administrator, this section will provide you with the essential knowledge needed to manage your customer data and gain valuable insights into customer behavior and preferences.\n\nUpdated over 1 year ago",
        "Jump to Content\nLog In\nv3.0\nHome\nDocumentation\nAPI Reference\nProduct Updates\nCommunity\nSearch\nQUICK START\nDeveloper Quickstart\nACCOUNT MANAGEMENT\nWhat is Vault?\nAccessing Accounts\nMonitoring Metrics and Alerts\nManaging Users\nAccount Settings\nData Policies\nCompliance\nAuthorizations\nAccess Tokens\nKEY CONCEPTS\nIdentity Resolution\nConsent & Privacy\nClient & Server Side Cookies\nContent Affinity\nArchitecture\nPIPELINE & PROFILES\nData Pipeline\nSchema Management\nIdentity\nWAREHOUSE ACCESS\nWhat is Cloud Connect?\nConnecting Warehouses\nData Models & Queries\nActivating Data Models\nCloud Connect Troubleshooting and FAQs\nAUDIENCES & ACTIVATION\nUser Profiles\nAudiences\nReports\nContent\nLookalike Audiences & Propensity Models\nActivation\nBehavioral Scores\nTUTORIALS\nLeveraging User Profiles\nData Collection & Onboarding\nData Management\nUse Cases\nSDKS & TOOLS\nWeb\nMobile\nChrome Extension\nINTEGRATIONS\nLytics Integration Options\nAcoustic\nAdobe\nAdRoll\nAirship\nAmplitude\nAmazon Ads\nAmazon Kinesis\nAmazon Pinpoint\nAmazon Redshift\nAmazon S3\nAmazon SQS\nAnsira\nBigCommerce\nBlueKai\nBlueshift\nBraze\nBrevo\nCampaign Monitor\nCheetah Digital\nClearbit\nContentful\nCordial\nCriteo\nCustomer.io\nDatabricks\nDotdigital\nDrift\nEpiserver\nFullContact\nGIGYA\nGoogle Optimize\nGoogle Ad Manager (DFP)\nGoogle Ads\nGoogle BigQuery\nGoogle Cloud Pub/Sub\nGoogle Cloud Storage\nGoogle Drive\nGoogle Marketing: Analytics, DV360, CM360\nGoogle Cloud Operations\nGoogle Tag Manager\nHubSpot\niContact\nInsider\nIterable\nJebbit\nKlaviyo\nLeadsquared\nLinkedIn\nLiveRamp\nLocalytics\nLooker\nLotame\nLytics File Service\nLytics Monitoring\nMailchimp\nMailgun\nMandrill\nMapp\nMapp: BlueHornet\nMarketo\nMaropost\nMediaMath\nMeta\nMicrosoft\nMicrosoft Azure\nMicrosoft Teams\nMixpanel\nNetSuite\nNew Relic\nOmeda\nOneSignal\nOneTrust\nOracle Marketing Cloud: Eloqua\nPinterest\nPostUp\nRadar\nReddit\nResponsys\nRetention Science\nSailthru\nSalesforce\nSalesforce DMP (Krux)\nSalesforce Marketing Cloud\nSalesforce Pardot\nSegment.com\nSelligent\nSendGrid\nShopify\nSitecore\nSlack\nSnapchat\nSnowflake\nSparkPost\nSurveyMonkey\nSwrve\nTaboola\nTealium\nTikTok\nThe Trade Desk\nUnified ID 2.0\nVersium\nWebhooks\nWebhook Templates\nWistia\nX Ads\nYahoo Ads\nZapier\nZendesk\nZuora\nContentstack\nPARTNERS\nPartner Types\nLytics Communication\nLEGACY\nInsights\nImporting External Experiences\nIDP-initiated SSO (legacy)\nWeb Personalization\nMerge Statistics\nSuggest Edits\n\nWhen data from multiple sources are combined into a single user profile, it is essential to ensure that the resulting data is accurate and consistent. Merge statistics in Lytics provide a way to monitor and optimize the merging process, ensuring that the data is adequately combined and reflects the most up-to-date and accurate information.\n\nIn the example below, you can see how merge stats can be used to uncover potential areas of issue. Out of the four available identifiers, user_id, is not responsible for any merges. This could be due to several reasons, such as no data populating that field, the relationship being defined inaccurately, etc.\n\nUpdated over 1 year ago"
    ],
    "zeotap": [
        "\t\nProducts\nIntegrate\nUnify\nSegment\nOrchestrate\nDashboard\nProtect\nTarget\nSecurity\nGDPR\nFor Developers\nAPI Docs\nCompany\nCareers\nPress\nAcceptable Use Policy\nSupport\nHelp Center\nContact us\nResources\nClear your Cache and Cookies\nCreate an HAR File\nPrivacy Terms Acceptable Use Policy\n ",
        "\t\nProducts\nIntegrate\nUnify\nSegment\nOrchestrate\nDashboard\nProtect\nTarget\nSecurity\nGDPR\nFor Developers\nAPI Docs\nCompany\nCareers\nPress\nAcceptable Use Policy\nSupport\nHelp Center\nContact us\nResources\nClear your Cache and Cookies\nCreate an HAR File\nPrivacy Terms Acceptable Use Policy\n ",
        "HOME\nWHAT'S NEW?\nAPI DOC\nINTEGRATIONS\nSearch Results\n\t\nFilter by:\n\t\nProducts\nIntegrate\nUnify\nSegment\nOrchestrate\nDashboard\nProtect\nTarget\nSecurity\nGDPR\nFor Developers\nAPI Docs\nCompany\nCareers\nPress\nAcceptable Use Policy\nSupport\nHelp Center\nContact us\nResources\nClear your Cache and Cookies\nCreate an HAR File\nPrivacy Terms Acceptable Use Policy\n ",
        "\t\n \nSHOW KEY CONCEPTS\nLOG IN\nProducts\nIntegrate\nUnify\nSegment\nOrchestrate\nDashboard\nProtect\nTarget\nSecurity\nGDPR\nFor Developers\nAPI Docs\nCompany\nCareers\nPress\nAcceptable Use Policy\nSupport\nHelp Center\nContact us\nResources\nClear your Cache and Cookies\nCreate an HAR File\nPrivacy Terms Acceptable Use Policy\n ",
        "\t\n \nSHOW KEY CONCEPTS\nLOG IN\nProducts\nIntegrate\nUnify\nSegment\nOrchestrate\nDashboard\nProtect\nTarget\nSecurity\nGDPR\nFor Developers\nAPI Docs\nCompany\nCareers\nPress\nAcceptable Use Policy\nSupport\nHelp Center\nContact us\nResources\nClear your Cache and Cookies\nCreate an HAR File\nPrivacy Terms Acceptable Use Policy\n ",
        "\t\n \nSHOW KEY CONCEPTS\nLOG IN\nProducts\nIntegrate\nUnify\nSegment\nOrchestrate\nDashboard\nProtect\nTarget\nSecurity\nGDPR\nFor Developers\nAPI Docs\nCompany\nCareers\nPress\nAcceptable Use Policy\nSupport\nHelp Center\nContact us\nResources\nClear your Cache and Cookies\nCreate an HAR File\nPrivacy Terms Acceptable Use Policy\n ",
        "\t\n \nSHOW KEY CONCEPTS\nLOG IN\nProducts\nIntegrate\nUnify\nSegment\nOrchestrate\nDashboard\nProtect\nTarget\nSecurity\nGDPR\nFor Developers\nAPI Docs\nCompany\nCareers\nPress\nAcceptable Use Policy\nSupport\nHelp Center\nContact us\nResources\nClear your Cache and Cookies\nCreate an HAR File\nPrivacy Terms Acceptable Use Policy\n ",
        "HOME\nWHAT'S NEW?\nAPI DOC\nINTEGRATIONS\n\t\nLearn how to use Zeotap CDP to collect, unify and activate your customer data across multiple platforms.\nGet Started with Zeotap CDP\n\n\nLearn how to use the different\u00a0modules within Zeotap CDP and get started.\nBrowse by product\n\nIntegrate\n\n\nDiscover how to use\u00a0Sources and Destinations to integrate your disparate data sources and activate your data across platforms.\n\nUnify\n\nDiscover how to use Catalogue, Calculated Attributes and ID Strategy to\u00a0unify\u00a0your customer data.\n\nSegment\n\nDiscover how to use\u00a0Audiences and Predictive Audience\u00a0to create cohorts and activate your customer data.\n\nOrchestrate\n\nDiscover how to use Journeys to orchestrate\u00a0personalised workflows with actionable recommendations.\n\nDashboard\n\nDiscover how to use Dashboard to\u00a0analyse\u00a0your platform usage, exploring consumption metrics and more.\n\nProtect\n\nDiscover how to use Consent, Customer 360 and Data Lifecycle to protect your customer data by being compliant with regulatory frameworks.\nDo more with Zeotap\n\nTarget\n\nDiscover how to use Target to create and activate deterministic third-party Audience.\n\nID+\n\nDiscover how to use ID+ to create custom journeys and provide\u00a0optimal actions.\n\nAdmin\n\nDiscover how to use Admin to create organisations and add users with specific roles and access.\nProducts\nIntegrate\nUnify\nSegment\nOrchestrate\nDashboard\nProtect\nTarget\nSecurity\nGDPR\nFor Developers\nAPI Docs\nCompany\nCareers\nPress\nAcceptable Use Policy\nSupport\nHelp Center\nContact us\nResources\nClear your Cache and Cookies\nCreate an HAR File\nPrivacy Terms Acceptable Use Policy\n ",
        "\t\n \nSHOW KEY CONCEPTS\nLOG IN\nProducts\nIntegrate\nUnify\nSegment\nOrchestrate\nDashboard\nProtect\nTarget\nSecurity\nGDPR\nFor Developers\nAPI Docs\nCompany\nCareers\nPress\nAcceptable Use Policy\nSupport\nHelp Center\nContact us\nResources\nClear your Cache and Cookies\nCreate an HAR File\nPrivacy Terms Acceptable Use Policy\n ",
        "\t\n \nSHOW KEY CONCEPTS\nLOG IN\nProducts\nIntegrate\nUnify\nSegment\nOrchestrate\nDashboard\nProtect\nTarget\nSecurity\nGDPR\nFor Developers\nAPI Docs\nCompany\nCareers\nPress\nAcceptable Use Policy\nSupport\nHelp Center\nContact us\nResources\nClear your Cache and Cookies\nCreate an HAR File\nPrivacy Terms Acceptable Use Policy\n ",
        "\t\n \nSHOW KEY CONCEPTS\nLOG IN\nProducts\nIntegrate\nUnify\nSegment\nOrchestrate\nDashboard\nProtect\nTarget\nSecurity\nGDPR\nFor Developers\nAPI Docs\nCompany\nCareers\nPress\nAcceptable Use Policy\nSupport\nHelp Center\nContact us\nResources\nClear your Cache and Cookies\nCreate an HAR File\nPrivacy Terms Acceptable Use Policy\n ",
        "\t\n \nSHOW KEY CONCEPTS\nLOG IN\nProducts\nIntegrate\nUnify\nSegment\nOrchestrate\nDashboard\nProtect\nTarget\nSecurity\nGDPR\nFor Developers\nAPI Docs\nCompany\nCareers\nPress\nAcceptable Use Policy\nSupport\nHelp Center\nContact us\nResources\nClear your Cache and Cookies\nCreate an HAR File\nPrivacy Terms Acceptable Use Policy\n "
    ]
}